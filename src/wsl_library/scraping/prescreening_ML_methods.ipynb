{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e135db0",
   "metadata": {},
   "source": [
    "# README \n",
    "\n",
    "Script rapide, très sale imo, mainly généré par IA pour voir vers quelle méthode aller.\n",
    "\n",
    "GPU needed\n",
    "\n",
    "TODO: \n",
    "* Comprendre l'éval du crossencodeur, j'ai pas pris le temps d'aller voir\n",
    "* Better text preprocessing\n",
    "* Test avec SciBERT !\n",
    "* Train le bert jusqu'a early stop pour voir ces capacités\n",
    "* Mesurer le temps d'inférence du BERT pour voir combien de temps ça nous prendrait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbol",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tempory/ehehboi/conda_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, util\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "assert load_dotenv()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJUe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80801/1682894360.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[\"true_label\"] = data[\"true_label\"].replace({\"Not About Sufficiency\": 0, \"About Sufficiency\": 1})\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/full_concat_dataset.csv\").dropna()\n",
    "\n",
    "data = data.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "data[\"true_label\"] = data[\"true_label\"].replace({\"Not About Sufficiency\": 0, \"About Sufficiency\": 1})\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[[\"primary_title\", \"abstract\"]],\n",
    "    data[\"true_label\"],\n",
    "    test_size=0.2,\n",
    "    stratify=data[\"true_label\"],\n",
    "    random_state=seed,\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # TODO search for already existing preprocessing functions\n",
    "    return text\n",
    "X_train[\"abstract\"] = X_train[\"abstract\"].apply(preprocess_text)\n",
    "X_test[\"abstract\"] = X_test[\"abstract\"].apply(preprocess_text)\n",
    "\n",
    "assert not X_train[\"abstract\"].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkHC",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Traditional Model...\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.52      0.61       394\n",
      "           1       0.78      0.89      0.83       753\n",
      "\n",
      "    accuracy                           0.77      1147\n",
      "   macro avg       0.75      0.71      0.72      1147\n",
      "weighted avg       0.76      0.77      0.76      1147\n",
      "\n",
      "[[206 188]\n",
      " [ 80 673]]\n"
     ]
    }
   ],
   "source": [
    "# Traditional tfidf \n",
    "def train_traditional_model(X_train, y_train, X_test, y_test):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train[\"abstract\"])\n",
    "    X_test_tfidf = vectorizer.transform(X_test[\"abstract\"])\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    print(\"Logistic Regression Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "# Train and evaluate the models\n",
    "print(\"Training Traditional Model...\")\n",
    "train_traditional_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEQa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BERT Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1722' max='1722' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1722/1722 07:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.553300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.437600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.306600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.65      0.72       394\n",
      "           1       0.83      0.92      0.88       753\n",
      "\n",
      "    accuracy                           0.83      1147\n",
      "   macro avg       0.82      0.79      0.80      1147\n",
      "weighted avg       0.83      0.83      0.82      1147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Transformer-based Model (BERT)\n",
    "def train_bert_model(X_train, y_train, X_test, y_test):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "    train_encodings = tokenizer(\n",
    "        X_train[\"abstract\"].tolist(), truncation=True, padding=True, max_length=512\n",
    "    )\n",
    "    test_encodings = tokenizer(\n",
    "        X_test[\"abstract\"].tolist(), truncation=True, padding=True, max_length=512\n",
    "    )\n",
    "\n",
    "    class AbstractDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "    train_dataset = AbstractDataset(train_encodings, y_train.tolist())\n",
    "    test_dataset = AbstractDataset(test_encodings, y_test.tolist())\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, args=training_args, train_dataset=train_dataset, eval_dataset=test_dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    predictions = trainer.predict(test_dataset).predictions\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    print(\"BERT Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    return (y_pred, y_test)\n",
    "\n",
    "\n",
    "print(\"Training BERT Model...\")\n",
    "y_pred_bert, y_test_bert = train_bert_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "PKri",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cross-Encoder Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='861' max='861' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [861/861 01:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.177700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sufficiency-eval_cosine_accuracy': 0.7646033129904097, 'sufficiency-eval_cosine_accuracy_threshold': 0.5688587427139282, 'sufficiency-eval_cosine_f1': 0.824126984126984, 'sufficiency-eval_cosine_f1_threshold': 0.47997307777404785, 'sufficiency-eval_cosine_precision': 0.7895377128953771, 'sufficiency-eval_cosine_recall': 0.8618857901726428, 'sufficiency-eval_cosine_ap': 0.889762105477786, 'sufficiency-eval_cosine_mcc': 0.44555617855751745}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cross-Encoder Model\n",
    "def train_cross_encoder_model(X_train, y_train, X_test, y_test):\n",
    "    model = SentenceTransformer(\"distilbert-base-nli-stsb-mean-tokens\")\n",
    "\n",
    "    train_examples = [\n",
    "        InputExample(texts=[row[\"primary_title\"], row[\"abstract\"]], label=label)\n",
    "        for row, label in zip(X_train.to_dict(\"records\"), y_train)\n",
    "    ]\n",
    "    test_examples = [\n",
    "        InputExample(texts=[row[\"primary_title\"], row[\"abstract\"]], label=label)\n",
    "        for row, label in zip(X_test.to_dict(\"records\"), y_test)\n",
    "    ]\n",
    "\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "    train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "    model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=100)\n",
    "\n",
    "    test_dataloader = DataLoader(test_examples, shuffle=False, batch_size=16)\n",
    "    evaluator = evaluation.BinaryClassificationEvaluator.from_input_examples(\n",
    "        test_examples, name=\"sufficiency-eval\"\n",
    "    )\n",
    "\n",
    "    result = model.evaluate(evaluator)\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Training Cross-Encoder Model...\")\n",
    "y_result_cross_encoder = train_cross_encoder_model(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xref",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
