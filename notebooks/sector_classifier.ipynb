{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc62620",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_list = [\n",
    "    \"buildings\",\n",
    "    \"digitalisation\",\n",
    "    \"freight\",\n",
    "    \"mobility\",\n",
    "    \"nutrition\",\n",
    "    \"urban_ecology\",\n",
    "    \"urban_governance\",\n",
    "    \"urban_infra\",\n",
    "    \"trade\",\n",
    "]\n",
    "df_list = []\n",
    "for sector in sector_list:\n",
    "    print(f\"Processing {sector} dataset\")\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(\n",
    "        f\"../data/{sector}_dataset.csv\", usecols=[\"title\", \"abstract\", \"true_label\"]\n",
    "    )\n",
    "    df = df[df[\"true_label\"] == \"About Sufficiency\"]\n",
    "    df[\"origin\"] = sector\n",
    "    df_list.append(df)\n",
    "df = pd.concat(df_list, ignore_index=True).dropna()\n",
    "df[\"true_label\"] = df[\"origin\"].astype(\"category\").cat.codes\n",
    "df.to_csv(\"../data/sector_positive.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb89ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/sector_positive.csv\")\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"abstract\"], df[\"true_label\"], test_size=0.2, random_state=42, stratify=df[\"true_label\"]\n",
    ")\n",
    "\n",
    "# Traditional Classifier: Logistic Regression\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, train_labels)\n",
    "\n",
    "# Evaluate the Logistic Regression model\n",
    "lr_predictions = lr_model.predict(X_test)\n",
    "print(\"Logistic Regression Performance:\")\n",
    "print(classification_report(test_labels, lr_predictions, target_names=df[\"origin\"].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b5601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Classifier\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\", cache_dir=\"../.cache\"\n",
    ")\n",
    "\n",
    "# Tokenize the dataset\n",
    "train_encodings = tokenizer(\n",
    "    list(train_texts), truncation=True, padding=True, max_length=512\n",
    ")\n",
    "test_encodings = tokenizer(\n",
    "    list(test_texts), truncation=True, padding=True, max_length=512\n",
    ")\n",
    "\n",
    "# Convert the data into Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": train_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "        \"labels\": train_labels,\n",
    "    }\n",
    ")\n",
    "test_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": test_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": test_encodings[\"attention_mask\"],\n",
    "        \"labels\": test_labels,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Load the BERT model\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(df[\"true_label\"].unique()),\n",
    "    cache_dir=\"../.cache\",\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=200,\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the BERT model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the BERT model\n",
    "bert_predictions = trainer.predict(test_dataset)\n",
    "bert_predicted_labels = bert_predictions.predictions.argmax(axis=1)\n",
    "\n",
    "print(\"BERT Classifier Performance:\")\n",
    "print(classification_report(test_labels, bert_predicted_labels, target_names=df[\"origin\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ebe973",
   "metadata": {},
   "source": [
    "# SciBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f701a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT model\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name, cache_dir=\"../.cache\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "train_encodings = tokenizer(\n",
    "    list(train_texts), truncation=True, padding=True, max_length=512\n",
    ")\n",
    "test_encodings = tokenizer(\n",
    "    list(test_texts), truncation=True, padding=True, max_length=512\n",
    ")\n",
    "\n",
    "# Convert the data into Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": train_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": train_encodings[\"attention_mask\"],\n",
    "        \"labels\": train_labels,\n",
    "    }\n",
    ")\n",
    "test_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": test_encodings[\"input_ids\"],\n",
    "        \"attention_mask\": test_encodings[\"attention_mask\"],\n",
    "        \"labels\": test_labels,\n",
    "    }\n",
    ")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=len(df[\"true_label\"].unique()), cache_dir=\"../.cache\"\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "# Train the BERT model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the BERT model\n",
    "bert_predictions = trainer.predict(test_dataset)\n",
    "bert_predicted_labels = bert_predictions.predictions.argmax(axis=1)\n",
    "\n",
    "print(\"BERT Classifier Performance:\")\n",
    "print(classification_report(test_labels, bert_predicted_labels, target_names=df[\"origin\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4033290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
