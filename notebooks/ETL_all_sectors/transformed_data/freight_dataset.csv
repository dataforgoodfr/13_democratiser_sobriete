title,abstract,true_label
Application of an interpretable classification model on Early Folding Residues during protein folding,"BackgroundMachine learning strategies are prominent tools for data analysis. Especially in life sciences, they have become increasingly important to handle the growing datasets collected by the scientific community. Meanwhile, algorithms improve in performance, but also gain complexity, and tend to neglect interpretability and comprehensiveness of the resulting models.ResultsGeneralized Matrix Learning Vector Quantization (GMLVQ) is a supervised, prototype-based machine learning method and provides comprehensive visualization capabilities not present in other classifiers which allow for a fine-grained interpretation of the data. In contrast to commonly used machine learning strategies, GMLVQ is well-suited for imbalanced classification problems which are frequent in life sciences. We present a Weka plug-in implementing GMLVQ. The feasibility of GMLVQ is demonstrated on a dataset of Early Folding Residues (EFR) that have been shown to initiate and guide the protein folding process. Using 27 features, an area under the receiver operating characteristic of 76.6% was achieved which is comparable to other state-of-the-art classifiers. The obtained model is accessible at https://biosciences.hs-mittweida.de/efpred/.ConclusionsThe application on EFR prediction demonstrates how an easy interpretation of classification models can promote the comprehension of biological mechanisms. The results shed light on the special features of EFR which were reported as most influential for the classification: EFR are embedded in ordered secondary structure elements and they participate in networks of hydrophobic residues. Visualization capabilities of GMLVQ are presented as we demonstrate how to interpret the results.",Not About Sufficiency
Strategies for addressing conflicts arising from blue growth initiatives: insights from three case studies in South Africa,"South Africa has vigorously embraced the concept of the 'blue economy' and is aggressively pursuing a blue growth strategy to expand the ocean economy, create jobs, and alleviate poverty. However, many of these 'blue initiatives' are leading to conflicts amongst various stakeholders with different histories, relationships with resources and areas, worldviews, and values. Investment in the ocean economy is being prioritized by government and planning, environmental assessment, and decision-making processes are being fast-tracked. Consequently, historical inequities as well as environmental and social justice considerations are not being given due consideration. Communities are not being effectively consulted. This has resulted in tensions and conflicts amongst proponents of these projects and local communities living in areas affected by these initiatives. We examine the drivers of conflict and then explore the strategies that local communities and their social partners have employed in these case studies to challenge contentious developments, defend coastal and marine areas, and make their voices heard. The cases involve conflicts over air quality in an expanding marine industrial zone at Saldanha Bay, prospecting and mining applications in the vicinity of the Olifants Estuary in the Western Cape, and the expansion of the Richard's Bay Port, mining activities, and conservation initiatives in KwaZulu-Natal. The barriers and potential opportunities to opening up deliberative spaces, shifting values and views, and co-producing knowledge, in contexts that are characterised by structural inequality, poverty, and power asymmetries, are discussed.",Not About Sufficiency
Transfer Learning Approaches In The Domain Of Radial-Axial Ring Rolling For Machine Learning Applications,"Due to increased data accessibility, data-centric approaches, such as machine learning, are getting more represented in the forming industry to improve resource efficiency and to optimise processes. Prior research shows, that a classification of the roundness of shaped rings, using machine learning algorithms, is applicable to radial-axial ring rolling. The accuracy of these predictions nowadays is still limited by the amount and quality of the data. Therefore, this paper will focus on how to make the best use of the limited amount of data, using transfer learning approaches. Since acquiring data for homogenised databases is time, energy and resource consuming, logged data gathered by the industry is often used in research. This paper takes both, industrial data from thyssenkrupp rothe erde Germany GmbH and a smaller dataset of an inhouse research plant, into account. Additionally, a synthetic dataset, created by generative adversarial networks, is considered. To accomplish an improvement of machine learning predictions using accessible data, three transfer learning approaches are investigated in order to extend existing models: ( I) transferring from a radial-axial ring rolling mill to a different mill containing less available data with a ratio of 20:1, (II) learning from unlabelled data using an autoencoder and (III) training on synthetic data. The obtained improvements are further evaluated. Based on these results, future possible investigations are elaborated, in particular the consideration of transfer learning from the less complex cold ring rolling process.",Not About Sufficiency
Defining Risk in Food Safety in the Philippines,"Food safety is a fundamental public health concern that is dependent on various factors such as changing global food production patterns, public expectations, and international trade policies.(1,2) As a member of the World Trade Organization, the Philippines has agreed to follow the Uruguay Round of Trade Organization, the Sanitary and Phytosanitary Agreement, and Technical Barriers to Trade that permits countries to take legitimate measures to protect the life and health of their consumers in relation to food safety matters while prohibiting them from using those measures in a way that unjustifiably restricts food trade.(3,4,5) The Philippines is also a member of the Codex Alimentarius Commission that aims to ensure consumer protection and to facilitate international trade.(6) With these objectives, Codex focuses on the development of food standards based on risk analysis and independent scientific advice provided by expert bodies organized by the Food and Agriculture Organization and the World Health Organization.(7) Risk analysis is a systematic and disciplined methodology that provides policymakers with the science-based information and evidence needed for effective and transparent decision-making, leading to improvements in food safety and public health.(8) In the Philippines, Republic Act No. 10611 or the Food Safety Act of 2013, serves as the framework for implementing the farm to fork food safety regulatory system which ensures a high level of consumer health protection, fair trade practices and global competitiveness of Philippine foods by controlling hazards in the food chain, adoption of precautionary measures based on scientific risk analysis, and adoption of international standards.",Not About Sufficiency
Prediction of Churning Behavior of Customers in Telecom Sector Using Supervised Learning Techniques,"Data mining is vast area that co-relates diverse branches i.e Statistics, Data Base, Machine learning and Artificial intelligence. Various applications are accessible in various areas. Churning of the Customer is the behavior when client never again needs to stay with his association with the company. Customer Churn Management is assuming essential job in client management. Nowadays different telecommunication companies are concentrating on distinguishing high esteemed and potential churning clients to expand benefit and share market. It is comprehended that making new clients are costlier than to holding existing client. There is a current issue that customer leave the organization because of obscure reasons. In our investigation, we predict churn behavior of the client by utilizing diverse data mining methods. It will in the long run help in breaking down client's behavior and characterize whether it is a churning client or not. We utilize online accessible data set available at Kaggle repository and for forecasting of Customer behavior we utilized different algorithms while we achieved 99.8% accuracy level using Bagging Algorithms.",Not About Sufficiency
"Recent Developments in Heavy Metals Detection: Modified Electrodes, Pretreatment Methods, Prediction Models and Algorithms","Heavy metal pollution has become an increasingly serious environmental issue, making the detection of heavy metals essential for safeguarding public health and the environment. This review aims to highlight the commonly used methods for detecting heavy metals (such as atomic absorption spectroscopy (AAS), atomic emission spectroscopy (AES), inductively coupled plasma-mass spectrometry (ICP-MS), square-wave anodic stripping voltammetry (SWASV), etc.), with a particular focus on electrochemical detection and electrode modification materials. Metal nanomaterials (such as titanium dioxide (TiO2), copper oxide (CuO), ZIF-8, MXene, etc.) are emphasized as promising candidates for enhancing the performance of sensors due to their high surface area and excellent catalytic properties. However, challenges such as interference from non-target heavy metal ions and the formation of organometallic complexes with organic compounds can complicate the detection process. To address these issues, two potential solutions have been proposed: the development of advanced algorithms (such as machine learning (ML), back-propagation neural network (BPNN), support vector machines (SVM), random forests (RF), etc.) for signal processing and the use of pretreatment methods (such as Fenton oxidation (FO), ozone oxidation, and photochemical oxidation) to suppress such interferences. This paper aims to review commonly used methods for detecting heavy metals, with a particular emphasis on electrochemical techniques. It will also highlight the challenges faced in these methods, such as interference and sensitivity limitations, and propose innovative solutions, including the use of metal nanomaterials for improved sensor performance and the integration of advanced algorithms and pretreatment techniques to address interference and enhance detection accuracy.",Not About Sufficiency
The spatial planning of housing and urban green space: A combined stated choice experiment and land-use modeling approach,"Urban green space (UGS) is receiving increasing attention as a means to make cities better adapted to climate change and to create high-quality living environments for citizens. The spatial planning of UGS generally asks a trade-off between optimizing land-use and accessibility characteristics which may differ between housing types. In this paper, we use a stated choice experiment to estimate residential location preferences related to neighborhood land-use, UGS and accessibility characteristics in the framework of a land-use model. A national sample of 394 Dutch homeowners participated in the experiments and mixed logit models were estimated by housing type distinguishing detached, row houses and apartments. The estimation results show that trade-offs made between a green neighborhood and accessibility to urban amenities tend to differ between the housing types. The estimates are used as parameters in a housing land-use allocation model. An application of the model to a residential area development problem shows that creating green buffers along road infrastructure is the most beneficial way of allocating UGS considering the housing value for residents. The application further shows that apartments benefit more strongly from UGS in the neighborhood than from high accessibility of urban amenities compared to detached and row houses. Finally, we find that the optimal spatial allocation of UGS depends on whether maximizing housing market-value or residents' utility is the prime objective.",Not About Sufficiency
Exploration of doped quantum magnets with ultracold atoms,"In the last decade, quantum simulators, and in particular cold atoms in optical lattices, have emerged as a valuable tool to study strongly correlated quantum matter. These experiments are now reaching regimes that are numerically difficult or impossible to access. In particular they have started to fulfill a promise which has contributed significantly to defining and shaping the field of cold atom quantum simulations, namely the exploration of doped and frustrated quantum magnets and the search for the origins of high-temperature superconductivity in the fermionic Hubbard model. Despite many future challenges lying ahead, such as the need to further lower the experimentally accessible temperatures, remarkable studies have already been conducted. Among them, spin-charge separation in one-dimensional systems has been demonstrated, extended-range antiferromagnetism in two-dimensional systems has been observed, connections to modern day large-scale numerical simulations were made, and unprecedented comparisons with microscopic trial wavefunctions have been carried out at finite doping. In many regards, the field has acquired new realms, putting old ideas to a new test and producing new insights and inspiration for the next generation of physicists. In the first part of this paper, we review the results achieved in cold atom realizations of the Fermi-Hubbard model in recent years. We put special emphasis on the new probes available in quantum gas microscopes, such as higher-order correlation functions, full counting statistics, the ability to study far-from -equilibrium dynamics, machine learning and pattern recognition of instantaneous snapshots of the many-body wavefunction, and access to non-local correlators. Our review is written from a theoretical perspective, but aims to provide basic understanding of the experimental procedures. We cover one- dimensional systems, where the phenomenon of spin-charge separation is ubiquitous, and two-dimensional systems where we distinguish between situations with and without doping. Throughout, we focus on the strong coupling regime where the Hubbard inter-actions U dominate and connections to t - J models can be justified. In the second part of this paper, with the stage set and the current state of the field in mind, we propose a new direction for cold atoms to explore: namely mixed-dimensional bilayer systems, where the charge motion is restricted to individual layers which remain coupled through spin-exchange. These systems can be directly realized experimentally and we argue that they have a rich phase diagram, potentially including a strongly correlated BEC-to-BCS cross-over and regimes with different superconducting order parameters, as well as complex parton phases and possibly even analogs of tetraquark states. In particular, we propose a novel, strong pairing mechanism in these systems, which puts the formation of hole pairs at experimentally accessible, elevated temperatures within reach. Ultimately we propose to explore how the physics of the mixed-dimensional bilayer system can be connected to the rich phenomenology of the single-layer Hubbard model. (C) 2021 Elsevier Inc. All rights reserved.",Not About Sufficiency
The Effect of World Happiness Aspects with COVID-19 Infected using Machine Learning Multiple Regression Model,"An outbreak of pneumonia was found in China in December 2019, known to be caused by a virus called COVID-19. Since then, the virus has spread all over the world causing countries to lock their borders to prevent the spread of infections. There is no known medicine for COVID-19 and other corona virus. The objective of this paper is to reveal whether five aspects of world happiness that was obtained from https://worldhappiness.report which are Freedom to make life choices, Social support, Healthy life expectancy, Logged GDP per capita and Ladder score have effect on the number COVID-19 infected cases where the dataset is taken from COVID-19 World Dataset from www.kaggle.com. Using Multiple Regression as the model, the result shows that the predicted and actual infected cases are similar and hence there are no effect between COVID-19 and those five aspects.",Not About Sufficiency
A disaster-severity assessment DSS comparative analysis,"This paper aims to provide a comparative analysis of fuzzy rule-based systems and some standard statistical and other machine learning techniques in the context of the development of a decision support system (DSS) for the assessment of the severity of natural disasters. This DSS, which will be referred to as SEDD, has been proposed by the authors to help decision makers inside those Non-Governmental Organizations (NGOs) concerned with the design and implementation of international operations of humanitarian response to disasters. SEDD enables a relatively highly accurate and interpretable assessment on the consequences of almost every potential disaster scenario to be obtained through a set of easily accessible information about that disaster scenario and historical data about similar ones. Thus, although SEDD's methodology is rather sophisticated, its data requirements are small, which, therefore, enables its use in the context of NGOs and countries requiring humanitarian aid. In this sense, SEDD opposes to some current tools which focuses on one phenomena-one place disaster scenarios (earthquakes in California, hurricanes in Florida, etc.) and/or have extensive and/or technologically sophisticated data requirements (real-time remote sensing information, exhaustive building census, etc.). Moreover, although focused on disaster response, SEDD can also be useful in other phases of disaster management, as disaster mitigation or preparedness. Particularly, the predictive accuracy and interpretability of SEDD fuzzy methodology is compared here in a disaster severity assessment context with those of multiple linear regression, linear discriminant analysis, classification trees and support vector machines. After an extensive validation over the EM-DAT disaster database, it is concluded that SEDD outperforms the methods above in the task of simultaneously providing an accurate and interpretable inference tool for the evaluation of the consequences of disasters.",Not About Sufficiency
Bringing Order to Chaos in MOOC Discussion Forums with Content-Related Thread Identification,This study addresses the issues of overload and chaos in MOOC discussion forums by developing a model to categorize and identify threads based on whether or not they are substantially related to the course content. Content-related posts were defined as those that give/seek help for the learning of course material and share/comment on relevant resources. A linguistic model was built based on manually-coded starting posts in threads from a statistics MOOC (n=837) and tested on thread starting posts from the second offering of the same course (n=304) and a different statistics course (n=298). The number of views and votes threads received were tested to see if they helped classification. Results showed that content-related posts in the statistics MOOC had distinct linguistic features which appeared to be unrelated to the subject-matter domain; the linguistic model demonstrated good cross-course reliability (all recall and precision > .77) and was useful across all time segments of the courses; number of views and votes were not helpful for classification.,Not About Sufficiency
Contextualizing mobility during the Ebola epidemic in Liberia,"Based on findings from focus groups and key informant interviews conducted at five sites in Liberia between 2018 and 2019, we explore some of the key factors that influenced people's motivation to travel during the 2014-2016 Ebola Virus Disease (EVD). We discuss how these factors led to certain mobility patterns and the implications these had for EVD response. The reasons for individual mobility during the epidemic were multiple and diverse. Some movements were related to relocation efforts as people attempted to extricate themselves from stigmatizing situations. Others were motivated by fear, convinced that other communities would be safer, particularly if extended family members resided there. Individuals also felt compelled to travel during the epidemic to meet other needs and obligations, such as attending burial rites. Some expressed concerns about obtaining food and earning a livelihood. Notably, these latter concerns served as an impetus to travel surreptitiously to evade quarantine directives aimed specifically at restricting mobility. Improvements in future infectious disease response could be made by incorporating contextually-based mobility factors, for example: the personalization of public health messaging through the recruitment of family members and trusted local leaders, to convey information that would help allay fear and combat stigmatization; activating existing traditional community surveillance systems in which entry into the community must first be approved by the community chief; and increased involvement of local leaders and community members in the provision of food and care to those quarantined so that the need to travel for these reasons is removed. Author summaryHuman mobility is an important element of infectious disease outbreaks, both in terms of its spread and control. Understanding and dealing with the complex issue of human mobility warrants serious consideration in efforts to limit the spread of the disease. For these and other reasons, public health responses to infectious disease outbreaks must first be predicated upon a firm understanding of the contextual issues surrounding human movement and agency, especially those pertaining to the social, cultural and political-economic dimensions involved. Focus groups and key informant interviews conducted at five sites in Liberia in 2018/19 offered insight into the reasons people gave to account for their movements during the 2014/16 EVD outbreaks. Based on the understanding of mobility we gained from this study, we recommend that infectious disease response could be improved through the personalizing of public health messaging through the recruitment of family members and trusted local leaders to convey information that would help allay fear and combat stigmatization; activating existing traditional community surveillance systems in which entry into the community must first be approved by the community chief; and increased involvement of local leaders and community members in the provision of food and care to those quarantined so that the need to travel for these reasons is removed.",Not About Sufficiency
VF-Pred: Predicting virulence factor using sequence alignment percentage and ensemble learning models,"This study introduces VF-Pred, a novel framework developed for the purpose of detecting virulence factors (VFs) through the analysis of genomic data. VFs are crucial for pathogens to successfully infect host tissue and evade the immune system, leading to the onset of infectious diseases. Identifying VFs accurately is of utmost importance in the quest for developing potent drugs and vaccines to counter these diseases. To accomplish this, VF-Pred combines various feature engineering techniques to generate inputs for distinct machine learning classification models. The collective predictions of these models are then consolidated by a final downstream model using an innovative ensembling approach. One notable aspect of VF-Pred is the inclusion of a novel Seq-Alignment feature, which significantly enhances the accuracy of the employed machine learning algorithms. The framework was meticulously trained on 982 features obtained from extensive feature engineering, utilizing a comprehensive ensemble of 25 models. The new downstream ensembling technique adopted by VF-Pred sur-passes existing stacking strategies and other ensembling methods, delivering superior performance in VF detection. There have been similar studies done earlier, VF-Pred stands out in comparison showing higher accuracy (83.5 %), higher sensitivity (87 %) towards identification of VFs. Accessible through a user-friendly web page, VF-Pred can be accessed by providing the identifier and protein sequence, enabling the prediction of high or low likelihoods of VFs. Overall, VF-Pred showcases a highly promising methodology for the identification of VFs, potentially paving the way for the development of more effective strategies in the battle against infectious diseases.",Not About Sufficiency
Affect-Aware Machine Learning Models for Deception Detection,"Automated deception detection systems can enhance societal well-being by helping humans detect deceivers and support people in high-stakes situations across health, social work, and legal domains. Existing computational approaches for detecting deception have not leveraged dimensional representations of affect, specifically valence and arousal, expressed during communication. My research presents a novel analysis of the potential for including affect in machine learning models for detecting deception. My work informs and motivates the development of affect-aware machine learning approaches for modeling deception and other social behaviors during human interactions in-the-wild. This research, independently defined and conducted by me, is from work-in progress towards my undergraduate thesis in the Department of Computer Science at the University of Southern California.",Not About Sufficiency
Digital Twin Fundamentals of mRNA In Vitro Transcription in Variable Scale Toward Autonomous Operation,"The COVID-19 pandemic caused the rapid development of mRNA (messenger ribonucleic acid) vaccines and new RNA-based therapeutic methods. However, the approval rate for candidates has the potential to be increased, with a significant number failing so far due to efficacy, safety, and manufacturing deficiencies, hindering equitable vaccine distribution during pandemics. This study focuses on optimizing the production of mRNA, a critical component of mRNA-based vaccines, using a scalable machine by investigating the key mechanisms of mRNA in vitro transcription. First, kinetic parameters for the mRNA production process were determined. The validity of the determination and the robustness of the model are demonstrated by predicting different reactions with and without substrate limitations as well as different transcripts. The optimized reaction conditions, including temperature, urea concentration, and concentration of reaction-enhancing additives, resulted in a 55% increase in mRNA yield with a 33% reduction in truncated mRNA. Additionally, the feasibility of a segmented flow approach allowed for high-throughput screening (HTS), enabling the production of 20 vaccine candidates within a short time frame, representing a 10-fold increase in productivity, compared to nonsegmented reactions limited by the residence time in the plug flow reactor. The findings presented for the first time here contribute to the development of a fully continuous and efficient manufacturing process for mRNA and other cell and gene therapy drugs/vaccine candidates as presented in our previous work, which discussed the integration of process analytical technologies and predictive process models in a Biopharma 4.0 facility to enable the production of clinical and large-scale doses, ensuring a rapid and resilient supply of critical therapeutics. The results in this study especially highlight that the same machine and equipment can be used for screening and manufacturing different drug candidates in continuous operation. By streamlining production and adhering to quality standards, this approach enhances the industry's ability to respond swiftly to pandemics and public health emergencies, addressing the urgent need for accessible and effective vaccines.",Not About Sufficiency
UHD Video Coding: A Light-Weight Learning-Based Fast Super-Block Approach,"The ultra high-definition (UHD) video format, which has recently become popular, aims to provide high spatial resolution, high temporal frame rate, high sample bit-depth, and wide pixel color gamut. Despite the continued development of global network capacities, it inevitably causes the increased bandwidth cost of catering to the requirement of delivering UHD video services. To address such challenges, this paper presents an improved super coding unit (SCU) method for UHD video coding in High Efficiency Video Coding (HEVC). Initially, the medium coding unit (MCU) is proposed to avoid unnecessary brute-force coding unit (CU) partitions of SCU. Furthermore, the SCU is proposed to be encoded by Direct-MCU and SCU-to-MCU modes: the Direct-MCU mode is intended to better adapt to the texture-rich region, which guarantees the compression efficiency by avoiding extra-size CU partition; the SCU-to-MCU mode is designed for the homogeneous region of UHD content, which saves the encoding time by skipping fine-grained CU partition search. Moreover, a learning-based fast SCU decision approach is proposed to speed up the determination process of Direct-MCU and SCU-to-MCU, where three representative handcrafted features are extracted. Experimental results show that our method achieves an affordable complexity and excellent coding efficiency (up to 7.30% Bjontegaard Delta rate savings) in UHD video coding compared to recent HEVC reference software.",Not About Sufficiency
A graph representation of molecular ensembles for polymer property prediction,"Synthetic polymers are versatile and widely used materials. Similar to small organic molecules, a large chemical space of such materials is hypothetically accessible. Computational property prediction and virtual screening can accelerate polymer design by prioritizing candidates expected to have favorable properties. However, in contrast to organic molecules, polymers are often not well-defined single structures but an ensemble of similar molecules, which poses unique challenges to traditional chemical representations and machine learning approaches. Here, we introduce a graph representation of molecular ensembles and an associated graph neural network architecture that is tailored to polymer property prediction. We demonstrate that this approach captures critical features of polymeric materials, like chain architecture, monomer stoichiometry, and degree of polymerization, and achieves superior accuracy to off-the-shelf cheminformatics methodologies. While doing so, we built a dataset of simulated electron affinity and ionization potential values for >40k polymers with varying monomer composition, stoichiometry, and chain architecture, which may be used in the development of other tailored machine learning approaches. The dataset and machine learning models presented in this work pave the path toward new classes of algorithms for polymer informatics and, more broadly, introduce a framework for the modeling of molecular ensembles.",Not About Sufficiency
"Pharmaceutical Patents, Innovation, and the Right to Health","There have been rapid strides in developing laws related to pharmaceutical patents in recent years at the global level. The scope of patentability of pharmaceutical patents has increased and has been codified under the Agreement on Trade-Related Aspects of Intellectual Property Rights (TRIPS). The adoption of the TRIPS Agreement resulted in significant changes to the laws related to patents for developing countries, which are the ers of the World Trade Organization (WTO). The developed nations have contended that patents in the medical field are essential for the promotion of research and innovation. They believe that only a strict patent protection regime catalyzes the development of better medicines and techniques. However, this often results in excessive patenting and monopoly over certain drugs. This drives the prices of the drugs up and makes them inaccessible to a large section of the population, particularly in developing and least-developed countries (LDC's). The scenario regarding vaccine-related patents during the Covid-19 Pandemic was grim since profits were prioritized over people's Health. The pharmaceutical companies were inclined to secure profits and protect their research from being used elsewhere. This amplifies the debate between the protection of Intellectual Property Rights and the Right to the Health of people. The developed countries have contended that the exceptions provided under TRIPS are sufficient to safeguard the health rights of the general populace in developing and LDC's. However, the experiences of the developing countries with the implementation of the exceptions to the TRIPS have yet to be fruitful, with developed countries repeatedly pressuring them into protecting patents at the expense of the Health of their citizens. Geopolitical tensions could further aggravate this. This is evidenced by the sanctions imposed on Russia by the West, which stares at a situation in which Western Pharmaceutical Companies can deny it access to medicines. The unavailability of modern medicine can adversely impact the Human Rights of its citizens in such a scenario. The tensions between different nations can flare over anytime, and the rigid application of TRIPS can be catastrophic to the health rights of the citizens. The article analyses the exceptions under TRIPS and their Effectiveness in protecting the Right to Health. It also suggests modification in the existing Intellectual Property Regime related to pharmaceutical patents to ensure that the Right to Health is not compromised.",Not About Sufficiency
"Influence of Activity-Travel Participation, Travel Mode Choice, and Multitasking Activities on Subjective Well-Being Using R","Multitasking activities (MTA) are typically thought to enhance general subjective well-being (SWB). However, performing MTA while operating a private vehicle is frequently challenging. Public transportation (PT) can provide an additional option to engage in more pleasurable activities while traveling. Several studies have been conducted on the engagement of different activities while using different transport modes and its influence on physical, social, and mental health. Moreover, numerous studies have been carried out on motorized transport and MTA that resulted in accidents, fatalities, injuries, and even disasters. In addition, several experts studied the influence of health parameters on daily activities. There have, however, only been a few studies on MTA while on PT and its influence on SWB. Therefore, the current study aims to investigate the travel mode choice, the performance of onboard MTA, and its influence on overall SWB. Using random sampling techniques, data on 732 individuals and 191 households-representing 0.029% of the overall population of Bandung, Indonesia-were gathered. Two different models were developed between independent, intermediate, and dependent variables. Statistical Package for Social Sciences (SPSS) was used for descriptive statistics, whereas R software was used for the multilevel linear regression analysis. The model estimation results show that MTA mediates the relationship among socio-demographic and economic variables, built environment, trip and travel parameters, and SWB. A unit increase in PT lines can provide a 1.5% greater opportunity to participate in more onboard MTA; however, a unit increase in MTA can enhance SWB by 5.1% where both the models show satisfactory coefficient of determination (R2). A unit increase in motorized transport caused a 12.9% negative association with MTA and 10.9% with SWB. A unit increase in NMT and PT are 21.7% and 10.2% positively associated with MTA and 19.2% and 13.1% positively associated with SWB. The current study helps policymakers to develop a policy based on PT which allows the individuals to engage in more MTA that enhance SWB and target sustainable transportation system.",Not About Sufficiency
Constructing a pollen proxy from low-cost Optical Particle Counter (OPC) data processed with Neural Networks and Random Forests,"Pollen allergies affect a significant proportion of the global population, and this is expected to worsen in years to come. There is demand for the development of automated pollen monitoring systems. Low-cost Optical Particle Counters (OPCs) measure particulate matter and have attractive advantages of real-time high time resolution data and affordable costs. This study asks whether low-cost OPC sensors can be used for meaningful monitoring of airborne pollen. We employ a variety of methods, including supervised machine learning techniques, to construct pollen proxies from hourly-average OPC data and evaluate their performance, holding out 40 % of observations to test the proxies. The most successful methods are supervised machine learning Neural Network (NN) and Random Forest (RF) methods, trained from pollen concentrations collected from a Hirst-type sampler. These perform significantly better than using a simple particle size-filtered proxy or a Positive Matrix Factorisation (PMF) source apportionment pollen proxy. Twelve NN and RF models were developed to construct a pollen proxy, each varying by model type, input features and target variable. The results show that such models can construct useful information on pollen from OPC data. The best metrics achieved (Spearman correlation coefficient = 0.85, coefficient of determination = 0.67) were for the NN model constructing a Poaceae (grass) pollen proxy, based on particle size information, temperature, and relative humidity. Ability to distinguish high pollen events was evaluated using F1 Scores, a score reflecting the fraction of true positives with respect to false positives and false negatives, with promising results (F1 <= 0.83). Modelconstructed proxies demonstrated the ability to follow monthly and diurnal trends in pollen. We discuss the suitability of OPCs for monitoring pollen and offer advice for future progress. We demonstrate an attractive alternative for automated pollen monitoring that could provide valuable and timely information to the benefit of pollen allergy sufferers.",Not About Sufficiency
From cloud AI to embedded AI in cardiac healthcare,"The health of the population is a global concern, and cardiac health is an increasingly studied aspect. In the era of artificial intelligence (AI), there are many possibilities to improve the diagnosis and monitoring of our cardiac system in a non-invasive and unobtrusive and way and accessible to a wider population. This article addresses issues such as the diagnosis of heart diseases in which the heartbeat becomes irregular using wearable systems containing embedded artificial intelligence. This paper is mainly a review of reported studies that have recently been developed that demonstrate how technology is moving from cloud-based AI systems to embedded AI systems in cardiac healthcare.",Not About Sufficiency
Accurate Prediction of Lysine Methylation Sites Using Evolutionary and Structural-Based Information,"Methylation is considered one of the proteins' most important post-translational modifications (PTM). Plasticity and cellular dynamics are among the many traits that are regulated by methylation. Currently, methylation sites are identified using experimental approaches. However, these methods are time-consuming and expensive. With the use of computer modelling, methylation sites can be identified quickly and accurately, providing valuable information for further trial and investigation. In this study, we propose a new machine-learning model called MeSEP to predict methylation sites that incorporates both evolutionary and structural-based information. To build this model, we first extract evolutionary and structural features from the PSSM and SPD2 profiles, respectively. We then employ Extreme Gradient Boosting (XGBoost) as the classification model to predict methylation sites. To address the issue of imbalanced data and bias towards negative samples, we use the SMOTETomek-based hybrid sampling method. The MeSEP was validated on an independent test set (ITS) and 10-fold cross-validation (TCV) using lysine methylation sites. The method achieved: an accuracy of 82.9% in ITS and 84.6% in TCV; precision of 0.92 in ITS and 0.94 in TCV; area under the curve values of 0.90 in ITS and 0.92 in TCV; F1 score of 0.81 in ITS and 0.83 in TCV; and MCC of 0.67 in ITS and 0.70 in TCV. MeSEP significantly outperformed previous studies found in the literature. MeSEP as a standalone toolkit and all its source codes are publicly available at https://github.com/arafatro/MeSEP.",Not About Sufficiency
"Making CNNs for Video Parsing Accessible Event Extraction from DOTA2 Gameplay Video using Transfer, Zero-Shot, and Network Pruning","The ability to extract sequences of game events for high-resolution e-sport games has traditionally required access to the game's engine. This serves as a barrier to groups who don't possess this access. It is possible to apply deep learning to derive these logs from gameplay video, but it requires computational power that serves as an additional barrier. These groups would benefit from access to these logs, such as small e-sport tournament organizers who could better visualize gameplay to inform both audience and commentators. In this paper we present a combined solution to reduce the required computational resources and time to apply a convolutional neural network (CNN) to extract events from e-sport gameplay videos. This solution consists of techniques to train a CNN faster and methods to execute predictions more quickly. This expands the types of machines capable of training and running these models, which in turn extends access to extracting game logs with this approach. We evaluate the approaches in the domain of DOTA2, one of the most popular e-sports. Our results demonstrate our approach outperforms standard backpropagation baselines.",Not About Sufficiency
Comparison of mortality patterns after the Fukushima Daiichi Nuclear Power Plant radiation disaster and during the COVID-19 pandemic,"The initial health impact caused by radiation disasters can be broadly classified into direct and indirect effects. Though no direct health hazards caused by radiation, such as acute radiation injury, were observed following the Fukushima Daiichi nuclear power plant accident, indirect deaths have been reported, including those caused by initial emergency evacuation and relocation, medical disruption, and psychological and social health effects. However, these indirect health effects have not been prioritised for addressal. We evaluated the radiation disaster experience with that of the coronavirus disease (COVID-19) pandemic that emerged while facing the challenges from the radiation disaster. Most of the health effects of COVID-19 are directly associated with infection, but indirect health effects of various scales and entities have been reported. The two disasters have similarities in terms of the strain on community healthcare and the large number of deaths. Adapting the measures implemented in the acute to subacute phases of the COVID-19 disaster to radiation disasters may help improve management following future radiation disasters. Based on the experience and findings during the COVID-19 pandemic, the pattern of deaths in radiation disasters can be divided into five major groups: direct deaths, and four indirect patterns of deaths due to a deteriorating supply-demand balance (a hospital-level problem), collapse of the healthcare system (a community-level problem), death due to neglect alongside underlying disease, and diseases other than direct invasion. From the similarities between the two disasters, three main issues should be prioritised as initial emergency evacuation measures in a radiation disaster: emergency exposure medicine, the establishment of a medical system, and protection of death with dignity. The validity of these priority issues needs to be verified in future research.",Not About Sufficiency
"Sustainable planning in Wuhan City during COVID-19: an analysis of influential factors, risk profiles, and clustered patterns","The outbreak of novel coronavirus pneumonia (COVID-19) is closely related to the intra-urban environment. It is important to understand the influence mechanism and risk characteristics of urban environment on infectious diseases from the perspective of urban environment composition. In this study, we used python to collect Sina Weibo help data as well as urban multivariate big data, and The random forest model was used to measure the contribution of each influential factor within to the COVID-19 outbreak. A comprehensive risk evaluation system from the perspective of urban environment was constructed, and the entropy weighting method was used to produce the weights of various types of risks, generate the specific values of the four types of risks, and obtain the four levels of comprehensive risk zones through the K-MEANS clustering of Wuhan's central urban area for zoning planning. Based on the results, we found: ?the five most significant indicators contributing to the risk of the Wuhan COVID-19 outbreak were Road Network Density, Shopping Mall Density, Public Transport Density, Educational Facility Density, Bank Density. Floor Area Ration, Poi Functional Mix ?After streamlining five indicators such as Proportion of Aged Population, Tertiary Hospital Density, Open Space Density, Night-time Light Intensity, Number of Beds Available in Designated Hospitals, the prediction accuracy of the random forest model was the highest. ?The spatial characteristics of the four categories of new crown epidemic risk, namely transmission risk, exposure risk, susceptibility risk and Risk of Scarcity of Medical Resources, were highly differentiated, and a four-level integrated risk zone was obtained by K-MEANS clustering. Its distribution pattern was in the form of ""multicenter-periphery"" gradient diffusion. For the risk composition of the four-level comprehensive zones combined with the internal characteristics of the urban environment in specific zones to develop differentiated control strategies. Targeted policies were then devised for each partition, offering a practical advantage over singular COVID-19 impact factor analyses. This methodology, beneficial for future public health crises, enables the swift identification of unique risk profiles in different partitions, streamlining the formulation of precise policies. The overarching goal is to maintain regular social development, harmonizing preventive measures and economic efforts.",Not About Sufficiency
Extracellular vesicles proteins for early cancer diagnosis: From omics to biomarkers,"Extracellular vesicles (EVs) are a promising source of early biomarkers for cancer diagnosis. They are enriched with diverse molecular content, such as proteins, DNA, mRNA, miRNA, lipids, and metabolites. EV proteins have been widely investigated as potential biomarkers since they reflect specific patient conditions. However, although many markers have been validated and confirmed using external cohorts of patients and different analytical approaches, no EV protein markers are approved for diagnostic use. This review presents the primary strategies adopted using mass spectrometry and immune-based techniques to identify and validate EV protein biomarkers. We report and discuss recent scientific research focusing on cancer biomarker discovery through EVs, emphasizing their significant potential for the tempestive diagnosis of several cancer typologies. Finally, recent advancements in the standardization of EV isolation and quantitation through the development of easy-touse and high-throughput kits for sample preparation-that should make protein EV biomarkers more reliable and accessible-are presented. The data reported here showed that there are still several challenges to be addressed before a protein vesicle marker becomes an essential tool in diagnosing cancer.",Not About Sufficiency
Utilisation of underground pedestrian systems for urban sustainability,"Underground pedestrian systems (UPS) have emerged as an urban phenomenon in the city centres of mega-cities, providing alternative walkways that are safe, accessible, efficient and pleasant for pedestrians. Despite many successful UPS in operation around the world, the application and performance of UPS are not yet well understood by local authorities. While previous studies debated the impacts on cities and people that the development of UPS would bring, an understanding of how to develop UPS to contribute to sustainable urban development, including economic viability, environmental livability and social equity, should be improved. This paper presents a detailed discussion of potential contributions and challenges in developing UPS within the context of sustainable urban development. It contains a comprehensive analysis of the relationship between UPS and urban development with regard to urban planning concepts such as the compact city, city resilience, sustainable transport and urban renewal, within the context of contemporary challenges such as the need to achieve economic sustainability, managing a non-renewable and vulnerable underground resource, and humanisation and social sustainability. It demonstrates why UPS development presents opportunities for and challenges to achieving economic viability, environmental livability and social equity, how to develop UPS so that they make effective contributions to sustainable urban development, and how the challenge of each issue has been addressed in light of the experiences of cities with UPS developments globally. (C) 2015 Elsevier Ltd. All rights reserved.",Not About Sufficiency
Common radiological findings in victims of intimate partner violence: A narrative review,"Physical, sexual, or emotional abuse by a current or former partner or spouse is known as intimate partner violence (IPV). It is the most prevalent type of violence against women. It's also the easiest to avoid among violent acts. Clinical decision making is influenced by imaging findings associated with IPV. Evaluation of pertinent patient demographic, trauma, and common radiological factors was conducted to determine the prevalence, nature, and severity of injuries in IPV patients. Medical experts often misdiagnose IPV, which affects millions of women. Radiologists and other healthcare personnel might raise awareness of this ""invisible"" yet pervasive public health issue by integrating adult IPV diagnosis in core training and employing machine learning. Longitudinal imaging and clinical data provide objective information and warnings that can change victim treatment and aid law enforcement.",Not About Sufficiency
Preventive healthcare policies in the US: solutions for disease management using Big Data Analytics,"Data-driven healthcare policy discussions are gaining traction after the Covid-19 outbreak and ahead of the 2020 US presidential elections. The US has a hybrid healthcare structure; it is a system that does not provide universal coverage, albeit few years ago enacted a mandate (Affordable Care Act-ACA) that provides coverage for the majority of Americans. The US has the highest health expenditure per capita of all western and developed countries; however, most Americans don't tap into the benefits of preventive healthcare. It is estimated that only 8% of Americans undergo routine preventive screenings. On a national level, very few states (15 out of the 50) have above-average preventive healthcare metrics. In literature, many studies focus on the cure of diseases (research areas such as drug discovery and disease prediction); whilst a minority have examined data-driven preventive measures-a matter that Americans and policy makers ought to place at the forefront of national issues. In this work, we present solutions for preventive practices and policies through Machine Learning (ML) methods. ML is morally neutral, it depends on the data that train the models; in this work, we make the case that Big Data is an imperative paradigm for healthcare. We examine disparities in clinical data for US patients by developing correlation and imputation methods for data completeness. Non-conventional patterns are identified. The data lifecycle followed is methodical and deliberate; 1000+clinical, demographical, and laboratory variables are collected from the Centers for Disease Control and Prevention (CDC). Multiple statistical models are deployed (Pearson correlations, Cramer's V, MICE, and ANOVA). Other unsupervised ML models are also examined (K-modes and K-prototypes for clustering). Through the results presented in the paper, pointers to preventive chronic disease tests are presented, and the models are tested and evaluated.",Not About Sufficiency
V2I-Calib: A Novel Calibration Approach for Collaborative Vehicle and Infrastructure LiDAR Systems,"Cooperative LiDAR systems integrating vehicles and road infrastructure, termed V2I calibration, exhibit substantial potential, yet their deployment encounters numerous challenges. A pivotal aspect of ensuring data accuracy and consistency across such systems involves the calibration of LiDAR units across heterogeneous vehicular and infrastructural endpoints. This necessitates the development of calibration methods that are both real-time and robust, particularly those that can ensure robust performance in urban canyon scenarios without relying on initial positioning values. Accordingly, this paper introduces a novel approach to V2I calibration, leveraging spatial association information among perceived objects. Central to this method is the innovative Overall Intersection over Union (oIoU) metric, which quantifies the correlation between targets identified by vehicle and infrastructure systems, thereby facilitating the real-time monitoring of calibration results. Our approach involves identifying common targets within the perception results of vehicle and infrastructure LiDAR systems through the construction of an affinity matrix. These common targets then form the basis for the calculation and optimization of extrinsic parameters. Comparative and ablation studies conducted using the DAIR-V2X dataset substantiate the superiority of our approach. For further insights and resources, our project repository is accessible at https://github.com/MassimoQu/v2i-calib.",Not About Sufficiency
Electronic Phenotyping to Identify Patients with Heart Failure Using a National Clinical Information Database in Japan,"Heart failure (HF) is a grave problem in the clinical and public health sectors. The aim of this study is to develop a phenotyping algorithm to identify patients with HF by using the medical information database network (MID-NET) in Japan. Methods: From April 1 to December 31, 2013, clinical data of patients with HF were obtained from MID-NET. A phenotyping algorithm was developed with machine learning by using disease names, examinations, and medications. Two doctors validated the cases by manually reviewing the medical records according to the Japanese HF guidelines. The algorithm was also validated with different cohorts from an inpatient database of the Department of Cardiovascular Medicine at Tohoku University Hospital. Results: The algorithm, which initially had low precision, was improved by incorporating the value of B-type natriuretic peptide and the combination of medications related to HF. Finally, the algorithm on a different cohort was verified with higher precision (35.0%. 87.8%). Conclusions: Proper algorithms can be used to identify patients with HF.",Not About Sufficiency
ATLAS Open Data to engage the public in Education and Research(?),"ATLAS Open Data is an initiative aimed at making the data, simulations, and documentary resources of the experiment accessible to a wide audience, in accordance with the CERN Open Data policy. The project has seen the release of numerous datasets of proton-proton collisions at center-of-mass energies of 8 TeV and 13 TeV collected at the LHC during Run-1 and Run-2, allowing for the investigation of typical phenomena in high-energy physics. To facilitate their dissemination, the data are shared in accessible and commonly used formats. Additionally, they are accompanied by software and web interfaces designed for easy use, without the need for installation or coding by the user. The objective is twofold. On the one hand, the goal is to promote these activities in outreach and educational contexts, such as Summer Schools, Masterclasses, university projects, as well as various initiatives within the ATLAS Collaboration itself. On the other hand, high-energy physics research becomes accessible to an interdisciplinary audience, involving experts from other fields to benefit from their expertise, such as machine learning and computer science. This work provides an overview of the ATLAS Open Data resources, with concrete examples of how they can be used to promote scientific education and research in the field of particle physics.",Not About Sufficiency
Microplastic occurrence and effects in commercially harvested North American finfish and shellfish: Current knowledge and future directions,"Scientific Significance Statement As global seafood consumption rises, it is important to understand the mechanisms by which fisheries are affected by microplastic pollution. A growing body of literature describes the occurrence and effects of microplastics in commercial species, primarily from Europe, Asia, and South America; however, there are far fewer studies conducted in North America. In this article, we review the evidence available for the presence and effects of microplastics on commercially valuable fishery species of North America and possible consequences of human consumption. We identify key priorities for future research on this topic including geographic and taxonomic representativeness; physiological, organismal, and population level effects; microplastics as multiple stressors; human health risks; and standardization of field and lab protocols. Commercial fisheries yield essential foods, sustain cultural practices, and provide widespread employment around the globe. Commercially harvested species face a myriad of anthropogenic threats including degraded habitats, changing climate, overharvest, and pollution. Microplastics are pollutants of increasing concern, which are pervasive in the environment and can harbor or adsorb pollutants from surrounding waters. Aquatic organisms, including commercial species, encounter and ingest microplastics, but there is a paucity of data about those caught and cultured in North America. Additional research is needed to determine prevalence, physiological effects, and population-level implications of microplastics in commercial species from Canada, the United States, and Mexico. Investigations into possible human health effects of microplastic exposure from seafood are also greatly needed. This synthesis summarizes current knowledge, identifies data gaps, and provides future research directions for addressing microplastics effects in commercially valuable North American fishery species.",Not About Sufficiency
Medical diagnosis assistant based on category ranking,"This paper presents a real-world application for assisting medical diagnosis which relies on the exclusive use of machine learning techniques. We have automatically processed an extensive biomedical literature to train a categorization algorithm in order to provide it with the capability of matching symptoms to MeSH diseases descriptors. To interact with the classifier, we have developed a web interface so that professionals in medicine can easily get some help in their diagnostical decisions. We also demonstrate the effectiveness of this approach with a test set containing several hundreds of real clinical histories. A full operative version can be accessed on-line through the following site: www.dlsi.ua.es/omda/index.php.",Not About Sufficiency
Promoting the social interaction in a historical community through participatory planning and design: An empirical study in Rural China,"Public participation in heritage conservation is an area that lacks proper investigation in the urban planning and design discipline. To address this gap, this paper presents an empirical study of participatory design methods promoting social interaction in Liuzhi, a historical community with cultural and historical relics in rural China. Based on data collected through surveys, poster mapping, in-situ exhibition, and photo collage, this paper presents perceptions and the needs of residents for the development of cultural heritage. The results indicate that the methodology facilitates the dissemination of expertise and local knowledge, as well as the bidirectional interactions between designers and residents to enhance an understanding of the historical value of heritage. Suggestions were proposed for balancing the contradictions of expertise and local knowledge, which is imperative in the development of proposals for heritage conservation and regeneration in rural areas.",Not About Sufficiency
Application of random forest model to predict the demand of essential medicines for non-communicable diseases management in public health facilities,"Introduction: recent initiatives in healthcare reform have pushed for a better understanding of data complexity and revolution. Given the global prevalence of Non-Communicable Diseases (NCD) and the economic and clinical burden they impose, it is recommended that the management of essential medicines used to treat them be renovated and optimized through the application of predictive modeling such a RF model. Methods: in this study, a series of data pre-processing activities were used to select the top seventeen (17) NCD essential medicines most commonly used for treating common and frequent NCD. The study focused on machine learning (ML) applications, whereby a random forest (RF) model was applied to predict the demand using essential medicines consumption data from 2015 to 2019 for approximately 500 medical products. Results: with a seventy-eight (78) percent accuracy rate for the training set and a 71 percent accuracy rate for the testing set, the RF model predicted the trend in demand for 17 NCD essential medicines. This was achieved by entering the month, year, district, and name of the NCD essential medicine. Based on historical consumption data, the RF model can thus be used to predict demand trends. Our findings showed that the RF model is talented to commendably perform as a predicting model. Conclusion: the study concluded that RF has the ability to optimize health supply chain planning and operational management by boosting the accuracy in predicting the demand trend for NCD essential medicines.",Not About Sufficiency
"Integrating knowledge on green infrastructure, health and well-being in ageing populations: Principles for research and practice","Ageing and urbanisation pose significant challenges for public health and urban planning. Ageing populations are at particular risk from hazards arising from urbanisation processes, some of which are in turn exacerbated by climate change. One approach for mitigating the negative effects of urbanisation on ageing populations is the leveraging of the beneficial effects of urban green infrastructure as a public health intervention in the planning process. We assessed the potential of available theoretical frameworks to provide the context for such leverage. This involved active engagement with academics and practitioners specialising in ageing, green infrastructure and health and well-being through a knowledge-brokering approach. We concluded that an integrated and comprehensive framework on the socio-cultural-ecological determinants of health is lacking. To address this, we present a set of principles for overcoming challenges to knowledge integration when working at the intersection of green infrastructure, ageing, health and well-being. Our findings-and the co-production process used to generate them-have wider significance for trans-disciplinary research into the benefits of the natural environment to human health and well-being as well as other complex and interconnected topics associated with global grand challenges.",Not About Sufficiency
Electret-based flexible pressure sensor for respiratory diseases auxiliary diagnosis system using machine learning technique,"Respiratory diseases have been increasingly affecting people worldwide, posing a major public health challenge due to rising morbidity and mortality rates. Subtle abnormal respiratory sounds s may present early in the pulmonary or respiratory tract diseases, therefore urging an instant intervention in critical clinical conditions. However, the current monitoring and signal analysis technologies pose significant challenges in achieving real-time, convenient, and accurate respiratory disease monitoring. Here, we propose a novel automatic auxiliary diagnosis system that utilizes a flexible electret-based self-powered sensor (FESS), signal processing technology, and machine learning algorithms. The FESS is based on a strain-enhanced laminated electret with an edge-to-edge hollow hemisphere array structure, which enables the system to detect and prevent various respiratory diseases with high accuracy rates of 99.43%, sensitivity of 98.30%, and specificity of 99.02%. Our system holds immense potential in reducing medical burden and improving the overall health of individuals.",Not About Sufficiency
Integrated surface-enhanced Raman spectroscopy and convolutional neural network for quantitative and qualitative analysis of pesticide residues on pericarp,"Pesticide residue poses a significant global public health concern, necessitating improved detection methods. Here, a novel platform was introduced based on surface-enhanced Raman spectroscopy (SERS) to detect ten distinct types of pesticides. Notably, the sensitivity of this approach is exemplified by detecting trace amounts of 50 pM (10 ppt) thiabendazole. The correlation between the characteristic peak intensity of coexisting pesticides and their concentrations displays an exceptional linear relationship (R2 = 0.9999), underscoring its utility for quantitative mixed pesticide detection. Additionally, qualitative analysis of five mixed pesticides was conducted leveraging distinctive peak labeling. Harnessing machine learning techniques, a model for classifying and pre-dicting pesticides on pericarps was developed. Remarkably, the convolutional neural network achieved classi-fication accuracy of 100 % and prediction accuracy of 99.62 %. This innovative approach accurately identifies and quantifies diverse pesticides, thus offering a feasible scheme for in-situ detection of pesticide residues. Ul-timately, this strategy contributes to ensuring food safety and public health.",Not About Sufficiency
Mapping urban green equity and analysing its impacted mechanisms: A novel approach,"Current assessment methods of Urban green equity (UGE) ignore the social differentiation of urban space, which may lead to a misallocation of green space resources, exacerbating differentiation, and potentially giving rise to green gentrification (GG). This study employs a novel evaluation framework and approach for UGE, considering the impact of landscape and greening on Spatial Differentiation of Residential Areas (SDORA), to evaluate the UGE of the west bank core area in Changsha city, Hunan Province, China, and explores its influencing mechanisms. The results show that (1) The study area exhibits significant greening differentiation, and residential areas with high property prices have higher quality landscapes and greening than those with low property prices. (2) The region displays significant green equity disparity (GEI = 0.497) and GG. (3) The cumulative contribution of landscape and greening to SDORA reaches 39.04 %, with the features of water body and mountain proximity as well as accessibility having the most significant impact on urban green space differentiation. This study enhances the understanding of UGE assessment, emphasizing the importance of considering spatial differentiation during UGE evaluation, and provides technical support and scientific guidance for the planning and construction of urban green space systems.",Not About Sufficiency
"GEOLOGICAL HAZARDS, VULNERABILITY, AND RISK ASSESSMENT USING GIS - MODEL FOR GLENWOOD-SPRINGS, COLORADO","Glenwood Springs, Colorado, lies at the junction of the Roaring Fork and Colorado Rivers, surrounded by the steep peaks of the Colorado Rocky Mountains.  Large parts of the region have had intensive sheet erosion, debris flows, and hyperconcentrated floods triggered by landslides and slumps.  The latter come from unstable slopes in the many tributary channels on the mountainsides, causing concentration of debris in channels and a large accumulation of sediment in colluvial wedges and debris fans that line the river valleys.  Many of the landslide and debris-flow deposits exist in a state resembling suspended animation, ready to be destabilized by intense precipitation and/or seismic activity. During this century urban development in the Roaring Fork River valley has increased rapidly.  The city of Glenwood Springs continues to expand over unstable debris fans without any construction of hazard mitigation structures.  Since 1900, Glenwood Springs has had at least 21 damaging debris flows and floods; on July 24, 1977 a heavy thunderstorm spread a debris flow over more than 80 ha of the city. This paper presents a method that uses Geographic Information Systems (GIS) to assess geological hazards, vulnerability, and risk in the Glenwood Springs area.  The hazards evaluated include subsidence, rockfall, debris flows, and floods, and in this paper we focus on debris flows and subsidence.  Information on topography, hydrology, precipitation, geomorphic processes, bedrock and surficial geology, structural geology, soils, vegetation, and land use, was processed for hazard assessment using a series of algorithms.  ARC/INFO and GRASS GIS softwares were used to produce maps and tables in a format accessible to urban planners. After geological hazards were defined for the study area, we estimated the vulnerability (V(e)) of various elements for an event of intensity i. Risk is assessed as a function of hazard and vulnerability.  We categorized the study area in 14 classes for planning procedures; 7 classes defined as areas suitable for human settlement, and 7 classes defined as unsuitable for building, and most effectively reserved for parks and forests.",Not About Sufficiency
Machine Learning Fusion Model Approach for the Real-Time Detection of Head Gestures using IMUs,"Modern sensor technology has contributed to the study of human behaviors during social interactions. Inertial movement units (IMUs) have shown great promise in the recognition of communication cues displayed by head gestures, which are important for healthy interactions. However, no gold standard exists to automatically detect head actions from IMUs. This paper presents the design of a real-time head-action detection (HAD) unit based on a new real-time fusion model architecture approach. An analysis of buffer sizes and feature contribution using a decision tree (DT) classifier and a predictor importance fusion is presented. The fusion model is composed of two classification stages wherein the first stage focus on recognizing head position and the second on recognizing head motion. The designed HAD unit uses a data buffer size of 3s, 7 features in total, and a DT classifier. Results show a testing accuracy of 97.91% and an Fl-score of 98.5% The use of the designed HAD unit and its architecture could allow for easy retraining to add recognition of additional head actions by having specialized head action classification models.",Not About Sufficiency
A Methodology for Flattening the Command and Control Problem Space,"Modeling Command and Control (C2) problems is notoriously complex across operating environments, systems, and missions. We seek to systematically decompose the complexities of C2, and present the foundational problems in a form that is accessible to the larger research community. Our first objective is to ""flatten"" core C2 problems by formulating and developing a surrogate environment that can foster new insights and evaluation. Using this framework, the second objective is to host a public challenge to acquire a diversity of solutions from outside the DoD community.",Not About Sufficiency
The Medical Profession in the Regulation of the Therapeutic Market in Colombia (1895-1948),"this article shows the process of regulating the pharmaceutical market in Colombia in the first half of the 20th century. This process took place in a context of tensions between apothecary and university medicine. The field of medicine was in the process of professionalization, it sought to subordinate health-related trades, including pharmacies, and defined the rules of the therapeutic market. In the midst of a flourishing market and an incipient pharmaceutical industry, pharmacists were the first to organize as a trade union and to think about the professionalization of their trade. The article follows three analytical axes: the standardization of the pharmacy by university medicine; the regulation of the pharmaceutical market as one of the fronts of public hygiene, and the struggle of certified doctors against medicines of doubtful efficacy. The analysis shows that the regulation of medicines is part of a broader impulse of medicalization of society. Arguments in defense of public health played a key role in the regulation of medicines and in the formation of a monopoly of the medical arts dominated by university medicine and supported by the State.",Not About Sufficiency
"Population growth, accessibility spillovers and persistent borders: Historical growth in West-European municipalities","Lack of cross-border transport supply has repeatedly been blamed for the fact that national borders limit spatial interaction and, consequently, the growth of border regions. This study applies an accessibility approach to investigate for most municipalities in ten countries in mainland West Europe if foreign transport supply is lagging behind, and if population growth in these municipalities has been affected by the limits that national borders have imposed on market access. To do so, data describing historical population changes and road networks between 1961 and 2011 have been used. The results show that in the study area, cross-border transport accessibility was not at a disadvantage in 1961 and has since then grown even more than domestic accessibility. However, municipal population growth has depended almost exclusively on domestic market access. Processes of economic international integration in the study area are found to coincide with the growth of cross-border accessibility, but do not have a clear coincidence with the effects of cross-border accessibility on population growth.",Not About Sufficiency
Epidemiological models for predicting Ross River virus in Australia: A systematic review,"Author summary As the most common human arbovirus infection in Australia, Ross River virus exerts a significant public health and economic burden on the population. Because the virus is transmitted by mosquitoes, incidence is influenced by climate, environment, and socio-economic factors. Using epidemiological models to predict incidence or outbreaks of RRV fully utilises these data to inform decision-making. In this systematic review, we summarised models and their predictive performance, and highlighted significant exposures in order to increase understanding of transmission. Ross River virus (RRV) is the most common and widespread arbovirus in Australia. Epidemiological models of RRV increase understanding of RRV transmission and help provide early warning of outbreaks to reduce incidence. However, RRV predictive models have not been systematically reviewed, analysed, and compared. The hypothesis of this systematic review was that summarising the epidemiological models applied to predict RRV disease and analysing model performance could elucidate drivers of RRV incidence and transmission patterns. We performed a systematic literature search in PubMed, EMBASE, Web of Science, Cochrane Library, and Scopus for studies of RRV using population-based data, incorporating at least one epidemiological model and analysing the association between exposures and RRV disease. Forty-three articles, all of high or medium quality, were included. Twenty-two (51.2%) used generalised linear models and 11 (25.6%) used time-series models. Climate and weather data were used in 27 (62.8%) and mosquito abundance or related data were used in 14 (32.6%) articles as model covariates. A total of 140 models were included across the articles. Rainfall (69 models, 49.3%), temperature (66, 47.1%) and tide height (45, 32.1%) were the three most commonly used exposures. Ten (23.3%) studies published data related to model performance. This review summarises current knowledge of RRV modelling and reveals a research gap in comparing predictive methods. To improve predictive accuracy, new methods for forecasting, such as non-linear mixed models and machine learning approaches, warrant investigation.",Not About Sufficiency
Does exercise participation promote happiness?: Mediations and heterogeneities,"This paper uses a nationally representative and large-scale dataset from China to empirically examine the relationship between exercise participation and happiness. To address the problem of reverse causality between the two factors, the instrumental variable (IV) approach is used to deal with endogeneity to some extent. It is demonstrated that higher frequencies of exercise participation are positively related to happiness. Findings also demonstrate that physical exercise could significantly decrease depressive disorders, improves self-rated health conditions and reduces the frequency of health problems affecting people's work and life. At the same time, all of above health factors significantly influence subjective wellbeing. When these health variables are included in regressions, the correlation between exercise participation and happiness declines. This confirms that physical activity helps to improve happiness by enhancing mental and overall health conditions. In addition, results show that physical activities are more prominently related to happiness for male, older and unmarried individuals and those living in rural areas, lacking social security and with higher levels of depression as well as lower socioeconomic status. Furthermore, a series of robustness checks are carried out and exercise participation's positive role in improving happiness is further confirmed using different happiness measures and instrumental variables, various IV models, as well as penalized machine learning methods and placebo tests. With the increasing emphasis of improving happiness as an important goal in the global public health policy, findings of this paper have important policy implications for enhancing subjective wellbeing.",Not About Sufficiency
Understanding the drivers of smallholder dairy cooperative participation in developing countries: Evidence from rural Zambia,"CONTEXT: Smallholder dairy farmers are among the primary dairy producers in developing countries. In Zambia, they contribute more than 80 % of the country's milk production, which amounts to approximately $80 million annually. Understanding the factors that influence smallholder dairy farmers' decisions to join cooperatives is crucial for enhancing cooperative participation and improving dairy production efficiency in the region. OBJECTIVE: The primary goal of this study is to investigate the determinants of smallholder dairy farmers' decisions to join cooperatives, while also comparing the predictive performance of the random effects logit model and the random forest model in identifying these factors. METHODS: Data were collected from 515 rural smallholder dairy farmers in Zambia. The analysis utilizes a random effects logit model and a random forest model to identify the factors influencing farmers' decisions to join dairy cooperatives. RESULTS AND CONCLUSIONS: Three primary findings were observed. First, the RF model exhibited superior predictive accuracy compared to the random effects logit model, aligning with existing literature on the enhanced predictive capabilities of machine learning techniques. Second, several key factors, including physical proximity to cooperative offices, educational attainment, and dairy farming experience, were identified from the random effects logit model as significantly influencing current farmers' decisions to join dairy cooperatives. Third, the random forest model indicated that demographic and economic characteristics-specifically age of the household head, household size, total cow ownership, dependency ratio, and farming experience-are expected to be the most influential predictors of cooperative membership in future scenarios. SIGNIFICANCE: Findings suggest the need for establishing cooperative offices closer to rural farming commu- nities in developing countries to enhance accessibility and encourage cooperative participation. Policies should focus on improving educational levels and providing accessible knowledge sources through governmental and non-governmental initiatives to foster cooperative membership. Addressing the reluctance of wealthier farmers to join cooperatives requires tailored interventions such as incentives, awareness campaigns, or targeted outreach efforts emphasizing the benefits of cooperative membership across different resource levels.",Not About Sufficiency
Forest Fire Susceptibility and Risk Mapping Using Social/Infrastructural Vulnerability and Environmental Variables,"Forests fires in northern Iran have always been common, but the number of forest fires has been growing over the last decade. It is believed, but not proven, that this growth can be attributed to the increasing temperatures and droughts. In general, the vulnerability to forest fire depends on infrastructural and social factors whereby the latter determine where and to what extent people and their properties are affected. In this paper, a forest fire susceptibility index and a social/infrastructural vulnerability index were developed using a machine learning (ML) method and a geographic information system multi-criteria decision making (GIS-MCDM), respectively. First, a forest fire inventory database was created from an extensive field survey and the moderate resolution imaging spectroradiometer (MODIS) thermal anomalies product for 2012 to 2017. A forest fire susceptibility map was generated using 16 environmental variables and a k-fold cross-validation (CV) approach. The infrastructural vulnerability index was derived with emphasis on different types of construction and land use, such as residential, industrial, and recreation areas. This dataset also incorporated social vulnerability indicators, e.g., population, age, gender, and family information. Then, GIS-MCDM was used to assess risk areas considering the forest fire susceptibility and the social/infrastructural vulnerability maps. As a result, most high fire susceptibility areas exhibit minor social/infrastructural vulnerability. The resulting forest fire risk map reveals that 729.61 ha, which is almost 1.14% of the study areas, is categorized in the high forest fire risk class. The methodology is transferable to other regions by localisation of the input data and the social indicators and contributes to forest fire mitigation and prevention planning.",Not About Sufficiency
Digital Epidemiological Approaches in HIV Research: a Scoping Methodological Review,"Purpose of ReviewThe purpose of this scoping review was to summarize literature regarding the use of user-generated digital data collected for non-epidemiological purposes in human immunodeficiency virus (HIV) research.Recent FindingsThirty-nine papers were included in the final review. Four types of digital data were used: social media data, web search queries, mobile phone data, and data from global positioning system (GPS) devices. With these data, four HIV epidemiological objectives were pursued, including disease surveillance, behavioral surveillance, assessment of public attention to HIV, and characterization of risk contexts. Approximately one-third used machine learning for classification, prediction, or topic modeling. Less than a quarter discussed the ethics of using user-generated data for epidemiological purposes.SummaryUser-generated digital data can be used to monitor, predict, and contextualize HIV risk and can help disrupt trajectories of risk closer to onset. However, more attention needs to be paid to digital ethics and the direction of the field in a post-Application Programming Interface (API) world.",Not About Sufficiency
Minority stress and structural stigma predict well-being in European LGBTQ plus parents,"ObjectiveThis study tested whether exposure to minority stress and structural stigma across multiple levels of the family system were associated with two indicators of well-being (life satisfaction, depressive symptoms) in LGBTQ+ parents across 19 European countries.BackgroundMinority stress (i.e., identity-based stress resulting from systemic oppression) and structural stigma (i.e., hostile legal environments, prejudicial social attitudes) are heterogeneous, yet well-documented risk factors of reduced well-being within LGBTQ+ populations. However, a comprehensive assessment stratifying both concepts across multiple levels of the family system (i.e., the individual, couple, and family level) is lacking for LGBTQ+ parents.MethodUsing data from the EU LGBTI Survey 2019, a sample of 3808 LGBTQ+ parents from 19 European countries was analyzed. Associations between self-reported minority stress indicators, objective structural stigma indicators, sociodemographic predictors, and well-being were tested using non-linear, machine learning-based techniques (gradient boosted decision tree models).ResultsSupporting preregistered hypotheses, exposure to individual-level minority stress and individual- and family-level structural stigma predicted life satisfaction and depressive symptoms. Couple-level minority stress predicted life satisfaction, but not depressive symptoms, and family-level minority stress predicted neither. Trans parents and those facing economic burdens were particularly vulnerable to low well-being.ConclusionsExposure to minority stress and structural stigma, particularly within highly stigmatizing regions, are risk factors for LGBTQ+ parents' well-being. Future research should examine the role of family-level minority stress using validated measures.",Not About Sufficiency
"The European Council, the Council, and the European Green Deal","We assess the response of the European Council and the Council of the European Union (hereafter the Council) to the emergence and development of the European Green Deal (EGD). First, we conduct a literature review of the historical role of the two intergovernmental institutions in EU climate policy development, drawing inspiration from new intergovernmentalism, historical institutionalism, and discursive institutionalism. Next, we provide an overview of the EGD itself and three of its core elements: (1) the ambition to achieve climate neutrality by 2050; (2) its systemic and integrative nature; and (3) the just transition approach. We then present the results of a qualitative content analysis of all Council and European Council Conclusions from 2018 to 2020. Our findings show that the European Council and the Council have declared support for the EGD and its underlying principles. The European Council engaged with all three elements but mentioned the objective of achieving net-zero emissions by 2050 most frequently and with growing intensity over the years studied. The Council similarly discussed the three elements of the EGD and gave increasing focus to the integrated/systemic transition over the course of the years 2018-2020. Our empirical analysis suggests that, on paper, the Council and the European Council may manage to govern through the organisational turbulence of member state divisions on climate governance. Furthermore, environmental turbulence arising from external contexts (e.g., economic and health crises) did not dampen their declared support towards the goals of the EGD.",Not About Sufficiency
Dengue Prediction in Latin America Using Machine Learning and the One Health Perspective: A Literature Review,"Dengue fever is a serious and growing public health problem in Latin America and elsewhere, intensified by climate change and human mobility. This paper reviews the approaches to the epidemiological prediction of dengue fever using the One Health perspective, including an analysis of how Machine Learning techniques have been applied to it and focuses on the risk factors for dengue in Latin America to put the broader environmental considerations into a detailed understanding of the small-scale processes as they affect disease incidence. Determining that many factors can act as predictors for dengue outbreaks, a large-scale comparison of different predictors over larger geographic areas than those currently studied is lacking to determine which predictors are the most effective. In addition, it provides insight into techniques of Machine Learning used for future predictive models, as well as general workflow for Machine Learning projects of dengue fever.",Not About Sufficiency
Monotonic and Non-Monotonic Infections on Networks,"The structure of a network can significantly affect the course of infections on it. For example, a human-to-human contact network affects the epidemiology of infectious diseases, affecting both the rate of new infections and the sizes of outbreaks. Related results are also known for infrastructure systems like communication and power transmission systems that experience cascading breakdowns. Despite this dependence, some characteristics of outbreaks are predictable based only on the infection being transmitted. Here we consider SIR-like infections, and give an elementary proof that for any network, increasing the probability of transmission monotonically increases the mean outbreak size. We also introduce a simple model, termed 2FleeSIR, in which susceptibles protect themselves by avoiding contacts with infectees. The dynamics of 2FleeSIR are fundamentally different from SIR dynamics because 2FleeSIR seems to exhibit no outbreak transition in densely-connected networks. Moreover, 2FleeSIR exhibits non-monotonic phenomena: for some networks, increasing transmissibility actually decreases the final extent. We show that in non-monotonic epidemics. public health officials might be able to intervene in a fundamentally new way to change the network so as to control the effect of unexpectedly-high transmissibility. However, interventions that decrease transmissibility might actually cause more people to become infected.",Not About Sufficiency
Machine learning models for Neisseria gonorrhoeae antimicrobial susceptibility tests,"Neisseria gonorrhoeae is an urgent public health threat due to the emergence of antibiotic resistance. As most isolates in the United States are susceptible to at least one antibiotic, rapid molecular antimicrobial susceptibility tests (ASTs) would offer the opportunity to tailor antibiotic therapy, thereby expanding treatment options. With genome sequence and antibiotic resistance phenotype data for nearly 20,000 clinical N. gonorrhoeae isolates now available, there is an opportunity to use statistical methods to develop sequence-based diagnostics that predict antibiotic susceptibility from genotype. N. gonorrhoeae, therefore, provides a useful example illustrating how to apply machine learning models to aid in the design of sequence-based ASTs. We present an overview of this framework, which begins with establishing the assay technology, the performance criteria, the population in which the diagnostic will be used, and the clinical goals, and extends to the choices that must be made to arrive at a set of features with the desired properties for predicting susceptibility phenotype from genotype. While we focus on the example of N. gonorrhoeae, the framework generalizes to other organisms for which large-scale genotype and antibiotic resistance data can be combined to aid in diagnostics development.",Not About Sufficiency
"Traces of the active Capitignano and San Giovanni faults (Abruzzi Apennines, Italy)","We present a 1: 20,000 scale map of the traces of the active Capitignano and San Giovanni faults in the area of the Montereale basin (central Apennines, Italy) covering an area of about 80 km(2). Detailed fault mapping is based on high-resolution topography from airborne LiDAR imagery validated by extensive ground truthing and geophysical prospecting. Our analysis allowed the recognition of several features related to fault activity, even in scarcely accessible areas characterized by dense vegetation cover and rugged terrain. The identified fault traces run at the base of the NW-SE striking Montereale basin-bounding mountain front and along the base of the southwestern slope of the Monte Mozzano ridge, and have a length of about 12 and 8 km, respectively. Improving the knowledge of fault geometry is a critical issue not only for the recognition of seismogenic sources but also for surface fault hazard assessment and for local urban planning. The knowledge of the exact location of the fault traces is also crucial for the seismogenic characterization of the active faults by means of paleoseismological trenching.",Not About Sufficiency
The City as a Power Hub for Boosting Renewable Energy Communities: A Case Study in Naples,"This study introduces an innovative methodology for designing sustainable urban energy districts using Geographic Information Systems (GIS). The scope is to identify specific parts of the urban fabric, suitable for becoming energy districts that can meet the energy needs of dwellings and activities and produce an energy surplus for the city. The method uses building archetypes to characterize the districts and perform simulations through an algorithm based on correction coefficients considering variables such as total building height, exposure, year of construction, and building typology. By leveraging GIS, this approach supports the creation of urban energy maps, which help identify and address potential energy-related issues in various urban contexts. Additionally, the research explores different scenarios for developing energy communities within the district, aiming to optimize energy use and distribution. A case study in Naples, Southern Italy, demonstrates that installing photovoltaic panels on the roofs of buildings can allow a complete electrical supply to the building stock. The final goal is to provide a robust tool that enhances confidence in urban energy planning decisions, contributing to more sustainable and efficient energy management at the district level. This approach may support the urban and territorial governance towards sustainable solutions by developing strategies for the creation of energy communities and optimizing the potential of specific sites.",Not About Sufficiency
Correlation between Neutrophil-to-Lymphocyte Ratio and Diabetic Neuropathy in Chinese Adults with Type 2 Diabetes Mellitus Using Machine Learning Methods,"Objective. One of the most frequent consequences of diabetes mellitus has been identified as diabetic peripheral neuropathy (DPN), and numerous inflammatory disorders, including diabetes, have been documented to be reflected by the neutrophil-to-lymphocyte ratio (NLR). This study aimed to explore the correlation between peripheral blood NLR and DPN, and to evaluate whether NLR could be utilized as a novel marker for early diagnosis of DPN among those with type 2 Diabetes Mellitus (T2DM). Methods. We reviewed the medical records of 1154 diabetic patients treated at Tongji Hospital Affiliated to Tongji University from January 2022 to March 2023. These patients did not have evidence of acute infections, chronic inflammatory status within the past three months. The information included the clinical, laboratory, and demographic characteristics of the patient. Finally, a total of 442 T2DM individuals with reliable, complete, and accessible medical records were recruited, including 216 T2DM patients without complications (DM group) and 226 T2DM patients with complications of DPN (DPN group). One-way ANOVA and multivariate logistic regression were applied to analyze data from the two groups, including peripheral blood NLR values and other biomedical indices. The cohort was divided in a 7 : 3 ratio into training and internal validation datasets following feature selection and data balancing. Based on machine learning, training was conducted using extreme gradient boosting (XGBoost) and support vector machine (SVM) methods. K-fold cross-validation was applied for model assessment, and accuracy, precision, recall, F1-score, and the area under the receiver operating characteristic curve (AUC) were used to validate the models' discrimination and clinical applicability. Using Shapley Additive Explanations (SHAP), the top-performing model was interpreted. Results. The values of 24-hour urine volume (24H UV), lower limb arterial plaque thickness (LLAB thickness), carotid plaque thickness (CP thickness), D-dimer and onset time were significantly higher in the DPN group compared to the DM group, whereas the values of urine creatinine (UCr), total cholesterol (TC), low-density lipoprotein (LDL), alpha-fetoprotein (AFP), fasting c-peptide (FCP), and nerve conduction velocity and wave magnitude of motor and sensory nerve shown in electromyogram (EMG) were considerably lower than those in the DM group (P < 0.05, respectively). NLR values were significantly higher in the DPN group compared to the DM group (2.60 +/- 4.82 versus 1.85 +/- 0.98, P < 0.05). Multivariate logistic regression analysis revealed that NLR (P = 0.008, C = 0.003) was a risk factor for DPN. The multivariate logistic regression model scores were 0.6241 for accuracy, 0.6111 for precision, 0.6667 for recall, 0.6377 for F1, and 0.6379 for AUC. Machine learning methods, XGBoost and SVM, built prediction models, showing that NLR can predict the onset of DPN. XGBoost achieved an accuracy of 0.6541, a precision of 0.6316, a recall of 0.7273, a F1 value of 0.6761, and an AUC value of 0.690. SVM scored an accuracy of 0.5789, a precision of 0.5610, a recall of 0.6970, an F1 value of 0.6216, and an AUC value of 0.6170. Conclusions. Our findings demonstrated that NLR is highly correlated with DPN and is an independent risk factor for DPN. NLR might be a novel indicator for the early diagnosis of DPN. XGBoost and SVM models have great predictive performance and could be reliable tools for the early prediction of DPN in T2DM patients.",Not About Sufficiency
Automatic Classification of Parkinson's Disease Using Best Parameters of Forward and Backward Walking,"This study aims to investigate the discriminative gait features of forward and backward walking to provide a combination of the most relevant parameters. These parameters would potentially help the clinicians to follow quantitative methods in diagnosing Parkinson's disease. In this paper, the statistically significant gait features were narrowed down from 46 to 30, 20, 10, and 5, using the minimal-redundancy-maximal-relevance feature selection method. The selected features were then fed to Random Forest and Support Vector Machine classifiers to evaluate the ability of features in discriminating Parkinson's disease and control groups. According to the results, we selected to use Random Forest classifier in our algorithm. Applying our algorithm on a database comprising 62 Parkinson's disease patients and 11 control participants, we achieved the average accuracy of 93.9 and 88 in 10 iterations of Random Forest and Support Vector Machine, respectively. Using the minimal-redundancy-maximal-relevance feature selection and mean decrease in accuracy and Gini index of the Random Forest classifier, we found the critical role of backward walking parameters such as the average of stance time, step length, and swing time in classification results.",Not About Sufficiency
The Potential Related to Microgeneration of Renewable Energy in Urban Spaces and Its Impact on Urban Planning,"This research aims to explore the potential of renewable energy sources in urban planning, focusing on microgeneration technologies, through a structured literature review. A systematic review was conducted using the PRISMA method, encompassing the identification, selection, eligibility, and analysis of studies related to renewable energy microgeneration in urban environments. The findings emphasize key areas such as policy development, energy security, and future scenario projections, with a particular focus on solar energy generation. The review highlights the importance of robust regulatory frameworks and monitoring systems for effectively managing prosumers and ensuring equitable energy distribution. Key challenges identified include the intermittency of renewable energy sources, regulatory complexities, monitoring systems, prosumer management, energy sizing risks, and the lifecycle of microgeneration technologies. The research accentuates the need for outstanding collaboration between academia, industry, and urban planners to accelerate the adoption and implementation of renewable energy solutions. The main conclusion is that such collaboration is essential for addressing challenges, driving innovation, and contributing to the development of sustainable urban energy systems.",Not About Sufficiency
"Elucidating the prospects of paddy straw as a potential source of nanosilica: A road map towards sustainable agriculture, bioeconomy and entrepreneurship","Paddy straw burning is an entrenched issue in the agricultural system, widely recognized for its adverse environmental and health impacts, particularly in South Asian countries. The revolutionary advancements in nanotechnology have paved the way for innovative solutions by valorizing silica-rich paddy straw (PS) into silica nanoparticles (SiNPs). SiNPs are known for their versatile role in agriculture sectors. Nonetheless, the high fabrication cost of SiNPs has limited its utility. Exploring agro-waste PS as a potential source of SiNPs promises resource efficiency and a sustainable solution towards stubble management. The review sheds light on various modes of synthesis of SiNPs, critically featuring the green synthesis approach using PS. The multifaceted potential of SiNPs in alleviating biotic and abiotic stress and delivering essential components in plants is discussed. Given the challenges of synthesizing well-characterized SiNPs, integrating machine learning in synthesis process and design of slow-release formulations is recommended. The review envisions future research encompassing Sustainable Agriculture Matrix, Sensitivity Analysis, and Life Cycle Assessment to seamlessly integrate SiNPs into agriculture. Under the umbrella of PS management, various laws and policies undertaken by the GOI have been discussed. The valorization of PS is proposed as a lucrative opportunity for entrepreneurship programs by authors. While research on SiNPs in agriculture is advancing, much remains to learn about their mechanisms of action, interactions with plants, soils, toxicity, and potential synergies/trade-offs with other agricultural inputs. Moreover, ensuring affordable access to SiNPs for small-scale farmers and resource-limited regions requires considerable attention. Hence, the review endeavors to capture the attention of stakeholders to embrace a comprehensive approach towards stubble management that will contribute towards SDGs and nurturing a thriving circular bioeconomy.",Not About Sufficiency
Stock Market Trend Prediction Using Recurrent Convolutional Neural Networks,"Short-term prediction of stock market trend has potential application for personal investment without high-frequency-trading infrastructure. Existing studies on stock market trend prediction have introduced machine learning methods with handcrafted features. However, manual labor spent on handcrafting features is expensive. To reduce manual labor, we propose a novel recurrent convolutional neural network for predicting stock market trend. Our network can automatically capture useful information from news on stock market without any handcrafted feature. In our network, we first introduce an entity embedding layer to automatically learn entity embedding using financial news. We then use a convolutional layer to extract key information affecting stock market trend, and use a long short-term memory neural network to learn context-dependent relations in financial news for stock market trend prediction. Experimental results show that our model can achieve significant improvement in terms of both overall prediction and individual stock predictions, compared with the state-of-the-art baseline methods.",Not About Sufficiency
Learning to Navigate The Synthetically Accessible Chemical Space Using Reinforcement Learning,"Over the last decade, there has been significant progress in the field of machine learning for de novo drug design, particularly in generative modeling of novel chemical structures. However, current generative approaches exhibit a significant challenge: they do not ensure that the proposed molecular structures can be feasibly synthesized nor do they provide the synthesis routes of the proposed small molecules, thereby seriously limiting their practical applicability. In this work, we propose a novel reinforcement learning (RL) setup for de novo drug design: Policy Gradient fothree HIV targets. Finally, we describe how the end-to-end training conceptualized in this study represents an important paradigm in radically expanding the synthesizable chemical space and automating the drug discovery process.r Forward Synthesis (PGFS), that addresses this challenge by embedding the concept of synthetic accessibility directly into the de novo drug design system. In this setup, the agent learns to navigate through the immense synthetically accessible chemical space by subjecting initial commercially available molecules to valid chemical reactions at every time step of the iterative virtual synthesis process. The proposed environment for drug discovery provides a highly challenging test-bed for RL algorithms owing to the large state space and high-dimensional continuous action space with hierarchical actions. PGFS achieves state-of-the-art performance in generating structures with high QED and clogP. Moreover, we validate PGFS in an in-silico proof-of-concept associated with three HIV targets. Finally, we describe how the end-to-end training conceptualized in this study represents an important paradigm in radically expanding the synthesizable chemical space and automating the drug discovery process.",Not About Sufficiency
Human adaptability research into the beginning of the third millennium,"Human adaptability, as a field of inquiry within human biology, became defined during the research activities of the International Biological Program (IBP) (1964-1974). During this period, research was focused on ecological, physiological, and genetic studies of human populations within the theoretical frameworks of adaptation and evolution. Other defining characteristics of the IBP human adaptability research were standardization of methods, multidisciplinary projects, international cooperation, and a concern with human health issues. Some observers suggest that this research contributed to the ongoing transformation of physical anthropology and related fields from a largely descriptive to an analytical science. During the 25 years between the end of the IBP and the present, a number of research trends have continued: Several new multidisciplinary projects were initiated and completed; a subfield of demography within human biology has matured; nutrition, infant and child growth, and health studies have proliferated; and molecular generics and DNA analysis have superseded the earlier population genetics. International programs today are geared toward more practical and applied studies with less emphasis on basic science. Continuation of human adapt ability research into the 21st century is likely to make contributions in 3 broad areas: population, environment, and health. Productive research is likely to contribute to these 3 areas in the following categories: reproduction, psychosocial stress, life span approaches to health, effects of losses in biodiversity on health, a human biology of poverty, emerging infectious diseases, epidemiology of modernization, evolutionary medicine, and aging. The success of much of this research in its contribution to knowledge will come from the integrated perspectives of a biobehavioral framework of inquiry.",Not About Sufficiency
Ensemble Siamese Network (ESN) Using ECG Signals for Human Authentication in Smart Healthcare System,"Advancements in digital communications that permit remote patient visits and condition monitoring can be attributed to a revolution in digital healthcare systems. Continuous authentication based on contextual information offers a number of advantages over traditional authentication, including the ability to estimate the likelihood that the users are who they claim to be on an ongoing basis over the course of an entire session, making it a much more effective security measure for proactively regulating authorized access to sensitive data. Current authentication models that rely on machine learning have their shortcomings, such as the difficulty in enrolling new users to the system or model training sensitivity to imbalanced datasets. To address these issues, we propose using ECG signals, which are easily accessible in digital healthcare systems, for authentication through an Ensemble Siamese Network (ESN) that can handle small changes in ECG signals. Adding preprocessing for feature extraction to this model can result in superior results. We trained this model on ECG-ID and PTB benchmark datasets, achieving 93.6% and 96.8% accuracy and 1.76% and 1.69% equal error rates, respectively. The combination of data availability, simplicity, and robustness makes it an ideal choice for smart healthcare and telehealth.",Not About Sufficiency
Channel Modeling of NB-PLC for Smart Grid,"Future electricity power network must be restructured to improve its efficiency in order to provide all the consumers with flexible, cost effective, reliable and accessible power delivery. Recent research focuses on the conversion of typical power grid to Smart Grid (SG) where energy generation is decentralized. In this regard Power Line Communications (PLC) is a vital part of Information and Communication Technologies (ICT) within the SG. This paper presents the analysis and modeling of different channels i.e., Low Voltage (LV), Medium Voltage (MV) and High Voltage (HV) transmission lines for Narrowband Power Line Communications (NB-PLC) using transmission line theory. Most of the literature regarding power line channel modeling deals with the statistical approaches which are relatively inaccurate as they provide a partial analysis of power line channel models which includes solely multipath effects. A comprehensive investigation has been carried out by evaluating and comparing the behaviors of LV, MV and HV channel models. A consequence for the validity of this research is to facilitate the International standardization process of various power line transfer functions especially in Asian countries where a lot more research is needed in the area of power line communications. Moreover characteristics of constant and frequency selective impedances of various channel models have been evaluated.",Not About Sufficiency
Binary classification of users of electronic cigarettes and smokeless tobacco through biomarkers to assess similarity with current and former smokers: machine learning applied to the population assessment of tobacco and health study,"BackgroundExposure to harmful and potentially harmful constituents in cigarette smoke is a risk factor for cardiovascular and respiratory diseases. Tobacco products that could reduce exposure to these constituents have been developed. However, the long-term effects of their use on health remain unclear. The Population Assessment of Tobacco and Health (PATH) study is a population-based study examining the health effects of smoking and cigarette smoking habits in the U.S. population. Participants include users of tobacco products, including electronic cigarettes and smokeless tobacco. In this study, we attempted to evaluate the population-wide effects of these products, using machine learning techniques and data from the PATH study.MethodsBiomarkers of exposure (BoE) and potential harm (BoPH) in cigarette smokers and former smokers in wave 1 of PATH were used to create binary classification machine-learning models that classified participants as either current (BoE: N = 102, BoPH: N = 428) or former smokers (BoE: N = 102, BoPH: N = 428). Data on the BoE and BoPH of users of electronic cigarettes (BoE: N = 210, BoPH: N = 258) and smokeless tobacco (BoE: N = 206, BoPH: N = 242) were input into the models to investigate whether these product users were classified as current or former smokers. The disease status of individuals classified as either current or former smokers was investigated.ResultsThe classification models for BoE and BoPH both had high model accuracy. More than 60% of participants who used either one of electronic cigarettes or smokeless tobacco were classified as former smokers in the classification model for BoE. Fewer than 15% of current smokers and dual users were classified as former smokers. A similar trend was found in the classification model for BoPH. Compared with those classified as former smokers, a higher percentage of those classified as current smokers had cardiovascular disease (9.9-10.9% vs. 6.3-6.4%) and respiratory diseases (19.4-22.2% vs. 14.2-16.7%).ConclusionsUsers of electronic cigarettes or smokeless tobacco are likely to be similar to former smokers in their biomarkers of exposure and potential harm. This suggests that using these products helps to reduce exposure to the harmful constituents of cigarettes, and they are potentially less harmful than conventional cigarettes.",Not About Sufficiency
Recommendations for uniform definitions used in newborn screening for severe combined immunodeficiency,"Background: Public health newborn screening (NBS) programs continuously evolve, taking advantage of international shared learning. NBS for severe combined immunodeficiency (SCID) has recently been introduced in many countries. However, comparison of screening outcomes has been hampered by use of disparate terminology and imprecise or variable case definitions for non-SCID conditions with T-cell lymphopenia. Objectives: This study sought to determine whether standardized screening terminology could overcome a Babylonian confusion and whether improved case definitions would promote international exchange of knowledge. Methods: A systematic literature review highlighted the diverse terminology in SCID NBS programs internationally. While, as expected, individual screening strategies and tests were tailored to each program, we found uniform terminology to be lacking in definitions of disease targets, sensitivity, and specificity required for comparisons across programs. Results: The study's recommendations reflect current evidence from literature and existing guidelines coupled with opinion of experts in public health screening and immunology. Terminologies were aligned. The distinction between actionable and nonactionable T-cell lymphopenia among non-SCID cases was clarified, the former being infants with T-cell lymphopenia who could benefit from interventions such as protection from infections, antibiotic prophylaxis, and live-attenuated vaccine avoidance. Conclusions: By bringing together the previously unconnected public health screening community and clinical immunology community, these SCID NBS deliberations bridged the gaps in language and perspective between these disciplines. This study proposes that international specialists in each disorder for which NBS is performed join forces to hone their definitions and recommend uniform registration of outcomes of NBS. Standardization of terminology will promote international exchange of knowledge and optimize each phase of NBS and follow-up care, advancing health outcomes for children worldwide.",Not About Sufficiency
Urban Rhapsody: Large-scale exploration of urban soundscapes,"Noise is one of the primary quality-of-life issues in urban environments. In addition to annoyance, noise negatively impacts public health and educational performance. While low-cost sensors can be deployed to monitor ambient noise levels at high temporal resolutions, the amount of data they produce and the complexity of these data pose significant analytical challenges. One way to address these challenges is through machine listening techniques, which are used to extract features in attempts to classify the source of noise and understand temporal patterns of a city's noise situation. However, the overwhelming number of noise sources in the urban environment and the scarcity of labeled data makes it nearly impossible to create classification models with large enough vocabularies that capture the true dynamism of urban soundscapes. In this paper, we first identify a set of requirements in the yet unexplored domain of urban soundscape exploration. To satisfy the requirements and tackle the identified challenges, we propose Urban Rhapsody, a framework that combines state-of-the-art audio representation, machine learning and visual analytics to allow users to interactively create classification models, understand noise patterns of a city, and quickly retrieve and label audio excerpts in order to create a large high-precision annotated database of urban sound recordings. We demonstrate the tool's utility through case studies performed by domain experts using data generated over the five-year deployment of a one-of-a-kind sensor network in New York City.",Not About Sufficiency
REVERSE STANDARDIZATION FROM PUBLIC E-HEALTH SERVICE,"Standardization activities exist for a range of e-health services concerning personal and public health, and many standard results are available. Yet these standards sometimes cover the same use area and it is difficult to select appropriate ones. This paper discusses e-health standardization activities and an e-health ecosystem targeting public health in anticipation of its continuous evolution. We introduce a portable health clinic with body area network (BAN-PHC) technologies providing affordable healthcare and telemedicine as a candidate service for health screenings that can be useful for emerging nations, which collectively have a massive population of around 5.8 billion people. The effectiveness of such health checks is evaluated through actual mass examinations in Bangladesh, and key features to accelerating standards deployment are raised. This success leads to adoption of the standards in emerging nations and can be reversely deployed in developed nations. Machine-to-machine (M2M) technologies are also important for providing scalable solutions and accelerating global integration of back-end systems. We propose integration of two key enablers - BAN-PHC and M2M technologies - to provide evolved services for quick and broad standard acceptance from emerging nations to developed nations.",Not About Sufficiency
Application of the Delphi method to the development of common data elements for social drivers of health: A systematic scoping review,"Collaborative data science requires standardized, harmonized, interoperable, and ethically sourced data. Developing an agreed-upon set of elements requires capturing different perspectives on the importance and feasibility of the data elements through a consensus development approach. This study reports on the systematic scoping review of literature that examined the inclusion of diverse stakeholder groups and sources of social drivers of health variables in consensus-based common data element (CDE) sets. This systematic scoping review included sources from PubMed, Embase, CINAHL, WoS MEDLINE, and PsycINFO databases. Extracted data included the stakeholder groups engaged in the Delphi process, sources of CDE sets, and inclusion of social drivers data across 11 individual and 6 social domains. Of the 384 studies matching the search string, 22 were included in the final review. All studies involved experts with healthcare expertise directly relevant to the developed CDE set, and only six (27%) studies engaged health consumers. Literature reviews and expert input were the most frequent sources of CDE sets. Seven studies (32%) did not report the inclusion of any demographic variables in the CDE sets, and each demographic SDoH domain was included in at least one study with age and sex assigned at birth included in all studies, and social driver domains included only in four studies (18%). The Delphi technique engages diverse expert groups around the development of SDoH data elements. Future studies can benefit by involving health consumers as experts. Having diverse expert stakeholders is an effective strategy to establish common data elements for social drivers of health, which is crucial for driving positive changes in patient outcomes. Collecting and capturing social factors that affect individuals' health is imperative. Social drivers of health data allow researchers to understand health disparities to make healthcare available, accessible, and affordable. However, collecting common health data elements has challenged researchers due to limited resources to facilitate change. Incorporating various stakeholders, such as individuals and patient advocacy groups, can effectively contribute to the research process as community advisors. This article reviews the studies that used the Delphi method and brings together experts to agree on guidelines for collecting common data elements. The article's findings reveal that experts are healthcare professionals and researchers, leaving out the crucial input from patients and caregivers. This article emphasized that developing a standard set of data elements can improve the standardization of social drivers of health. Common data elements provide the opportunity to improve patients' and social circumstances and their efforts toward health outcomes. Graphical Abstract",Not About Sufficiency
Waste frying oil derived carbon nano-onions as a cost-effective cathode material for high-voltage zinc-ion hybrid supercapacitors,"Energy crisis and environmental pollution are two major worldwide issues that can be solved by developing sustainable, commercial, and long-durability energy storage systems from the reutilization of waste material. Waste frying oil, a known waste, is used as a carbon source for the synthesis of carbon nano-onions (CNOs) using economically viable and environmentally friendly wick pyrolysis techniques. The as-synthesized CNOs were used as a cost-effective cathode in zinc-ions hybrid supercapacitors (Zn-HSCs). CNOs with a fractal-like structure in an interconnected network and more accessible active available surface area provide great advantage as a cathode to fabricate Zn-HSCs. The CNOs-based active cathode material in aqueous Zn-HSCs presents a high specific capacitance of 342 F g-1 at a current density of 0.5 A g-1 under a high working potential window (0-1.9 V). Zn-HSCs achieve a maximum 8.2 kW kg- 1 of power density and 164.33 W h kg- 1 of energy density. More significantly, excellent long-term durability with 83 % of capacity retention and 100 % retention of coulombic efficiency was also achieved after 10,000 cycles at a high 10 A g-1 of current density. The overall work represents a way of utilizing waste frying oil as a cost-effective and efficient cathode material for aqueous Zn-HSCs, which could aid-in the commercialization of Zn-HSCs.",Not About Sufficiency
Development of a Risk Model to Predict Social Determinants of Health Needs Using Electronic Health Record and Social Vulnerability Index Data With Machine Learning,,Not About Sufficiency
"The Impact of Increasing Disease Prevalence, False Omissions, and Diagnostic Uncertainty on Coronavirus Disease 2019 (COVID-19) Test Performance","Context.-Coronavirus disease 2019 (COVID-19) test performance depends on predictive values in settings of increasing disease prevalence. Geospatially distributed diagnostics with minimal uncertainty facilitate efficient point-of-need strategies. Objectives.-To use original mathematics to interpret COVID-19 test metrics; assess US Food and Drug Administration Emergency Use Authorizations and Health Canada targets; compare predictive values for multiplex, antigen, polymerase chain reaction kit, point-of-care antibody, and home tests; enhance test performance; and improve decision-making. Design.-PubMed/newsprint-generated articles documenting prevalence. Mathematica and open access software helped perform recursive calculations, graph multivariate relationships, and visualize performance by comparing predictive value geometric mean-squared patterns. Results.-Tiered sensitivity/specificity comprised: T1, 90%, 95%; T2, 95%, 97.5%; and T3, 100%, >= 99%. Tier 1 false negatives exceeded true negatives at >90.5% prevalence; false positives exceeded true positives at <5.3% prevalence. High-sensitivity/specificity tests reduced false negatives and false positives, yielding superior predictive values. Recursive testing improved predictive values. Visual logistics facilitated test comparisons. Antigen test quality fell off as prevalence increased. Multiplex severe acute respiratory syndrome (SARS)-CoV-2)*influenza A/B*respiratory syncytial virus testing performed reasonably well compared with tier 3. Tier 3 performance with a tier 2 confidence band lower limit will generate excellent performance and reliability. Conclusions.-The overriding principle is to select the best combined performance and reliability pattern for the prevalence bracket. Some public health professionals recommend repetitive testing to compensate for low sensitivity. More logically, improved COVID-19 assays with less uncertainty conserve resources. Multiplex differentiation of COVID-19 from influenza A/B-respiratory syncytial virus represents an effective strategy if seasonal flu surges next year.",Not About Sufficiency
ProkBERT family: genomic language models for microbiome applications,"BackgroundIn the evolving landscape of microbiology and microbiome analysis, the integration of machine learning is crucial for understanding complex microbial interactions, and predicting and recognizing novel functionalities within extensive datasets. However, the effectiveness of these methods in microbiology faces challenges due to the complex and heterogeneous nature of microbial data, further complicated by low signal-to-noise ratios, context-dependency, and a significant shortage of appropriately labeled datasets. This study introduces the ProkBERT model family, a collection of large language models, designed for genomic tasks. It provides a generalizable sequence representation for nucleotide sequences, learned from unlabeled genome data. This approach helps overcome the above-mentioned limitations in the field, thereby improving our understanding of microbial ecosystems and their impact on health and disease.MethodsProkBERT models are based on transfer learning and self-supervised methodologies, enabling them to use the abundant yet complex microbial data effectively. The introduction of the novel Local Context-Aware (LCA) tokenization technique marks a significant advancement, allowing ProkBERT to overcome the contextual limitations of traditional transformer models. This methodology not only retains rich local context but also demonstrates remarkable adaptability across various bioinformatics tasks.ResultsIn practical applications such as promoter prediction and phage identification, the ProkBERT models show superior performance. For promoter prediction tasks, the top-performing model achieved a Matthews Correlation Coefficient (MCC) of 0.74 for E. coli and 0.62 in mixed-species contexts. In phage identification, ProkBERT models consistently outperformed established tools like VirSorter2 and DeepVirFinder, achieving an MCC of 0.85. These results underscore the models' exceptional accuracy and generalizability in both supervised and unsupervised tasks.ConclusionsThe ProkBERT model family is a compact yet powerful tool in the field of microbiology and bioinformatics. Its capacity for rapid, accurate analyses and its adaptability across a spectrum of tasks marks a significant advancement in machine learning applications in microbiology. The models are available on GitHub (https://github.com/nbrg-ppcu/prokbert) and HuggingFace (https://huggingface.co/nerualbioinfo) providing an accessible tool for the community.",Not About Sufficiency
Factorization Machines Leveraging Lightweight Linked Open Data-Enabled Features for Top-N Recommendations,"With the popularity of Linked Open Data (LOD) and the associated rise in freely accessible knowledge that can be accessed via LOD, exploiting LOD for recommender systems has been widely studied based on various approaches such as graph-based or using different machine learning models with LOD-enabled features. Many of the previous approaches require construction of an additional graph to run graph-based algorithms or to extract path-based features by combining user-item interactions (e.g., likes, dislikes) and background knowledge from LOD. In this paper, we investigate Factorization Machines (FMs) based on particularly lightweight LOD-enabled features which can be directly obtained via a public SPARQL Endpoint without any additional effort to construct a graph. Firstly, we aim to study whether using FM with these lightweight LOD-enabled features can provide competitive performance compared to a learning-to-rank approach leveraging LOD as well as other well-established approaches such as kNN-item and BPRMF. Secondly, we are interested in finding out to what extent each set of LOD-enabled features contributes to the recommendation performance. Experimental evaluation on a standard dataset shows that our proposed approach using FM with lightweight LOD-enabled features provides the best performance compared to other approaches in terms of five evaluation metrics. In addition, the study of the recommendation performance based on different sets of LOD-enabled features indicate that property-object lists and PageRank scores of items are useful for improving the performance, and can provide the best performance through using them together for FM. We observe that subject-property lists of items does not contribute to the recommendation performance but rather decreases the performance.",Not About Sufficiency
An Ensemble Learning Approach for Addressing the Class Imbalance Problem in Twitter Spam Detection,"Being an important source for real-time information dissemination in recent years, Twitter is inevitably a prime target of spammers. It has been showed that the damage caused by Twitter spam can reach far beyond the social media platform itself. To mitigate the threat, a lot of recent studies use machine learning techniques to classify Twitter spam and report very satisfactory results. However, most of the studies overlook a fundamental issue that is widely seen in real-world Twitter data, i.e., the class imbalance problem. In this paper, we show that the unequal distribution between spam and non-spam classes in the data has a great impact on spam detection rate. To address the problem, we propose an ensemble learning approach, which involves three steps. In the first step, we adjust the class distribution in the imbalanced data set using various strategies, including random oversampling, random undersampling and fuzzy-based oversampling. In the next step, a classification model is built upon each of the redistributed data sets. In the final step, a majority voting scheme is introduced to combine all the classification models. Experimental results obtained using real-world Twitter data indicate that the proposed approach can significantly improve the spam detection rate in data sets with imbalanced class distribution.",Not About Sufficiency
Property Rights and Social Justice,,Not About Sufficiency
A Combined Convolutional Neural Network for Urban Land-Use Classification with GIS Data,"The classification of urban land-use information has become the underlying database for a variety of applications including urban planning and administration. The lack of datasets and changeable semantics of land-use make deep learning methods suffer from low precision, which prevent improvements in the effectiveness of using AI methods for applications. In this paper, we first used GIS data to produce a well-tagged and high-resolution urban land-use image dataset. Then, we proposed a combined convolutional neural network named DUA-Net for complex and diverse urban land-use classification. The DUA-Net combined U-Net and Densely connected Atrous Spatial Pyramid Pooling (DenseASPP) to extract Remote Sensing Imagers (RSIs) features in parallel. Then, channel attention was used to efficiently fuse the multi-source semantic information from the output of the double-layer network to learn the association between different land-use types. Finally, land-use classification of high-resolution urban RSIs was achieved. Experiments were performed on the dataset of this paper, the publicly available Vaihingen dataset and Potsdam dataset with overall accuracy levels reaching 75.90%, 89.71% and 89.91%, respectively. The results indicated that the complex land-use types with heterogeneous features were more difficult to extract than the single-feature land-cover types. The proposed DUA-Net method proved suitable for high-precision urban land-use classification, which will be of great value for urban planning and national land resource surveying.",Not About Sufficiency
Field-Deployable Computer Vision Wood Identification of Peruvian Timbers,"Illegal logging is a major threat to forests in Peru, in the Amazon more broadly, and in the tropics globally. In Peru alone, more than two thirds of logging concessions showed unauthorized tree harvesting in natural protected areas and indigenous territories, and in 2016 more than half of exported lumber was of illegal origin. To help combat illegal logging and support legal timber trade in Peru we trained a convolutional neural network using transfer learning on images obtained from specimens in six xylaria using the open source, field-deployable XyloTron platform, for the classification of 228 Peruvian species into 24 anatomically informed and contextually relevant classes. The trained models achieved accuracies of 97% for five-fold cross validation, and 86.5 and 92.4% for top-1 and top-2 classification, respectively, on unique independent specimens from a xylarium that did not contribute training data. These results are the first multi-site, multi-user, multi-system-instantiation study for a national scale, computer vision wood identification system evaluated on independent scientific wood specimens. We demonstrate system readiness for evaluation in real-world field screening scenarios using this accurate, affordable, and scalable technology for monitoring, incentivizing, and monetizing legal and sustainable wood value chains.",Not About Sufficiency
Intelligent Textiles for Physical Human-Environment Interactions,"Physical human-environment interaction is a fundamental aspect of our daily lives, involving the constant use of our sensory and motor systems to extract, process, and communicate information. Capturing, modeling, and augmenting these physical interactions are crucial for enhancing human well-being and promoting intelligent system designs. However, the pervasive and diverse nature of these interactions poses challenges that require scalable and adaptable systems. To address these challenges, I adopt an integrated approach that combines digital fabrication and machine learning techniques. The approach involves developing a digital design and fabrication pipeline to integrate sensing and actuation capabilities into textile-based platforms, and capturing diverse datasets on human-environment interactions to enable intelligent and adaptive applications. The dissertation showcases past and ongoing works on intelligent textile-based sensing and actuating platforms that embody this approach, including tactile sensing garments, an intelligent carpet for human pose estimation, programmable textile-based actuators for assistive wearables, and smart gloves for adaptive tactile interaction transfer. Moving forward, I aim to explore applications of the developed systems in healthcare, robotics, and human behaviors intervention, and expand to diverse sensing and actuation modalities.",Not About Sufficiency
UEMPLOY: ONLINE CONSULTANCY FOR EMPLOYMENT INCLUSION,"The research analyses innovative online education methods to address to specific target groups: a) training business consultants to advise companies how to deal professionally with recruiting or retaining disabled employees, within a framework of equal opportunities best practice; b) training company human resource management (HRM) departments in effective employment practices for disabled workers and in evaluating the change management processes needed for success in this process; c) demonstrating to company management how proactive equal opportunities practices in personnel selection can enhance quality of services and products offered, increase profitability and reduce risk of complaints and discrimination. The innovative training modules address strategic planning, investment in personnel and a modern vision of enterprise development that reflects advanced levels of corporate social responsibility. The project supporting implementation of this research and training is a two-year Leonardo Da Vinci project, UEmploy (510784-LLP-1-2010-1-RO-LEONARDO-LMP) with 7 partners from BG, HU, FI, IE and RO. This project underlines that while European employment inclusion is an important objective, there is no common standardized scheme. The online component of the training contributes to standardization of approaches and contents. The focus of the project is modern rehabilitation, a range of services and processes designed to enable all disabled people to live their lives as fully as possible. For organizations, SME's and professionals the project provides an opportunity to introduce/train innovative employment inclusion and rehabilitation approaches in dealing with people with disabilities throughout all work processes. Methods' and professional support (in English and partner languages) facilitates SMEs to employ people with disabilities. Handbooks and materials for company management and HR departments have already been developed and tested in each partner country. Social partners have validated materials and a blended learning system is in place for all the categories of beneficiaries. The project addresses employer representatives: organizations, SMEs and professionals. UEmploy provides training opportunities for innovative employment inclusion and rehabilitation supports. UEmploy addresses employment inclusion from the perspectives of both employees and employers, facilitating access and cooperation. Many European countries still have very low employment rates for disabled citizens, despite some EU policy success over the past decade. People with disabilities are still significantly excluded from work activities, which could provide financial support, employment integration and continuing professional development. However, companies increasingly realize that inclusive employment produces significant added value. Added skills and competence is linked to inclusion strategies. Employers benefit from enhanced design, creative access and real added value in developing proactive learning strategies in access and services.",Not About Sufficiency
Automatic and objective gradation of 114 183 terrorist attacks using a machine learning approach,"Catastrophic events cause casualties, damage property, and lead to huge social impacts. To build common standards and facilitate international communications regarding disasters, the relevant authorities in social management rank them in subjectively imposed terms such as direct economic losses and loss of life. Terrorist attacks involving uncertain human factors, which are roughly graded based on the rule of property damage, are even more difficult to interpret and assess. In this paper, we collected 114 183 open-source records of terrorist attacks and used a machine learning method to grade them synthetically in an automatic and objective way. No subjective claims or personal preferences were involved in the grading, and each derived common factor contains the comprehensive and rich information of many variables. Our work presents a new automatic ranking approach and is suitable for a broad range of gradation problems. Furthermore, we can use this model to grade all such attacks globally and visualize them to provide new insights.",Not About Sufficiency
Expert insights on digital contact tracing: interviews with contact tracing policy professionals in New Zealand,"Digital contact tracing (DCT) is the application of digital tools to assist with identifying and informing close contacts of a COVID-19 case. DCT is a potential solution to capacity constraints of current manual contact tracing processes. Expert opinion from contact tracing professionals rarely informs public discourse on the benefits and limitations of DCT solutions. Three focus groups were undertaken in New Zealand to understand benefits and limitations of DCT solutions from contact tracing professionals. One was with the National Investigation and Tracing Centre (NITC) and two were with Public Health Units (PHUs). Participants highlighted four key themes including: (i) equity, (ii) privacy, (iii) communication and public perception and (iv) the operational model. Participants were concerned DCT solutions could exacerbate existing health inequities due to lack of access to, or familiarity with, technology. Poor communication and public understanding of DCT were seen as a major threat to both the efficacy of DCT solutions and the wider COVID-19 response. Most importantly, end-users were cautious of the operational model for DCT data that might: (i) attempt to replace manual processes that cannot or should not be automated by technology (case investigations, follow-ups); (ii) place undue burden on citizens and (iii) increase the workload for the current system beyond its capacity, for unproven or limited benefit. To be effective, contact tracing professionals believed DCT technologies must have strong privacy safeguards, a clear and simple communication strategy, interoperability with the existing contact tracing system and a foundation of health equity.",Not About Sufficiency
Fintech Agents: Technologies and Theories,"Many financial technology (fintech) applications have incorporated interactive computer agents to act as mediators between the user and the fintech system. This paper provides a comprehensive review of interactive fintech agents from technological and social science perspectives. First, we explain the general fintech landscape and define interactive fintech agents. Next, we review the major technologies involved in creating fintech: (1) artificial intelligence and machine learning, (2) big data, (3) cloud computing, and (4) blockchain; as well as the specific key technologies enabling the following aspects of interactive fintech agents: (1) intelligence, (2) understanding of users, and (3) manifestation as social actors. Following the technology review, we examine issues and theories related to human-fintech agent interaction in the following areas: (1) agents' understanding of users, (2) agents' manifestation as social actors (via embodiment, emotion, and personality), and (3) users' social interaction with agents. Finally, we suggest directions for future research on fintech agents.",Not About Sufficiency
Retinal electroretinogram features can detect depression state and treatment response in adults: A machine learning approach,"Background: Major depressive disorder (MDD) is a major public health problem. The retina is a relevant site to indirectly study brain functioning. Alterations in retinal processing were demonstrated in MDD with the pattern electroretinogram (PERG). Here, the relevance of signal processing and machine learning tools applied on PERG was studied. Methods: PERG - whose stimulation is reversible checkerboards - was performed according to the International Society for Clinical Electrophysiology of Vision (ISCEV) standards in 24 MDD patients and 29 controls at the inclusion. PERG was recorded every 4 weeks for 3 months in patients. Amplitude and implicit time of P50 and N95 were evaluated. Then, time/frequency features were extracted from the PERG time series based on wavelet analysis. A statistical model has been learned in this feature space and a metric aiming at quantifying the state of the MDD patient has been derived, based on minimum covariance determinant (MCD) mahalanobis distance. Results: MDD patients showed significant increase in P50 and N95 implicit time (p = 0,006 and p = 0,0004, respectively, Mann-Whitney U test) at the inclusion. The proposed metric extracted from the raw PERG provided discrimination between patients and controls at the inclusion (p = 0,0001). At the end of the follow-up at week 12, the difference between the metrics extracted on controls and patients was not significant (p = 0,07), reflecting the efficacy of the treatment. Conclusions: Signal processing and machine learning tools applied on PERG could help clinical decision in the diagnosis and the follow-up of MDD in measuring treatment response.",Not About Sufficiency
Optimizing Outcomes of Treatment-Resistant Depression in Older Adults (OPTIMUM): Study Design and Treatment Characteristics of the First 396 Participants Randomized,"Objective: Evidence from clinical trials comparing effectiveness and safety of pharmacological strategies in older adults unresponsive to first-line antidepressants is limited. The study, Optimizing Outcomes of Treatment-Resistant Depression in Older Adults (OPTIMUM), tests three hypotheses concerning pharmacotherapy strategies for treatment-resistant late-life depression: 1) augmentation strategies will provide greater improvement than switching monotherapies; 2) augmentation strategies will have lower tolerability and more safety concerns than switching monotherapies; and 3) age will moderate the effectiveness and safety differences between treatment strategies. The authors describe the methodology, processes for stakeholder engagement, challenges, and lessons learned in the early phases of OPTIMUM. Methods: This pragmatic randomized clinical trial located in five North American regions will enroll 1,500 participants aged 60 years and older unresponsive to two or more antidepressant trials. The authors evaluate two strategies (medication augmentation versus switch) using four medications (aripiprazole, bupropion, lithium, and nortriptyline) via a stepwise, prespecified protocol. Primary outcomes include: 1) symptom remission (Montgomery Asberg Depression scale <= 10); 2) psychological well-being, comprising positive affect, general life satisfaction, and purpose; and 3) safety (rates of serious adverse events and prevalence of falls and fall-related injuries). Results: To date, 396 participants have been randomized. The authors report on four challenges: 1) engagement and recruitment; 2) increasing polypharmacy in older adults, resulting in potentially hazardous scenarios; 3) reporting adverse events and procedure standardization across sites; and 4) dissemination of results. Conclusion: Solutions to these challenges, including early inclusion of stake holders, will inform future pragmatic studies in older adults with depression.",Not About Sufficiency
Planning for Urban Social Sustainability: Towards a Human-Centred Operational Approach,"In Europe, growing concerns about social segregation and social stability have pushed calls to make cities 'inclusive, safe, resilient and sustainable' higher on policy agendas. However, how to approach such generic policy objectives and operationalise them for planning practices is still largely unclear. This article makes a conceptual contribution to the operational understanding of social sustainability in urban planning practices. The article argues that, between theoretical concept and operational forms, different evaluative approaches towards social sustainability may be taken. Evaluating three dimensions of policy operationalisations in The Netherlands, we argue that Amartya Sen's capability approach provides a promising conceptual framework for operationalising social sustainability in cities in Europe and beyond. We compare capabilities with a more commonly applied resource-based conception to show that the former is more accurate and potentially more effective, because it shifts the evaluative space of social sustainability from means (i.e., urban resources) to ends: the eventual well-being of urban citizens.",Not About Sufficiency
A Review of Medical Federated Learning: Applications in Oncology and Cancer Research,"Machine learning has revolutionized every facet of human life, while also becoming more accessible and ubiquitous. Its prevalence has had a powerful impact in healthcare, with numerous applications and intelligent systems achieving clinical level expertise. However, building robust and generalizable systems relies on training algorithms in a centralized fashion using large, heterogeneous datasets. In medicine, these datasets are time consuming to annotate and difficult to collect centrally due to privacy concerns. Recently, Federated Learning has been proposed as a distributed learning technique to alleviate many of these privacy concerns by providing a decentralized training paradigm for models using large, distributed data. This new approach has become the defacto way of building machine learning models in multiple industries (e.g. edge computing, smartphones). Due to its strong potential, Federated Learning is also becoming a popular training method in healthcare, where patient privacy is of paramount concern. In this paper we performed an extensive literature review to identify state-of-the-art Federated Learning applications for cancer research and clinical oncology analysis. Our objective is to provide readers with an overview of the evolving Federated Learning landscape, with a focus on applications and algorithms in oncology space. Moreover, we hope that this review will help readers to identify potential needs and future directions for research and development.",Not About Sufficiency
"City-Scale Bike Lane Facility Management through LiDAR, Weakly Supervised Learning, and Dimensionality Reduction","City-scale facility management plays a critical role in public health. This research aims to analyze city-scale infrastructure through a cost-effective approach by utilizing light detection and ranging (LiDAR) point cloud acquisition, dimensionality reduction processing, and advanced machine learning segmentation methods. A case study in a southwestern city in the United States examined bike lane infrastructure along 38.6 km using a backpack-mounted light detection and ranging LiDAR scanner on an electric scooter. The collected point cloud data were projected into two dimensions (2D) along the z-axis (up/down) and partitioned into crops in image format. A weakly supervised learning approach was employed for segmentation. The neural network segmented moving car trajectories, bike lane marking, and road curb segmentation. The postprocessed outputs evaluated city-scale bike infrastructure by measuring (1) lane widths; and (2) the closest distance between moving cars and the lanes, providing insights into the current state of road safety for cyclists. The results indicated that 85% of the roads examined had unsafe conditions due to reduced lane widths or shorter lateral distances, thus falling short of design guidelines. This study demonstrates a cost-effective and scalable method for city-scale data collection and analysis to inform infrastructure planning and cyclist safety assessments.",Not About Sufficiency
"Response to Waddington et al. on ""J-value assessment of relocation measures following the nuclear power plant accidents at Chernobyl and Fukushima Daiichi""","This is a response to Waddington et al. who argue that there was insufficient public health benefit to justify the relocation of several hundred thousand individuals after the Chernobyl and perhaps Fukushima nuclear accidents. We believe the arguments are not defensible either from a radiological/public health point of view, or from a consideration of costs, and we are skeptical of the use of the ""J-value assessment"" as a foundation to reach these conclusions. There is debate about the health consequences of exposure to ionizing radiation; data on health impacts are often uncertain and incomplete, and therefore not the basis on which to hesitate to evacuate those affected. (C) 2018 Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved.",Not About Sufficiency
Analyzing the effects of memory biases and mood disorders on social performance,"Realistic models of decision-making and social interactions, considering the nature of memory and biases, continue to be an area of immense interest. Emotion and mood are a couple of key factors that play a major role in decisions, nature of social interactions, size of the social network, and the level of engagement. Most of the prior work in this direction focused on a single trait, behavior, or bias. However, this work builds an integrated model that considers multiple traits such as loneliness, the drive to interact, the memory, and mood biases in an agent. The agent system comprises of rational, manic, depressed, and bipolar agents. The system is modeled with an interconnected network, and the size of the personal network of each agent is based on its nature. We consider a game of iterated interactions where an agent cooperates based on its past experiences with the other agent. Through simulation, the effects of various biases and comparative performances of agent types is analyzed. Taking the performance of rational agents as the baseline, manic agents do much better, and depressed agents do much worse. The payoffs also exhibit an almost-linear relationship with the extent of mania. It is also observed that agents with stronger memory perform better than those with weaker memory. For rational agents, there is no significant difference between agents with a positive bias and those with a negative bias. Positive bias is linked with higher payoffs in manic and bipolar agents. In depressed agents, negative bias is linked with higher payoffs. In manic agents, an intermediate value of mood dependence offers the highest payoff. But the opposite is seen in depressed agents. In bipolar agents, agents with weak mood dependence perform the best.",Not About Sufficiency
Machine learning techniques in cardiac risk assessment,"Background: The objective of this study was to predict the mortality risk of patients during or shortly after cardiac surgery by using machine learning techniques and their learning abilities from collected data. Methods: The dataset was obtained from Acibadem Maslak Hospital. Risk factors of the European System for Cardiac Operative Risk Evaluation (EuroSCORE) were used to predict mortality risk. First, Standard EuroSCORE scores of patients were calculated and risk groups were determined, because 30-day follow-up information of patients was not available in the dataset. Models were created with five different machine learning algorithms and two different datasets including age, serum creatinine, left ventricular dysfunction, and pulmonary hypertension were numeric in Dataset 1 and categorical in Dataset 2. Model performance evaluation was performed with 10-fold cross-validation. Results: Data analysis and performance evaluation were performed with R, RStudio and Shiny. C4.5 was selected as the best algorithm for risk prediction (accuracy= 0.989) in Dataset 1. This model indicated that pulmonary hypertension, recent myocardial infarct, surgery on thoracic aorta are the primary three risk factors that affect the mortality risk of patients during or shortly after cardiac surgery. Also, this model is used to develop a dynamic web application which is also accessible from mobile devices (https://elifkartal. shinyapps.io/euSCR/). Conclusion: The C4.5 decision tree model was identified as having the highest performance in Dataset 1 in predicting the mortality risk of patients. Using the numerical values of the risk factors can be useful in increasing the performance of machine learning models. Development of hospital-specific local assessment systems using hospital data, such as the application in this study, would be beneficial for both patients and doctors.",Not About Sufficiency
Evidence for the efficacy of anti-inflammatory plants used in Brazilian traditional medicine with ethnopharmacological relevance,"Ethnopharmacological relevance: When exacerbated, inflammatory processes can culminate in physical and emotional disorders and, if not stopped, can be lethal. The high prevalence of inflammation has become a public health problem, and the need for new drugs to treat this pathology is imminent. The use of medicinal plants has emerged as an alternative, and a survey of data that corroborates its application in inflammatory diseases is the starting point. Furthermore, Brazil harbors a megadiversity, and the traditional use of plants is relevant and needs to be preserved and carefully explored for the discovery of new medicines. Aim of the study: This review sought to survey the medicinal plants traditionally used in Brazil for the treatment of inflammatory processes and to perform, in an integrative way, a data survey of these species and analysis of their phytochemical, pharmacological, and molecular approaches. Materials and methods: Brazilian plants that are traditionally used for inflammation (ophthalmia, throat inflammation, orchitis, urinary tract inflammation, ear inflammation, and inflammation in general) are listed in the DATAPLAMT database. This database contains information on approximately 3400 native plants used by Brazilians, which were registered in specific documents produced until 1950. These inflammatory disorders were searched in scientific databases (PubMed/Medline, Scopus, Web of Science, Lilacs, Scielo, Virtual Health Library), with standardization of DECS/MESH descriptors for inflammation in English, Spanish, French, and Portuguese, without chronological limitations. For the inclusion criteria, all articles had to be of the evaluated plant species, without association of synthesized substances, and full articles free available in any of the four languages searched. Duplicated articles and those that were not freely available were excluded. Results: A total of 126 species were identified, culminating in 6181 articles in the search. After evaluation of the inclusion criteria, 172 articles representing 40 different species and 38 families were included in the study. Comparison of reproducibility in intra-species results became difficult because of the large number of extraction solvents tested and the wide diversity of evaluation models used. Although the number of in vitro and in vivo evaluations was high, only one clinical study was found ( Abrus precatorius). ). In the phytochemical analyses, more than 225 compounds, mostly phenolic compounds, were identified. Conclusion: This review allowed the grouping of preclinical and clinical studies of several Brazilian species traditionally used for the treatment of many types of inflammation, corroborating new searches for their pharmacological properties as a way to aid public health. Furthermore, the large number of plants that have not yet been studied has encouraged new research to revive traditional knowledge.",Not About Sufficiency
"Machine Learning on the COVID-19 Pandemic, Human Mobility and Air Quality: A Review","The ongoing COVID-19 global pandemic is touching every facet of human lives (e.g., public health, education, economy, transportation, and the environment). This novel pandemic and non-pharmaceutical interventions of lockdown and confinement implemented citywide, regionally or nationally are affecting virus transmission, people's travel patterns, and air quality. Many studies have been conducted to predict the diffusion of the COVID-19 disease, assess the impacts of the pandemic on human mobility and on air quality, and assess the impacts of lockdown measures on viral spread with a range of Machine Learning (ML) techniques. This literature review aims to analyze the results from past research to understand the interactions among the COVID-19 pandemic, lockdown measures, human mobility, and air quality. The critical review of prior studies indicates that urban form, people's socioeconomic and physical conditions, social cohesion, and social distancing measures significantly affect human mobility and COVID-19 viral transmission. During the COVID-19 pandemic, many people are inclined to use private transportation for necessary travel to mitigate coronavirus-related health problems. This review study also noticed that COVID-19 related lockdown measures significantly improve air quality by reducing the concentration of air pollutants, which in turn improves the COVID-19 situation by reducing respiratory-related sickness and deaths. It is argued that ML is a powerful, effective, and robust analytic paradigm to handle complex and wicked problems such as a global pandemic. This study also explores the spatio-temporal aspects of lockdown and confinement measures on coronavirus diffusion, human mobility, and air quality. Additionally, we discuss policy implications, which will be helpful for policy makers to take prompt actions to moderate the severity of the pandemic and improve urban environments by adopting data-driven analytic methods.",Not About Sufficiency
IRIA: The information retrieval intelligent assistant,"The modern work environment demands the ability to search and manage large amounts of information from a wide variety of applications. The Information Research Intelligent Assistant (IRIA) addresses this problem by acting as an autonomous assistant to a user, unobtrusively building a map of accessible relevant information and using it to enable users to find information quickly. To do so, IRIA applies experience-based agency, a cognitive science approach to managing information access in large databases, to information search. In empirical tests, IRIA demonstrated the ability to monitor both user and workgroup web searches to preactively find and recommend relevant information.",Not About Sufficiency
Machine learning optimization model for reducing the electricity loads in residential energy forecasting,"Recently, households have consumed more power by overusing refrigerators, mobiles, washing machines, and electrical appliances. The incoming energy needs to be optimized, and this research uses machine learning for energy forecasting to reduce the overconsumption of power. Intelligent buildings are additionally associated with Heating, Air Conditioning (HVAC), and Ventilation Units. Due to the limited devices, it acquires only minimum communication capability. In this paper, deep learning with Meta-heuristic based algorithm has been proposed to address the constraints such as multi-objective optimization and fitness function and address the energy consumption of HVAC units. It uses a gated recurrent circuit (GRU) with a gorilla troop optimizer (GTO) to optimize the energy efficiently. This study uses the HVAC model to analyze smart buildings and addresses power consumption through HVAC systems based on power loss, price management, and reactive power. The simulation environment is performed using python at various experiments with various scenarios for proposed evaluation. Here, integrity in intelligent buildings is considered. The results outcomes are compared with other HVAC devices using different metrics and communication protocols. It has been proven that the proposed model is more effective in reducing the system's energy consumption than other approaches.",Not About Sufficiency
"Low-Carbon Territorial Spatial Detailed Planning in the Context of Climate Change: A Case Study of the Wenzhou Garden Expo Park Area, China","In the context of global climate change, promoting the low-carbon transformation of cities has become an important strategy to cope with environmental challenges. This paper takes Wenzhou Garden Expo Park area as the research object, combines its practical experience as a pilot of a national low-carbon city, and discusses how to effectively control carbon emission in the spatial planning of national territory. The study systematically evaluated the impact of different land use types and development intensities on carbon emissions, as well as the relationship between daytime temperature and carbon emissions, by constructing a carbon emission measurement model and a random forest regression model. This evaluation was based on an analysis of remote sensing data and land use changes from 2000 to 2023. The results show that between 2000 and 2023, the carbon emission from building land in the Garden Expo Park area will increase by about 70%, while the carbon emission can be reduced by more than 25% through rational land use layout and development intensity control. At the same time, the expansion of green space and forest land increases the carbon sink capacity by about 16.7%. With rising temperatures, carbon emissions exhibit a significant upward trend. This study suggests that specific optimization strategies for low-carbon planning, along with an indicator system-particularly through increasing the allocation of green spaces such as arboreal forests and parks-can significantly improve regional carbon balance. This study may provide a reference for other rapidly urbanizing regions to balance economic development and carbon emissions.",Not About Sufficiency
Restructuring of Beijing's social space,"Four China-, U.S.-, and UK-based senior specialists on the urban geography of Beijing team up to present the results of their investigation of the change in the city's social landscape on the basis of restricted subdistrict-level data covering the period from 1982 to 2000. Adopting a factor ecology approach, the authors seek to identify the extent to which factors that shape urban social landscapes in Western countries have played a role in Beijing during a period of economic transition (e.g., from administrative toward market allocation of housing). Similarly, they investigate the relative strength of processes of social differentiation vis-A-vis mixing accruing from the restructuring of Beijing's physical space (e.g., housing cost differentiation and accelerated in-migration that lead to the emergence of concentrated areas inhabited by migrants and/or minorities, and of relatively low density suburbs) revealed by the 2000 Census. The paper includes a section on developments since 2000, citing data (through 2006) on population, scarcity of affordable housing, traffic congestion, and industrial and residential relocation, relating in part to the forthcoming Olympic Games in 2008.",Not About Sufficiency
"PRERISK: A Personalized, Artificial Intelligence-Based and Statistically-Based Stroke Recurrence Predictor for Recurrent Stroke","BACKGROUND: Predicting stroke recurrence for individual patients is difficult, but individualized prediction may improve stroke survivors' engagement in self-care. We developed PRERISK: a statistical and machine learning classifier to predict individual risk of stroke recurrence. METHODS: We analyzed clinical and socioeconomic data from a prospectively collected public health care-based data set of 41 975 patients admitted with stroke diagnosis in 88 public health centers over 6 years (2014-2020) in Catalonia-Spain. A new stroke diagnosis at least 24 hours after the index event was considered as a recurrent stroke, which was considered as our outcome of interest. We trained several supervised machine learning models to provide individualized risk over time and compared them with a Cox regression model. Models were trained to predict early, late, and long-term recurrence risk, within 90, 91 to 365, and >365 days, respectively. C statistics and area under the receiver operating characteristic curve were used to assess the accuracy of the models. RESULTS: Overall, 16.21% (5932 of 36 114) of patients had stroke recurrence during a median follow-up of 2.69 years. The most powerful predictors of stroke recurrence were time from previous stroke, Barthel Index, atrial fibrillation, dyslipidemia, age, diabetes, and sex, which were used to create a simplified model with similar performance, together with modifiable vascular risk factors (glycemia, body mass index, high blood pressure, cholesterol, tobacco dependence, and alcohol abuse). The areas under the receiver operating characteristic curve were 0.76 (95% CI, 0.74-0.77), 0.60 (95% CI, 0.58-0.61), and 0.71 (95% CI, 0.69-0.72) for early, late, and long-term recurrence risk, respectively. The areas under the receiver operating characteristic curve of the Cox risk class probability were 0.73 (95% CI, 0.72-0.75), 0.59 (95% CI, 0.57-0.61), and 0.67 (95% CI, 0.66-0.70); machine learning approaches (random forest and AdaBoost) showed statistically significant improvement (P<0.05) over the Cox model for the 3 recurrence time periods. Stroke recurrence curves can be simulated for each patient under different degrees of control of modifiable factors. CONCLUSIONS: PRERISK is a novel approach that provides a personalized and fairly accurate risk prediction of stroke recurrence over time. The model has the potential to incorporate dynamic control of risk factors.",Not About Sufficiency
Developing and deploying deep learning models in brain magnetic resonance imaging: A review,"Magnetic resonance imaging (MRI) of the brain has benefited from deep learning (DL) to alleviate the burden on radiologists and MR technologists, and improve throughput. The easy accessibility of DL tools has resulted in a rapid increase of DL models and subsequent peer-reviewed publications. However, the rate of deployment in clinical settings is low. Therefore, this review attempts to bring together the ideas from data collection to deployment in the clinic, building on the guidelines and principles that accreditation agencies have espoused. We introduce the need for and the role of DL to deliver accessible MRI. This is followed by a brief review of DL examples in the context of neuropathologies. Based on these studies and others, we collate the prerequisites to develop and deploy DL models for brain MRI. We then delve into the guiding principles to develop good machine learning practices in the context of neuroimaging, with a focus on explainability. A checklist based on the United States Food and Drug Administration's good machine learning practices is provided as a summary of these guidelines. Finally, we review the current challenges and future opportunities in DL for brain MRI.",Not About Sufficiency
"An Application of Machine-Learning Model for Analyzing the Impact of Land-Use Change on Surface Water Resources in Gauteng Province, South Africa","The change in land-use diversity is attributed to the anthropogenic factors sustaining life. The surface water bodies and other crucial natural resources in the study area are being depleted at an alarming rate. This study explored the implications of the changing land-use diversity on surface water resources by using a random forest (RF) classifier machine-learning algorithm and remote-sensing models in Gauteng Province, South Africa. Landsat datasets from 1993 to 2022 were used and processed in the Google Earth Engine (GEE) platform, using the RF classifier. The results indicate nine land-use diversity classes having increased and decreased tendencies, with high F-score values ranging from 72.3% to 100%. In GP, the spatial coverage of BL has shrunk by 100.4 km(2) every year over the past three decades. Similarly, BuA exhibits an annual decreasing rate of 42.4 km(2) due to the effect of dense vegetation coverage within the same land use type. Meanwhile, water bodies, marine quarries, arable lands, grasslands, shrublands, dense forests, and wetlands were expanded annually by 1.3, 2.3, 2.9, 5.6, 11.2, 29.6, and 89.5 km(2), respectively. The surface water content level of the study area has been poor throughout the study years. The MNDWI and NDWI values have a stronger Pearson correlation at a radius of 5 km (r = 0.60, p = 0.000, n = 87,260) than at 10 and 15 km. This research is essential to improve current land-use planning and surface water management techniques to reduce the environmental impacts of land-use change.",Not About Sufficiency
Transforming Mixed Data Bases for Machine Learning: A Case Study,"Structured Data Bases which include both numerical and categorical attributes (Mixed Databases or MD) ought to be adequately pre-processed so that machine learning algorithms may be applied to their analysis and further processing. Of primordial importance is that the instances of all the categorical attributes be encoded so that the patterns embedded in the MD be preserved. We discuss CESAMO, an algorithm that achieves this by statistically sampling the space of possible codes. CESAMO's implementation requires the determination of the moment when the codes distribute normally. It also requires the approximation of an encoded attribute as a function of other attributes such that the best code assignment may be identified. The MD's categorical attributes are thusly mapped into purely numerical ones. The resulting numerical database (ND) is then accessible to supervised and non-supervised learning algorithms. We discuss CESAMO, normality assessment and functional approximation. A case study of the US census database is described. Data is made strictly numerical using CESAMO. Neural Networks and Self-Organized Maps are then applied. Our results are compared to classical analysis. We show that CESAMO's application yields better results.",Not About Sufficiency
A GIS Approach to Estimation of Building Population for Micro-spatial Analysis,"Population data used in GIS analyses is generally assumed to be homogeneous and planar (i.e. census tracts, townships or prefectures) due to the public unavailability of building population data. However, information on building population is required for micro-spatial analysis for improved disaster management and emergency preparedness, public facility management for urban planning, consumer and retail market analysis, environment and public health programs and other demographic studies. This article discusses a GIS approach using the Areametric and Volumetric methods for estimating building population based on census tracts and building footprint datasets. The estimated results were evaluated using actual building population data by visual, statistical and spatial means, and validated for use in micro-spatial analysis. We have also implemented a standalone GIS tool (known as 'PopShape GIS') for generating new building footprint with population attribute information based on user-defined criteria.",Not About Sufficiency
A hybrid learning model for efficient classification of Land Use and land change from satellite images,"With Deep Learning (DL) outperforming previous Machine Learning (ML) techniques in classifying images, the remote sensing community has recently shown an increased interest in using these algorithms to classify Land Use and Land Cover (LULC) using multispectral and hyperspectral data. Land Use (LU) and Land Cover (LC) are two types of cartographic data that are used to develop smart cities and monitor the environment. LULC clas-sification can benefit greatly from successfully applying remote sensing Image Classification (IC) using high spatial resolution data. The acquisition of spatiotemporal data for LULC classification has been made more accessible because of recent improvements in spatial analysis and Deep Learning (DL) technology. Considering the quality of Deep Neural Networks (DNN) in related Computer Vision (CV) tasks and the enormous volume of remotely sensed data accessible, DL methods appear to be particularly promising for modelling many remote sensing problems. However, there are several issues with ground-truth, resolution, and the nature of data that have a significant impact on categorization performance. We propose a Reversible Residual Network (RAVNet), a hybrid residual attention sensitive segmentation approach, to precisely categorize LULC in this study. The suggested network is based on the VNet model, which extracts relevant information by mixing low-level and high-level Feature Maps (FM). The attention-aware features change adaptively to the integration of residual modules. Our system was tested on the National Agriculture Imagery Program (NAIP) dataset, and the findings demonstrate that our architecture is competitive against other learning models.",Not About Sufficiency
Development and validation of explainable machine-learning models for carotid atherosclerosis early screening,"BackgroundCarotid atherosclerosis (CAS), an important factor in the development of stroke, is a major public health concern. The aim of this study was to establish and validate machine learning (ML) models for early screening of CAS using routine health check-up indicators in northeast China.MethodsA total of 69,601 health check-up records from the health examination center of the First Hospital of China Medical University (Shenyang, China) were collected between 2018 and 2019. For the 2019 records, 80% were assigned to the training set and 20% to the testing set. The 2018 records were used as the external validation dataset. Ten ML algorithms, including decision tree (DT), K-nearest neighbors (KNN), logistic regression (LR), naive Bayes (NB), random forest (RF), multiplayer perceptron (MLP), extreme gradient boosting machine (XGB), gradient boosting decision tree (GBDT), linear support vector machine (SVM-linear), and non-linear support vector machine (SVM-nonlinear), were used to construct CAS screening models. The area under the receiver operating characteristic curve (auROC) and precision-recall curve (auPR) were used as measures of model performance. The SHapley Additive exPlanations (SHAP) method was used to demonstrate the interpretability of the optimal model.ResultsA total of 6315 records of patients undergoing carotid ultrasonography were collected; of these, 1632, 407, and 1141 patients were diagnosed with CAS in the training, internal validation, and external validation datasets, respectively. The GBDT model achieved the highest performance metrics with auROC of 0.860 (95% CI 0.839-0.880) in the internal validation dataset and 0.851 (95% CI 0.837-0.863) in the external validation dataset. Individuals with diabetes or those over 65 years of age showed low negative predictive value. In the interpretability analysis, age was the most important factor influencing the performance of the GBDT model, followed by sex and non-high-density lipoprotein cholesterol.ConclusionsThe ML models developed could provide good performance for CAS identification using routine health check-up indicators and could hopefully be applied in scenarios without ethnic and geographic heterogeneity for CAS prevention.",Not About Sufficiency
Remote Sensing of Water Quality Parameters over Lake Balaton by Using Sentinel-3 OLCI,"The Ocean and Land Color Instrument (OLCI) onboard Sentinel 3A satellite was launched in February 2016. Level 2 (L2) products have been available for the public since July 2017. OLCI provides the possibility to monitor aquatic environments on 300 m spatial resolution on 9 spectral bands, which allows to retrieve detailed information about the water quality of various type of waters. It has only been a short time since L2 data became accessible, therefore validation of these products from different aquatic environments are required. In this work we study the possibility to use S3 OLCI L2 products to monitor an optically highly complex shallow lake. We test S3 OLCI-derived Chlorophyll-a (Chl-a), Colored Dissolved Organic Matter (CDOM) and Total Suspended Matter (TSM) for complex waters against in situ measurements over Lake Balaton in 2017. In addition, we tested the machine learning Gaussian process regression model, trained locally as a potential candidate to retrieve water quality parameters. We applied the automatic model selection algorithm to select the combination and number of spectral bands for the given water quality parameter to train the Gaussian Process Regression model. Lake Balaton represents different types of aquatic environments (eutrophic, mesotrophic and oligotrophic), hence being able to establish a model to monitor water quality by using S3 OLCI products might allow the generalization of the methodology.",Not About Sufficiency
Modeling and Feature Analysis of Air Traffic Management Technical Support System Based on Weighted Complex Network,"In order to accurately analyze the distribution characteristics of Air Traffic Management Technical Support System(ATMTSS), based on complex network theory, the weighted network model of air traffic management system was established. From the three aspects of mobility, efficiency, vulnerability, feature analysis index set is established under the weighted condition. Taking the southwest China area as an example, results show that the weighted network is uneven distribution and few nodes played a key role in the network nodes. The cumulative degree distribution and node weighted distribution obey a power-law distribution. The Network has a scale-free feature. The Poisson distribution value of the navigation device node degree is 0.0085. The network is a random network. The average path length of the network is 3.4, which shows the small world feature. The concept of rationality of device establishment location can basically reflect the business relationship between ATMTSS and flight flow. The node with high centrality score is a hub node that connects different local networks to a whole network. The network is poor in the resistance to malicious attacks. These theories have laid the foundation for the further development planning of the air traffic management system based on business sustainability research.",Not About Sufficiency
Evaluating the Privacy Exposure of Interpretable Global and Local Explainers,"During the last few years, the abundance of data has significantly boosted the performance of Machine Learning models, integrating them into several aspects of daily life. However, the rise of powerful Artificial Intelligence tools has introduced ethical and legal complexities. This paper proposes a computational framework to analyze the ethical and legal dimensions of Machine Learning models, focusing specifically on privacy concerns and interpretability. In fact, recently, the research community proposed privacy attacks able to reveal whether a record was part of the black-box training set or inferring variable values by accessing and querying a Machine Learning model. These attacks highlight privacy vulnerabilities and prove that GDPR regulation might be violated by making data or Machine Learning models accessible. At the same time, the complexity of these models, often labelled as ""black-boxes"", has made the development of explanation methods indispensable to enhance trust and facilitate their acceptance and adoption in high-stake scenarios. Our study highlights the trade-off between interpretability and privacy protection. By introducing REVEAL, this paper proposes a framework to evaluate the privacy exposure of black-box models and their surrogate-based explainers, whether local or global. Our methodology is adaptable and applicable across diverse black-box models and various privacy attack scenarios. Through an in-depth analysis, we show that the interpretability layer introduced by explanation models might jeopardize the privacy of individuals in the training data of the black-box, particularly with powerful privacy attacks requiring minimal knowledge but causing significant privacy breaches.",Not About Sufficiency
"Historical land use reconstruction for South Asia: Current understanding, challenges, and solutions","To deal with global warming and biodiversity loss, historical land use in South Asia received wide attention because of its huge population and rich biodiversity. We reviewed representative global and regional historical land use reconstructions for South Asia from their data sources, methods of area estimation and spatial mapping, and outcomes. Then we made some prospects on developing historical land use for South Asia. The land use area of South Asia in global reconstructions was mainly estimated using population. For the spatial pattern, most of them assumed that the historical distribution of land use in South Asia mimics satellite-based land use patterns or that the land use area was pixelized based on land suitability for cultivation and grazing. Large discrepancies exist among these global reconstructions in the land use area and spatial patterns with increasing magnitude with time in the past. The regional reconstructions of historical land use for South Asia used more regional historical records, but most covered only about one hundred years. The closer to the past, the lower the spatial-temporal resolution of these regional reconstructions. Great discrepancies also exist between these regional re-constructions because of the differences in cropland definition. Expansion of archaeological data sources, meta-analysis, and crowdsourcing will bring insights into area estimation of historical land use. Remote sensing and machine learning will facilitate spatial pattern mapping. Reconstruction region-by-region, and then integration, is also a solution for current challenges in reconstructing historical land use for South Asia.",Not About Sufficiency
A generalized model for estimating adsorption energies of single atoms on doped carbon materials,"Single metal atoms on doped carbons constitute a new class of extremely appealing materials, as they present the best metal utilization for catalysis. However, their stability can be compromised by metal aggregation and the formation of nanoparticles, which often results in reduced activity or even catalyst deactivation. In many cases, the carbon hosts are generated via thermal processes, leading to poorly controlled materials. This causes a structural and compositional diversity that is modeled via indirect procedures and by comparison to a collection of structural models with different compositions. Our aim in this work is to develop a general framework based on machine learning techniques to determine the stability of the different structures against aggregation as nanoparticles. Here, we built machine learning models for the cavities and identified the robust features characterizing the metal-support interaction, considering different heteroatoms in the decorative cavity and single metal atoms. The descriptors presented here are accessible and cost-effective, such as the cavity size, electronegativity of the metal and heteroatoms, different covalent radii, and the metal electronic density. These can then be employed in the search for a mathematical equation that describes the adsorption energy via the Bayesian machine scientist. The algorithm is able to separate coordination, and covalent and ionic contributions expressed by the descriptors. This approach paves the way towards general modeling of single atoms in modified carbons particularly addressing one of the crucial features, stability. Single metal atoms on doped carbons constitute a new class of extremely appealing materials, as they present the best metal utilization for catalysis.",Not About Sufficiency
Oxytocin effects on the resting-state mentalizing brain network,"Oxytocin (OT) has modulatory effects in both human behavior and in the brain, which is not limited in the specific brain area but also with the potential effect on connectivity with other brain regions. Evidence indicates that OT effects on human behavior are multifaceted, such as trust behavior, decrease anxiety, empathy and bonding behavior. For the vital role of mentalizing in understanding others, here we examine whether OT has a general effect on mentalizing brain network which is associated to the effect of related social behavioral and personality traits. Using a randomized, double-blind placebo-controlled group design, we investigate the resting-state functional magnetic resonance imaging after intranasal OT or placebo. The functional connectivity (FC) maps with seed in left/right temporoparietal junction (lTPJ/rTPJ) showed that OT significantly increased connectivity between rTPJ and default attention network (DAN), but decreased the FC between lTPJ and medial prefrontal network (MPN). With machine learning approach, we report that identified altered FCs of TPJ can classify OT and placebo (PL) group. Moreover, individual's empathy trait can modulate the FC between left TPJ and right rectus (RECT), which shows a positive correlation with empathic concern in PL group but a negative correlation in OT group. These results demonstrate that OT has significant effect on FC with lTPJ and rTPJ, brain regions where are critical for mentalizing, and the empathy concern can modulate the FC. These findings advance our understanding of the neural mechanisms by which OT modulates social behaviors, especially in social interaction involving mentalizing.",Not About Sufficiency
Urban expertise and study: the end of a militant model?,"Olivier CHATELAN. Urban expertise and study: the end of a militant model? The working group ""Cadre de Vie"" was founded in 1973 within the ""Economie et Humanisme"" association and gathered a number of leading figures involved to differing degrees in urban planning projects for three years in the Lebret Center of Paris. This small team wished to debate the acknowledgement of ""human needs"" in contemporary urban planning projects in several French cities. However, the purpose and sense of the group's work soon appeared unclear to its members themselves: wasn't there a redundancy with the studies of ""Economie et Humanisme"" of previous decades? What kind of visibility should be given to such studies? The working group having few prospects and being led for the most part by both Andre Villette and Robert Caillot, it failed to generate true research dynamics. The reasons for this failure have less to do with the personalities of the members themselves than to an identity crisis within a militant movement confronted with the professionalization of urban planning.",Not About Sufficiency
Developing a machine learning-based instrument for subjective well-being assessment on Weibo and its psychological significance: An evaluative and interpretive research,"Demystifying machine learning (ML) approaches through the synergy of psychology and artificial intelligence can achieve a balance between predictive and explanatory power in model development while enhancing rigor in validation and reporting standards. Accordingly, this study aimed to bridge this research gap by developing a subjective well-being (SWB) prediction model on Weibo, serving as a psychological assessment instrument and explaining the model construction based on psychological knowledge. The model establishment involved the collection of SWB scores and posts from 1,427 valid Weibo users. Multiple machine learning algorithms were employed to train the model and fine-tune its parameters. The optimal model was selected by comparing its criterion validity and split-half reliability performance. Furthermore, SHAP values were calculated to rank the importance of features, which were then used for model interpretation. The criterion validity for the three dimensions of SWB ranged from 0.50 to 0.52 (P < 0.001), and the split-half reliability ranged from 0.94 to 0.96 (P < 0.001). The identified relevant features were related to four main aspects: cultural values, emotions, morality, and time and space. This study expands the application scope of SWB-related psychological theories from a data-driven perspective and provides a theoretical reference for further well-being prediction.",Not About Sufficiency
Multi-Standard Quadratic Optimization: interior point methods and cone programming reformulation,"A Standard Quadratic Optimization Problem (StQP) consists of maximizing a (possibly indefinite) quadratic form over the standard simplex. Likewise, in a multi-StQP we have to maximize a (possibly indefinite) quadratic form over the Cartesian product of several standard simplices (of possibly different dimensions). Among many other applications, multi-StQPs occur in Machine Learning Problems. Several converging monotone interior point methods are established, which differ from the usual ones used in cone programming. Further, we prove an exact cone programming reformulation for establishing rigorous yet affordable bounds and finding improving directions.",Not About Sufficiency
Feature Extractions for Computationally Predicting Protein Post-Translational Modifications,"Background: Post-translational modifications (PTMs) are a key regulating mechanism in the cellular process. It is of importance to quickly and accurately identify PTMs. Both next generation sequencing as well as bioinformatics techniques greatly facilitated discovery of PTMs. Most bioinformatics techniques followed the machine learning framework where feature extraction occupies a key position. Conclusion: The article focuses mainly on reviewing various feature extractions from protein sequence, structure, function, physicochemical and biochemical property and evolution conservation, which were used for predicting PTMs in the machine learning-based methods. The binary encoding, amino acid composition, pseudo amino acid composition, composition of K-spaced amino acid pairs, auto correlation functions, position weight amino acids composition and position-specific amino acid propensity extracted features directly from protein sequences. Encoding based on grouped weight is a hybrid way of feature extraction integrating information both on physicochemical and biochemical property and on sequences. The information on protein structure, especially secondary structure, accessible surface and disorder was used for encoding proteins. The feature extraction from the evolution conservation included position-specific scoring matrix and k-nearest neighbor score. In addition, we discussed some existing problems in the feature extractions.",Not About Sufficiency
Electroencephalogram-based adaptive closed-loop brain-computer interface in neurorehabilitation: a review,"Brain-computer interfaces (BCIs) represent a groundbreaking approach to enabling direct communication for individuals with severe motor impairments, circumventing traditional neural and muscular pathways. Among the diverse array of BCI technologies, electroencephalogram (EEG)-based systems are particularly favored due to their non-invasive nature, user-friendly operation, and cost-effectiveness. Recent advancements have facilitated the development of adaptive bidirectional closed-loop BCIs, which dynamically adjust to users' brain activity, thereby enhancing responsiveness and efficacy in neurorehabilitation. These systems support real-time modulation and continuous feedback, fostering personalized therapeutic interventions that align with users' neural and behavioral responses. By incorporating machine learning algorithms, these BCIs optimize user interaction and promote recovery outcomes through mechanisms of activity-dependent neuroplasticity. This paper reviews the current landscape of EEG-based adaptive bidirectional closed-loop BCIs, examining their applications in the recovery of motor and sensory functions, as well as the challenges encountered in practical implementation. The findings underscore the potential of these technologies to significantly enhance patients' quality of life and social interaction, while also identifying critical areas for future research aimed at improving system adaptability and performance. As advancements in artificial intelligence continue, the evolution of sophisticated BCI systems holds promise for transforming neurorehabilitation and expanding applications across various domains.",Not About Sufficiency
FMARS: ANNOTATING REMOTE SENSING IMAGES FOR DISASTER MANAGEMENT USING FOUNDATION MODELS,"Very-High Resolution (VHR) remote sensing imagery is increasingly accessible, but often lacks annotations for effective machine learning applications. Recent foundation models like GroundingDINO [1] and Segment Anything (SAM) [2] provide opportunities to automatically generate annotations. This study introduces FMARS (Foundation Model Annotations in Remote Sensing), a methodology leveraging VHR imagery and foundation models for fast and robust annotation. We focus on disaster management and provide a large-scale dataset with labels obtained from pre-event imagery over 19 disaster events, derived from the Maxar Open Data initiative. We train segmentation models on the generated labels, using Unsupervised Domain Adaptation (UDA) techniques to increase transferability to real-world scenarios. Our results demonstrate the effectiveness of leveraging foundation models to automatically annotate remote sensing data at scale, enabling robust downstream models for critical applications. Code and dataset are available at https://github.com/links-ads/igarss-fmars.",Not About Sufficiency
Estimating the Prevalence of Schizophrenia in the General Population of Japan Using an Artificial Neural Network-Based Schizophrenia Classifier: Web-Based Cross- Sectional Survey,"Background: Estimating the prevalence of schizophrenia in the general population remains a challenge worldwide, as well as in Japan. Few studies have estimated schizophrenia prevalence in the Japanese population and have often relied on reports from hospitals and self-reported physician diagnoses or typical schizophrenia symptoms. These approaches are likely to underestimate the true prevalence owing to stigma, poor insight, or lack of access to health care among respondents. To address these issues, we previously developed an artificial neural network (ANN)-based schizophrenia classification model (SZ classifier) using data from a large-scale Japanese web-based survey to enhance the comprehensiveness of schizophrenia case identification in the general population. In addition, we also plan to introduce a population-based survey to collect general information and sample participants matching the population's demographic structure, thereby achieving a precise estimate of the prevalence of schizophrenia in Japan. Objective: This study aimed to estimate the prevalence of schizophrenia by applying the SZ classifier to random samples from the Japanese population. Methods: We randomly selected a sample of 750 participants where the age, sex, and regional distributions were similar to Japan's demographic structure from a large-scale Japanese web-based survey. Demographic data, health-related backgrounds, physical comorbidities, psychiatric comorbidities, and social comorbidities were collected and applied to the SZ classifier, as this information was also used for developing the SZ classifier. The crude prevalence of schizophrenia was calculated through the proportion of positive cases detected by the SZ classifier. The crude estimate was further refined by excluding false-positive cases and including false-negative cases to determine the actual prevalence of schizophrenia. Results: Out of 750 participants, 62 were classified as schizophrenia cases by the SZ classifier, resulting in a crude prevalence of schizophrenia in the general population of Japan of 8.3% (95% CI 6.6%-10.1%). Among these 62 cases, 53 were presumed to be false positives, and 3 were presumed to be false negatives. After adjustment, the actual prevalence of schizophrenia in the general population was estimated to be 1.6% (95% CI 0.7%-2.5%). Conclusions: This estimated prevalence was slightly higher than that reported in previous studies, possibly due to a more comprehensive disease classification methodology or, conversely, model limitations. This study demonstrates the capability of an ANN-based model to improve the estimation of schizophrenia prevalence in the general population, offering a novel approach to public health analysis.",Not About Sufficiency
Intent Classification using Feature Sets for Domestic Violence Discourse on Social Media,"Domestic Violence against women is now recognized to be a serious and widespread problem worldwide. Domestic Violence and Abuse is at the root of so many issues in society and considered as the societal tabooed topic. Fortunately, with the popularity of social media, social welfare communities and victim support groups facilitate the victims to share their abusive stories and allow others to give advice and help victims. Hence, in order to offer the immediate resources for those needs, the specific messages from the victims need to be alarmed from other messages. In this paper, we regard intention mining as a binary classification problem (abuse or advice) with the use-case of abuse discourse. To address this problem, we extract rich feature sets from the raw corpus, using psycholinguistic clues and textual features by term-class interaction method. Machine learning algorithms are used to predict the accuracy of the classifiers between two different feature sets. Our experimental results with high classification accuracy give a promising solution to understand a big social problem through big social media and its use in serving information needs of various community welfare organizations.",Not About Sufficiency
AI-Driven Educational Transformation in Secondary Schools: Leveraging Data Insights for Inclusive Learning Environments,"In recent years, in the field of education, there has been a progressive trend towards teaching that is more personalised to students' characteristics and some models of prediction failure that are more accurate. In this sense, machine learning techniques have contributed to this realisation. This transformation has been significantly influenced by the integration of machine learning techniques, which have played a crucial role in harnessing data to enhance educational practices. This paper investigates the transformative potential of Artificial Intelligence (AI) within secondary education, focusing on the utilization of student assessment data and socioeconomic contextual information. The primary objective of this paper is to investigate the transformative potential of Artificial Intelligence (AI) within secondary education, with a specific focus on the utilization of student assessment data and socioeconomic contextual information. So, this paper explores the application of AI algorithms to create tailored learning pathways, adaptive support mechanisms, and targeted interventions that accommodate diverse student backgrounds. The integration of AI in secondary education is envisioned not only as a means to enhance academic outcomes but also as a tool to promote social equity and inclusivity. By leveraging data insights, educators can identify and respond to the unique needs of each student, fostering an environment where learning is optimized for individual growth. Furthermore, the paper scrutinizes the ethical considerations and challenges inherent in deploying AI systems in educational settings, emphasizing the pivotal role of equity, transparency, and data privacy in these implementations. This research aims to offer educators, policymakers, and stakeholder's insights into harnessing AI to foster adaptable, student-centric learning environments that bridge educational gaps and promote holistic academic development in secondary schools. Ethical guidelines and frameworks are discussed to ensure responsible AI deployment in educational contexts, safeguarding the rights and privacy of students. The data explored is taken from the management system of a secondary school in the municipality of Braga, relating to students taking Maths A, Maths B or Maths Applied to the Social Sciences (MACS). A total of 621 students were analysed, of which: 520 students attended Mathematics A in the Science and Technology and Socio-Economic Sciences courses, 20 attended Mathematics B in the Visual Arts course and 81 attended Mathematics Applied to Social Sciences in the Languages and Humanities course. The different maths subjects were analysed separately and at the end a comparative study was carried out between the three strands. Grounded in the analysis of these integrated datasets, the study sheds light on the pivotal role of AI in revolutionizing secondary school education. By closely examining student assessment data alongside socioeconomic indicators, such as academic performance and behavioral patterns, the paper identifies opportunities for AI integration to personalize learning experiences, address educational disparities, and cultivate inclusive learning environments. In Maths A there are 268 female and 252 male students. In Maths B there are 18 females and 2 males (most of the arts subjects are taken by women). In MACS the distribution is as follows: 45 female and 36 male. All these students, regardless of the area they chose at the start of secondary school, have in common the choice of Maths, however Maths A will be taken throughout secondary school, while the other two strands are only taken in the first two years of secondary school. Data fusuion techniques have achieved better performance in this type of study, since the models themselves combine the predictions of two or more base models.",Not About Sufficiency
Archaeology and ethnobiology of Late Holocene bird remains from the northern Oregon coast,"Archaeological bird remains from the Oregon coast provide important insight into local environments and the interactions between birds and people on the North American Pacific Coast. We contribute to this discussion with an analysis of bird remains from the Late Holocene Par-Tee site (35CLT20) in Seaside, Oregon. We sampled the Par-Tee avifaunal assemblage to near-redundancy, generating the largest sample from a single site on the Oregon Coast to date (N = 7204). The Par-Tee assemblage is dominated by nearshore or estuarine birds including scoters (Melanitta spp.) and Common Murres (Uria aalge), as well as pelagic Sooty Shearwaters (Ardenna grisea). Because of the large size of the sample, we identified unique species such as the California Condor (Gymnogyps californianus), which are currently endangered and face conservation challenges. Although the Par-Tee avifaunal assemblage is diverse, site residents appear to have focused on acquiring the most accessible species in the nearshore habitat complemented by opportunistic pelagic hunting and/or scavenging of beached birds. Most birds appear to have been processed for dietary consumption, with possible preferential use of larger-winged birds for tool manufacture. These findings underscore the value and challenges of using legacy collections for evaluating past human-environmental interactions in coastal and other aquatic regions.",Not About Sufficiency
Machine Learning practices and infrastructures,"Machine Learning (ML) systems, particularly when deployed in high-stakes domains, are deeply consequential. They can exacerbate existing inequities, create new modes of discrimination, and reify outdated social constructs. Accordingly, the social context (i.e. organisations, teams, cultures) in which ML systems are developed is a site of active research for the field of AI ethics, and intervention for policymakers. This paper focuses on one aspect of social context that is often overlooked: interactions between practitioners and the tools they rely on, and the role these interactions play in shaping ML practices and the development of ML systems. In particular, through an empirical study of questions asked on the Stack Exchange forums, the use of interactive computing platforms (e.g. Jupyter Notebook and Google Colab) in ML practices is explored. I find that interactive computing platforms are used in a host of learning and coordination practices, which constitutes an infrastructural relationship between interactive computing platforms and ML practitioners. I describe how ML practices are co-evolving alongside the development of interactive computing platforms, and highlight how this risks making invisible aspects of the ML life cycle that AI ethics researchers' have demonstrated to be particularly salient for the societal impact of deployed ML systems.",Not About Sufficiency
Learning to recognize rat social behavior: Novel dataset and cross-dataset application,"Background: Social behavior is an important aspect of rodent models. Automated measuring tools that make use of video analysis and machine learning are an increasingly attractive alternative to manual annotation. Because machine learning-based methods need to be trained, it is important that they are validated using data from different experiment settings. New method: To develop and validate automated measuring tools, there is a need for annotated rodent interaction datasets. Currently, the availability of such datasets is limited to two mouse datasets. We introduce the first, publicly available rat social interaction dataset, RatSI. Results: We demonstrate the practical value of the novel dataset by using it as the training set for a rat interaction recognition method. We show that behavior variations induced by the experiment setting can lead to reduced performance, which illustrates the importance of cross-dataset validation. Consequently, we add a simple adaptation step to our method and improve the recognition performance. Comparison with existing methods: Most existing methods are trained and evaluated in one experimental setting, which limits the predictive power of the evaluation to that particular setting. We demonstrate that cross-dataset experiments provide more insight in the performance of classifiers. Conclusions: With our novel, public dataset we encourage the development and validation of automated recognition methods. We are convinced that cross-dataset validation enhances our understanding of rodent interactions and facilitates the development of more sophisticated recognition methods. Combining them with adaptation techniques may enable us to apply automated recognition methods to a variety of animals and experiment settings. (C) 2017 Elsevier B.V. All rights reserved.",Not About Sufficiency
NATURAL PRODUCTS AS ANTIBACTERIAL AGENTS,"For thousands of years medicinal plants have played a significant role in the treatment of a wide range of medical conditions, including infectious diseases. Some naturally occurring chemical compounds serve as models for a large percentage clinically proven drugs, and many are now being re-assessed as antimicrobial agents. The primary reason for this renaissance is the fact that infectious disease remains a significant cause of morbidity and mortality worldwide, accounting for approximately 50% of all deaths in tropical countries and as much as 20% of deaths in the Americas. Despite the significant progress made in microbiology and the control of microorganisms, sporadic incidents of epidemics due to drug resistant microorganisms and previously unknown disease-causing microbes pose an enormous threat to global public health. These negative health trends call for a global initiative for the development of new strategies for the prevention and treatment of infectious disease, including natural products. Literally thousands of plant species have been tested against hundreds of bacterial strains in vitro, and many medicinal plants are active against a wide range of gram-positive and gram-negative bacteria. However, very few of these medicinal plant extracts have been tested in animal or human studies to determine safety and efficacy. This review focuses on the medicinal plants and phytochemical for which there is significant published in vitro, in vivo and clinical data available. The examples provided in this review such as St. John's wort, tree tea oil, and green tea demonstrate that even commonly used plant extracts may offer prospective new treatment of bacterial infections, including multi-drug resistant bacteria. One interesting example is St. John's wort (Hypericum perforatum) and its active constituent hyperforin, both of which have significant activity against MRSA in microgram concentrations. This antibacterial discovery was based on the ethnomedical use of St. John's wort to treat skin infections and wounds. Review of the published data indicates that medicinal plants offer significant potential for the development of novel antibacterial therapies and adjunct treatments (i.e. MDR pump inhibitors). However, new investigations should employ modern methodology, including using nationally recognized protocols and standards for microbial testing, the generation of minimum inhibitory concentrations, as well as standardization of the quality of plant materials used for testing.",Not About Sufficiency
Understanding Fluctuations in Public Opinion toward COVID-19 Vaccines: Insights from Social Media Analysis,"As of early 2019, the COVID-19 outbreak has ensued in millions of deaths, making it one of the worst pandemics in history. In addition to wearing masks, increasing sanitation, and avoiding crowds, widespread vaccination is crucial for preventing virus transmission. Despite significant progress in vaccine research and policy implementations, widespread immunization remains challenging. Analysis of exchanges on social media regarding COVID-19 vaccines has revealed significant uncertainty and mistrust in vaccines. As a result, ongoing evaluation of trust and confidence in COVID-19 vaccines is critical to crafting successful communication approaches for promoting extensive vaccination. This study aims to use content analysis of tweets about COVID-19 vaccines while also examining the user accounts generating them to provide evidence of fluctuations in public views toward COVID-19 vaccines. The proposed framework collects and processes data from social media networks, particularly Twitter, before presenting various analytics based on the different analyses performed through machine learning and deep learning algorithms. We hypothesize that a qualitative study starting from the pandemic would identify themes in public discourses (particularly those with negative sentiment or evidence of misleading information) that circulated during the developmental and mass release phases of COVID-19 vaccines. Therefore, it could inform and aid healthcare officials, public health agencies, and policymakers in increasing awareness and educational interventions for COVID-19 vaccines.",Not About Sufficiency
Exploring the Impact of Public Health Emergencies on Urban Vitality Using a Difference-In-Difference Model,"Urban vitality, a multifaceted construct, is influenced by economic conditions and urban structural characteristics, and can significantly be impacted by public health emergencies. While extensive research has been conducted on urban vitality, prevailing studies often rely on singular data sources, limiting the scope for holistic assessment. Moreover, there is a conspicuous absence of longitudinal analyses on urban vitality's evolution and a dearth of quantitative causal evaluations of the effects of public health emergencies. Addressing these gaps, this study devises a comprehensive framework for evaluating urban vitality, assessing Wuhan's vitality from 2018 to 2020 across economic, social, spatial, and ecological dimensions. Utilizing a Difference-In-Difference (DID) model, the impact of public health emergencies is quantified. The findings indicate pronounced spatial variations in Wuhan's urban vitality, with a gradational decline from the city center; public health emergencies exhibit differential impacts across vitality dimensions, detrimentally affecting economic, social, and spatial aspects, while bolstering ecological vitality. Moreover, high population and high public budget revenue are identified as factors enhancing urban vitality and bolstering the city's resilience against sudden adversities. This study offers valuable insights for geographers and urban planners, contributing to the refinement of urban development strategies.",Not About Sufficiency
A Technical Tool for Urban Upgrading: An Application for Cultural Heritage Preservation and Planning for Affordable Housing,"A technical tool to support projects for urban reforms and the implementation of current land policies is presented together with an example for its application in a project concerning the preservation of privately-owned residential buildings listed as ""protected"" cultural heritage (CH) constructions, urban planning and planning for affordable housing provision. The projects should be based on the voluntary participation of current property owners and an agreement signed between them and the private developer (team of professionals), so that the project will be self-financed through value capture measures to be decided by the state. The application presented here is based on the assumption that the state, by example, has set the rules for an increase in FAR in order to apply affordable housing policy and the preservation of listed CH residential private constructions. The state also provides the rules for identifying the target group of beneficiaries for affordable housing. Current property owners contribute the land, while the developer's team undertakes all project costs. New property units are shared accordingly with the developer, the current owners and the beneficiaries for affordable housing. No additional public funds for the ""affordable housing"" units or for expropriation of the protected CH buildings is required.",Not About Sufficiency
"Extreme events, energy security and equality through micro- and macro-levels: Concepts, challenges and methods","Low-income households face long-standing challenges of energy insecurity and inequality (EII). During extreme events (e.g., disasters and pandemics) these challenges are especially severe for vulnerable populations reliant on energy for health, education, and well-being. However, many EII studies rarely incorporate the micro- and macro-perspectives of resilience and reliability of energy and internet infrastructure and social-psychological factors. To remedy this gap, we first address the impacts of extreme events on EII among vulnerable populations. Second, we evaluate the driving factors of EII and how they change during disasters. Third, we situate these inequalities within broader energy systems and pinpoint the importance of equitable infrastructure systems by examining infrastructure reliability and resilience and the role of renewable technologies. Then, we consider the factors influencing energy consumption, such as energy practices, socio-psychological factors, and internet access. Finally, we propose interdisciplinary research methods to study these issues during extreme events and provide recommendations.",Not About Sufficiency
Emergency Situation Awareness: Twitter Case Studies,"The Emergency Situation Awareness (ESA) system provides all-hazard situation awareness information for emergency managers using content gathered from the public Twitter API. It collects, filters and analyses Tweets from specific regions of interest in near-real-time, enabling effective alerting for unexpected incidents and monitoring of emergency events with results accessible via an interactive website. ESA was developed in close collaboration with users to ensure fitness-for-purpose for the tasks performed by emergency services agencies. ESA processes large volumes of Twitter data and identifies discussion threads, trends and hot topics using language models. A burst detector generates alerts for unusually high frequency words that are filtered using text mining techniques and machine learning algorithms to identify Tweets of interest to emergency managers. An overview of the ESA platform is presented along with example case studies of its use to detect earthquakes, identify bushfire events and provide all-hazard monitoring in a crisis coordination centre.",Not About Sufficiency
Treatment System Adaptations during War: Lessons from Ukrainian Addiction Treatment Providers,"Background: The war in Ukraine has posed significant challenges to the healthcare system. This paper draws upon expert consultations, held between December 2022 and February 2023, focused on HIV/AIDS, addiction, and mental health service delivery during the first year of this war, and following the Global Mental Health Humanitarian Coalition panel discussion in May 2022. Objectives: This commentary presents the experiences of frontline healthcare workers in Ukraine, challenges, and local adaptations to meet the increased mental health needs of healthcare providers. We aimed to document the adaptations made in the addiction healthcare system and to acknowledge the changes in vulnerabilities and lessons learned. Results: Burnout among healthcare providers delivering addiction, HIV/AIDS and mental health services became more visible after the second half of 2022. Challenges included increased workload, contextual threats, lack of job relocation strategies, and money-follow-the-patient policies. Recommendations: The lessons from the first year of war in Ukraine hold significant generalizability to other contexts. These include enabling bottom-up approaches to tailoring services and allowing healthcare providers to respond to the dynamics of war in an effective and active manner. Other recommendations include departmental-specific resources and strategies, particularly as vulnerable groups and challenges are unstable in humanitarian contexts. Conclusions: Globally and in Ukraine, healthcare workers need more than applause. Along with monetary incentives, other strategies to prevent burnout, ensure sustainable capacity building, job relocation opportunities, and bespoke adaptations are imperative to protect healthcare providers' wellbeing and overall public health.",Not About Sufficiency
Comparing Natural Language Processing and Structured Medical Data to Develop a Computable Phenotype for Patients Hospitalized Due to COVID-19: Retrospective Analysis,"Background: Throughout the COVID-19 pandemic, many hospitals conducted routine testing of hospitalized patients for SARS-CoV-2 infection upon admission. Some of these patients are admitted for reasons unrelated to COVID-19 and incidentally test positive for the virus. Because COVID-19-related hospitalizations have become a critical public health indicator, it is important to identify patients who are hospitalized because of COVID-19 as opposed to those who are admitted for other indications.Objective: We compared the performance of different computable phenotype definitions for COVID-19 hospitalizations that use different types of data from electronic health records (EHRs), including structured EHR data elements, clinical notes, or a combination of both data types. Methods: We conducted a retrospective data analysis, using clinician chart review-based validation at a large academic medical center. We reviewed and analyzed the charts of 586 hospitalized individuals who tested positive for SARS-CoV-2 in January 2022. We used LASSO (least absolute shrinkage and selection operator) regression and random forests to fit classification algorithms that incorporated structured EHR data elements, clinical notes, or a combination of structured data and clinical notes. We used natural language processing to incorporate data from clinical notes. The performance of each model was evaluated based on the area under the receiver operator characteristic curve (AUROC) and an associated decision rule based on sensitivity and positive predictive value. We also identified top words and clinical indicators of COVID-19- specific hospitalization and assessed the impact of different phenotyping strategies on estimated hospital outcome metrics.Results: Based on a chart review, 38.2% (224/586) of patients were determined to have been hospitalized for reasons other than COVID-19, despite having tested positive for SARS-CoV-2. A computable phenotype that used clinical notes had significantly better discrimination than one that used structured EHR data elements (AUROC: 0.894 vs 0.841; P<.001) and performed similarly to a model that combined clinical notes with structured data elements (AUROC: 0.894 vs 0.893; P=.91). Assessments of hospital outcome metrics significantly differed based on whether the population included all hospitalized patients who tested positive for SARS-CoV-2 or those who were determined to have been hospitalized due to COVID-19.Conclusions: These findings highlight the importance of cause-specific phenotyping for COVID-19 hospitalizations. More generally, this work demonstrates the utility of natural language processing approaches for deriving information related to patient hospitalizations in cases where there may be multiple conditions that could serve as the primary indication for hospitalization.",Not About Sufficiency
Expanding eVision's Scope of Influenza Forecasting,"According to the United States Center for Disease Control and Prevention (CDC) between 39 and 56 million people in the United States experienced flu like symptoms in the 2019-20 flue season. From which, 410 to 740 thousand were hospitalized and 24 to 62 thousand (most of them children or elderly) succumbed to the disease. Hence, the presence of an early warning mechanism that can alert pharmaceuticals, hospitals, and governments to the trends of the influenza season, would serve as a significant step in helping combat communicable diseases and reduce the mortality of child under the age of five. Both of which are among the targets for the 3rd United Nations (UN) Sustainable Development Goal (SDG): to ensure healthy lives and promote well-being for all at all ages. As reported in the [ACM Special Interest Group in Computers and Society (SIGCAS) 2020 Computers and Sustainable Societies (COMPASS)] and [IEEE Technology and Engineering Management Society (TEMS) 2020 International Conference on Artificial Intelligence for Good (AI4G)] Long Short-Term Memory (LSTM) neural networks are utilized by Santa Clara University's EPIC (Ethical, Pragmatic, and Intelligent Computing) and BioInnovation & Design laboratories for continued research and development of an eVision (Epidemic Vision) tool to predict the trend of influenza cases throughout the flu season. There we reported eVision's success in making 3 to 7 weeks in advance predictions for the 2018-2019 United States flu season with 90.15% accuracy on 7 week predictions and delineated future steps of 1) expanding eVision's scope to study the effects of augmenting predictions with concurrent data from neighboring, near by, and developmentally similar countries/states with similar environmental conditions and 2) the introduction of confidence intervals for the predictions in order to account for the average error and thus increase the trustworthiness of eVision's results. This paper is to report that as a result of those steps, both the Californian and Chilean 7 week forecasts improved by 1.98% and 7.89% respectively.",Not About Sufficiency
Three-Dimensional Inverse Opal TiO2 Coatings to Enable the Gliding of Viscous Oils,"As a result of the increasing emphasis on accessing unconventional deposits of heavy oil and bitumen to meet global energy needs, there is an intense focus on addressing the rheological challenges involved in the transportation, handling, and processing of viscous hydrocarbons. While the design of superhydrophobic surfaces has been extensively explored, the fabrication of surfaces nonwetted by low-surface-tension and high-viscosity oils that can be scaled to meet industrial needs remains to be adequately addressed. Here, we demonstrate that colloidally templated architectures of TiO2 particles applicable through a facile spray deposition process can form 3D inverse opal coatings adhered to low-alloy steels. Low-temperature sintering induces necking of particles, giving rise to an interconnected framework of plastrons surrounded by necked TiO2 ligaments. Surface functionalization with 1H,1H,2H,2H-perfluorooctanephosphonic acid yields a helical surface monolayer with pendant trifluoromethyl moieties. The combination of interconnected plastrons, re-entrant curvature, and low surface energy suspends liquid droplets, of both water and heavy oil, in the Cassie-Baxter regime, yielding contact angles of 164 degrees +/- 5 degrees and 161 degrees +/- 2 degrees, respectively. The interconnected network of plastrons further enables the facile gliding of heavy oil (<100 s) upon immersion within a bath, whereas a comparable untreated surface remains completely fouled. The performance of this coating suggests a promising solution to mitigate the challenges of handling viscous oils in midstream applications and furthermore delineates a route to designing coatings for a broad host of rheologically challenging fluids.",Not About Sufficiency
Data Link for the Creation of Digital Twins,"A digital twin offers various features such as visualizations, simulations with real data, and performance monitoring. Several of these features or parts of them can be implemented employing existing systems storing product data. However, to create a digital twin, these data being scattered to different systems is needed to be merged. For merging data and straightforward linking of these systems, this paper proposes a Data Link. The Data Link offers a single interface to access the systems via an API (Application Programming Interface) gateway and makes all data of the physical product available and accessible. In addition, it stores metadata about the systems and offers a user interface that allows searching the system that contains the required piece of information or implements the needed feature. The Data Link was implemented for an industrial overhead crane bringing operational data, control, and CAD (computer-aided design) models behind a single interface. The example implementation indicated that the Data Link based architecture for digital twins allows easy implementation of digital twins by using existing systems.",Not About Sufficiency
Leveraging Traditional Design for Reliability Techniques for Artificial Intelligence,"Across academia, industry, and government there has been a resurgence of interest and development of Artificial Intelligence (AI) as it emerges from a second winter. With advances in microchips, networks, sensors, and data storage, AI and Machine Learning (ML) are becoming more accessible and feasible for utilization in fielded products to assist end Users with faster decision making and lightened cognitive load. However, with these advances in technology there is also concern of deploying products that may behave in unpredictable, erratic - or, in general, unreliable ways. These concerns, among others, have been voiced by boards, organizations, and government appointed commissions such as the Defense Innovation Board (DIB) and National Security Commission on AI (NSCAI). Even at the highest levels of leadership, the Secretary of Defense has stated that ""Our development, deployment, and use of AI must always be responsible, equitable, traceable, reliable, and governable"" [1]. All the voices and reports have a common theme and request: a push for means and methods to ensure assurance of AI enabled systems, and ultimately trust from evaluators and users. The concepts of assurance and trust depend on the system exhibiting expected or desired behaviors. This can be closely correlated to the definition of reliability in an engineering sense: the probability of a system performing its intended function under expected conditions for a given period of time. While there have been pushes across government and academia through working groups and commissions to develop means to assure AI, it is imperative to derive lessons learned from how reliable and assured systems have been developed in the past. There is a vast compendium of knowledge of reliability that can and should be leveraged in the design and development of AI enabled systems. While the introduction of AI may bring with it new challenges, the basic tenets and principles ultimately are steadfast. The use of Design for Reliability (DfR) tools and techniques should be applied to design and development, just like for any other traditional system. It is understood that there may be some required augmentation or adaptation, but the intent of tools originally designed for traditional systems can be leveraged for AI enabled applications. The DfR tools reviewed in this paper will include the Failure Modes and Effects Analysis (FMEA), Reliability Block Diagram (RBD), Highly Accelerated Life Testing (HALT) and Physics of Failure Analysis (PoF). Successful and effective execution of these tools requires the integration of reliability engineers into design and development teams to truly influence the design and build in reliability. These tools have been demonstrated and recommended for use in the development of traditional systems and will be presented here as means to develop and design for reliability and assurance in AI enabled systems. The application of such tools and processes will begin to establish a framework to develop, qualify, and release reliable and assured AI.",Not About Sufficiency
Hybrid Machine Learning Approach to Zero-Inflated Data Improves Accuracy of Dengue Prediction,"Background Spatiotemporal dengue forecasting using machine learning (ML) can contribute to the development of prevention and control strategies for impending dengue outbreaks. However, training data for dengue incidence may be inflated with frequent zero values because of the rarity of cases, which lowers the prediction accuracy. This study aimed to understand the influence of spatiotemporal resolutions of data on the accuracy of dengue incidence prediction using ML models, to understand how the influence of spatiotemporal resolution differs between quantitative and qualitative predictions of dengue incidence, and to improve the accuracy of dengue incidence prediction with zero-inflated data.Methodology We predicted dengue incidence at six spatiotemporal resolutions and compared their prediction accuracy. Six ML algorithms were compared: generalized additive models, random forests, conditional inference forest, artificial neural networks, support vector machines and regression, and extreme gradient boosting. Data from 2009 to 2012 were used for training, and data from 2013 were used for model validation with quantitative and qualitative dengue variables. To address the inaccuracy in the quantitative prediction of dengue incidence due to zero-inflated data at fine spatiotemporal scales, we developed a hybrid approach in which the second-stage quantitative prediction is performed only when/where the first-stage qualitative model predicts the occurrence of dengue cases.Principal findings At higher resolutions, the dengue incidence data were zero-inflated, which was insufficient for quantitative pattern extraction of relationships between dengue incidence and environmental variables by ML. Qualitative models, used as binary variables, eased the effect of data distribution. Our novel hybrid approach of combining qualitative and quantitative predictions demonstrated high potential for predicting zero-inflated or rare phenomena, such as dengue.Significance Our research contributes valuable insights to the field of spatiotemporal dengue prediction and provides an alternative solution to enhance prediction accuracy in zero-inflated data where hurdle or zero-inflated models cannot be applied. In our study, we tackled the complex challenge of predicting dengue fever outbreaks, a crucial task in the field of epidemiology. Dengue prediction is complicated because it relies on the quality of data, which may be affected by the temporal and spatial resolution. We explored different machine learning algorithms across various spatial (village, city and region) and temporal resolutions (weekly and monthly). A key hurdle we encountered was the high frequency of zero values in reported dengue cases, a common issue known as zero-inflated data. This phenomenon makes accurate predictions difficult, especially at finer resolutions. To overcome this obstacle, we first made qualitative predictions about the presence or absence of dengue cases. Then, in scenarios indicating disease presence, we estimated the magnitude of cases quantitatively. This innovative method we designated as hybrid approach and significantly enhanced prediction accuracy in zero-inflated data. This approach can be applied to continuous data where zero-inflated or hurdle models cannot be applied. Our findings have broader implications beyond dengue prediction, shedding light on the challenges of dealing with zero-inflated data in various real-world situations. By improving our understanding of these complexities, our research contributes valuable insights that not only benefit scientists working in epidemiology but also have practical applications in public health strategies ensuring more effective and targeted interventions.",Not About Sufficiency
High-Level Multiplexing in Digital PCR with Intercalating Dyes by Coupling Real-Time Kinetics and Melting Curve Analysis,"Digital polymerase chain reaction (dPCR) is a mature technique that has enabled scientific breakthroughs in several fields. However, this technology is primarily used in research environments with high-level multiplexing, representing a major challenge. Here, we propose a novel method for multiplexing, referred to as amplification and melting curve analysis (AMCA), which leverages the kinetic information in real-time amplification data and the thermodynamic melting profile using an affordable intercalating dye (EvaGreen). The method trains a system composed of supervised machine learning models for accurate classification, by virtue of the large volume of data from dPCR platforms. As a case study, we develop a new 9-plex assay to detect mobilized colistin resistant genes as clinically relevant targets for antimicrobial resistance. Over 100,000 amplification events have been analyzed, and for the positive reactions, the AMCA approach reports a classification accuracy of 99.33 +/- 0.13%, an increase of 10.0% over using melting curve analysis. This work provides an affordable method of high-level multiplexing without fluorescent probes, extending the benefits of dPCR in research and clinical settings.",Not About Sufficiency
Remote Blood Oxygen Estimation From Videos Using Neural Networks,"Peripheral blood oxygen saturation (SpO(2)) is an essential indicator of respiratory functionality and received increasing attention during the COVID-19 pandemic. Clinical findings show that COVID-19 patients can have significantly low SpO(2) before any obvious symptoms. Measuring an individual's SpO(2) without having to come into contact with the person can lower the risk of cross contamination and blood circulation problems. The prevalence of smartphones has motivated researchers to investigate methods for monitoring SpO(2) using smartphone cameras. Most prior schemes involving smartphones are contact-based: They require using a fingertip to cover the phone's camera and the nearby light source to capture reemitted light from the illuminated tissue. In this paper, we propose the first convolutional neural network based noncontact SpO(2) estimation scheme using smartphone cameras. The scheme analyzes the videos of an individual's hand for physiological sensing, which is convenient and comfortable for users and can protect their privacy and allow for keeping face masks on. We design explainable neural network architectures inspired by the optophysiological models for SpO(2) measurement and demonstrate the explainability by visualizing the weights for channel combination. Our proposed models outperform the state-of-the-art model that is designed for contact-based SpO(2) measurement, showing the potential of the proposed method to contribute to public health. We also analyze the impact of skin type and the side of a hand on SpO(2) estimation performance.",Not About Sufficiency
"Reality of compact development in a developing country: focusing on perceived quality of life in Jakarta, Indonesia","Recent urban planning tends to encourage higher density assuming that density would increase urban quality of life (UQoL) and sustainability. However, in the developing world, numerous primate cities are already highly densified. For example, Jakarta's density is over 1,500 people/ha. The population grew from 6.5 million in 1980 to 10 million in 2015. Jakarta's urban area also expanded to neighbouring jurisdictions including Bogor, Depok, Tangerang, and Bekasi and became a metropolitan area with 23 million people. Although Jakarta is dense, it is inefficient, causing Jakarta to encourage more compact development to improve quality of life (QoL) and sustainability from 2005. Yet, the relationship between compactness and QoL has remained unquestioned. This study aimed to investigate the perceived QoL of citizens living in three types of neighbourhood - high density and high mixed-use (HH), high density and low mixed-use (HL), and low density and high mixed-use (LH). This study finds that residents in LH find QoL to be higher compared with other types of residents. It also reveals that mixed use plays a more important role in improving residents' QoL than does density. In addition, social cohesion and inter-personal relationships are more likely to occur in LH. High density tends to lead to lower QoL. This study calls for revisiting our beliefs in high density and more in-depth studies on the relationship between urban form and QoL.",Not About Sufficiency
NfL predicts relapse-free progression in a longitudinal multiple sclerosis cohort study,"Background: Easily accessible biomarkers enabling the identification of those patients with multiple sclerosis (MS) who will accumulate irreversible disability in the long term are essential to guide early therapeutic decisions. We here examine the utility of serum neurofilament light chain (sNfL) for forecasting relapse-free disability progression and conversion to secondary progressive MS (SPMS) in the prospective Neurofilament and longterm outcome in MS (NaloMS) cohort. Methods: The predictive ability of sNfL at Baseline and sNfL follow-up (FU)/ Baseline (BL) ratio with regard to disability progression was assessed within a development cohort (NaloMS, n=19 6 patients with relapsing-remitting MS (RRMS) or clinically isolated syndrome) and validated with an external independent cohort (Du euro sseldorf, Essen, n=2 04). Both relapse-free EDSS-progression (RFP: inflamma-tory-independent EDSS-increase 12 months prior to FU) and SPMS-transition (minimum EDSS-score of 3.0) were investigated. Findings: During the study period, 17% (n=34) of NaloMS patients suffered from RFP and 14% (n=27) converted to SPMS at FU (validation cohort RFP n=42, SPMS-conversion n=24). sNfL at BL was increased in patients with RFP (10.8 pg/ml (interquartile range (IQR) 7.7-15.0) vs. 7.2 pg/ml (4.5-12.5), p<0.017). In a multivariable logistic regression model, increased sNfL levels at BL (Odds Ratio (OR) 1.02, 95% confidence interval (CI) 1.01-1.04, p=0.012) remained an independent risk factor for RFP and predicted individual RFP risk with an accuracy of 82% (NaloMS) and 83% (validation cohort) as revealed by support vector machine. In addition, the sNfL FU/BL ratio was increased in SPMS-converters (1.16 (0.89-1.70) vs. 0.96 (0.75-1.23), p=0.011). This was confirmed by a multivariable logistic regression model, as sNfL FU/BL ratio remained in the model (OR 1.476, 95%CI 1.078-2,019, p=0.015) and individual sNfL FU/BL ratios showed a predictive accuracy of 72% in NaloMS (63% in the validation cohort) as revealed by machine learning. Interpretation: sNfL levels at baseline predict relapse-free disability progression in a prospective longitudinal cohort study 6 years later. While prediction was confirmed in an independent cohort, sNfL further discriminates patients with SPMS at follow-up and supports early identification of patients at risk for later SPMS conversion. (C) 2021 The Author(s). Published by Elsevier B.V.",Not About Sufficiency
Machine Learning Approaches Reveal That the Number of Tests Do Not Matter to the Prediction of Global Confirmed COVID-19 Cases,"Coronavirus disease 2019 (COVID-19) has developed into a global pandemic, affecting every nation and territory in the world. Machine learning-based approaches are useful when trying to understand the complexity behind the spread of the disease and how to contain its spread effectively. The unsupervised learning method could be useful to evaluate the shortcomings of health facilities in areas of increased infection as well as what strategies are necessary to prevent disease spread within or outside of the country. To contribute toward the well-being of society, this paper focusses on the implementation of machine learning techniques for identifying common prevailing public health care facilities and concerns related to COVID-19 as well as attitudes to infection prevention strategies held by people from different countries concerning the current pandemic situation. Regression tree, random forest, cluster analysis and principal component machine learning techniques are used to analyze the global COVID-19 data of 133 countries obtained from the Worldometer website as of April 17, 2020. The analysis revealed that there are four major clusters among the countries. Eight countries having the highest cumulative infected cases and deaths, forming the first cluster. Seven countries, United States, Spain, Italy, France, Germany, United Kingdom, and Iran, play a vital role in explaining the 60% variation of the total variations by us of the first component characterized by all variables except for the rate variables. The remaining countries explain only 20% of the variation of the total variation by use of the second component characterized by only rate variables. Most strikingly, the analysis found that the variable number of tests by the country did not play a vital role in the prediction of the cumulative number of confirmed cases.",Not About Sufficiency
Sociodemographic predictors of COVID-19 vaccine acceptance: a nationwide US-based survey study,"Objectives: Acceptance of COVID-19 vaccination is attributable to sociodemographic factors and their complex interactions. Attitudes towards COVID-19 vaccines in the United States are changing frequently, especially since the launch of the vaccines and as the United States faces a third wave of the pandemic. Our primary objective was to determine the relative influence of sociodemographic predictors on COVID-19 vaccine acceptance. The secondary objectives were to understand the reasons behind vaccine refusal and compare COVID-19 vaccine acceptance with influenza vaccine uptake. Study design: This was a nationwide US-based survey study. Methods: A REDCap survey link was distributed using various online platforms. The primary study outcome was COVID-19 vaccine acceptance (yes/no). Sociodemographic factors, such as age, ethnicity, gender, education, family income, healthcare worker profession, residence regions, local healthcare facility and 'vaccine launch' period (pre vs post), were included as potential predictors. The differences in vaccine acceptance rates among sociodemographic subgroups were estimated by Chi-squared tests, whereas logistic regression and neural networks computed the prediction models and determined the predictors of relative significance. Results: Among 2978 eligible respondents, 81.1% of participants were likely to receive the vaccine. All the predictors demonstrated significant associations with vaccine acceptance, except vaccine launch period. Regression analyses eliminated gender and vaccine launch period from the model, and the machine learning model reproduced the regression result. Both models precisely predicted individual vaccine acceptance and recognised education, ethnicity and age as the most important predictors. Fear of adverse effects and concern with efficacy were the principal reasons for vaccine refusal. Conclusions: Sociodemographic predictors, such as education, ethnicity and age, significantly influenced COVID-19 vaccine acceptance, and concerns of side-effects and efficacy led to increased vaccine hesitancy. (C) 2021 The Royal Society for Public Health. Published by Elsevier Ltd. All rights reserved.",Not About Sufficiency
"Designing for happiness, building for resilience: a systematic review of key factors for cities","Cities today face the dual challenge of increasing vulnerability to disruptions and a growing demand for improved quality of life. By prioritising happiness and resilience, cities can create sustainable environments that promote health, social cohesion, and adaptability to various stresses and withstand economic, social, and environmental shocks while fostering the well-being, inclusivity, and quality of life of all residents. This study aims to understand what indicators most impact urban resilience and happiness through a systematic literature review. Our analysis uncovered that factors with a dual impact on improving urban resilience and happiness fit into the following categories: individual, social, socio-economic, environmental, infrastructural, technological, resource-based, place-based, urban planning, and housing-related. This study provided a comprehensive understanding of how urban design and policies can enhance both resilience and happiness in cities. Policymakers and planners can adapt and use the identified indicators and strategies to enhance citizens' happiness and create a resilient community in various contexts and under diverse stressors.",Not About Sufficiency
Quantifying uncertainty in machine learning classifiers for medical imaging,"Purpose Machine learning (ML) models in medical imaging (MI) can be of great value in computer aided diagnostic systems, but little attention is given to the confidence (alternatively, uncertainty) of such models, which may have significant clinical implications. This paper applied, validated, and explored a technique for assessing uncertainty in convolutional neural networks (CNNs) in the context of MI. Materials and methods We used two publicly accessible imaging datasets: a chest x-ray dataset (pneumonia vs. control) and a skin cancer imaging dataset (malignant vs. benign) to explore the proposed measure of uncertainty based on experiments with different class imbalance-sample sizes, and experiments with images close to the classification boundary. We also further verified our hypothesis by examining the relationship with other performance metrics and cross-checking CNN predictions and confidence scores with an expert radiologist (available in the Supplementary Information). Additionally, bounds were derived on the uncertainty metric, and recommendations for interpretability were made. Results With respect to training set class imbalance for the pneumonia MI dataset, the uncertainty metric was minimized when both classes were nearly equal in size (regardless of training set size) and was approximately 17% smaller than the maximum uncertainty resulting from greater imbalance. We found that less-obvious test images (those closer to the classification boundary) produced higher classification uncertainty, about 10-15 times greater than images further from the boundary. Relevant MI performance metrics like accuracy, sensitivity, and sensibility showed seemingly negative linear correlations, though none were statistically significant (p >= 0.05). The expert radiologist and CNN expressed agreement on a small sample of test images, though this finding is only preliminary. Conclusions This paper demonstrated the importance of uncertainty reporting alongside predictions in medical imaging. Results demonstrate considerable potential from automatically assessing classifier reliability on each prediction with the proposed uncertainty metric.",Not About Sufficiency
Introduction to Fairness in Algorithmic Decision Making mini-track,"A vast application of machine learning and decision-making algorithms for decision support in various areas of life caused the need for the algorithms to take into account additional constraints, such as non-discriminatory behavior or imposing fairness, or social welfare prior to proposing decisions to decision makers. These constraints can be fulfilled by carefully guiding the whole decision-making and data governance process, by adjusting decision-making, data mining and machine learning algorithms to fulfill additional constraints. For example, by adapting CRISP-DM methodology to account for possible biases, by imposing instance-dependent cost-sensitive learning, or enforcing equality in data envelopment analysis as presented in this mini-track.",Not About Sufficiency
"Advanced Digital Solutions for Food Traceability: Enhancing Origin, Quality, and Safety Through NIRS, RFID, Blockchain, and IoT","The rapid growth of the human population, the increase in consumer needs regarding food authenticity, and the sub-par synchronization between agricultural and food industry production necessitate the development of reliable track and tracing solutions for food commodities. The present research proposes a simple and affordable digital system that could be implemented in most production processes to improve transparency and productivity. The system combines non-destructive, rapid quality assessment methods, such as near infrared spectroscopy (NIRS) and computer/machine vision (CV/MV), with track and tracing functionalities revolving around the Internet of Things (IoT) and radio frequency identification (RFID). Meanwhile, authenticity is provided by a self-developed blockchain-based solution that validates all data and documentation ""from farm to fork"". The system is introduced by taking certified Hungarian sweet potato production as a model scenario. Each element of the proposed system is discussed in detail individually and as a part of an integrated system, capable of automatizing most production flows while maintaining complete transparency and compliance with authority requirements. The results include the data and trust model of the system with sequence diagrams simulating the interactions between participants. The study lays the groundwork for future research and industrial applications combining digital tools to improve the productivity and authenticity of the agri-food industry, potentially increasing the level of trust between participants, most importantly for the consumers.",Not About Sufficiency
Geographic Concentration and Correlates of Nursing Home Closures: 1999-2008,"Background: While demographic shifts project an increased need for long-term care for an aging population, hundreds of nursing homes close each year. We examine whether nursing home closures are geographically concentrated and related to local community characteristics such as the racial and ethnic population mix and poverty. Methods: National Online Survey Certification and Reporting data were used to document cumulative nursing facility closures over a decade, 1999 through 2008. Census 2000 zip code level demographics and poverty rates were matched to study facilities. The weighted Gini coefficient was used to measure geographic concentration of closures, and geographic information system maps to illustrate spatial clustering patterns of closures. Changes in bed supply due to closures were examined at various geographic levels. Results: Between 1999 and 2008, a national total of 1776 freestanding nursing homes closed (11%), compared with 1126 closures of hospital-based facilities (nearly 50%). Combined, there was a net loss of over 5% of beds. The relative risk of closure was significantly higher in zip code areas with a higher proportion of blacks or Hispanics or a higher poverty rate. The weighted Gini coefficient for closures was 0.55 across all metropolitan statistical areas and 0.71 across zip codes. Closures tended to be spatially clustered in minority-concentrated zip codes around the urban core, often in pockets of concentrated poverty. Conclusions: Nursing home closures are geographically concentrated in minority and poor communities. Since nursing home use among the minority elderly population is growing while it is declining among whites, these findings suggest that disparities in access will increase.",Not About Sufficiency
Fault Pattern Recognition in Power Distribution Integrated Network with Renewable Energy Source,"The challenge with most developing countries is to maintain a reliable and sustainable electricity supply. This has a depleting effect on the economic development of most states. In order to reduce the impact of energy shortages, there has been an extensive attempt to use renewable energy sources to generate electricity. There are however technical challenges of integrating the existing power system grid with the renewable external sources. These challenges include adequate power system protection, energy security, and reliability of external sources. In this paper, we investigate the fault pattern recognition and detection in a power distribution grid integrated with the wind energy source. A reduced Eskom 22kV and wind power energy source integrated is modeled using MATLAB/Simulink. From the integrated model, various types of power systems faults are generated. We further investigated the use of local polynomial approximation (LPA) for signal decomposition and support vector machine (SVM) for fault classification and detection. We also tested the performance of the naive Bayes classifier. In this paper, a hybrid technique based on LPA and SVM is proposed for fault pattern recognition and detection in a power distribution integrated system with the wind energy source. The proposed method was further tested using machine learning platforms WEKA and Orange. The results of the classifiers gave the accuracy of between 98 and 99 %.",Not About Sufficiency
Analysis of Walkable Street Networks by Using the Space Syntax and GIS Techniques: A Case Study of cankiri City,"Nowadays, city forms are changing due to rapid urbanization and increasing population. In urban morphology studies, walkable street network is examined through the city form to create sustainable cities. This study aims to examine accessibility of street network that shapes the city form by using central street line retrieved from OSM. Accessibility of the street network, one of the criteria of walkability, was evaluated in cankiri, a micro city in Turkey. The space syntax and GIS methods were used to examine the physical accessibility of the street network. As differences in the topography are not taken into consideration in the space syntax, it was integrated with the GIS in this study. With this method, spatial accessibility, the correlation between integration and choice values of street network, was examined at first. Secondly, land slope was classified according to the standards of pedestrian accessibility and the study area was analyzed using the GIS. Finally, streets with low slope percentage and high integration value were overlaid. The results revealed that the longest, continuous, and main axes located in the area with low slope and high integration values are accessible. The accessible streets obtained by a collaborative integration of the space syntax and GIS methods are lower than the area obtained just from the space syntax method. The use of a combination of these methods is beneficial in terms of understanding the land in three dimensions, but focusing on land surface slopes is only one of the possible synergies between the two tools. The walkable street network obtained by using this method gives an idea about urban mobility. While this method works with hilly lands, other GIS data may be needed for different land types. However, it should also be extended to multi-source information and quantitative analysis methods in bigger cities, as urban walkability is at the core of the 15-minute city model, which is of high actuality of the agenda of urban planning and sustainable urban development.",Not About Sufficiency
Pharmacological affinity fingerprints derived from bioactivity data for the identification of designer drugs,"Facing the continuous emergence of new psychoactive substances (NPS) and their threat to public health, more effective methods for NPS prediction and identification are critical. In this study, the pharmacological affinity fingerprints (Ph-fp) of NPS compounds were predicted by Random Forest classification models using bioactivity data from the ChEMBL database. The binary Ph-fp is the vector consisting of a compound's activity against a list of molecular targets reported to be responsible for the pharmacological effects of NPS. Their performance in similarity searching and unsupervised clustering was assessed and compared to 2D structure fingerprints Morgan and MACCS (1024-bits ECFP4 and 166-bits SMARTS-based MACCS implementation of RDKit). The performance in retrieving compounds according to their pharmacological categorizations is influenced by the predicted active assay counts in Ph-fp and the choice of similarity metric. Overall, the comparative unsupervised clustering analysis suggests the use of a classification model with Morgan fingerprints as input for the construction of Ph-fp. This combination gives satisfactory clustering performance based on external and internal clustering validation indices.",Not About Sufficiency
The elephant in the room is really a cow: using consumption corridors to define sustainable meat consumption in the European Union,"Implementing the European Green Deal requires a consistent food systems' policy that involves not only targeting the supply side but also conducting extensive changes in diets at the consumer level. Reducing meat consumption is an obvious strategy to put the European food system on track to meet the Green Deal's goals. This cannot be achieved by focusing solely on consumer choice and individual responsibility. Stronger governance is required to reduce the scale of meat consumption to sustainable levels. Such governance needs to be informed by a holistic definition of ""sustainable meat consumption"", designed to ensure that important sustainability priorities are not neglected, and to account for all emissions associated with EU consumption, regardless of where production takes place. This article presents a conceptual framework to define ""sustainable meat consumption"" based on the concept of consumption corridors (CCs). A CC is the space between a minimum (the floor) and maximum (the ceiling) consumption level, which allows everybody to satisfy their needs without compromising others' ability to meet their own. Embedded in a powerful set of principles (recognizing universal needs; tackling both over and under-consumption; framing food as a common good; promoting public participation; and addressing environmental justice and planetary sustainability), CCs are attuned to the Green Deal's ambition to ""leave no one behind"", in the EU and beyond. CCs provide a demand-side solution encompassing a more equitable alternative to discuss what is actually a ""fair share"" of the world's limited resources when it comes to meat consumption.",Not About Sufficiency
Public Transport Systems' Connectivity: Spatiotemporal Analysis and Failure Detection,"Public Transport (PT) plays a major role in passenger flow as an affordable and efficient mode contributing for sustainable transportation by way of traffic congestion and air pollution reduction. Those advantages are impaired if the PT system does not provides a continuous accessibility and connectivity for all prospect passengers. Hence, it is imperative to assess the performance of PT systems based on the system's spatial and temporal properties. For the failure detection, three connectivity indicators are being used: a) transportation network coverage (direct and indirect); and b) stop transfer potential. These indicators are used for the identification of connectivity issues and flaws. Each indicator provides the means to identify the causes in terms of network coverage, routes structure and coverage, stops locations, frequencies, and transfers synchronization. A case study of Dolo area, which is part of Veneto region (Italy), is introduced. The analysis is focused on the hospital connectivity. The current PT system is analyzed, followed by identifying connectivity failures, and improvements recommendations. Results show that connectivity to the hospital by PT is characterized by long ingress and egress distances, low frequencies, and lack of fast and efficient transfers. The local authorities can easily use the tool to pinpoint stops to be relocated, as well as time-tables change, all in order to increase the connectivity by PT to the hospital. (C) 2014 The Authors. Published by Elsevier B.V.",Not About Sufficiency
Self-Aware Wearable Systems in Epileptic Seizure Detection,"Today, wearable systems are facing fundamental barriers in terms of battery lifetime and quality of their results. The main challenge in wearable systems is to increase the battery lifetime, while maintaining the machine-learning performance of the system. A recently proposed concept for overcoming this challenge is self-awareness, which increases system's knowledge of itself and the surrounding environment. This is precisely what health monitoring wearable systems require to adapt to different situations. To demonstrate the impact of introducing self-awareness in wearable technologies, we consider the epileptic seizure detection problem, as a case study. Epilepsy affects around 1% of the world's population, which can dramatically degrade the quality of life and represents a major public health issue. As a result, detection of epileptic seizures has become more important over the past decades. In this paper, we aim to introduce a new generation of self-aware wearable systems to decrease energy consumption and improve their seizures detection capabilities by introducing the notion of self-awareness in such systems. These techniques include switching to low-power mode to reduce the energy consumption and machine-learning model enhancement to improve detection quality. We incorporated our proposed techniques in the machine learning module, which detects epileptic seizures by monitoring the cardiac and respiratory systems. We evaluated the performance of our approach based on an epilepsy database of more than 141 hours, provided by the Lausanne University Hospital (CHUV). Our self-aware wearable system achieves 36% reduction in computational complexity and 10.51% improvement in detection performance.",Not About Sufficiency
Settling in or moving out? Exploring the effect of mobility intentions on public housing exits,"This paper seeks to understand how public housing residents' mobility intentions affect their actual exits. The results suggest that mobility intentions do have a significant effect on public housing exits. However, the rate of exit among those who intend to move out of public housing was similar to those who did not intend to leave. In addition, tenure had a significant effect on the odds of exiting alluding to issues of duration dependence. However, neighbourhood conditions did not fully explain public housing exits. Our proxy for policy reform had a large effect on the odds of exiting of public housing. This result suggests that changes in housing assistance programmes and urban housing policy could largely account for public housing exits. Overall, the results imply that while public housing residents may have positive and negative mobility intentions, their exits may primarily be due to shifts in housing policy and social welfare programmes versus individual characteristics and neighbourhood conditions.",Not About Sufficiency
Embodied geographies of liveability and urban parks,"Urban parks are currently enshrined within liveable forms of sustainable urban planning for high-density city living. This article draws on Giles Deleuze and Felix Guattari's idea of territory to critically explore the embodied geographies of liveability. The concept of territory draws attention to the emplacement of subjectivities constituted not only through the discursive but also the emotional and affectual forces or flows between and through bodies and proximate objects. We argue that the embodied geographies of liveability are both performed and folded through the emotional and affectual circulations flowing through the body. To investigate these performances, flows and connections, an affective mapping exercise of urban park visits was conducted with 18 apartment dwellers in Wollongong, New South Wales, Australia. We address the concerns about social exclusion raised by the agenda of liveable cities and how the concept of 'territory' offers policy-relevant conclusions.",Not About Sufficiency
The Swedish Strategy and Method for Development of a National Healthcare Information Architecture,"We need a precise framework of regulations in order to maintain appropriate and structured health care documentation that ensures that the information maintains a sufficient level of quality to be used in treatment, in research and by the actual patient. The users shall be aided by clearly and uniformly defined terms and concepts, and there should be an information structure that clarifies what to document and how to make the information more useful. Most of all, we need to standardize the information, not just the technical systems."" (eHalsa - nytta och naring, Riksdag report 2011/12: RFR5, p. 37). In 2010, the Swedish Government adopted the National e-Health - the national strategy for accessible and secure information in healthcare. The strategy is a revision and extension of the previous strategy from 2006, which was used as input for the most recent efforts to develop a national information structure utilizing business-oriented generic models. A national decision on healthcare informatics standards was made by the Swedish County Councils, which decided to follow and use EN/ISO 13606 as a standard for the development of a universally applicable information structure, including archetypes and templates. The overall aim of the Swedish strategy for development of National Healthcare Information Architecture is to achieve high level semantic interoperability for clinical content and clinical contexts. High level semantic interoperability requires consistently structured clinical data and other types of data with coherent traceability to be mapped to reference clinical models. Archetypes that are formal definitions of the clinical and demographic concepts and some administrative data were developed. Each archetype describes the information structure and content of overarching core clinical concepts. Information that is defined in archetypes should be used for different purposes. Generic clinical process model was made concrete and analyzed. For each decision-making step in the process where information is processed, the amount and type of information and its structure were defined in terms of reference templates. Reference templates manage clinical, administrative and demographic types of information in a specific clinical context. Based on a survey of clinical processes at the reference level, the identification of specific clinical processes such as diabetes and congestive heart failure in adults were made. Process-specific templates were defined by using reference templates and populated with information that was relevant to each health problem in a specific clinical context. Throughout this process, medical data for knowledge management were collected for each health problem. Parallel with the efforts to define archetypes and templates, terminology binding work is on-going. Different strategies are used depending on the terminology binding level.",Not About Sufficiency
A New Calibration Strategy for Transfer-Power Measurement of Wireless Charging of Electric Vehicles,"Accurate measurement is a hinge pin of a functioning society, engendering a sense of value, fairness, trust, and safety. A particularly salient example is wireless energy transfer to electric vehicles. A robust calibration method is needed for the fair and accurate measurement of the wireless power transferred to electric vehicles using the TPM (Transfer-Power Measurement) method. A prevalent obstacle to affordable, reliable, and accurate measurements in the field is lack of suitable transfer standards. This paper presents both a new transfer standard and a calibration path for TPM towards the standardization of power and energy metering in wireless power transfer.",Not About Sufficiency
The supporting role of Artificial Intelligence and Machine/Deep Learning in monitoring the marine environment: a bibliometric analysis,"The widespread interest towards a sustainable and effective monitoring of the environment is increasingly demanding the development of modern and more affordable technologies to support or even replace the traditional time-consuming, high -cost sampling surveys at a multi -scale level. Researchers are highly benefitting from the recent enormous progresses achieved in the Artificial Intelligence (AI) field, with Machine/Deep Learning (ML/DL) applications increasing at sight. This gives a remarkable contribution to the environmental monitoring at sea, further allowing to develop efficient, smart and low-cost solutions to support the wide variety of tasks dealing with this objective. This study explores the global scientific literature on AI and ML/DL applications for the environmental monitoring over the last years. The VOSviewer software has been used to create maps based on the bibliographic network data: this allowed to display the relationships among scientific journals, researchers, and countries and to analyze the co -occurrence of different terms connected to the research. The resulting bibliometric analysis aims at verifying the major research interests and at providing the community with interesting findings and new perspectives on this very important topic, highlighting the great potential and flexibility of these methodologies and the excellent achievements they obtained in the last years.",Not About Sufficiency
Accessing green space in Melbourne: Measuring inequity and household mobility,"Economically disadvantaged communities living in neighborhoods with low access to green space are known to experience a heightened burden of health issues, leading to intergenerational well-being problems. However, relatively little is known about the extents and causes of green space inequalities among different social communities. To explore this in the metropolitan Melbourne area, we use the 2016 Census data of equivalised household income and calculate local indicators of spatial association (LISA) between low-income proportion and green space access at a suburb level. We show that the distribution of green space in Melbourne favors more affluent communities, meaning that there are lower concentrations of low-income households in greener areas. The Mann-Whitney U statistics applied to LISA clusters also indicates statistically significant inequity in access to green space for low-income communities. Secondly, the paper shows that low-income households' relocation and provision of human-modified green space exacerbate inequality in green space access over time. Mobility patterns show the movement of low-income people from high-green areas to low-green areas over time. The spatial analysis of green space types reveals that the location of human-modified green spaces has a significant correlation with (non-randomly distributed) natural green spaces.",Not About Sufficiency
Data-driven design of inorganic materials with the Automatic Flow Framework for Materials Discovery,"The expansion of programmatically accessible materials data has cultivated opportunities for data-driven approaches. Workflows such as the Automatic Flow Framework for Materials Discovery not only manage the generation, storage, and dissemination of materials data, but also leverage the information for thermodynamic formability modeling, such as the prediction of phase diagrams and properties of disordered materials. In combination with standardized parameter sets, the wealth of data is ideal for training machine-learning algorithms, which have already been employed for property prediction, descriptor development, design rule discovery, and the identification of candidate functional materials. These methods promise to revolutionize the path to synthesis, and ultimately transform the practice of traditional materials discovery to one of rational and autonomous materials design.",Not About Sufficiency
Improving the diagnosis of myocardial infarction with machine learning,"Machine learning models that integrate cardiac troponin concentrations and clinical features to compute the probability of myocardial infarction outperform current care pathways that use fixed troponin thresholds or risk scores. Adoption of these models could reduce inequalities, prevent unnecessary admissions, and accelerate the diagnosis and treatment of patients with myocardial infarction.",Not About Sufficiency
Lifetime health costs of intimate partner violence: A prospective longitudinal cohort study with linked data for out-of-hospital and pharmaceutical costs,"The health effects of intimate partner violence (IPV) can be long term, developing years after the IPV began and persisting after it has ceased. We aim to quantify the excess lifetime out-of-hospital and pharmaceutical health costs of women who experience IPV in Australia by applying a novel combination of econometric and actuarial techniques to a large and unique dataset. We find that women with a history of IPV have AUD48,413 (2020) higher lifetime health costs per person than women who do not experience IPV. This suggests that the adverse health impact of IPV leads to increased health costs over one's lifetime regardless of whether the initial IPV experience is early or later in life, and policies reducing the incidence of IPV will have long-term impacts on the healthcare system.",Not About Sufficiency
"Furries, freestylers, and the engine of social change: The struggle for recognition in a mediatized world","This article merges the 'terminologies of social change' from recognition theory and mediatization research to argue that the mediatization of society has eased and accelerated processes of what recognition theorist Axel Honneth calls individualization and social inclusion. This, however, cannot be understood unambiguously as moral progress. Thus, the first part of the article outlines the conceptualization of social change in Honneth's recognition-theoretical framework, including the critique of recognition theory's account of power, which problematizes Honneth's inherent idea of moral progress. Considering this critique, the article follows Douglas Giles' suggestion that we must distinguish between affirmational and transformational struggles for recognition. The discussion is subsequently related to the conceptualization of social change in mediatization research, which shows that the closely interrelated phenomena of a social relocation of media and a re-orientation of the self help facilitate and constitute the individual's increased possibilities, but also dependence, with respect to intersubjective recognition from peer groups in many contemporary societies. This provides the starting point for exemplifying how the conditions for affirmational and transformational recognition struggles, and thus individualization and social inclusion, have changed in a mediatized world. This exemplification draws primarily on an in-depth case study of mediated recognition in the lives of four young Danes aged 15 to 30 years with different subcultural identity positions. The main empirical illustrations involve the experiences of the 22-year-old hijabi football freestyler Maarika, who uses her Muslim identity actively on her social media channels to be a role model and to counter negative stereotypes of Muslims and female footballers.",Not About Sufficiency
Machine Learning-Based Indices Assessing Different Aspects of Beta-Cell Function in Pregnancy,"Accurate assessment of pancreatic beta-cell function parameters, such as glucose sensitivity (G-Sens), rate sensitivity (R-Sens), and potentiation factor ratio (PFR), relies on mathematical modelling coupled with C-peptide measurement, both not always accessible in clinical settings. Machine learning may provide surrogate markers of model-and-C-peptide-based parameters. Aim of the study was to leverage machine learning to build predictive equations of G-Sens, R-Sens, and PFR in pregnant women, without the need of modeling and C-peptide. To this aim, predictive approaches were implemented (multivariate polynomial regressions), under different scenarios of data availability. We found that G-Sens prediction showed good performance (R-adj(2) = 0.45, p<0.0001 in test set), whereas results were unsatisfactory for R-Sens. PFR prediction showed moderate performance (R-adj(2) = 0.33, p < 0.01 in test set). In conclusion, machine learning is appropriate for G-Sens and PFR prediction, while R-Sens prediction appears hardly feasible.",Not About Sufficiency
"Forgotten whales, fading codfish: Perceptions of 'natural' ecosystems inform visions of future recovery","Perceptions of past ecological change affect views of current ecosystem state, but how do baselines help to shape stakeholders' visions of an idealized future?Here, we investigate links between perceptions of natural baselines and visions for the nearshore Gulf of Maine among a key stakeholder group, active lobster fishers. We ask three related questions: (1) What do fishers perceive as a 'natural' Gulf of Maine? (2) How do perceptions of the past predict individual and collective visions of an ideal future? and (3) How is existing management perceived as supporting these visions?We found that fishers perceived the ecosystem to be 'natural' an average of one decade before they started fishing. Three species dominated views of natural systems: cod Gadus morhua, lobster Homarus americanus, and herring Clupea harengus, but while long-time fishers associated abundant cod with a natural nearshore Gulf of Maine, memories of a historically cod-rich Gulf of Maine were fading among some younger fishers who began their careers after the cod crash in the 1990s. Perceptions of 'natural' ecosystems dictated future visions for the majority of taxa; on average, fishers remembered and desired abundant cod and herring, but perceived halibut Hippoglossus hippoglossus and endangered right whales Eubalaena glacialis to have always been rare.Fishers described a vision for the future based on views of past ecological and social baselines, including fisheries deconsolidation and diversification, but expressed a lack of shared vision with and trust in federal management institutions to achieve these goals. In particular, memories of cod abundance in the 1970s and 1980s were coupled with memories of a diversified and accessible fishery, but fishers doubted that the recovery of cod would result in their restored access to cod fisheries.Together our results demonstrate that past personal experiences limit perceptions of what is possible, highlighting both the value and limitations of local ecological knowledge in places that have experienced ecological change over centuries. They also demonstrate how stakeholder perceptions of both social and ecological baselines shape visions for future ecosystems but are mediated by contemporary issues like trust in institutions and fisheries access.",Not About Sufficiency
Determining Stingray Movement Patterns in a Wave-Swept Coastal Zone Using a Blimp for Continuous Aerial Video Surveillance,"Stingrays play a key role in the regulation of nearshore ecosystems. However, their movement ecology in high-energy surf areas remains largely unknown due to the notorious difficulties in conducting research in these environments. Using a blimp as an aerial platform for video surveillance, we overcame some of the limitations of other tracking methods, such as the use of tags and drones. This novel technology offered near-continuous coverage to characterise the fine-scale movements of stingrays in a surf area in Kiama, Australia, without any invasive procedures. A total of 98 stingray tracks were recorded, providing 6 h 27 min of movement paths. The tracking data suggest that stingrays may use a depth gradient located in the sandflat area of the bay for orientating their movements and transiting between locations within their home range. Our research also indicates that stingray behaviour was influenced by diel periods and tidal states. We observed a higher stingray occurrence during the afternoon, potentially related to foraging and anti-predatory strategies. We also saw a reduced route fidelity during low tide, when the bathymetric reference was less accessible due to stranding risk. Considering the increasing threat of anthropogenic development to nearshore coastal environments, the identification of these patterns can better inform the management and mitigation of threats.",Not About Sufficiency
From Privacy to Algorithms' Fairness,"This article aims to show how the legal and ethical debate - as far as ethics has become an indispensable complementary normative tool within legal frameworks - on the digital world in the United States (US) and the European Union (EU) has significantly opened up to include new dimensions other than privacy, particularly in connection with machine learning algorithms and Big Data. If privacy still remains the main interpretive construct to normatively forge the digital space, increasingly issues of discrimination, equal opportunity, fairness and, more broadly, models of justice, are entering the picture. While offering some examples of the inadequacy of privacy to cover new normative concerns related to Big Data and machine learning, the article also argues that attempts to grant algorithmic fairness represent just the first step in addressing the wider question about what models of digital justice we are willing to apply.",Not About Sufficiency
The Study of the Factors That Influence the Community to Survive Living in the Disaster Area,"Lapindo mud disaster areas (Zone 1) is an area region located at a radius of 0-1.5 Km from the Lapindo mud embankment and the area has been designated by Badan Penanggulangan Lumpur Sidoarjo (BPLS) as an area which not decent to live. There are seven inhabited village that includes areas of Zone 1 which is an area of this research, among others Kalitengah, Kedungbendo, Ketapang, Gedang, Mindi, Glagaharum and Besuki. This research aims to know what the factors are influencing the people to survive living in this area and know the living conditions of the people who lived in the area of Lapindo mud disaster. This type of research used in this study is a survey research. Location of the research conducted in villages including areas of Zone 1 Lapindo mud disaster. The population in this study is a community based on families who live in Zone 1. The sample was 100 households spread over 7 villages. Data collection techniques in this research is through interview, observation and documentation. Data collected each variable is then performed the scoring, then calculated the percentage and compared with the percentage of other variables. Data analysis techniques in this study using quantitative descriptive percentages. The results of this research about factors that affected people to living in the area of Lapindo mud disaster, shows that public perception that the land they occupy is the birthplace is dominant factors that influence (87%). Community work, which are mostly traders/entrepreneurs and trade around their neighborhood have an influence (74%), because such work they cannot leave and this work is for daily needs. The inexpensive price of compensation offered is also influenced people to stay living in the area Lapindo mud disaster (81%), besides the public perception about the high prices of houses in the relocation sites (67%) were also influential for people to still live in the area Lapindo mud disaster. Transportation also affects the people with a percentage of 56%, although this influence is the smallest of the others.",Not About Sufficiency
A Method of Coding Video Segments in Spectral-Cluster Space with Detection of Structural Features,"The directions for improving the efficiency of providing remote video services using the aviation info-communication segment in the process of support and decision-making in critical infrastructure systems are substantiated. It is justified that in order to localize or eliminate problematic aspects, it is necessary to apply technologies for reducing the bit intensity of the video stream. This will reduce the information load on the network and create conditions for the involvement of redundant code structures. Accordingly, it is necessary to integrate video data compression technologies for on-board systems. The effectiveness of clustering video segments in spectral space based on the structural feature of the number of series of units in the binary description of their components is substantiated. A method of statistical coding of transformants in structural space has been developed. The basic component here is the establishment of statistical dependencies within structural clusters, taking into account: local features of spectral components in their range based on structural features in their binary description; reduced power of the statistical space and increase the level of uneven distribution of the clustered components of the transformant. On the basis of the conducted experimental studies, it is shown that the use of the created method of encoding clustered transformants allows to increase the level of reliability of aerial photographs by the indicator of the peak signal/noise ratio by an average of 50%.",Not About Sufficiency
FabricNET: A Microscopic Image Dataset of Woven Fabrics for Predicting Texture and Weaving Parameters through Machine Learning,"This research presents an approach aimed at enhancing texture recognition and weaving parameter estimation in the textile industry to align with sustainability goals and improve product quality. By utilizing low-cost handheld microscopy and machine learning, this method offers the potential for more precise production outcomes. In this study, textile images were manually labeled for texture, specific mass, weft, and warp parameters, followed by the extraction of various texture features, resulting in a comprehensive dataset comprising four hundred and fifty-eight inputs and four outputs. Prominent machine learning algorithms, including XGBoost, RF, and MLP, were applied, resulting in noteworthy achievements. Specifically, XGBoost demonstrated an impressive texture classification accuracy of 0.987, while RF yielded the lowest MAE (5.121 g/cm) in specific mass prediction. Additionally, weft and warp estimations displayed superior accuracy compared to manual measurements. This research emphasizes the crucial role of AI in improving efficiency and sustainability within the textile industry, potentially reducing resource wastage, enhancing worker safety, and increasing productivity. These advancements hold the promise of significant positive environmental and social impacts, marking a substantial step forward in the industry's pursuit of its objectives.",Not About Sufficiency
ApIsoT: An IoT Function Aggregation Mechanism for Detecting Varroa Infestation in Apis mellifera Species,"In recent years, the global reduction in populations of the Apis mellifera species has generated a worrying deterioration in the production of essential foods for human consumption. This phenomenon threatens food security, as it reduces the pollination of vital crops, negatively affecting the health and stability of ecosystems. The three main factors generating the loss of the bee population are industrial agriculture, climate changes, and infectious diseases, mainly those of parasitic origin, such as the Varroa destructor mite. This article proposes an IoT system that uses accessible, efficient, low-cost devices for beekeepers in developing countries to monitor hives based on temperature, humidity, CO2, and TVOC. The proposed solution incorporates nine-feature aggregation as a data preprocessing strategy to reduce redundancy and efficiently manage data storage on hardware with limited capabilities, which, combined with a machine learning model, improves mite detection. Finally, an evaluation of the energy consumption of the solution in each of its nodes, an analysis of the data traffic injected into the network, an assessment of the energy consumption of each implemented classification model, and, finally, a validation of the solution with experts is presented.",Not About Sufficiency
"Investigating of transportation systems development for urban districts, costs and social equity: a case of Sanandaj, Kurdistan","The transportation system is one of the essential subjects for any country because it directly concerns the daily costs and social problems. Researchers have proposed different methods to develop transportation systems, each with its strengths and weaknesses. This paper has proposed a new bi-objective mathematical model based on three social equity theories, the first of which considers infrastructure development costs for a transportation system and includes operational, environmental, and pollution costs. The second objective focuses on enhancing social equity in investment distributions in the transportation system for a long time. Three theories, including Utilitarianism, Rawl's, and Sadr's, were modeled and used to assess social equity. The model was examined for an original case of data in Sanandaj city, and an improved version of the augmented e-constraint method was developed to solve it. According to the general results, each social equity theory has its own merits. The results show that Rawls' theory, with minimum infrastructure costs, establishes the best social equity (70%) after 5 years of application; therefore, the Sanandaj city would be a sustainable city based on Rawls' theory which enhances region 1, and the three regions would be in balance after 5 years. Urban planners and transport network managers can use the developed model for equitable investment in an urban transportation system.",Not About Sufficiency
DeepBCE: Evaluation of deep learning models for identification of immunogenic B-cell epitopes,"B-Cell epitopes (BCEs) can identify and bind with receptor proteins (antigens) to initiate an immune response against pathogens. Understanding antigen-antibody binding interactions has many applications in biotechnology and biomedicine, including designing antibodies, therapeutics, and vaccines. Lab-based experimental identifi-cation of these proteins is time-consuming and challenging. Computational techniques have been proposed to discover BCEs, but most lack of significant accomplishments. This work uses classical and deep learning models (DLMs) with sequence-based features to predict immunity stimulator BCEs from proteomics sequences. The proposed convolutional neural network-based model outperforms other models with an accuracy (ACC) of 0.878, an F-measure of 0.871, and an area under the receiver operating characteristic curve (AUC) of 0.945. The proposed strategy achieves 58.7% better results on average than other state-of-the-art approaches based on the Mathews Correlation Coefficient (MCC) results. The established model is accessible through a web application located at http://deeplbcepred.pythonanywhere.com.",Not About Sufficiency
Galaxy Training: A powerful framework for teaching!,"There is an ongoing explosion of scientific datasets being generated, brought on by recent technological advances in many areas of the natural sciences. As a result, the life sciences have become increasingly computational in nature, and bioinformatics has taken on a central role in research studies. However, basic computational skills, data analysis, and stewardship are still rarely taught in life science educational programs, resulting in a skills gap in many of the researchers tasked with analysing these big datasets. In order to address this skills gap and empower researchers to perform their own data analyses, the Galaxy Training Network (GTN) has previously developed the Galaxy Training Platform (), an open access, community-driven framework for the collection of FAIR (Findable, Accessible, Interoperable, Reusable) training materials for data analysis utilizing the user-friendly Galaxy framework as its primary data analysis platform. Since its inception, this training platform has thrived, with the number of tutorials and contributors growing rapidly, and the range of topics extending beyond life sciences to include topics such as climatology, cheminformatics, and machine learning. While initially aimed at supporting researchers directly, the GTN framework has proven to be an invaluable resource for educators as well. We have focused our efforts in recent years on adding increased support for this growing community of instructors. New features have been added to facilitate the use of the materials in a classroom setting, simplifying the contribution flow for new materials, and have added a set of train-the-trainer lessons. Here, we present the latest developments in the GTN project, aimed at facilitating the use of the Galaxy Training materials by educators, and its usage in different learning environments.",Not About Sufficiency
Leveraging artificial intelligence for inclusive maternity care: Enhancing access for mothers with disabilities in Africa,"Women with disabilities face significant barriers in accessing maternal healthcare, which increases their risk of adverse pregnancy outcomes, particularly in Africa, where resources are limited. Artificial intelligence (AI) presents a unique opportunity to improve inclusivity and accessibility to antenatal care, skilled birth attendance and postnatal care for these women. This paper explores the potential of AI to address the socio-economic, physical, and institutional barriers that limit the utilisation of maternal healthcare services by women with disabilities. AI-driven technologies, such as virtual assistants, predictive analytics, and wearable devices, can enhance maternal health outcomes by improving monitoring during pregnancy, providing real-time health data, and facilitating access to skilled care. However, the successful implementation of AI in maternal healthcare in Africa faces challenges, including technological infrastructure, data quality, and ethical concerns. Collaborative efforts between governments, healthcare providers, and AI developers are necessary to overcome these challenges and ensure AI tools are inclusive, culturally sensitive, and accessible. Integrating AI into maternal healthcare services could lead to improved maternal outcomes, reduce mortality rates, and promote equity for women with disabilities in Africa.",Not About Sufficiency
Optimizing influenza vaccine allocation: A predictive analytics approach for informed public health planning,"Disclaimer: In an effort to expedite the publication of articles, AJHP is posting manuscripts online as soon as possible after acceptance. Accepted manuscripts have been peer-reviewed and copyedited, but are posted online before technical formatting and author proofing. These manuscripts are not the final version of record and will be replaced with the final article (formatted per AJHP style and proofed by the authors) at a later time. Purpose: Excessive purchasing of influenza vaccine can lead to costly overages and waste of resources. Insufficient quantities, however, can jeopardize population health. Our project aimed to use predictive analytics to determine the influenza vaccine quantities that would be needed for the next influenza season while minimizing vaccine waste and meeting patient care demands. Methods: Several data sources were evaluated to develop a predictive analytics model to better estimate future influenza vaccine orders during upcoming influenza seasons. A retrospective analysis of influenza vaccine administrations over the last 4 influenza seasons allowed the team to develop an algorithm to predict influenza vaccine needs. Two regions within Mayo Clinic were selected to determine the validity of our ordering process. These 2 regions, identified as regions 3 and 5, ordered influenza vaccines based on the algorithm, while the other 3 regions acted as control groups, ordering though traditional methods based on purchasing data. Results: Predictive analysis for the 2 intervention regions resulted in a savings of over $1 million when compared to traditional ordering methods. The model predicted that the quantity of vaccine ordered should be 17,574.16 and 9,164.29 quadrivalent influenza vaccines for regions 3 and 5, respectively. On the basis of actual administration data, 15,902 vaccines for region 3 and 9,016 vaccines for region 5 will be administered by the end of the season, both of which are less than the predicted amount needed, demonstrating the accuracy of the analytics. Conclusion: Compared to the traditional ordering method, ordering using predictive analytics allowed the team to more accurately determine future order volumes and spend, yielding significant cost savings.",Not About Sufficiency
A Machine Learning-Based Technique with Intelligent WordNet Lemmatize for Twitter Sentiment Analysis,"Laterally with the birth of the Internet, the fast growth of mobile strategies has democratised content production owing to the widespread usage of social media, resulting in a detonation of short informal writings. Twitter is microblogging short text and social networking services, with posted millions of quick messages. Twitter analysis addresses the topic of interpreting users' tweets in terms of ideas, interests, and views in a range of settings and fields. This type of study can be useful for a variation of academics and applications that need knowing people's perspectives on a given topic or event. Although sentiment examination of these texts is useful for a variety of reasons, it is typically seen as a difficult undertaking due to the fact that these messages are frequently short, informal, loud, and rich in linguistic ambiguities such as polysemy. Furthermore, most contemporary sentiment analysis algorithms are based on clean data. In this paper, we offers a machine-learning-based sentiment analysis method that extracts features from Term Frequency and Inverse Document Frequency (TF-IDF) and needs to apply deep intelligent wordnet lemmatize to improve the excellence of tweets by removing noise. We also utilise the Random Forest network to detect the emotion of a tweet. To authenticate the proposed approach performance, we conduct extensive tests on publically accessible datasets, and the findings reveal that the suggested technique significantly outperforms sentiment classification in",Not About Sufficiency
InterPepRank: Assessment of Docked Peptide Conformations by a Deep Graph Network,"Peptide-protein interactions between a smaller or disordered peptide stretch and a folded receptor make up a large part of all protein-protein interactions. A common approach for modeling such interactions is to exhaustively sample the conformational space by fast-Fourier-transform docking, and then refine a top percentage of decoys. Commonly, methods capable of ranking the decoys for selection fast enough for larger scale studies rely on first-principle energy terms such as electrostatics, Van der Waals forces, or on pre-calculated statistical potentials. We present InterPepRank for peptide-protein complex scoring and ranking. InterPepRank is a machine learning-based method which encodes the structure of the complex as a graph; with physical pairwise interactions as edges and evolutionary and sequence features as nodes. The graph network is trained to predict the LRMSD of decoys by using edge-conditioned graph convolutions on a large set of peptide-protein complex decoys. InterPepRank is tested on a massive independent test set with no targets sharing CATH annotation nor 30% sequence identity with any target in training or validation data. On this set, InterPepRank has a median AUC of 0.86 for finding coarse peptide-protein complexes with LRMSD < 4 angstrom. This is an improvement compared to other state-of-the-art ranking methods that have a median AUC between 0.65 and 0.79. When included as a selection-method for selecting decoys for refinement in a previously established peptide docking pipeline, InterPepRank improves the number of medium and high quality models produced by 80% and 40%, respectively.",Not About Sufficiency
Interpretable genotype-to-phenotype classifiers with performance guarantees,"Understanding the relationship between the genome of a cell and its phenotype is a central problem in precision medicine. Nonetheless, genotype-to-phenotype prediction comes with great challenges for machine learning algorithms that limit their use in this setting. The high dimensionality of the data tends to hinder generalization and challenges the scalability of most learning algorithms. Additionally, most algorithms produce models that are complex and difficult to interpret. We alleviate these limitations by proposing strong performance guarantees, based on sample compression theory, for rule-based learning algorithms that produce highly interpretable models. We show that these guarantees can be leveraged to accelerate learning and improve model interpretability. Our approach is validated through an application to the genomic prediction of antimicrobial resistance, an important public health concern. Highly accurate models were obtained for 12 species and 56 antibiotics, and their interpretation revealed known resistance mechanisms, as well as some potentially new ones. An open-source disk-based implementation that is both memory and computationally efficient is provided with this work. The implementation is turnkey, requires no prior knowledge of machine learning, and is complemented by comprehensive tutorials.",Not About Sufficiency
"Physiological, Psychological, and Functional Health Determinants of Depressive Symptoms Among the Elderly in India: Evaluation of Classification Performance of XGBoost Models","Background: Depression among the elderly is a growing public health concern, especially in India. This study aimed to investigate the predictive validity of physiological, psychological, and functional health factors in classifying the level of depressive symptoms among the elderly using the extreme gradient boosting (XGBoost) technique. Additionally, we compared the performance of models trained on original and resampled data. Methods: This study is entirely based on secondary data analysis of the Longitudinal Aging Study in India wave 1 data. We classified the observations into ""high depressive symptom"" and ""low/no depressive symptom"" groups based on the predictors, including physiological, psychological, and functional health factors, along with socio-demographic factors. We developed three models (Models 1, 2, and 3) trained on original, over-sampled, and under-sampled data, respectively. Model performance was evaluated using the metrics of balanced accuracy, sensitivity, specificity, and area under the receiver operating characteristics curve (AUC). Results: The study included 26,065 individuals aged 60 and above. Model 3, trained on under-sampled data, demonstrated the best overall performance. It achieved a balanced accuracy of 64%, with a sensitivity of 62.8% and specificity of 65.2%. The AUC for Model 3 was 0.692. Feature importance analysis revealed that life satisfaction, instrumental activities of daily living, mobility, caste, and monthly per capita expenditure quintiles were among the most influential factors in predicting the level of depressive symptoms. Conclusion: The XGBoost models demonstrate promise in predicting depressive symptoms among the elderly. These findings suggest that machine learning models can be envisaged for early detection and management of depression, especially in primary care.",Not About Sufficiency
"The Government Land Sales programme and developers' willingness to pay for accessibility in Singapore, 1990-2015","Singapore has been internationally referred to as a successful model of state-led spatial development alongside world-class infrastructure investment, but there is a paucity of evidence on the effectiveness of the city-state's Government Land Sales (GLS) programme as an instrument of strategic spatial integration used to increase land productivity/profitability via accessibility enhancements at various locations in dynamic sequences. This study examines whether developers' willingness to pay for accessible land around major transport and green infrastructure facilities was consistent with the city-state's strategic spatial vision by analysing GLS transaction cases for the period 1990-2015. The spatiotemporal regressions of the GLS programme's transaction unit prices, attributable to tendering conditions, permitted development parameters, and locational characteristics, including the availability of major infrastructure facilities within certain distances, reveal that accessibility premiums appeared to be most significant within 500 m of mass rapid transit (MRT) stations, where Singapore's statutory agencies were likely to distribute state-owned sites reactively after the new MRT stations came into operation. Singapore's experience over a quarter of a century implies that government land ownership and development guidance could bring about competitive and sustainable market outcomes if a range of site sales implemented by local agencies are collectively programmed for strategic spatial planning in fast-growing regions of the world.",Not About Sufficiency
Phenotypic Characterization of Chronic Kidney Patients Through Hierarchical Clustering,"Chronic kidney disease is a major public health problem around the world and this disease early diagnosis is still a great challenge as it is asymptomatic in its early stages. Thus, in order to identify variables capable of assisting CKD diagnosis and monitoring, machine learning techniques and statistical analysis use has shown itself to be extremely promising. For this work, unsupervised machine learning, statistical analysis techniques and discriminant analysis were used. Clinical Relevance - Discriminating variables characterization assist to differentiate groups of patients in different stages of Chronic Kidney Disease and it has important outcomes in the development of future models to aid clinical decision-making, as they can generate models with a greater predictive capacity for Chronic Kidney Disease, predominantly aiding the early diagnosis capacity of this pathology.",Not About Sufficiency
Prediction of coal self-ignition tendency using machine learning,"Self-ignition of coal emits hazardous particles and toxic gases, polluting environment and threatening people's health. Prediction of self-ignition tendency of coal is of great significance to prevent hazards of coal self-ignition. However, it is very challenging to forecast the self-ignition tendacy of coal, because of complex physicochemical processes and highly nonlinear correlation between factors and self-ignition tendency. In this work, machine learning methods (Multilayer Perceptron (MLP) and Random Forest (RF)) are used to represent the complex physicochemical processes and effects of external factors. The regression prediction models with regarding to crossing point temperature (CPT) and 13 input features are established. The dependence of input features is examined using the feature engineering. Two hundreds and four CPT samples are collected, in which 142 (70%) samples and 62 (30%) samples are divided as training data and testing data, respectively. Results show that the accuracy of both MLP and RF predicted CPTs in the testing data reaches 90%, which proves good predictability of machine-learning based models with several hundreds of samples. This work improves prediction of the self-ignition tendency of coal impacted by complex physicochemical properties and a variety of external factors. It may help to predict other fuels susceptible to self-ignition e.g., oil shale and biomass fuels.",Not About Sufficiency
Machine learning models on a web application to predict short-term postoperative outcomes following anterior cervical discectomy and fusion,"Background The frequency of anterior cervical discectomy and fusion (ACDF) has increased up to 400% since 2011, underscoring the need to preoperatively anticipate adverse postoperative outcomes given the procedure's expanding use. Our study aims to accomplish two goals: firstly, to develop a suite of explainable machine learning (ML) models capable of predicting adverse postoperative outcomes following ACDF surgery, and secondly, to embed these models in a user-friendly web application, demonstrating their potential utility.Methods We utilized data from the National Surgical Quality Improvement Program database to identify patients who underwent ACDF surgery. The outcomes of interest were four short-term postoperative adverse outcomes: prolonged length of stay (LOS), non-home discharges, 30-day readmissions, and major complications. We utilized five ML algorithms - TabPFN, TabNET, XGBoost, LightGBM, and Random Forest - coupled with the Optuna optimization library for hyperparameter tuning. To bolster the interpretability of our models, we employed SHapley Additive exPlanations (SHAP) for evaluating predictor variables' relative importance and used partial dependence plots to illustrate the impact of individual variables on the predictions generated by our top-performing models. We visualized model performance using receiver operating characteristic (ROC) curves and precision-recall curves (PRC). Quantitative metrics calculated were the area under the ROC curve (AUROC), balanced accuracy, weighted area under the PRC (AUPRC), weighted precision, and weighted recall. Models with the highest AUROC values were selected for inclusion in a web application.Results The analysis included 57,760 patients for prolonged LOS [11.1% with prolonged LOS], 57,780 for non-home discharges [3.3% non-home discharges], 57,790 for 30-day readmissions [2.9% readmitted], and 57,800 for major complications [1.4% with major complications]. The top-performing models, which were the ones built with the Random Forest algorithm, yielded mean AUROCs of 0.776, 0.846, 0.775, and 0.747 for predicting prolonged LOS, non-home discharges, readmissions, and complications, respectively.Conclusions Our study employs advanced ML methodologies to enhance the prediction of adverse postoperative outcomes following ACDF. We designed an accessible web application to integrate these models into clinical practice. Our findings affirm that ML tools serve as vital supplements in risk stratification, facilitating the prediction of diverse outcomes and enhancing patient counseling for ACDF.",Not About Sufficiency
Urban greening co-creation: Participatory spatial modelling to bridge data-driven and citizen-centred approaches,"The unprecedented growth of metropolitan areas creates challenges for maintaining liveable and biodiverse cities. Urban areas face multiple demands on sparse space whilst various stakeholders similarly aim to promote greening efforts. In this context, authorities need to balance various policy objectives with demands from diverse urban stakeholders. Part of this challenge is the question how generalized data-driven green space planning approaches can be connected to local, contextualized understandings, practices and values related to green space. In this paper, we present a participative application of GIS that contributes to bridging the gap between datadriven and citizen-centred approaches in urban greening. Through an empirical study in Amsterdam, we show how this can link local priorities with larger-scale policy frameworks through deliberative and data-driven cocreation. In our case study, local stakeholders and researchers jointly identified criteria for greening, translated these into indicators and eventually identified potential locations for small-scale greening. Site visits by local experts helped to validate the model results and translate this into concrete plans for greening several locations. Our approach promoted a dialogue between stakeholders, linking spatial data with practical experiences and aligning local priorities with policy programmes. Combining various knowledges from involved stakeholders also contributed to the quality of analysis and validation of modelling results. By increasing transparency and inclusiveness of planning, it also contributed to acceptance of process outcomes and empowering local stakeholders. With increasing urgencies for environmental measures in many cities, we emphasize the potential of transdisciplinary GIS-approaches to navigate different interests and integrate various types of valuable knowledge. We suggest that similar approaches may be applied to other environmental challenges that have a strong spatial character.",Not About Sufficiency
The role of correspondence analysis in medical research,"Correspondence analysis (CA) is a multivariate statistical and visualization technique. CA is extremely useful in analyzing either two- or multi-way contingency tables, representing some degree of correspondence between columns and rows. The CA results are visualized in easy-to-interpret ""bi-plots,"" where the proximity of items (values of categorical variables) represents the degree of association between presented items. In other words, items positioned near each other are more associated than those located farther away. Each bi-plot has two dimensions, named during the analysis. The naming of dimensions adds a qualitative aspect to the analysis. Correspondence analysis may support medical professionals in finding answers to many important questions related to health, wellbeing, quality of life, and similar topics in a simpler but more informal way than by using more complex statistical or machine learning approaches. In that way, it can be used for dimension reduction and data simplification, clustering, classification, feature selection, knowledge extraction, visualization of adverse effects, or pattern detection.",Not About Sufficiency
An IoT-Enabled mHealth Sensing Approach for Remote Detection of Keratoconus Using Smartphone Technology,"Keratoconus (KC) is a progressive eye disease and a major cause of vision impairment and blindness worldwide. Early diagnosis is crucial for effective management, yet conventional diagnostic methods rely on expensive and bulky imaging devices, limiting accessibility, especially in resource-constrained settings. This paper introduces a novel smartphone-based approach for the early detection of KC, leveraging screen-projected Placido disc patterns and an advanced image processing framework. Unlike traditional corneal topographers, our method utilizes a unique Placido disc projection technique and a machine learning-based classification model to analyze corneal irregularities with high precision. With a sensitivity of 96.08%, specificity of 97.96%, and overall accuracy of 97% on our dataset, the proposed system demonstrates exceptional diagnostic reliability. By transforming a standard smartphone into an effective screening tool, this innovation provides an affordable, portable, and user-friendly solution for early KC detection, bridging the gap in eye care accessibility and reducing the global burden of undiagnosed keratoconus.",Not About Sufficiency
Open Metaverse with Open Software,"The metaverse is an increasingly popular social interaction, gaming, and education platform. In recent years there has been a growing interest in the Metaverse by various commercial firms in order to promote economic transactions and the development of applications in the world of digital and virtual interaction. With the widespread adoption of the metaverse concept, there is a growing need for open standards and open-source code related to the metaverse to ensure interoperability and enable innovation. Open standards enable different systems to communicate and share data, while open-source code promotes collaboration and transparency in development. This manuscript describes the importance of open standards and open-source code for developing metaverse technologies. We discuss the benefits of open standards for interoperability and innovation and examine case studies of successful open-source metaverse projects. We also discuss challenges in implementing open standards and open source code in metaverse development, including intellectual property concerns and governance issues. The open standards and open source code are essential for the metaverse's growth and sustainability. The developers and policymakers should prioritize these principles to ensure a the development of inclusive and open virtual environment. Our work aims to generate immersive, multiverse reality scenarios using dedicated graphics platforms for generating virtual environments.",Not About Sufficiency
A Wireless Sensor Array System Coupled With AI-Driven Data Analysis Towards Remote Monitoring of Human Breaths,"The development of low-cost point-of-care sensor systems is essential for the screening and diagnostics of different diseases. However, this type of application requires effective integration of different sensor hardware and electronics in a portable, wireless, and reliable platform. We report herein the development of such a platform by integrating a nanostructured chemiresistive sensing array (CSA) with a low-current multichannel electronics board (MEB) and a Raspberry Pi board (RPB). The system allows the collection of data from the sensor array responses to volatile organic compounds (VOCs) and human breaths (HBs), then transfers the data through a serial connection from MEB to RPB. After processing and restructuring the data, RPB will wirelessly upload it to MongoDB Atlas (MDBA) cloud database. A workstation periodically retrieves the data from the MDBA cloud database and trains them with customized machine learning models. The best result feeds back to the MDBA cloud server, providing a pretrained model for a future prediction or disease identification. At the same time, the real-time sensor response data are displayed on the Thing speak portal. Once programed, the system runs in an independent mode without a PC connection with various functions, including remote monitoring services and ad hoc applications that are typically not accessible from traditional stationary monitoring systems housed in hospitals and laboratories. Some of these functions are demonstrated by testing the performance in sensing HB samples with and without simulated lung cancer-specific VOCs, showing promises for potential applications in remote breath monitoring and screening of lung cancer.",Not About Sufficiency
Multi-step influenza forecasting through singular value decomposition and kernel ridge regression with MARCOS-guided gradient-based optimization,"This research delves into the significance of influenza outbreaks in public health, particularly the importance of accurate forecasts using weekly Influenza-like illness (ILI) rates. The present work develops a novel hybrid machine-learning model by combining singular value decomposition with kernel ridge regression (SKRR). In this context, a novel hybrid model known as H-SKRR is developed by combining two robust forecasting approaches, SKRR and ridge regression, which aims to improve multi-step-ahead predictions for weekly ILI rates in Southern and Northern China. The study begins with feature selection via XGBoost in the preprocessing phase, identifying optimal precursor information guided by importance factors. It decomposes the original signal using multivariate variational mode decomposition (MVMD) to address non-stationarity and complexity. H-SKRR is implemented by incorporating significant lagged-time components across sub-components. The aggregated forecasted values from these sub-components generate ILI values for two horizons (i.e., 4-and 7-weekly ahead). Employing the gradient-based optimization (GBO) algorithm fine-tunes model parameters. Furthermore, the deep random vector functional link (dRVFL), Ridge regression, and gated recurrent unit neural network (GRU) models were employed to validate the MVMD-H-SKRR-GBO paradigm's effectiveness. The outcomes, assessed using the MARCOS (Measurement of alternatives and ranking according to compromise solution) method as a multicriteria decision-making method, highlight the superior accuracy of the MVMD-H-SKRR-GBO model in predicting ILI rates. The results clearly highlight the exceptional performance of the MVMD-H-SKRR-GBO model, with outstanding precision demonstrated by impressive R, RMSE, IA, and U95 % values of 0.946, 0.388, 0.970, and 1.075, respectively, at t + 7.",Not About Sufficiency
Statistical analysis of S/N-curves by means of a fatigue database for polypropylene,"Material databases of plastics are becoming more and more the focus of applied science and commercial use in industry. Material properties of material manufacturers are often provided in publicly accessible material databases, which usually contain processing and mechanical properties under static loadings. Fatigue strength values are usually not accessible. The fatigue data for thermoplastics is of particular interest, as these materials have a particularly high lightweight construction potential and can be processed with a high degree of automation and reproducibility. Individual fatigue strength parameters for a specific material, environmental condition, geometry and loadings have been investigated in numerous publications. However, no work has been found in which fundamental interactions of different materials, environmental conditions, geometries and loadings on the course of the S/N-Curve have been investigated. In this paper, different effect relationships between temperature, filler type and filler content, fiber orientation and load ratio will be presented for the material Polypropylene (PP). A fatigue strength database of 11 different material manufacturers, from 71 different S/N-Curves with 606 tested samples, serves as a basis. The fatigue database enables a digital twin, which is used for the design of structural components to add a third dimension with artificial intelligence, and which is trained by an engineer. From the determined effect relationships, fatigue factors are to be derived and can be used to evaluate the fatigue strength of a component in the design process and to train the digital twin. The fatigue-strength values from the database also allow a statistical consideration of the slope k and the scattering of the S/N-Curve. The different S/N-Curves are transferred into a Haigh diagram, from which the functional course of the mean stress is determined.",Not About Sufficiency
Developing Pronunciation Models in New Languages Faster by Exploiting Common Grapheme-to-Phoneme Correspondences Across Languages,"We discuss two methods that let us easily create grapheme-to-phoneme (G2P) conversion systems for languages without any human-curated pronunciation lexicons, as long as we know the phoneme inventory of the target language and as long as we have some pronunciation lexicons for other languages written in the same script. We use these resources to infer what grapheme-to-phoneme correspondences we would expect, and predict pronunciations for words in the target language with minimal or no language-specific human work. Our first approach uses finitestate transducers, while our second approach uses a sequence-to-sequence neural network. Our G2P models reach high degrees of accuracy, and can be used for various applications, e.g. in developing an automatic speech recognition system. Our methods greatly simplify a task that has historically required extensive manual labor.",Not About Sufficiency
Non-Financial Reporting in SMEs: a new approach to measure corporate well-being based on employee perception,"The paper focuses on social sustainability within manufacturing industries by measuring workplace well-being. Firstly, the study focused on what the scientific literature says about the main variables and aspects that impact on employee well-being and deepen the productivity and efficiency of industries. To better understand the problems and address the best solution to improve the company environment, the paper analyses more than 20 innovative startups operating in the field of well-being and Human Resource Management. After having defined the impactful and meaningful variables, it was structured a particular case study on a manufacturing field, regarding a Calabrian SME. Using Business Intelligence tools and applying machine learning techniques, comes to light an overall result of company well-being and the value of the main areas. In addition, different company cluster analyses are conducted. Moreover, it shows a correlation network between variables highlighting the most affecting ones on well-being. Thanks to the information obtained, a strategic investment can be made to enhance people's well-being and to make companies more productive. (c) 2023 The Authors. Published by ELSEVIER B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)",Not About Sufficiency
A Primer on Using Behavioral Data for Testing Theories in Advertising Research,"Interactions with and between customers in digital, social, and mobile environments are commonly recorded, producing behavioral data that have the potential to advance advertising research. This article provides an accessible guide on how to leverage such data for advertising researchers who may have thus far relied mostly on lab experiment or survey data. Specifically, we suggest potential sources for behavioral data and present a process for analyzing and interpreting behavioral data. Each step of the process is discussed: exploring, understanding and preparing data; specifying and estimating models; and interpreting and presenting the results. Some fundamental issues with using multiple regression to analyze such data are covered, including standardization, outliers, transformations, multicollinearity, and the omitted variable bias. We also discuss issues that are especially problematic with using behavioral data in advertising research, including endogeneity, count data, data with many zeros, and grouped data. More advanced versions of regression that address these issues are surveyed, including instrumental variables, propensity scoring, generalized linear models, and mixed models. General advice for thinking about behavioral data is provided.",Not About Sufficiency
Spatiotemporal Prediction of Conflict Fatality Risk Using Convolutional Neural Networks and Satellite Imagery,"As both satellite imagery and image-based machine learning methods continue to improve and become more accessible, they are being utilized in an increasing number of sectors and applications. Recent applications using convolutional neural networks (CNNs) and satellite imagery include estimating socioeconomic and development indicators such as poverty, road quality, and conflict. This article builds on existing work leveraging satellite imagery and machine learning for estimation or prediction, to explore the potential to extend these methods temporally. Using Landsat 8 imagery and data from the Armed Conflict Location & Event Data Project (ACLED) we produce subnational predictions of the risk of conflict fatalities in Nigeria during 2015, 2017, and 2019 using distinct models trained on both yearly and six-month windows of data from the preceding year. We find that predictions at conflict sites leveraging imagery from the preceding year for training can predict conflict fatalities in the following year with an area under the receiver operating characteristic curve (AUC) of over 75% on average. While models consistently outperform a baseline comparison, and performance in individual periods can be strong (AUC > 80%), changes based on ground conditions such as the geographic scope of conflict can degrade performance in subsequent periods. In addition, we find that training models using an entire year of data slightly outperform models using only six months of data. Overall, the findings suggest CNN-based methods are moderately effective at detecting features in Landsat satellite imagery associated with the risk of fatalities from conflict events across time periods.",Not About Sufficiency
Towards a Dynamic Composability Approach for using Heterogeneous Systems in Remote Sensing,"Influenced by the advances in data and computing, the scientific practice increasingly involves machine learning and artificial intelligence driven methods which requires specialized capabilities at the system-, science- and service-level in addition to the conventional large-capacity supercomputing approaches. The latest distributed architectures built around the composability of data-centric applications led to the emergence of a new ecosystem for container coordination and integration. However, there is still a divide between the application development pipelines of existing supercomputing environments, and these new dynamic environments that disaggregate fluid resource pools through accessible, portable and re-programmable interfaces. New approaches for dynamic composability of heterogeneous systems are needed to further advance the data-driven scientific practice for the purpose of more efficient computing and usable tools for specific scientific domains. In this paper, we present a novel approach for using composable systems in the intersection between scientific computing, artificial intelligence (AI), and remote sensing domain. We describe the architecture of a first working example of a composable infrastructure that federates Expanse, an NSF-funded supercomputer, with Nautilus, a Kubernetes-based GPU geo-distributed cluster. We also summarize a case study in wildfire modeling, that demonstrates the application of this new infrastructure in scientific workflows: a composed system that bridges the insights from edge sensing, AI and computing capabilities with a physics-driven simulation.",Not About Sufficiency
Comprehensive Benefit Assessment of the Middle Route of South-to-North Water Diversion Project Based on Markowitz Theory,"In the context of global climate change and the water-carbon peak target, improving water security in arid regions is a persistent challenge in global water resources management. Water diversion projects can serve as an important measure to effectively alleviate the uneven distribution of water resources, achieve rational allocation and efficient utilization of water resources. However, how to achieve the maximization of comprehensive benefits during the process of water allocation is also an urgent problem that needs to be solved. This study focuses on the Middle Route Project of the South to North Water Diversion Project in China, selecting four important municipalities and provinces during 2015 to 2021, namely Beijing, Tianjin, Hebei Province, and Henan Province, based on the actual benefits of the water receiving areas of the middle line project. Nine representative indicators related to social, economic, and ecological benefits were selected to evaluate the optimal combination of water resource allocation in the water receiving areas along the central line, in order to achieve the maximum comprehensive benefits and solve the problems of high water safety guarantee requirements and difficult balanced water distribution in urban agglomerations in the water receiving areas. Through the calculation of the Markovsky theoretical model, the results show that when 79.9% of the water conveyance is used to generate social benefits, 15.8% of the water conveyance is used to generate ecological benefits, and 4.5% of the water conveyance is used to generate economic benefits, the project achieves the maximum comprehensive benefits. This computational model method can be used to provide technical support and scientific reference for the optimal allocation of water resources in cross regional water transfer projects.",Not About Sufficiency
Assessing the impact of alternative land-use zoning policies on future ecosystem services,"Land use conversions rank among the most significant drivers of change in ecosystem services worldwide, affecting human wellbeing and threatening the survival of other species. Hence, predicting the effects of land use decisions on ecosystem services has emerged as a crucial need in spatial planning, and in the associated Strategic Environmental Assessment (SEA) practice. The paper presents a case-study research aimed at empirically exploring how the implementation of different land-use zoning policies affect the future provision of a set of ecosystem services (water purification, soil conservation, habitat for species, carbon sequestration and timber production). The study area is located in The Araucania, one of Chile's Administrative Regions. The first part of the methods consisted in the construction of land-use scenarios associated to different policies. Subsequently, the effects of the land-use scenarios on the provision of the selected ecosystem services were assessed in a spatially explicit way, by using modeling tools. Finally, a set of metrics was developed to compare scenarios, and trade-offs in the provision of different ecosystem services were made explicit through trade-off curves. The results indicate that, for this case study, spatial configuration of land uses is as an important factor as their size. This suggests that the analysis of land-use patterns deserves attention, and that this information should be included in scenario exercises aimed to support spatial planning. The paper concludes by discussing the potential contribution of the approach to support SEA of spatial plans. (C) 2012 Elsevier Inc. All rights reserved.",Not About Sufficiency
A Deliberative Rural Community Consultation to Assess Support for Flood Risk Management Policies to Strengthen Resilience in Malawi,"As disasters increase in frequency and magnitude with adverse effects on population health, governments will be forced to implement disaster risk management policies that may include forced relocation. Ineffective public consultation has been cited as one reason for failure of these policies. Using the deliberative polling method, this study assessed the capacity of rural communities to participate in flood risk management policy priority setting and the impact of providing accurate and balanced information on policies by comparing pre-and post -deliberation data. The study also assessed the level of trust on whether government and community would use the results of this study. Results indicated strong community support for policy options to reduce vulnerability in communities and strong resistance to relocation. As all the top five ranked policy options were concerned with population pressure, gender, and social service issues, which are all conceptually considered social determinants of a healthy community, this study concludes that public health considerations are central to flood risk policy development and implementation. The study revealed high levels of trust in government and the community relating to flood risk management, which policymakers in low-to-middle income countries can capitalise on for meaningful community consultation for effective disaster risk management.",Not About Sufficiency
Report of the third conference on next-generation sequencing for adventitious virus detection in biologics for humans and animals,"Next-generation sequencing (NGS) has been proven to address some of the limitations of the current testing methods for adventitious virus detection in biologics. The International Alliance for Biological Standardization (IABS), the U.S. Food and Drug Administration (FDA), and the European Directorate for the Quality of Medicines and Healthcare (EDQM) co-organized the ""3rd Conference on Next-generation Sequencing for Adventitious Virus Detection in Biologics for Humans and Animals"", which was held on September 27-28, 2022, in Rockville, Maryland, U.S.A. The meeting gathered international representatives from regulatory and public health authorities and other government agencies, industry, contract research organizations, and academia to present the current status of NGS applications and the progress on NGS standardization and validation for detection of viral adventitious agents in biologics, including human and animal vaccines, gene therapies, and biotherapeutics. Current regulatory expectations were discussed for developing a scientific consensus regarding using NGS for detection of adventitious viruses. Although there are ongoing improvements in the NGS workflow, the development of reference materials for facilitating method qualification and validation support the current use of NGS for adventitious virus detection.",Not About Sufficiency
Securing IoT-Empowered Fog Computing Systems: Machine Learning Perspective,"The Internet of Things (IoT) is an interconnected network of computing nodes that can send and receive data without human participation. Software and communication technology have advanced tremendously in the last couple of decades, resulting in a considerable increase in IoT devices. IoT gadgets have practically infiltrated every aspect of human well-being, ushering in a new era of intelligent devices. However, the rapid expansion has raised security concerns. Another challenge with the basic approach of processing IoT data on the cloud is scalability. A cloud-centric strategy results from network congestion, data bottlenecks, and longer response times to security threats. Fog computing addresses these difficulties by bringing computation to the network edge. The current research provides a comprehensive review of the IoT evolution, Fog computation, and artificial-intelligence-inspired machine learning (ML) strategies. It examines ML techniques for identifying anomalies and attacks, showcases IoT data growth solutions, and delves into Fog computing security concerns. Additionally, it covers future research objectives in the crucial field of IoT security.",Not About Sufficiency
EFFECTIVENESS OF FINANCING MEASURES TO REDUCE LOW EMISSIONS IN POLAND - A CASE STUDY,"Local authorities are increasingly facing the challenges of climate change, which requires adaptation of various elements of local government systems, including infrastructure, communities and local economies. Operating in an ever-changing reality, local government units should plan for the necessary sustainable local government investments, with particular attention to structural gaps and financial mismatches. Local government is responsible for ensuring local energy security and thus for 'environmental sustainability'. The overall objective of this study is to assess the effectiveness of financing low-emission lowering measures from the Environmental Protection and Water Management Fund in municipalities in Lubusz Province in 2020. The article uses cause-effect relationship analysis (regression models). Investments implemented by local governments generally have a positive impact on the reduction of low emissions, although the results of the study indicate a negative impact of financing investments in alternative energy sources (RES). This may be related to large investment outlays, as well as difficulties in spatial planning, environmental impact and administrative inefficiency.",Not About Sufficiency
Workers' Satisfaction during the COVID-19 Pandemic in Central and Eastern Europe,"This article analyses the determinants of worker satisfaction in Central and Eastern European countries, focusing on the effects of the COVID-19 pandemic. For this purpose, the latest European Social Survey data were utilised in a multilevel framework, covering 5681 workers from eight countries. The results suggest that both the general stringency of policies aimed at containing the spread of the COVID-19 contagion and the resulting disruptions to individuals' working lives significantly affected the well-being of workers. Workers' satisfaction with their jobs was negatively affected mainly through decreases in work income, but the impact on their overall life satisfaction was more complex. While income reductions and workplace relocation negatively impacted life satisfaction, the opportunity and possible necessity to stay home more positively affected the life satisfaction of workers. Nevertheless, the overall stringency of governmental policies related to COVID-19 significantly reduced workers' life satisfaction.",Not About Sufficiency
"Foundations of AI for future physicians: A practical, accessible curriculum","What was the educational challenge?The integration of machine learning (ML) and large language models (LLMs) into healthcare is transforming diagnostics, patient care, and administrative workflows. However, most clinicians lack the foundational knowledge to critically engage with these tools, creating risks of overreliance and missed oversight. Just as understanding computed tomography (CT) physics became essential for its safe application, clinicians must acquire basic AI literacy. Practical AI education remains absent from most medical curricula.What was the solution?We propose a modular curriculum using Colab notebooks to teach foundational AI concepts. Colab's free, cloud-based, and interactive environment makes it accessible and engaging, even for non-data scientists. This hands-on approach emphasizes practical applications, enabling learners to explore datasets, build ML models, and interact with locally run LLMs, fostering critical engagement with AI tools.How was the solution implemented?The curriculum consists of five interconnected modules: introduction to data science, exploring datasets, predictive modeling, advanced ML techniques and imaging, and working with LLMs. Designed to integrate into medical school data science threads, the curriculum provides structured, progressive learning tailored to clinical contexts.What lessons were learned?Global accessibility, hands-on engagement, and modular design make this approach adaptable across diverse settings. Emphasizing ethical considerations and local relevance enhances its impact.What are the next steps?The next step is to integrate the Colab notebook-based curriculum into the authors' medical school data science thread. To support broader adoption, adaptable teaching guides will be developed, enabling implementation at other medical schools, including those in low-resource settings, while leveraging Colab's accessibility for regional customization.",Not About Sufficiency
"Respiratory pandemics, urban planning and design: A multidisciplinary rapid review of the literature","COVID-19 is the most recent respiratory pandemic to necessitate better knowledge about city planning and design. The complex connections between cities and pandemics, however challenge traditional approaches to reviewing literature. In this article we adopted a rapid review methodology. We review the historical literature on respiratory pandemics and their documented connections to urban planning and design (both broadly defined as being concerned with cities as complex systems). Our systematic search across multidisciplinary databases returned a total of 1323 sources, with 92 articles included in the final review. Findings showed that the literature represents the multi-scalar nature of cities and pandemics - pandemics are global phenomena spread through an interconnected world, but require regional, city, local and individual responses. We characterise the literature under ten themes: scale (global to local); built environment; governance; modelling; non-pharmaceutical interventions; socioeconomic factors; system preparedness; system responses; underserved and vulnerable populations; and future-proofing urban planning and design. We conclude that the historical literature captures how city planning and design intersects with a public health response to respiratory pandemics. Our thematic framework provides parameters for future research and policy responses to the varied connections between cities and respiratory pandemics.",Not About Sufficiency
Corporate governance and innovation: a predictive modeling approach using machine learning,"The examination of the associations between internal corporate governance (CG) mechanisms and innovation faces challenges due to nonlinear patterns and complex interactions. Consequently, existing literature rarely reaches a consensus on the directions or strengths of these relationships. Furthermore, to investigate the CG-innovation association, prior research has predominantly relied on explanatory modeling, which involves applying statistical models to data to test correlational or causal hypotheses about theoretical constructs. These are the reasons why it remains unclear whether internal CG mechanisms, when considered collectively as an extensive array of interconnected variables, offer valuable insights for accurately predicting innovation. To address this gap, we analyze a dataset of research and development (R&D) projects from the Brazilian electricity sector by employing predictive modeling, which entails using statistical models or data mining algorithms to predict new observations, particularly using supervised machine learning (ML) methods. Our study demonstrates that a comprehensive set of variables representing internal CG mechanisms significantly enhances the predictive capabilities of ML algorithms for innovation. Furthermore, we illustrate how ML can illuminate nonlinear and non-monotonic patterns, and interactions among variables, in the CG-innovation relationship. Our contribution to the literature encompasses three key aspects: introducing a predictive modeling approach to the discourse on the role of CG in innovation attainment through R&D endeavors, which can complement and enrich existing explanatory research; investigating non-linear and non-monotonic relationships, as well as interactions, in innovation prediction; and affirming the emerging body of literature that recognizes supervised ML as a valuable tool accessible to management researchers.",Not About Sufficiency
Interlinking FinTech and eHealth: a qualitative study,"Introduction This study investigates the integration of financial technology (FinTech) and electronic health (eHealth) to explore the opportunities, challenges, and implications arising from their interlinkage in Saudi Arabia.Methods Utilizing qualitative semi-structured interviews with 26 participants-including physicians, patients, technical and administrative managers, and FinTech consultants-the research adopts an inductive approach to understand diverse perspectives.Results Key findings reveal significant benefits such as improved efficiency in administrative processes, enhanced access to healthcare services, increased financial inclusion, better decision-making, improved patient experience, and the promotion of innovation and sustainability. However, barriers including regulatory challenges, data privacy and security concerns, interoperability issues, the digital divide, resistance to change, and cost implications were also identified.Conclusion Overall, the integration of FinTech and eHealth holds substantial promise for advancing healthcare delivery in Saudi Arabia. Future implications include the expansion of telehealth services, an increase in startups, the integration of wearable health devices, blockchain-based systems, evolving regulatory frameworks, and heightened collaborations. Addressing the identified challenges is crucial for realizing the full potential of this integration.",Not About Sufficiency
Acceptability of sustainable mobility policies under a post-COVID-19 scenario. Evidence from Spain,"The COVID-19 pandemic has suddenly modified the lifestyle of a large portion of the population around the world. This pandemic is also the first one in decades that has severely impacted many countries of the Global North. Governments have had to adopt wide-scope and desperate measures to face the abnormal situation and to reduce the stress of their health care systems. These measures have been based on reducing the physical-social interaction and mobility (closing schools and some economic activities, or fostering telework, among others), increasing the physical distance between people, and recommending washing hands frequently and wearing masks. Thus, the COVID-19 may change many habits of people and the ways we interact with others after the current pandemic. It would also imply changes in mobility habits. Many questions arise about the willingness and acceptability of changes, and who would have to impulse them and how. This paper aims to study and understand individuals' acceptability towards a set of generic measures related to urban mobility in Spain, one of the countries most affected by the COVID-19 pandemic. To that end, we conducted an online survey during the lockdown in Spring (2020). More than 75% of respondents would accept restrictions on car use after the return to normal, and more than 90% agree on increasing the space for pedestrians and cyclists on streets. Furthermore, 75% of respondents would change the primary transport mode towards a more sustainable transport mode if it would decrease the incidence or severity of the COVID-19. These results show that the respondents are overall in favor of a new urban hierarchy that gives more importance to the most sustainable modes, reducing the public space devoted to the car, which means the possibility of turning the COVID-19 crisis into an opportunity to make Spanish cities more sustainable.",Not About Sufficiency
An investigation into social support networks of parents of children with intellectual disability in Bangladesh,"Drawing upon the theories of social cognition, social networks and social capital, this article investigates the structure and function of social support networks of parent of children with intellectual disability in Bangladesh, a developing country in South Asia. The primary data for the study were collected using a semi-structured questionnaire. Snowball sampling method was used to reach to participants. It also used psychometric tools such as the Lubben Social Network Scale - Revised (LSNS-R) and the Inventory of Socially Supportive Behaviors (ISSB) to measure the social network and support among the participants. The results show that parents of children with intellectual disability have smaller social network than parents of children without intellectual disabilities. The parents of intellectual disability have relatively a smaller number of close friends and relatives for seeking help, sharing private matters and consulting while making any important decisions. Multiple factors influence their possibility of getting social support. The factors include the number of relatives who see or hear at least once a month; number of relatives to share private matters; availability of relatives for making a decision; number of close friends and the number of close friends who live within five miles of their residence. This article concludes that the social network interventions can be a mechanism for individualized supports and services for the parents.",Not About Sufficiency
Alzheimer's as a Systems-Level Disease Involving the Interplay of Multiple Cellular Networks,"Alzheimer's disease (AD), and many neurodegenerative disorders, are multifactorial in nature. They involve a combination of genomic, epigenomic, interactomic and environmental factors. Progress is being made, and these complex diseases are beginning to be understood as having their origin in altered states of biological networks at the cellular level. In the case of AD, genomic susceptibility and mechanisms leading to (or accompanying) the impairment of the central Amyloid Precursor Protein (APP) processing and tau networks are widely accepted as major contributors to the diseased state. The derangement of these networks may result in both the gain and loss of functions, increased generation of toxic species (e.g., toxic soluble oligomers and aggregates) and imbalances, whose effects can propagate to supra -cellular levels. Although well sustained by empirical data and widely accepted, this global perspective often overlooks the essential roles played by the main counteracting homeostatic networks (e.g., protein quality control/ proteostasis, unfolded protein response, protein folding chaperone networks, disaggregases, ER-associated degradation/ubiquitin proteasome system, endolysosomal network, autophagy, and other stress-protective and clearance networks), whose relevance to AD is just beginning to be fully realized. In this chapter, an integrative perspective is presented. Alzheimer's disease is characterized to be a result of: (a) intrinsic genomic/epigenomic susceptibility and, (b) a continued dynamic interplay between the deranged networks and the central homeostatic networks of nerve cells. This interplay of networks will underlie both the onset and rate of progression of the disease in each individual. Integrative Systems Biology approaches are required to effect its elucidation. Comprehensive Systems Biology experiments at different `omics levels in simple model organisms, engineered to recapitulate the basic features of AD may illuminate the onset and sequence of events underlying AD. Indeed, studies of models of AD in simple organisms, differentiated cells in culture and rodents are beginning to offer hope that the onset and progression of AD, if detected at an early stage, may be stopped, delayed, or even reversed, by activating or modulating networks involved in proteostasis and the clearance of toxic species. In practice, the incorporation of next generation neuroimaging, high-throughput and computational approaches are opening the way towards early diagnosis well before irreversible cell death. Thus, the presence or co-occurrence of: (a) accumulation of toxic AP oligomers and tau species; (b) altered splicing and transcriptome patterns; (c) impaired redox, proteostatic, and metabolic networks together with, (d) compromised homeostatic capacities may constitute relevant 'AD hallmarks at the cellular level' towards reliable and early diagnosis. From here, preventive lifestyle changes and tailored therapies may be investigated, such as combined strategies aimed at both lowering the production of toxic species and potentiating homeostatic responses, in order to prevent or delay the onset, and arrest, alleviate, or even reverse the progression of the disease.",Not About Sufficiency
Reexamine the value of urban pocket parks under the impact of the COVID-19,"While the focus of the wide-spread coronavirus is its impacts on people's lives and economic wellbeing around the world, the pandemic substantially limits people's available options of physical activities and exacerbates an enduring problem of large urban populations lack accessible green space to fulfill the essential physical and mental health needs. Under the current pandemic situation available green space is further reduced when some parks are closed or open with limited functions to reduce the spread of coronavirus. At the same time, the demand for green space has increased because of the unavailability of other activities. In this essay, we call the attention of urban planners and designers to pocket parks. Studies have shown that the tiny size of pocket parks makes them an easier fit into vacant properties scattered throughout the urban fabric. Therefore, pocket parks can improve health and encourage social cohesion of residents in often underserved high density urban neighborhoods. The potential of pocket parks in providing accessible urban green space to all urban population may have been considered desirable before the coronavirus outbreak and now it should be considered a necessary 'lifeline' to improve urban residents' health during the coronavirus. In addition, with the long-overlooked value of accessible urban green space waken by the global-scale crisis, proper attention and improvement strategy, such as introducing more pocket park could lead to a better future after the COVID-19.",Not About Sufficiency
Using matrix assisted laser desorption ionisation mass spectrometry combined with machine learning for vaccine authenticity screening,"The global population is increasingly reliant on vaccines to maintain population health with billions of doses used annually in immunisation programmes. Substandard and falsified vaccines are becoming more prevalent, caused by both the degradation of authentic vaccines but also deliberately falsified vaccine products. These threaten public health, and the increase in vaccine falsification is now a major concern. There is currently no coordinated global infrastructure or screening methods to monitor vaccine supply chains. In this study, we developed and validated a matrix-assisted laser desorption/ionisation-mass spectrometry (MALDI-MS) workflow that used open-source machine learning and statistical analysis to distinguish authentic and falsified vaccines. We validated the method on two different MALDI-MS instruments used worldwide for clinical applications. Our results show that multivariate data modelling and diagnostic mass spectra can be used to distinguish authentic and falsified vaccines providing proof-of-concept that MALDI-MS can be used as a screening tool to monitor vaccine supply chains.",Not About Sufficiency
Integration of Augmented Reality with Building Information Modeling: Design Optimization and Construction Rework Reduction Perspective,"The construction industry is on the brink of a transformative shift with the integration of Building Information Modelling (BIM) and Augmented Reality (AR) to enhance project efficiency and accuracy. This study presents a comprehensive analysis and model that outlines the potential of BIM-AR integration in optimizing design processes and minimizing reworks in the construction industry. The study applied a systematic literature review methodology to highlight the potential of this integration in revolutionising construction practices. Key findings reveal that this integration facilitates a robust digital-physical bridge, ensures real-time data accessibility, and extends across the project's lifecycle. The model underscores the pivotal role of AR technologies and BIM authoring tools in realizing this potential, while also recognizing hardware constraints, software compatibility, and scalability as primary limitations. Remarkable challenges such as technology integration, data management, and user adoption are discussed, highlighting the need for industry-wide education and a cultural shift towards new technological practices. The study charts a future research trajectory focusing on standardization, affordable solutions, AI advancements, user experience, and sustainability investigations. By enabling superior visualization, communication, and collaboration, the BIM-AR convergence is set to revolutionize construction practices, driving the industry towards more sustainable, efficient, and error-minimized operations. This integration model serves as a roadmap for researchers and practitioners to outline the current state and future directions for BIM-AR in construction.",Not About Sufficiency
HIV drug resistance prediction with weighted categorical kernel functions,"BackgroundAntiretroviral drugs are a very effective therapy against HIV infection. However, the high mutation rate of HIV permits the emergence of variants that can be resistant to the drug treatment. Predicting drug resistance to previously unobserved variants is therefore very important for an optimum medical treatment. In this paper, we propose the use of weighted categorical kernel functions to predict drug resistance from virus sequence data. These kernel functions are very simple to implement and are able to take into account HIV data particularities, such as allele mixtures, and to weigh the different importance of each protein residue, as it is known that not all positions contribute equally to the resistance.ResultsWe analyzed 21 drugs of four classes: protease inhibitors (PI), integrase inhibitors (INI), nucleoside reverse transcriptase inhibitors (NRTI) and non-nucleoside reverse transcriptase inhibitors (NNRTI). We compared two categorical kernel functions, Overlap and Jaccard, against two well-known noncategorical kernel functions (Linear and RBF) and Random Forest (RF). Weighted versions of these kernels were also considered, where the weights were obtained from the RF decrease in node impurity. The Jaccard kernel was the best method, either in its weighted or unweighted form, for 20 out of the 21 drugs.ConclusionsResults show that kernels that take into account both the categorical nature of the data and the presence of mixtures consistently result in the best prediction model. The advantage of including weights depended on the protein targeted by the drug. In the case of reverse transcriptase, weights based in the relative importance of each position clearly increased the prediction performance, while the improvement in the protease was much smaller. This seems to be related to the distribution of weights, as measured by the Gini index. All methods described, together with documentation and examples, are freely available at https://bitbucket.org/elies_ramon/catkern.",Not About Sufficiency
Predicting energy poverty with combinations of remote-sensing and socioeconomic survey data in India: Evidence from machine learning,"Identifying energy poverty and targeting interventions require up-to-date and comprehensive survey data, which are expensive, time-consuming, and difficult to conduct, especially in rural areas of developing countries. This paper examined the potential of satellite remote sensing data in energy poverty prediction combined with so-cioeconomic survey data in response to these challenges. We found that a machine learning algorithm incor-porating geographical and environmental remotely collected indicators could identify 90.91% of the districts with high energy poverty and performs better than those using socioeconomic indicators only. Specifically, precipitation and fine particulate matter (PM2.5) offer the most significant contribution. Moreover, the algorithm, which was trained using a dataset from 2015, could also perform well to predict energy poverty using two environment indicators: precipitation and PM2.5 concentration.",Not About Sufficiency
Using machine learning models to predict the effects of seasonal fluxes on Plesiomonas shigelloides population density,"Seasonal variations (SVs) affect the population density (PD), fate, and fitness of pathogens in environmental water resources and the public health impacts. Therefore, this study is aimed at applying machine learning intelligence (MLI) to predict the impacts of SVs on P. shigelloides population density (PDP) in the aquatic milieu. Physicochemical events (PEs) and PDP from three rivers acquired via standard microbiological and instrumental techniques across seasons were fitted to MLI algorithms (linear regression (LR), multiple linear regression (MR), random forest (RF), gradient boosted machine (GBM), neural network (NN), K-nearest neighbour (KNN), boosted regression tree (BRT), extreme gradient boosting (XGB) regression, support vector regression (SVR), decision tree regression (DTR), M5 pruned regression (M5P), artificial neural network (ANN) regression (with one 10-node hidden layer (ANN10), two 6- and 4-node hidden layers (ANN64), and two 5- and 5-node hidden layers (ANN55)), and elastic net regression (ENR)) to assess the implications of the SVs of PEs on aquatic PDP. The results showed that SVs significantly influenced PDP and PEs in the water (p < 0.0001), exhibiting a site-specific pattern. While MLI algorithms predicted PDP with differing absolute flux magnitudes for the contributing variables, DTR predicted the highest PDP value of 1.707 log unit, followed by XGB (1.637 log unit), but XGB (mean-squared-error (MSE) = 0.0025; root-mean-squared-error (RMSE) = 0.0501; R-2 =0.998; medium absolute deviation (MAD) = 0.0275) outperformed other models in terms of regression metrics. Temperature and total suspended solids (TSS) ranked first and second as significant factors in predicting PDP in 53.3% (8/15) and 40% (6/15), respectively, of the models, based on the RMSE loss after permutations. Additionally, season ranked third among the 7 models, and turbidity (TBS) ranked fourth at 26.7% (4/15), as the primary significant factor for predicting PDP in the aquatic milieu. The results of this investigation demonstrated that MLI predictive modelling techniques can promisingly be exploited to complement the repetitive laboratory-based monitoring of PDP and other pathogens, especially in low-resource settings, in response to seasonal fluxes and can provide insights into the potential public health risks of emerging pathogens and TSS pollution (e.g., nanoparticles and micro- and nanoplastics) in the aquatic milieu. The model outputs provide low-cost and effective early warning information to assist watershed managers and fish farmers in making appropriate decisions about water resource protection, aquaculture management, and sustainable public health protection.",Not About Sufficiency
Pixel-wise statistical analysis of myocardial injury in STEMI patients with delayed enhancement MRI,"ObjectivesMyocardial injury assessment from delayed enhancement magnetic resonance images is routinely limited to global descriptors such as size and transmurality. Statistical tools from computational anatomy can drastically improve this characterization, and refine the assessment of therapeutic procedures aiming at infarct size reduction. Based on these techniques, we propose a new characterization of myocardial injury up to the pixel resolution. We demonstrate it on the imaging data from the Minimalist Immediate Mechanical Intervention randomized clinical trial (MIMI: NCT01360242), which aimed at comparing immediate and delayed stenting in acute ST-Elevation Myocardial Infarction (STEMI) patients.MethodsWe analyzed 123 patients from the MIMI trial (62 & PLUSMN; 12 years, 98 male, 65 immediate 58 delayed stenting). Early and late enhancement images were transported onto a common geometry using techniques inspired by statistical atlases, allowing pixel-wise comparisons across population subgroups. A practical visualization of lesion patterns against specific clinical and therapeutic characteristics was also proposed using state-of-the-art dimensionality reduction.ResultsInfarct patterns were roughly comparable between the two treatments across the whole myocardium. Subtle but significant local differences were observed for the LCX and RCA territories with higher transmurality for delayed stenting at lateral and inferior/inferoseptal locations, respectively (15% and 23% of myocardial locations with a p-value <0.05, mainly in these regions). In contrast, global measurements were comparable for all territories (no statistically significant differences for all-except-one measurements before standardization / for all after standardization), although immediate stenting resulted in more subjects without reperfusion injury.ConclusionOur approach substantially empowers the analysis of lesion patterns with standardized comparisons up to the pixel resolution, and may reveal subtle differences not accessible with global observations. On the MIMI trial data as illustrative case, it confirmed its general conclusions regarding the lack of benefit of delayed stenting, but revealed subgroups differences thanks to the standardized and finer analysis scale.",Not About Sufficiency
The Present and Future of Whole Genome Sequencing (WGS) and Whole Metagenome Sequencing (WMS) for Surveillance of Antimicrobial Resistant Microorganisms and Antimicrobial Resistance Genes across the Food Chain,"Antimicrobial resistance (AMR) surveillance is a critical step within risk assessment schemes, as it is the basis for informing global strategies, monitoring the effectiveness of public health interventions, and detecting new trends and emerging threats linked to food. Surveillance of AMR is currently based on the isolation of indicator microorganisms and the phenotypic characterization of clinical, environmental and food strains isolated. However, this approach provides very limited information on the mechanisms driving AMR or on the presence or spread of AMR genes throughout the food chain. Whole-genome sequencing (WGS) of bacterial pathogens has shown potential for epidemiological surveillance, outbreak detection, and infection control. In addition, whole metagenome sequencing (WMS) allows for the culture-independent analysis of complex microbial communities, providing useful information on AMR genes occurrence. Both technologies can assist the tracking of AMR genes and mobile genetic elements, providing the necessary information for the implementation of quantitative risk assessments and allowing for the identification of hotspots and routes of transmission of AMR across the food chain. This review article summarizes the information currently available on the use of WGS and WMS for surveillance of AMR in foodborne pathogenic bacteria and food-related samples and discusses future needs that will have to be considered for the routine implementation of these next-generation sequencing methodologies with this aim. In particular, methodological constraints that impede the use at a global scale of these high-throughput sequencing (HTS) technologies are identified, and the standardization of methods and protocols is suggested as a measure to upgrade HTS-based AMR surveillance schemes.",Not About Sufficiency
How to update a living systematic review and keep it alive during a pandemic: a practical guide,"BackgroundThe covid-19 pandemic has highlighted the role of living systematic reviews. The speed of evidence generated during the covid-19 pandemic accentuated the challenges of managing high volumes of research literature.MethodsIn this article, we summarise the characteristics of ongoing living systematic reviews on covid-19, and we follow a life cycle approach to describe key steps in a living systematic review.ResultsWe identified 97 living systematic reviews on covid-19, published up to 7th November 2022, which focused mostly on the effects of pharmacological interventions (n = 46, 47%) or the prevalence of associated conditions or risk factors (n = 30, 31%). The scopes of several reviews overlapped considerably. Most living systematic reviews included both observational and randomised study designs (n = 45, 46%). Only one-third of the reviews has been updated at least once (n = 34, 35%). We address practical aspects of living systematic reviews including how to judge whether to start a living systematic review, methods for study identification and selection, data extraction and evaluation, and give recommendations at each step, drawing from our own experience. We also discuss when it is time to stop and how to publish updates.ConclusionsMethods to improve the efficiency of searching, study selection, and data extraction using machine learning technologies are being developed, their performance and applicability, particularly for reviews based on observational study designs should improve, and ways of publishing living systematic reviews and their updates will continue to evolve. Finally, knowing when to end a living systematic review is as important as knowing when to start.",Not About Sufficiency
Real-time water quality monitoring using AI-enabled sensors: Detection of contaminants and UV disinfection analysis in smart urban water systems,"This study introduces a novel method for assessing water quality, employing a cutting-edge sensor system integrated with artificial intelligence (AI) technologies. Addressing the global challenge of water scarcity and pollution, the research focuses on the innovative use of spectroscopic analysis for real-time water quality monitoring. The study evaluates the effectiveness of this system in distinguishing between clean, contaminated, and UV-disinfected water samples, highlighting its precision in detecting variations in water quality. Central to the research is the deployment of advanced machine learning algorithms, including Random Forest, Support Vector Machines (SVM), and Neural Networks, to process and classify spectral data. These models demonstrate remarkable accuracy in real-time classification, underscoring the synergy between AI and environmental science in addressing critical public health issues. Significantly, the study showcases the potential of UV disinfection in water treatment, as evidenced by the spectral changes observed in disinfected water samples. This aspect of the research emphasizes the role of spectral analysis in verifying the efficacy of water treatment processes. Overall, this study paves the way for more sophisticated and accessible water quality monitoring systems, offering a promising solution to one of the most pressing environmental challenges. The integration of AI and spectral analysis in this research offers a breakthrough in ensuring a safe water supply and effective water resource management. This study utilizes advanced machine learning algorithms, including Random Forest, Support Vector Machines (SVM), and Neural Networks, for water quality assessment. These models process and classify spectral data with high precision, highlighting variations in water quality.",Not About Sufficiency
Placebo urban interventions: Observing Smart City narratives in Santiago de Chile,"The implementation of the Smart City (SC) model in Santiago, Chile has not heralded any significant interventions in terms of scale, urban impact, amount invested, technological innovation or architectural design. Instead, material interventions have been small and have had little more than a superficial impact upon the perceptions of citizens. The significance of observing 'Smart' interventions in Santiago involves analysing their implementation under a provincialising lens in order to observe the way local experience transforms monist ways of thinking about SCs. Based on ethnographic observation of an SC intervention (in Paseo Bandera, Santiago de Chile), four principles of intervention were identified: democratisation of the city, spatial appropriation by citizens, social and technological innovation and local and territorialised interventions. These principles help to identify the intervention as an urban placebo, which the article argues works through the fictions of effective interventions and urban image improvement that seek to participate in worlding practices whilst, in reality, very little is being improved or effectively addressed in the city. Paseo Bandera SC intervention presents a narrative of modern, sustainable and technologically advanced urban planning in the form of specific material interventions, when in fact it involves very little modernity, sustainability or technology, and is little more than a continuation and evolution of the neoliberal urban model that exists in Chile.",Not About Sufficiency
"Implementing Findable, Accessible, Interoperable, Reusable (FAIR) Principles in Child and Adolescent Mental Health Research: Mixed Methods Approach","Background: The FAIR (Findable, Accessible, Interoperable, Reusable) data principles are a guideline to improve the reusability of data. However, properly implementing these principles is challenging due to a wide range of barriers. Objectives: To further the field of FAIR data, this study aimed to systematically identify barriers regarding implementing the FAIR principles in the area of child and adolescent mental health research, define the most challenging barriers, and provide recommendations for these barriers. Methods: Three sources were used as input to identify barriers: (1) evaluation of the implementation process of the Observational Medical Outcomes Partnership Common Data Model by 3 data managers; (2) interviews with experts on mental health research, reusable health data, and data quality; and (3) a rapid literature review. All barriers were categorized according to type as described previously, the affected FAIR principle, a category to add detail about the origin of the barrier, and whether a barrier was mental health specific. The barriers were assessed and ranked on impact with the data managers using the Delphi method. Results: Thirteen barriers were identified by the data managers, 7 were identified by the experts, and 30 barriers were extracted from the literature. This resulted in 45 unique barriers. The characteristics that were most assigned to the barriers were, respectively, external type (n=32/45; eg, organizational policy preventing the use of required software), tooling category (n=19/45; ie, software and databases), all FAIR principles (n=15/45), and not mental health specific (n=43/45). Consensus on ranking the scores of the barriers was reached after 2 rounds of the Delphi method. The most important recommendations to overcome the barriers are adding a FAIR data steward to the research team, accessible step-by-step guides, and ensuring sustainable funding for the implementation and long-term use of FAIR data. Conclusions: By systematically listing these barriers and providing recommendations, we intend to enhance the awareness of researchers and grant providers that making data FAIR demands specific expertise, available tooling, and proper investments.",Not About Sufficiency
How to manage urban disturbances: Focused on social-ecological vulnerability to fine dust pollution,"Since the pattern of fine dust pollution varies regionally, it is necessary to identify areas vulnerable to fine dust pollution based on local community and environmental infrastructure and then plan green infrastructure for these areas. The purpose of this study is to analyze the suitability area of green infrastructure systems to improve urban resilience with regards to fine dust pollution. Regarding the research methods, first, a search for previous studies is conducted to establish an evaluation index for the resilience to fine dust pollution. Second, data on the environmental and social factors of the case study site (Suwon, Republic of Korea) are collected and mapped based on the resilience evaluation indices established for fine dust pollution. Third, these indices are used to confirm that regional differences caused by landscape fragmentation exist with regards to resilience to fine dust pollution. This study yields two key outcomes. Firstly, the indicators associated with resilience against fine dust pollution are categorized into environmental and social factors. Environmental factors encompass the composition of pollutants (including concentrations of PM2.5, PM10, NO2, O3, CO, and SO2), proximity to roads, and environmental assets such as well-established roadside green spaces, parks, forests, and thriving vegetation. Meanwhile, social factors incorporate considerations of social welfare services and air pollutant emission facilities. These factors contribute to the comprehensive evaluation indices, encompassing exposure, sensitivity, and adaptive capacity concerning fine dust pollution. Secondly, the spatial analysis of fine dust pollution within the case study area reveals that regions exhibiting robust social-ecological system adaptive capacities tend to demonstrate heightened resilience against fine dust pollution. Conversely, areas characterized by elevated exposure and sensitivity to this issue exhibit reduced resilience. Moreover, the investigation into landscape fragmentation highlights a strong correlation between resilience against fine dust pollution and the fragmentation of green spaces. This study deviates from previous research endeavors through its innovative approach of strategically planning green infrastructure in vulnerable zones, aligning with local characteristics. It further introduces distinct resilience evaluation indicators tailored to fine dust pollution and integrates considerations of the spatial distribution of social-ecological systems, thereby contributing to a nuanced understanding of this complex issue.",Not About Sufficiency
"Sympatric Seals, Satellite Tracking and Protected Areas: Habitat-Based Distribution Estimates for Conservation and Management","Marine predator populations are crucial to the structure and functioning of ecosystems. Like many predator taxa, pinnipeds face an increasingly complex array of natural and anthropogenic threats. Understanding the relationship between at-sea processes and trends in abundance at land-based monitoring sites requires robust estimates of at-sea distribution, often on multi-region scales. Such an understanding is critical for effective conservation management, but estimates are often limited in spatial extent by spatial coverage of animal-borne tracking data. Grey (Halichoerus grypus) and harbour seals (Phoca vitulina) are sympatric predators in North Atlantic shelf seas. The United Kingdom (UK) and Ireland represents an important population centre for both species, and Special Areas of Conservation (SACs) are designated for their monitoring and protection. Here we use an extensive high-resolution GPS tracking dataset, unprecedented in both size (114 grey and 239 harbour seals) and spatial coverage, to model habitat preference and generate at-sea distribution estimates for the entire UK and Ireland populations of both species. We found regional differences in environmental drivers of distribution for both species which likely relate to regional variation in diet and population trends. Moreover, we provide SAC-specific estimates of at-sea distribution for use in marine spatial planning, demonstrating that hotspots of at-sea density in UK and Ireland-wide maps cannot always be apportioned to the nearest SAC. We show that for grey seals, colonial capital breeders, there is a mismatch between SACs (where impacts are likely to be detected) and areas where impacts are most likely to occur (at sea). We highlight an urgent need for further research to elucidate the links between at-sea distribution during the foraging season and population trends observed in SACs. More generally, we highlight that the potential for such a disconnect needs to be considered when designating and managing protected sites, particularly for species that aggregate to breed and exhibit partial migration (e.g. grey seals), or spatial variation in migration strategies. We demonstrate the use of strategic tracking efforts to predict distribution across multiple regions, but caution that such efforts should be mindful of the potential for differences in species-environment relationships despite similar accessible habitats.",Not About Sufficiency
Inclusion of Gender Views for the Evaluation and Mitigation of Urban Vulnerability: A Case Study in Castellon,"The inclusion of gender views in every field and, especially, in urbanism, has become crucial for urban planning. Considering both men's and women's interests in an integrated gender equality perspective provides better results that improve the quality of public spaces and engenders a more sustainable society. However, to realize such benefits, efforts are required not only to understand the needs and preferences of urban residents but also to shape policies and develop strategies to mitigate vulnerability with population involvement. In order to help decision makers at the urban level evaluate vulnerability with the inclusion of gender views, this study proposes a model that incorporates the specificities of urban fabric users that face adverse conditions. The model is based on a structured and standardized checklist of key topics that could be applied to any urban development. From this checklist, a list of categories, subcategories, and indicators were proposed and validated using the inter-judge agreement technique. To illustrate this model, this paper presents the case study of Castellon (Spain) in which deprived neighborhoods were analyzed, updating a previous model intended only to detect vulnerability. The results help link policy making to social vulnerability and indicate strategies to reach inclusive neighborhoods via a gender equality approach.",Not About Sufficiency
Association between handgrip strength and small airway disease in patients with stable chronic obstructive pulmonary disease,"Background: Chronic obstructive pulmonary disease (COPD) is associated with airflow limitation resulting from a combination of small airway disease (SAD) and parenchymal destruction. Although various diagnostic methods for SAD exist, access to these tools can be limited.Objectives: This study aimed to explore the correlation between handgrip strength (HGS) and SAD in COPD patients.Design: Cross-sectional prospective study.Methods: HGS was measured using a hand dynamometer. SAD was evaluated using impulse oscillometry, with results reported as the difference between respiratory resistance at 5 and 20 Hz (R5-R20). SAD was defined as R5-R20 >= 0.07 kPa/L/s. The receiver operator characteristic (ROC) curves, sensitivity, and specificity values were calculated to determine the optimal cutoff value of HGS for predicting SAD.Results: Sixty-four patients (90.6% male) were included. The average age was 72.1 +/- 8.3 years, and body mass index was 23.4 +/- 4.2 kg/m2. FEV1 was 71.6 +/- 21.3%, and HGS was 30.2 +/- 8.1 kg. R5-R20 was 0.11 +/- 0.08 kPa/L/s. SAD was found in 64.1% of patients. A negative correlation between HGS and R5-R20 was observed (r = -0.332, p = 0.007). The best cutoff value for HGS in detecting SAD was determined to be 28.25 kg, with a sensitivity of 73.9%, specificity of 65.9%, and an area under ROC curve of 0.685 (95% CI 0.550-0.819, p = 0.015).Conclusion: SAD is common in COPD patients, and HGS is significantly negatively correlated with SAD. This tool might serve as an alternative or adjunctive assessment for small airway dysfunction in COPD patients.Registration: This study was registered with ClinicalTrials.gov with number NCT06223139. Association of handgrip strength with small airway disease in COPDIn chronic obstructive pulmonary disease (COPD), the condition of small airways (those less than 2 mm wide) is closely linked to disease severity. Small airway disease (SAD) leads to reduced lung function, increased retention of excess gas in the lung, and poorer health in COPD patients. While various diagnostic methods exist for SAD, they may not always be accessible. This study investigated handgrip strength as a potential alternative way to assess SAD. Our findings show a significant negative correlation between handgrip strength and SAD in COPD patients. This suggests that handgrip strength could be used alongside existing methods to evaluate small airway dysfunction in these patients.",Not About Sufficiency
Quantitative Evaluation of the Equity of Public Service Facility Layout in Urumqi City for Sustainable Development,"Since the start of the new century, the focus of China's socioeconomic development has gradually shifted from prioritizing efficiency to social equity, which is an important ingredient of sustainable development. The accessibility of public service facilities (PSFs) is vital for achieving social-spatial sustainability. As a basic tool for arranging PSFs, however, traditional urban plannings mainly focus on the spatial uniform distribution of facilities rather than the variance of the spatial distribution of populations they serve. So, by taking the dual perspective view, this paper quantitatively measures the balance of PSFs' spatial distribution of and populations of Urumqi City at the sub-district level. Based on point of interest (POI) data, this paper calculates and analyzes Gini coefficients and location entropy of three basic PSF types: living service facilities (LSFs), primary schools and kindergartens (PSAKs), and medical facilities (MFs). The research finds that the Gini coefficients of LSFs, PSAKs, and MFs in Urumqi City are 0.42, 0.36, and 0.34, respectively. Moreover, there are three significant mismatch areas: an extremely high PSF index value in low-population sub-districts, an extremely low index value in remote suburbs, and an extremely low index value in the city center. These findings indicate an obvious imbalance between the spatial distribution of PSFs and the population in Urumqi, which may be a critical impediment to sustainable development. Based on these, this paper offers guidance for achieving sustainability in the allocation of spatial resources.",Not About Sufficiency
"Digital transformation, productive services agglomeration and innovation performance","Innovation is a necessary guarantee for sustainable development. Stepping into the digital age, digital transformation has triggered the innovation revolution. This paper takes 30 provinces in China from 2012 to 2022 as the research sample, we verify whether digital transformation has improved innovation performance. Based on the Solow growth model and agglomeration economics theory, we also explore the moderating role and threshold effect of agglomeration in productive service industry between digital transformation and innovation performance. To achieve this, we apply the methods of machine learning and text analysis to construct an evaluation index of regional digital transformation and measure it. The paper finds that China's digital transformation index is increasing, but there is a digital divide between regions. We also determine that digital transformation significantly and positively contributes to the level of innovation performance. Considering the threshold effect of agglomeration in productive service industry, the impact of digital transformation on innovation performance exhibits non-linear characteristics, As the level of agglomeration continues to exceed the threshold, the innovation-driven effect of digital transformation increases. The research results help clarify the relationship between digital transformation and innovation performance, and provide favorable policy directions for regional governments to identify digital divides and make reasonable industrial layouts. Thus, it can promote the construction of digital China and innovation power, injecting strong innovation force into the realization of SDGs.",Not About Sufficiency
The gentrification of a post-industrial English rural village: Querying urban planetary perspectives,"Recent years have seen the growth of planetary perspectives related to urbanisation and gentrification that have challenged the significance of differentiations of rural and urban space. This paper explores the arguments advanced in these perspectives, highlighting claims that they are based on a critique of methodological territorialism that has long been employed in rural and urban studies, as well as exhibiting an urbanormativity that was arguably absent from some earlier critiques of people such as Ray Pahl. This paper seeks to develop a study of rural gentrification that avoids urbanormativity and methodological territorialism. After reviewing debates related to academic and lay conceptions of the urban and rural, the paper highlights how territorial representations may warrant investigation even when social practices may be seen to routinely traverse boundaries of, for example, the rural. The relevance of these ideas to gentrification is then explored through an investigation of the gentrification of a village, which like many 'urban' settlements, has experienced both industrialisation and de industrialisation. Drawing on the results of a 'mixed-method questionnaire' conducted in this village in Calderdale, England, the paper explores how the lives of residents are connected into more and less distant urban spaces through an analysis of migrational movements and employment relations, including commuting patterns. It is argued that, in line with arguments advanced within studies of planetary urbanisation/gentrification, there is considerable interconnection between the village and areas that have been classified as urban. However, it is also shown that neither this interconnection, nor the areas industrial past, means that symbolic and affective senses of rurality are insignificant to village residents, or to the practices of gentrification that have emerged. This did not mean that representations of rurality were unimpacted by industry and urban connectivity, with the paper detailing that whilst the village was widely seen as rural, it was also often seen as exhibiting significant unconformity from expectations of rurality. The paper ends by considering how senses of separation and connectivity to urban and industrial spaces link to different types of gentrifiers colonising the village.",Not About Sufficiency
On-Board Diagnostics and Driver Profiling,"Automobile manufacturing industry is an ever-developing industry driven to satisfy consumer's needs focusing on key factors like performance, comfort, luxury, eco-friendliness and most importantly driver and passenger safety. Driver profiling in modern times is a decisive and convenient method ensuing driver and passenger safety. This paper offers a knowledge base outline to the consumer for On-Board Diagnostics (OBD) and Driver Profiling. On-Board Diagnostics assists the user in car maintenance by enlisting Diagnostic Trouble Codes (DTC) from the car's Engine Control Unit (ECU) using an Android application. The second aspect of this paper, i.e. to profile the driver's behaviour is proposed in two different approaches, offering mathematical, visual and analytical analysis of consumer's driving behaviour. A simple, immediate and cost-effective approach to profile driver behaviours using GPS co-ordinates is accessible on the same Android application proposed for On-Board diagnostics. An alternate detailed, novel approach to profile driver behaviour is based on car engine parameters such as Engine speed/rpm, Vehicular Speed, Engine load & Throttle valve; is modeled using Data Analytics & unsupervised Machine Learning techniques.",Not About Sufficiency
Interpretable genotype-to-phenotype classifiers with performance guarantees,"Understanding the relationship between the genome of a cell and its phenotype is a central problem in precision medicine. Nonetheless, genotype-to-phenotype prediction comes with great challenges for machine learning algorithms that limit their use in this setting. The high dimensionality of the data tends to hinder generalization and challenges the scalability of most learning algorithms. Additionally, most algorithms produce models that are complex and difficult to interpret. We alleviate these limitations by proposing strong performance guarantees, based on sample compression theory, for rule-based learning algorithms that produce highly interpretable models. We show that these guarantees can be leveraged to accelerate learning and improve model interpretability. Our approach is validated through an application to the genomic prediction of antimicrobial resistance, an important public health concern. Highly accurate models were obtained for 12 species and 56 antibiotics, and their interpretation revealed known resistance mechanisms, as well as some potentially new ones. An open-source disk-based implementation that is both memory and computationally efficient is provided with this work. The implementation is turnkey, requires no prior knowledge of machine learning, and is complemented by comprehensive tutorials.",Not About Sufficiency
"Simultaneous localization, calibration, and tracking in an ad hoc sensor network","We introduce Simultaneous Localization and Tracking, called SLAT, the problem of tracking a target in a sensor network while simultaneously localizing and calibrating the nodes of the network. Our proposed solution, LaSLAT, is a Bayesian filter that provides on-line probabilistic estimates of sensor locations and target tracks. It does not require globally accessible beacon signals or accurate ranging between the nodes. Real hardware experiments are presented for 2D and 3D, indoor and outdoor, and ultrasound and audible ranging-hardware-based deployments. Results demonstrate rapid convergence and high positioning accuracy.",Not About Sufficiency
Moving toward resilience and sustainability in the built environment,"Developing and managing infrastructure requires considering the system's physical performance and operational, financial, social, environmental, and managerial aspects. These aspects interact in a dynamic environment that evolves and changes continuously. Changes in demand, socio-economic pressures, and the increasing frequency and intensity of natural events - exacerbated by climate change - make resilience and sustainability essential for the built environment's current and future performance. Sustainable infrastructures add value to society and maintain social equity and justice through time. To achieve this, it is necessary to consider socioeconomic as well as environmental aspects. On the other hand, resilience focuses on recovery in case of a damaging event, a critical property that minimizes functionality disruptions. This paper presents a conceptual discussion about the role of resilience and sustainability in relationship with infrastructure and the built environment's design and operation. It provides insight into risk-informed decision-making for evolving infrastructure systems, and change based on a systems-thinking approach. These concepts are central to the built environment's safe and responsible evolution and growth. In the end, the paper identifies challenges and proposes future research paths.",Not About Sufficiency
"The PM2.5-bound polycyclic aromatic hydrocarbon behavior in indoor and outdoor environments, part II: Explainable prediction of benzo[a] pyrene levels","Among the polycyclic aromatic hydrocarbons (PAH), benzo[a]pyrene (B[a]P) has been considered more relevant than other species when estimating the potential exposure-related health effects and has been recognized as a marker of carcinogenic potency of air pollutant mixture. The current understanding of the factors which govern non-linear behavior of B[a]P and associated pollutants and environmental processes is insufficient and further research has to rely on the advanced analytical approach which averts the assumptions and avoids simplifications required by linear modeling methods. For the purpose of this study, we employed eXtreme Gradient Boosting (XGBoost), SHapley Additive exPlanations (SHAP) attribution method, and SHAP value fuzzy clustering to investigate the concentrations of inorganic gaseous pollutants, radon, PM2.5 and particle constituents including trace metals, ions, 16 US EPA priority PM2.5-bound PAHs and 31 meteorological variables, as key factors which shape indoor and outdoor PM2.5-bound B[a]P distribution in a university building located in the urban area of Belgrade (Serbia). According to the results, the indoor and outdoor B[a]P levels were shown to be highly correlated and mostly influenced by the concentrations of Chry, B[b]F, CO, B[a]A, I[cd]P, B[k]F, Flt, D[ah]A, Pyr, B[ghi]P, Cr, As, and PM2.5 in both indoor and outdoor environments. Besides, high B[a]P concentration events were recorded during the periods of low ambient temperature (<12 degrees C), unstable weather conditions with precipitation and increased soil humidity.",Not About Sufficiency
Spatial and temporal forecasting of groundwater anomalies in complex aquifer undergoing climate and land use change,"Monitoring groundwater (GW) level variations, or anomalies in multiple wells, over long periods of time is essential to understanding changes in regional groundwater resource availability. However, it is challenging to predict these GW anomalies over the long term in agricultural areas due to complicated boundary conditions, heterogeneous hydrogeological characteristics, and groundwater extraction, as well as nonlinear interactions among these factors. To overcome this challenge, we developed an advanced modeling framework based on a recurrent neural network of long short -term memory (LSTM) as an alternative to complex and computationally expensive physical models. GW anomalies were forecast two months in advance (t + 2) based on the evaluation of drivers that influence GW dynamics in densely irrigated agricultural regions. An application and evaluation of this new approach was conducted in the Wisconsin Central Sands (WCS) region in the U.S., one of the most productive agricultural regions. The modeling framework was developed for the period 1958 - 2020 by utilizing easily accessible dynamic and static variables to represent hydrometeorological and geological characteristics. GW anomaly observations were acquired from 26 piezometers (wells) installed in the sandy aquifer in WCS over 10 - 60 years. The subset of GW observations from - 10 - 60 years, not used in model training, can forecast GW anomalies two months out with a coefficient of determination R 2 - 0.8. Additionally, MAE was less than 0.34 m/ month across the study region for both temporal and spatial modeling frameworks. Groundwater anomalies showed high spatiotemporal variability, and their responses are influenced differently by boundary conditions, catchment geology, climate, and topography across locations. Sites with higher autocorrelation with previous two-months GW anomalies reduced bias by increasing R 2 . Land use change and irrigation pumping have interactive effects on GW anomalies forecasting. The novelty of this study is identifying the regional drivers of GW fluxes. This case-specific information and location-related simplification, modification, and assumption of LSTM is a unique contribution to the existing literature. Our framework can be used as an alternative method of simulating water availability and groundwater level changes in areas where subsurface properties are unknown or difficult to determine.",Not About Sufficiency
"BPAGS: a web application for bacteriocin prediction via feature evaluation using alternating decision tree, genetic algorithm, and linear support vector classifier","The use of bacteriocins has emerged as a propitious strategy in the development of new drugs to combat antibiotic resistance, given their ability to kill bacteria with both broad and narrow natural spectra. Hence, a compelling requirement arises for a precise and efficient computational model that can accurately predict novel bacteriocins. Machine learning's ability to learn patterns and features from bacteriocin sequences that are difficult to capture using sequence matching-based methods makes it a potentially superior choice for accurate prediction. A web application for predicting bacteriocin was created in this study, utilizing a machine learning approach. The feature sets employed in the application were chosen using alternating decision tree (ADTree), genetic algorithm (GA), and linear support vector classifier (linear SVC)-based feature evaluation methods. Initially, potential features were extracted from the physicochemical, structural, and sequence-profile attributes of both bacteriocin and non-bacteriocin protein sequences. We assessed the candidate features first using the Pearson correlation coefficient, followed by separate evaluations with ADTree, GA, and linear SVC to eliminate unnecessary features. Finally, we constructed random forest (RF), support vector machine (SVM), decision tree (DT), logistic regression (LR), k-nearest neighbors (KNN), and Gaussian naive Bayes (GNB) models using reduced feature sets. We obtained the overall top performing model using SVM with ADTree-reduced features, achieving an accuracy of 99.11% and an AUC value of 0.9984 on the testing dataset. We also assessed the predictive capabilities of our best-performing models for each reduced feature set relative to our previously developed software solution, a sequence alignment-based tool, and a deep-learning approach. A web application, titled BPAGS (Bacteriocin Prediction based on ADTree, GA, and linear SVC), was developed to incorporate the predictive models built using ADTree, GA, and linear SVC-based feature sets. Currently, the web-based tool provides classification results with associated probability values and has options to add new samples in the training data to improve the predictive efficacy. BPAGS is freely accessible at https://shiny.tricities.wsu.edu/bacteriocin-prediction/.",Not About Sufficiency
Lacustrine Urban Blue Spaces: Low Availability and Inequitable Distribution in the Most Populated Cities in Mexico,"Lacustrine blue spaces provide benefits to the urbanites and wildlife habitat. Their availability varies depending on the city in which they are established and intra-urban social interactions. We analyzed the presence, distribution, and size of lentic water bodies in Mexico's 145 most populated cities. We searched for patterns in their distribution concerning demographic, socioeconomic, and geographic data, aiming to understand their socio-ecological interactions in cities. We digitized lacustrine spaces to obtain their number per city, total surface, area of blue space per inhabitant, and surface as a percentage of the city's total area. We tested for relationships between their number and surface and city population, hydrological regions, and urban marginalization index through linear and generalized linear models. We delimited 1834 lacustrine blue spaces, finding almost two-thirds of them artificial. Their presence and surface in Mexican cities were generally low, except for hydrological regions close to the Gulf of Mexico. Their number and surface decreased as the urban marginalization index increased. The lack of equitable provision of lacustrine space at the national level has implications for urban planning and land management. Blue spaces should maximize their ecosystem services' provision for the whole society to promote cities' sustainability and resilience.",Not About Sufficiency
Application of the ISHTAR Suite for the assessment of the health effects of Rome environmental policy,"The ISHTAR Suite is an innovative software tool that integrates several models for the simulation of the effects of transport and land use policies on the urban environment, population health and artistic heritage. Starting from the simulation of the effects of the postulated measure on the citizens behaviour in terms of daily movements, the suite calculation path goes through the modelling of transport, vehicles safety and emissions of pollutants and noise, pollutants dispersion and noise propagation, exposure to pollutants, noise and accidents and related risk assessment, monuments degradation, up to the overall comparison of the alternative scenarios in terms of cost-benefit or multi criteria analysis. The software modules are integrated by a Suite Manager that controls the tools execution by means of dedicated software 'connectors', and is linked to a User Interface, a suite database and a commercial Geographic Information System. The ISHTAR Suite is now being tested in the seven cities involved in the FP5 EESD Programme ISHTAR Project: Athens, Bologna, Brussels, Graz, Grenoble, Paris and Rome, with the analysis of different measures and policies. In the Rome application the effects on health of a traffic banning policy were studied.",Not About Sufficiency
A reinforcement learning-based online learning strategy for real-time short-term load forecasting,"Real-time Short-Term Load Forecasting (STLF) is crucial for energy management and power system operations. Conventional Machine Learning (ML) methodologies for STLF are often challenged by the inherent variability in energy demand. To tackle the challenge associated with inherent variability, this paper presents a novel Reinforcement Learning (RL)-enhanced STLF method. Different from conventional methods, our method dynamically improves the STLF model by selecting optimal training data to capture recent power usage trends and possible variations in demand patterns. By doing so, our method can significantly reduce the impact of unforeseen fluctuations in real-time forecasting. In addition to the novel RL-enhanced STLF method, we propose a comprehensive evaluation framework, encompassing three key dimensions: accuracy, runtime efficiency, and robustness. Tested on three distinct real-world energy datasets, our RL-enhanced method demonstrates superior forecasting performance across three evaluation metrics by achieving accurate and robust predictions in real-time under varying scenarios. Furthermore, our approach provides uncertainty bounds for practical predictions applications. These results underscore the significant advancements made by our RL-based method in forecasting precision, efficiency, and robustness. We have made our algorithm openly accessible online to promote continued development and advancement of STLF methods.",Not About Sufficiency
Sustainability Indicators in Restaurants: The Development of a Checklist,"This study aimed to develop and carry out content validation, semantic evaluation, reproducibility, and internal consistency of a checklist designed to verify the sustainability indicators in foodservice. The preliminary version of the checklist was prepared based on the international standards ISO (International Organization for Standardization) 14000, ISO 14001, ISO 14004 and documents from the Sustainable Restaurant Association (SRA) Certification, Green Seal Certifications, and Green Restaurant Association (GRA) certification, in addition to the American Dietetic Association (ADA) position. Thirteen experts in the study topic performed the content validation and semantic evaluation of the checklist (a minimum of 80% agreement among experts and mean value >= 4 on a 5-point Likert scale were needed to keep the item in the instrument). After consensus was reached by the experts' panel, two different researchers applied the checklist in 20 restaurants (at the same time, in the same place, without communication between them) for the analysis of reproducibility and internal consistency (Federal District, Brazil). The agreement among answers was verified by Cohen's Kappa coefficient. The final version of the checklist consisted of 76 items, divided into three sections (1. water, energy, and gas supply; 2. menu and food waste; 3. waste reduction, construction materials, chemicals, employees, and social sustainability). The developed checklist was validated concerning the content, approved in the semantic evaluation, reproducible, and with good reliability (Intraclass Correlation Coefficient (ICC) > 0.9 and alpha > 0.672).",Not About Sufficiency
Emergency reconstruction of large general hospital under the perspective of new COVID-19 prevention and control,"Objective To summarize the successful experience of timely crisis management, correct measures, and successful display of the hospital image in the First Affiliated Hospital of Zhejiang University (FAHZU), to improve the ability of emergency response. Methods The FAHZU, as the earliest designated hospital, accomplished the transformation from general hospital to infectious disease hospital under the guiding ideology of centralized patients, centralized experts, centralized resources, and centralized treatment with measures to transfer the Zhijiang campus hospitalized patients quickly, complete the space layout, create diagnosis and treatment space, streamline logistics, and transform logistics facilities within 48 & x202f;h. As of 5 March, the hospital had admitted 104 patients. Results Of the severe cases in Zhejiang province 95% underwent centralized treatment with the goal of zero deaths for severely ill patients, zero misdiagnoses for infected patients, and zero infections for medical staff, and this served as a reference for large medical institutions regarding how to manage such a public health emergency. Conclusion The successful cases of FAHZU provided a valuable experience for large medical institutions on how to address public health emergencies and how to carry out diagnosis and treatment and streamline the layout and related facilities in emergency reconstruction.",Not About Sufficiency
Measuring Cluster-Based Spatial Access to Shopping Stores under Real-Time Travel Time,"Shopping stores are an important part of retail facilities and indispensable public facilities in a city. They are not only concentrated in shopping malls, but also distributed independently throughout the city, and often agglomerated in space. This paper attempts to measure the rationality of the spatial layout of all shopping stores in the city. Residents will visit multiple shopping stores in one trip to meet their demands. Based on this characteristic, this paper studies shopping store clusters and proposes a cluster two-step floating catchment area (C-2SFCA) method to analyze the accessibility differences of shopping stores in urban areas. Using the case of Beijing within the Fifth Ring Road, this paper implements the C-2SFCA method in a study unit of traffic analysis zones (TAZ) considering three transport modes (car, public transport, walking) with the support of real-time travel time collected from an internet map. The results show that spatial accessibility differed greatly under different transport modes and also had an uneven distribution pattern. Among these three results, the spatial variation of public transport accessibility was the highest. The results can provide references for urban planners in facility configuration and decision-making.",Not About Sufficiency
Toward Control and Coordination in Cognitive Autonomous Networks,"The incorporation of Artificial Intelligence (AI) and Machine Learning (ML) in mobile networks is expected to raise the degree of automation by proposing Cognitive Autonomous Networks (CAN). In CAN, learning based functions, called Cognitive Functions (CFs), adjust network control parameters to optimize specific Key Performance Indicators (KPIs). The CFs share the same resources, and this very often introduces an overlap among their target control parameter adjustment, i.e., at one point of time, multiple CFs may want to change the same control parameter albeit by different amounts depending on their respective levels of interest in that parameter. Correspondingly, a Controller is required in CAN to coordinate the sharing of the parameter among the independent CFs to meet their varying extents of interests. Although a Nash Social Welfare Function (NSWF) based Controller was introduced at first, to overcome the problems of this Controller a second Controller was introduced based on Eisenberg-Gale Solution (EGS). To use an EGS based Controller, impact of each network control parameter on each CF, called Config-Weight (CW), needs to be calculated. In this paper we propose a Shapley value based method for CW calculation, prove the optimality of the method mathematically and by simulation, provide a comparison between the Controllers in a simulation environment that resembles 5G network and find that up to 9.18% improvement can be obtained using the EGS based Controller.",Not About Sufficiency
Industrial air pollution and self-reported respiratory and irritant health effects on adjacent residents: a case study of Islamabad Industrial Estate (IEI),"Human health deterioration due to industrial air emissions is gaining global attention. These health impacts are coupled mainly with chronic and acute health issues that are persistent in residents in close proximity to the industrial estates. Similarly, the inhabitants of Islamabad Industrial Estate (IEI) are exposed to intense industrial pollutants that have caused respiratory and irritant health issues. This cross-sectional study has compared two groups on deploying 378 close-ended questionnaires at the household level, after adjusting the potential confounders. The groups were framed as per their distance from IEI as 'Band I' - residence <= 650 m and 'Band II' - residence >= 650-1300 m. The distances were calculated from the respondent's residence to the outer digitized boundary (perimeter of IEI), whereas the perimeter was digitized using Google Earth and imported into a Geographical Information System. The results of multiple logistic regression for odds ratios confirmed significant increase in chronic respiratory problems (chronic bronchitis OR 1.93 (1.05-3.55), phlegm OR 2.5 (1.4-4.5), dyspnoea OR 2.18 (0.3-1.81)) and acute irritation issues (eye irritation OR 2.59 (1.63-3.90), throat irritation OR 1.782 (0.876-2.725)) in case of Band I. The same diseases were found in Band II but in less severity; however, their existence remained life-threatening for the locality. The study calls for preventive measures by the local residents, health safety measures by the federal government and relocation of populace to safe areas/sectors.",Not About Sufficiency
"The Quebec Semantic Memory Battery: Development, Standardization, and Psychometric Assessment of a Semantic Memory Battery in French","Objective People with aphasia often experience semantic memory (SM) impairment. To improve diagnostic outcomes, SM tasks should recruit various sensory input channels (oral, written, and pictographic), permitting accessible, complete evaluation. There is a need for SM batteries for French-speaking Quebecers that use multiple input channels. The present study, therefore, describes the development of a novel French-language semantic battery: la Batterie quebecoise de la memoire semantique (BQMS), the assessment of the BQMS's psychometric properties, and the establishment of normative data for the BQMS.Method We first developed eight SM tasks. Following a pilot validation study, we determined the BQMS's reliability and validity, to ensure consistent, accurate detection of SM impairment. Among French-speaking Quebecers with cerebrovascular aphasia (n = 10), people with the semantic variant of Primary Progressive Aphasia (n = 4), and healthy controls (n = 14), we examined its convergent validity, concurrent validity, test-retest reliability, and internal consistency. Finally, we established normative data for the BQMS by calculating cut-off scores per task that indicate SM impairment (in 93 cognitively healthy French-speaking Quebecers), stratified by sociodemographic variables associated with performance.Results The BQMS shows high concurrent, discriminant, and convergent validity, as well as good test-retest reliability and internal consistency. The cut-off score indicating SM impairment ranged from the 2nd to 25th percentiles (stratified by task, age, and sex).Conclusions The BQMS's psychometric properties indicate that it could be a valuable clinical tool for detecting SM impairment. Our normative data will help clinicians detect such impairments.",Not About Sufficiency
Unsupervised Clustering and Active Learning of Hyperspectral Images With Nonlinear Diffusion,"The problem of unsupervised learning and segmentation of hyperspectral images is a significant challenge in remote sensing. The high dimensionality of hyperspectral data, presence of substantial noise, and overlap of classes all contribute to the difficulty of automatically clustering and segmenting hyperspectral images. We propose an unsupervised learning technique called spectral-spatial diffusion learning (DLSS) that combines a geometric estimation of class modes with a diffusion-inspired labeling that incorporates both spectral and spatial information. The mode estimation incorporates the geometry of the hyperspectral data by using diffusion distance to promote learning a unique mode from each class. These class modes are then used to label all the points by a joint spectral-spatial nonlinear diffusion process. A related variation of DLSS is also discussed, which enables active learning by requesting labels for a very small number of well-chosen pixels, dramatically boosting overall clustering results. Extensive experimental analysis demonstrates the efficacy of the proposed methods against benchmark and state-of-the-art hyperspectral analysis techniques on a variety of real data sets, their robustness to choices of parameters, and their low computational complexity.",Not About Sufficiency
Sensing Our World Using Wireless Signals,"In the last few years, pervasive wireless signals have been established as a powerful medium for ubiquitous sensing. Wireless sensing quickly becomes an active cross-disciplinary research area that involves wireless communication, signal processing, human-computer interaction, machine learning, and even hardware design. This paper provides an accessible introduction to this exciting area and presents the key technologies enabling it. It highlights the latest achievements in this field, and the unaddressed research challenges as well as how emerging wireless standards can better support this promising technique.",Not About Sufficiency
Standardization of digitized heritage: a review of implementations of 3D in cultural heritage,"The value of three-dimensional virtual objects are proven in a great variety of applications; their flexibility allowing for a substantial amount of utilization purposes. In cultural heritage this has been used for many years already, and the amount of users continue to grow as acquisition methods and implementations are becoming more approachable. Nonetheless, there are still many apparent issues with making use of all the possible benefits of 3D data in the field, varying from lack of knowledge, infrastructure, or coherent workflows. This review aims to underline the current limitations in implementing 3D workflows for various cultural heritage purposes. 45 projects and institutions are reviewed, along with the most prominent guidelines for workflows and ways of implementing the 3D data on the web. We also cover how each project manage and make their data accessible to the public. Prominent and recurring issues with standardization, interoperability, and implementation is highlighted and scrutinized. The review is concluded with a discussion on the current utilization's of 3D data for cultural heritage purposes, along with suggestions for future developments.",Not About Sufficiency
How to Further Promote Bicycle Transport? Be Inspired by Denmark,"Today Denmark is a laboratory of cycling, new trends and ideas are connected here with years of experience, and the Danish experts share them with the Czech ones at the conference in April. They recommended four fundamental principles for promotional marketing of the city cycling. 1. ""From the point A to the point B"" philosophy - quickly, comfortably, safely: the aim is to project and frame a modern city convenient for cyclists too. 2. Perception is (also) reality: the aim is to change general perception of which types of mobility are the most useful for both the citizen and the society. 3. People-friendly cities require mobility for all: the aim is to have the cities comfortable for living, well accessible, enabling their inhabitants be mobile. 4. Relocation of the cycling potential: the aim is to redemocratize bicycle and locate it among the main means of city transport.",Not About Sufficiency
Skluma: An extensible metadata extraction pipeline for disorganized data,"To mitigate the effects of high-velocity data expansion and to automate the organization of filesystems and data repositories, we have developed Skluma a system that automatically processes a target filesystem or repository, extracts content- and context-based metadata, and organizes extracted metadata for subsequent use. Skluma is able to extract diverse metadata, including aggregate values derived from embedded structured data; named entities and latent topics buried within free-text documents; and content encoded in images. Skluma implements an overarching probabilistic pipeline to extract increasingly specific metadata from files. It applies machine learning methods to determine file types, dynamically prioritizes and then executes a suite of metadata extractors, and explores contextual metadata based on relationships among files. The derived metadata, represented in JSON, describes probabilistic knowledge of each file that may be subsequently used for discovery or organization. Skluma's architecture enables it to be deployed both locally and used as an on-demand, cloud-hosted service to create and execute dynamic extraction workflows on massive numbers of files. It is modular and extensible allowing users to contribute their own specialized metadata extractors. Thus far we have tested Skluma on local filesystems, remote FTP-accessible servers, and publicly-accessible Globus endpoints. We have demonstrated its efficacy by applying it to a scientific environmental data repository of more than 500,000 files. We show that we can extract metadata from those files with modest cloud costs in a few hours.",Not About Sufficiency
Private incentives for the emergence of co-production of offshore wind energy and mussel aquaculture,"Technological solutions to increase the efficiency of spatial use can play a key role as part of the toolbox of marine spatial planning. Co-locating of multiple ocean uses can potentially increase the production and enjoyment of the ocean while limiting impacts. However, a basic precondition for co-locating or coproduction is that all parties' private incentives are aligned. We use a case study of co-locating an offshore wind energy firm and a mussel aquaculture firm to assess the incentive structure for cooperation and to demonstrate that social benefits from co-locating exist. We find that there is room for cooperation between firms based on potential cost sharing and that the demonstrated social benefits may arise without government intervention. (C) 2014 Elsevier B.V. All rights reserved.",Not About Sufficiency
"Equitable buyouts? Learning from state, county, and local floodplain management programs","Climate change-exacerbated flooding has renewed interest in property buyouts as a pillar of managed retreat from coastal zones and floodplains in the United States. However, federal buyout programs are widely critiqued for being inaccessible and inequitable. To learn whether and how subnational buyout programs overcome these limitations, we examined five leading US state, county, and local buyout programs to see what they teach us about redesigning future federal policies. Our mixed-methods research used interviews and document analysis to develop case studies, juxtaposed subnational strategies against a review of critiques of federal buyouts, and focus group discussions with subnational buyout managers and experts to identify limitations of their programs. We find that subnational programs can be more inclusive and better respond to resident needs as compared to existing federal programs due to their access to dedicated, non-federal funding and their standing institutional status, which allows them to learn and evolve over time. Nevertheless, these programs lack coordination with and control over agencies that permit development and produce affordable housing. This gives buyout programs limited power in shaping the overall equity of who lives in floodplains and who has access to affordable, resilient housing after a buyout. Their experiences suggest federal programs can support managed retreat nationwide by increasing support for institutional and staff capacity at state and county levels, encouraging efforts to bridge institutional silos at subnational levels, and holistically mainstream climate considerations into regional floodplain development, affordable housing production, and flood risk mitigation.",Not About Sufficiency
In pursuit of responsible innovation for precision agriculture technologies,"Agricultural decision support systems (DSSs) are hardware and software tools that utilize big data collected from satellites and drones, ground-based sensors, and analyzed with machine learning algorithms to provide site-specific farming recommendations. Despite the promise of DSSs to address many challenges of the farm economy, there are social and ethical concerns that need to be addressed. Utilizing a mixed-methods approach that consisted of focus group discussions and a follow-up survey questionnaire, we highlight the experiences and affectations of heterogeneous food system actors from Vermont and South Dakota. We find that DSSs transform agricultural knowledge production, reconfigure labor arrangements and unevenly distribute benefits and burdens among farmers. We suggest that agritech developers implement inclusive and deliberative processes when redesigning DSSs to engender ethical, equitable and sustainable improvements to food production systems. Inclusive processes of open deliberation are modalities of responsible innovation, tasked with mitigating frictions within socio-technical systems.",Not About Sufficiency
A comprehensive review of water quality indices for lotic and lentic ecosystems,"Freshwater resources play a pivotal role in sustaining life and meeting various domestic, agricultural, economic, and industrial demands. As such, there is a significant need to monitor the water quality of these resources. Water quality index (WQI) models have gradually gained popularity since their maiden introduction in the 1960s for evaluating and classifying the water quality of aquatic ecosystems. WQIs transform complex water quality data into a single dimensionless number to enable accessible communication of the water quality status of water resource ecosystems. To screen relevant articles, the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method was employed to include or exclude articles. A total of 17 peer-reviewed articles were used in the final paper synthesis. Among the reviewed WQIs, only the Canadian Council for Ministers of the Environment (CCME) index, Irish water quality index (IEWQI) and Hahn index were used to assess both lotic and lentic ecosystems. Furthermore, the CCME index is the only exception from rigidity because it does not specify parameters to select. Except for the West-Java WQI and the IEWQI, none of the reviewed WQI performed sensitivity and uncertainty analysis to improve the acceptability and reliability of the WQI. It has been proven that all stages of WQI development have a level of uncertainty which can be determined using statistical and machine learning tools. Extreme gradient boosting (XGB) has been reported as an effective machine learning tool to deal with uncertainties during parameter selection, the establishment of parameter weights, and determining accurate classification schemes. Considering the IEWQI model architecture and its effectiveness in coastal and transitional waters, this review recommends that future research in lotic or lentic ecosystems focus on addressing the underlying uncertainty issues associated with the WQI model in addition to the use of machine learning techniques to improve the predictive accuracy and robustness and increase the domain of application.",Not About Sufficiency
An Overview of Unsupervised Deep Feature Representation for Text Categorization,"High-dimensional features are extensively accessible in machine learning and computer vision areas. How to learn an efficient feature representation for specific learning tasks is invariably a crucial issue. Due to the absence of class label information, unsupervised feature representation is exceedingly challenging. In the last decade, deep learning has captured growing attention from researchers in a broad range of areas. Most of the deep learning methods are supervised, which is required to be fed with a large amount of accurately labeled data points. Nevertheless, acquiring sufficient accurately labeled data is unaffordable in numerous real-world applications, which is suggestive of the needs of unsupervised learning. Toward this end, quite a few unsupervised feature representation approaches based on deep learning have been proposed in recent years. In this paper, we attempt to provide a comprehensive overview of unsupervised deep learning methods and compare their performances in text categorization. Our survey starts with the autoencoder and its representative variants, including sparse autoencoder, stacked autoencoder, contractive autoencoder, denoising autoencoder, variational autoencoder, graph autoencoder, convolutional autoencoder, adversarial autoencoder, and residual autoencoder. Aside from autoencoders, deconvolutional networks, restricted Boltzmann machines, and deep belief nets are introduced. Then, the reviewed unsupervised feature representation methods are compared in terms of text clustering. Extensive experiments in eight publicly available data sets of text documents are conducted to provide a fair test bed for the compared methods.",Not About Sufficiency
Large scale indoor occupant tracking using distributed acoustic sensing and machine learning,"The recent COVID-19 pandemic has brought attention to the criticality of implementing measures to prevent viral transmission and reduce the risk of infection. It is necessary to identify crowd-gathering hazards and implement appropriate engineering control measures. Consequently, obtaining comprehensive footfall information at a large scale becomes significant. Vibration-based sensors are currently the preferred choice for footfall detection, with geophones being the commonly utilized sensors known for their effective performance. However, the deployment of geophones in large-scale scenarios presents challenges due to the substantial amount of manual labor involved. To address these limitations, the Distributed Acoustic Sensing (DAS) system, a nonintrusive sensor, is introduced as an alternative solution. While DAS has been successfully applied in various long-distance scenarios, such as pipeline surveillance and real-time train tracking, its effectiveness in footfall detection requires further evaluation. Therefore, this study proposes a machine learning method to achieve footstep recognition using DAS, considering the inherent time dependency and spatial continuity present in footfall trajectories. To further evaluate the effectiveness of DAS, geophones are selected as the benchmark sensors. The evaluation process encompasses footstep recognition and additional data implementation. The results demonstrate that the proposed recognition method achieves high accuracy for both sensors, thereby establishing DAS as a more applicable solution for footfall detection compared to geophones. In conclusion, DAS offers the advantage of flexible space resolution settings and adaptable deployment options, rendering it suitable for applications in diverse scenarios. Its capabilities make it suitable for large-scale occupancy monitoring scenarios.",Not About Sufficiency
Neighbourhood built environment and leisure-time physical activity: A cross-sectional study in southern China,"This study conducted scientific evidence linking neighbourhood built environment to adults' leisure-time physical activity (LTPA) among adults in China. Data were obtained from a questionnaire survey conducted from April to July 2017 among 1002 adults aged 18-69 & x202f;years old in Pingshan District, Shenzhen, China. Chinese Walkable Environment Scale for urban community residents and International Physical Activity Questionnaire were used to measure participants' neighbourhood built environment and leisure-time physical activity, which was categorised into leisure-time walking (LTW) and leisure-time moderate-to-vigorous physical activity (LTMVPA). A total of 986 participants (mean age = 40.7 years, 53.3% females) were included in this research. Descriptive statistics were used to describe the socio-demographic variables, LTW and LTMVPA by sex. Multivariable logistic regression models were used to examine the association between neighbourhood environment characteristics and the likelihood of engaging in active LTW and LTMVPA. Only 20.7% of participants engaged in active LTW and 17.8% active LTMVPA. Better road condition was associated with higher likelihood of active (at least 150 min/week) LTW and LTMVPA. High perceived esthetic was positively associated with LTW and LTMVPA. Active LTW was related to better perception of traffic condition as well. The improvement of the neighbourhood environment characteristics can promote active LTPA among adults living in Shenzhen, China. Our findings support the importance of considering population health effects in urban planning and development.",Not About Sufficiency
Clinician's perspectives of the relocation of a regional child and adolescent mental health service from co-located to stand alone premises,"Introduction: Australia's National Mental Health Strategy's statement of rights and responsibilities states that children and adolescents admitted to a mental health facility or community program have the right to be separated from adult patients and provided with programs suited to their developmental needs. However, in rural Australia, where a lack of healthcare services, financial constraints, greater service delivery areas and fewer mental healthcare specialists represent the norm, Child and Adolescent Mental Health Services (CAMHS) are sometimes co-located with adult mental health services. The aim of the present study was to evaluate the impact of a recent relocation of a regional CAMHS in Victoria from co-located to stand alone premises. Method: Six CAMHS clinicians who had experienced service delivery at a co-located setting and the current stand-alone CAMHS setting were interviewed about their perceptions of the impact of the relocation on service delivery. An exploratory interviewing methodology was utilized due to the lack of previous research in this area. Interview data were transcribed and analysed according to interpretative phenomenological analysis techniques. Results: Findings indicated a perception that the relocation was positive for clients due to the family-friendly environment at the new setting and separation of CAMHS from adult psychiatric services. However, the impact of the relocation on clinicians was marked by a perceived loss of social capital from adult psychiatric service clinicians. Conclusion: These results provide increased understanding of the effects of service relocation and the influence of co-located versus stand-alone settings on mental health service delivery - an area where little prior research exists.",Not About Sufficiency
Web-based application for online self testing and knowledge evaluation in the field of Microbiology,"Our Web application has been developed as a Web-based education tool and is addressed to the students and residents in medicine, helping them in understanding and refining the concepts and in easily evaluate their knowledge and needed improvement areas. The described methodology was experimentally tested in the domain of bacteriology, virusology and parasitology. Many studies conducted in the domain of Web-based education tools have showed students and teachers interest in using this kind of applications and its benefits in the professional knowledge development of the students. When using the new instructional technology like online learning and online examination, institutions and teachers have to consider also the professional code and ethical obligations associated with this use like diversity, objectivity, web accessibility, security and copyright. The aim of our study is to implement a modem solution that will address this issue with a low cost and in the same time to make it available and easy accessible to teachers, students and medical residents. The implementation of this web-based application offers all the advantages provided by the Web technology (interoperability, multiple users, reusability, and extendibility) and will also support the categories mentioned above in evaluating and extending their knowledge in the field of Medical Microbiology. The design and concept of the application allows its extension in any educational field with minimum effort needed. The innovative aspect consists in a module that will preserve the same set of questions in the examination form for all the tested users, realized by creating an examination template that will arrange the questions in randomized manner, the options for each response being presented to each user in a different order. Our application offers great potential for improving the assessment and self-testing of students and residents in medicine and many advantages over traditional assessment methods.",Not About Sufficiency
Health Impact Assessments Are Needed In Decision Making About Environmental And Land-Use Policy,"The importance to public health of environmental decisions-including those about land use, transportation, power generation, agriculture, and environmental regulation-is increasingly well documented. Yet many decision makers in fields not traditionally focused on health continue to pay little if any attention to the important health effects of their work. This article examines the emerging practice of health impact assessment and offers real-world examples of its effective implementation, including studying the impact of nearby highways-a major source of air pollution-on proposed new housing for seniors. The article argues that officials at the federal, state, and local levels should consult health experts and consider using health impact assessments when their decisions on such issues as urban planning, land use, and environmental regulation have the potential to directly affect the conditions in which people live and work.",Not About Sufficiency
Atmospheric optical parameter server for atmospheric corrections of remote sensing data,"Significant atmospheric correction improvements can only be achieved by accurately representing the dynamic variability of the intrinsic atmospheric optical parameters that serve as input to the radiative transfer model. The challenge is to render the existing array of ground and satellite based (input) atmospheric optical measurements coherent and standard. This is accomplished by assimilating the atmospheric optical measurements and their expected errors with a spatio-temporal atmospheric model driven by meteorological-scale wind fields. Such models act as intelligent interpolators in space and time, and as a tool for product QA and standardization. The concept of a meteorological scale optical parameter prediction system for atmospheric corrections (NOMAD for ""Networked On-line Mapping of Atmospheric Data"") is presented in this communication along with examples of ongoing developmental work. The system is viewed as multi-level in that it will output atmospheric optical parameter estimates ranging from the meteorological to climatological scales and from the synoptic scale to the lower mesoscale (AVHRR scale to SPOT scale). The operational concept is that it will reside on a central server that will be accessible to any atmospheric correction system. Aside from the obvious advantage of atmospheric corrections operationalization, this concept will transfer the responsibility of quality assuring atmospheric parameters from the user to the science and technical staff maintaining the atmospheric server.",Not About Sufficiency
epiDAMIK 5.0: The 5th International Workshop on Epidemiology meets Data Mining and Knowledge Discovery,"Similar to previous iterations, the epiDAMIK @ KDD workshop is a forum to promote data driven approaches in epidemiology and public health research. Even after the devastating impact of COVID-19 pandemic, data driven approaches are not as widely studied in epidemiology, as they are in other spaces. We aim to promote and raise the profile of the emerging research area of data-driven and computational epidemiology, and create a venue for presenting state-of-the-art and in-progress results-in particular, results that would otherwise be difficult to present at a major data mining conference, including lessons learnt in the 'trenches'. The current COVID-19 pandemic has only showcased the urgency and importance of this area. Our target audience consists of data mining and machine learning researchers from both academia and industry who are interested in epidemiological and public-health applications of their work, and practitioners from the areas of mathematical epidemiology and public health. Homepage: https://epidamik.github.io/.",Not About Sufficiency
"Social interactions, residential segregation and the dynamics of tipping","We develop an analytically tractable population dynamics model of heterogeneous agents to characterize how social interactions within a neighborhood determine the dynamic evolution of its ethnic composition. We characterize the conditions under which integration or segregation will occur, which depends on the majority's social externality parameter and net benefit from leaving, and the minority's leaving probability. Minority segregation may result from the process of tipping, which may arise from three possible channels: two are related to exogenous shocks (migration flows and changes in tipping points) and one is related to the endogenous probabilistic features of our framework (endogenous polarization). This characterization of integration and segregation conditions yields interesting policy implications for social and urban planning policies to mitigate segregation.",Not About Sufficiency
XGBoost model as an efficient machine learning approach for PFAS removal: Effects of material characteristics and operation conditions,"Due to the implications of poly-and perfluoroalkyl substances (PFAS) on the environment and public health, great attention has been recently made to finding innovative materials and methods for PFAS removal. In this work, PFAS is considered universal contamination which can be found in many wastewater streams. Conven-tional materials and processes used to remove and degrade PFAS do not have enough competence to address the issue particularly when it comes to eliminating short-chain PFAS. This is mainly due to the large number of complex parameters that are involved in both material and process designs. Here, we took the advantage of artificial intelligence to introduce a model (XGBoost) in which material and process factors are considered simultaneously. This research applies a machine learning approach using data collected from reported articles to predict the PFAS removal factors. The XGBoost modeling provided accurate adsorption capacity, equilibrium, and removal estimates with the ability to predict the adsorption mechanisms. The performance comparison of adsorbents and the role of AI in one dominant are studied and reviewed for the first time, even though many studies have been carried out to develop PFAS removal through various adsorption methods such as ion ex-change, nanofiltration, and activated carbon (AC). The model showed that pH is the most effective parameter to predict PFAS removal. The proposed model in this work can be extended for other micropollutants and can be used as a basic framework for future adsorbent design and process optimization.",Not About Sufficiency
Discourse of Urban Resilience and 'Inclusive Development' in the Johannesburg Growth and Development Strategy 2040,"Drawing on various disciplinary perspectives, this article critically explores the transposition of the concept of resilience into urban planning, and the debates surrounding it. In particular, it examines how the links between urban resilience, sustainability and inclusive development are being constructed in the literature, and the implications in terms of governance. The article focuses on the example of the Johannesburg Growth and Development Strategy 2040, which uses the concept of resilience as an analytical framework and as a basis for planning a more inclusive development pathway. The article argues that despite overcoming some of the critiques made of the transposition of 'resilience thinking' in social sciences, there remain weaknesses and gaps in the reasoning linking resilience, economic growth and inclusive development, as well as unresolved issues of operationalization of resilience. In its endeavour to embrace resilience thinking, the city fails to size up the full implications of 'resilience planning'.",Not About Sufficiency
Phenotype and endotype based treatment of preschool wheeze,"IntroductionPreschool wheeze (PSW) is a significant public health issue, with a high presentation rate to emergency departments, recurrent symptoms, and severe exacerbations. A heterogenous condition, PSW comprises several phenotypes that may relate to a range of pathobiological mechanisms. However, treating PSW remains largely generalized to inhaled corticosteroids and a short acting beta agonist, guided by symptom-based labels that often do not reflect underlying pathways of disease.Areas coveredWe review the observable features and characteristics used to ascribe phenotypes in children with PSW and available pathobiological evidence to identify possible endotypes. These are considered in the context of treatment options and future research directions. The role of machine learning (ML) and modern analytical techniques to identify patterns of disease that distinguish phenotypes is also explored.Expert opinionDistinct clusters (phenotypes) of severe PSW are characterized by different underlying mechanisms, some shared and some unique. ML-based methodologies applied to clinical, biomarker, and environmental data can help design tools to differentiate children with PSW that continues into adulthood, from those in whom wheezing resolves, identifying mechanisms underpinning persistence and resolution. This may help identify novel therapeutic targets, inform mechanistic studies, and serve as a foundation for stratification in future interventional therapeutic trials.",Not About Sufficiency
Sustainable urban Planning mapping using remote sensing and GIS in Malaysia,"This study discusses the applicability of remote sensing and GIS techniques to classify urban land uses in Malaysia based on object-oriented classification, in order to extract the three components of urban areas, namely residential, commercial and industrial. The areas of interest in this study is Kuala Lumpur. The satellite data of SPOT 5 with fine spatial resolution of 2.5 meter and temporal resolution are tested. E-cognition software develops the rule sets of thematic layer attributes to extract the types of land use. The result shows that the three land use types are clearly identified, with low standard deviation of the classification. The variation is not too dispersed from the mean which proves that the result is acceptable. The findings also show that there have been some changes in the total acreage of recent classification due to the rapid development in the study area. Finally, the application of the object-oriented classification confirms the effectiveness of using remote sensing and GIS techniques in classifying urban land uses. This study also put forth some questions for future researches, which can help strengthen the potentials of remote sensing applications, especially in regards of further explorations of urban issues at various administration levels.",Not About Sufficiency
Urban environmental quality and human well-being - Towards a conceptual framework and demarcation of concepts; a literature study,"Construction of a multidisciplinary conceptual framework of environmental quality and quality of life is required to advance the field of urban development, environmental quality and human well-being. such a framework would allow for a more theory-based choice of indicators and for the development of tools to evaluate multidimensional aspects of urban environmental quality. These tools are required to assess the current and future quality of the urban environment and to have, eventually, the ability to assess the implications of spatial and urban planning policies with respect to these dimensions. Against this background, the National Institute for Public Health and the Environment in the Netherlands (RIVM) performed a major literature review [Leidelmeijer, van Kamp, 2002, in press] to identify various concepts in the literature concerning environmental quality, the relationships between these various concepts, as well as their respective theoretical bases. This paper summarises the outcomes of this survey. It reviews the main (types of) concepts of livability, environmental quality, quality of life and sustainability, and presents examples of underlying conceptual models. Different notions and concepts are compared along the dimensions of domain, indicator, scale, time-frame and context as described by [Urban Environmental Quality-a social geographical perspective, this issue]. It is concluded that a multidisciplinary conceptual framework of environmental quality and quality of life that will go beyond the disciplinary differences found in the current literature is needed if the field is to advance. (C) 2003 Elsevier Science B.V. All rights reserved.",Not About Sufficiency
COMPUTER-ASSISTED TEACHING AND NEW TECHNOLOGIES,"Many procedures in a broad spectrum of clinical specialities can be managed through a minimally invasive surgical (MIS) approach. Indeed, minimal access surgery has become increasingly accepted by the surgical community, but the surgeon is faced with the necessity of following a specific training. Over the past two decades, advances in computer-assisted technology and telecommunications have deeply altered surgical education. The role of the traditional ""Professor of Surgery"" has been superseded by the quantity and quality of knowledge available not only in the media and on the Internet, but also in CD-ROMs, DVDs, stream videos, webcasts and telementoring. Nowadays, information and knowledge is readily accessible for all to find. What is more, the knowledge of highly skilled teachers is available worldwide thanks to the Internet and other multimedia technologies. As new perspectives are opening up on the surgical horizon, such as natural orifice transluminal endoscopic surgery (NOTES), the need to apply new technologies to teaching and training as well as in daily clinical practice has become evident. This encapsulates multimedia, computer-assisted learning, videoconferencing, virtual reality and computer-assisted simulation in order to maximize surgical education. Telementoring through videoconferencing renders the acquisition of the cognitive aspects of surgical procedures easier through the observation of experts during hands-on practice. The development of computer technology allows to recreate a virtual 3D model of the patient, capable of simulating the patient's anatomy on a computer; computerized 3D images also allow virtual navigation around and into the anatomical structures, simulation of various interventions as well as automated volume calculation. In addition, a virtual reconstruction of the patient may also enhance diagnosis and enable a preoperative simulation and a preoperative training. Finally, the computerized data will allow the scheduling of automated surgical procedures. Robotics have demonstrated their ability in performing long-distance, remote-controlled surgical procedures. Surgeons require access to information whenever and wherever, and the Internet offers them that very chance, along with observation, training and education. Websites dedicated to surgery such as www.websurg.com incorporate all aspects of surgical training including fully illustrated and animated operative techniques, videos and experts' recommendations that, with the continuous updating of information, represent the future tool for the transfer of surgical and scientific knowledge. Using all these technologies, education may be improved through WeBSurg's virtual university as it gathers academic knowledge, education and expertise from around the world without any constraints of time and/or space. It will also facilitate the standardization of surgical treatments.",Not About Sufficiency
"Assessment of HIV Rapid Test Kits Inventory Management Practice and Challenges in Public Health Facilities of Addis Ababa, Ethiopia (vol 11, pg 85, 2022)",,Not About Sufficiency
An Investigation Into Design Criteria for Affordable Housing Supply,"Affordable housing provision constitutes a very large scheme in any countries due to income distribution and national development. The supply of housing depends on many activities and processes. The purpose of the houses is to meet the requirement of the householders therefore design criteria must address the users' requirements. The establishment of the design criteria constitutes an important activity at the initial phase of the housing development. The design criteria are prepared by the design team in collaboration with many stakeholders especially the householders. Housinsg design criteria are the requirements that must be considered prior to construction of the housing. Lack of adequate information on the design criteria would lead to poor householders' satisfactions, increase in maintenance cost, abandonments and completed but not occupy housing. In Malaysia, many of these consequences are prevalent. However, while information on the house owners' requirements is inconclusive, this current research set out to investigate the design criteria of affordable housing. The increase in the affordable housing gap in Malaysia can be reduced if the designers have a comprehensive understanding of users' requirements and the design criteria. Through a cross sectional survey questionnaire, comprising 25 criteria, 7 criteria were found to be extremely critical. The Kaiser-Meyer-Olkin measure of sampling adequacy indicated that the strength of the relationships among variables was strong (KMO =0.716). Bartlett's test of sphericity, which tests the overall significance of all the correlations within the correlation matrix, was significant chi(2) (325) = 1825.075, p<0.001), indicating the data were drawn from the same population and that the criteria were related. Sustainability considerations are now being considered by the providers of affordable housing. Deductively, the results lead to the conclusion that a major factor responsible for the poor homebuyers / homeo ccupiers dissatisfactions is the lack of considerations of the criteria that impact significantly on the buyers/occupiers choice. The findings are significant for developers, urban planners, banks, policy makers and the academia.",Not About Sufficiency
Consumer Attitudes toward Energy Reduction and Changing Energy Consumption Behaviors,"This editorial paper tackles the issue of the consumer attitudes toward energy reduction and changing energy consumption behaviors. This topic is of special relevance today as Europe faces an unprecedented energy crisis as a result of diverting from Russian supplies of oil and gas due to the war in Ukraine. For many years now, Europe has relied upon cheap and affordable Russian oil and gas (in fact, the European Green Deal and the strategy for the decarbonization of the economy by 2030 were indirectly based on it), but the transition to the renewable future now appears to be jeopardized. As energy prices are soaring globally, it is not yet clear whether this would have an effect on significantly changing consumer behavior and increasing energy efficiency and security as many consumers are reluctant to change their old habits and are used to having their energy on demand and for any possible occasion. However, changing energy consumption behaviors would be beneficial not only for handling the current energy crisis but also for setting the long-term trends with respect to energy saving, which is crucial for fighting global warming and climate change while sustaining economic growth.",Not About Sufficiency
"Marine parks for coastal cities: A concept for enhanced community well-being, prosperity and sustainable city living","Coastal cities continue to experience rapid urbanisation and population growth worldwide, linked to the diverse economic and social benefits flowing from proximity to the sea. Growing concern over human impacts upon coastal waters and global strategic goals for healthier cities requires that coastal cities develop innovative ways to inspire and empower communities to embrace and cherish city seascapes. Coastal city communities have much to gain from a healthier relationship with the sea. This paper proposes a collaborative community-led marine park concept that celebrates a city's connection to the marine environment, enhances sustainable economic prosperity and enables communities to participate in activities that deepen understanding, value, care and enjoyment of the city seascape. A city marine park (CMP) is not a marine protected area because it does not have biodiversity and heritage protection or ecosystem governance as a primary goal and does not aim to restrict human activities. A CMP enables city communities to collaborate towards a shared vision of elevated status and value for the city seascape. A CMP considers socio-economic and geographical context, including land-sea connectivity, and is integrated within a coastal city's strategic urban planning. This paper highlights core themes of a CMP and the diverse and wide-ranging benefits from coordinated activities that better connect the city community with its seascape. If co-created by the coastal city community and civic leaders, a CMP will form an enduring spatial nexus for progress toward healthy cities addressing multiple interlinked global sustainable development goals.",Not About Sufficiency
"Formalizing informal homes, a bad idea: The credibility thesis applied to China's ""extra-legal"" housing","Discussions about informal housing in developing, emerging economies often revolve around the need for prohibition, privatization and formalization. Private title is seen as a guarantee against indiscriminate expropriation leading to tenure security, better access to utilities and mortgage, and higher investments. However, the argument that formalization and privatization out of necessity lead to better rights of otherwise ""victimized slum-dwellers"" can be questioned. In addition, prohibition of informal housing can marginalize socially weaker groups, while drawing on critical resources for enforcement. We argue that to avoid externalities, one first needs to probe into the perceived function of existing property rights before considering institutional form, irrespective whether formal or informal. China's extra-legal housing or ""Small Property Rights' Housing"" (SPRH) is a case-in-point. Extra-legal housing is estimated to account for one-third of the Chinese urban housing stock. In light of this scale, we maintain that extra-legal housing performs a vital function in providing social security, i.e. affordable housing for low(er) income groups. The argument is supported through a survey amongst 300 respondents in 7 medium and large-sized cities. The survey finds that despite alleged tenure insecurity SPRH rallies a high level of perceived credibility along three dimensions: economic, social and psychological. Our findings indicate that urban planning and housing policy should consider institutional differences in line with existing functions. In this case, based on the CSI Checklist (Credibility Scales and Intervention), maintaining status-quo or co-optation might be more sensible as credibility is perceived to be high. (C) 2016 Elsevier Ltd. All rights reserved.",Not About Sufficiency
Security Exceptions in the WTO System: Bridge or Bottle-Neck for Trade and Security?,"The General Agreement on Tariffs and Trade (GATT) Article XXI remains intact, without any modification, since the inception of the GATT in 1947. Recent economic and political developments, however, are not well addressed in the Security Exceptions enshrined in Article XXI. Moreover, the security exceptions that have been incorporated into the General Agreement on Trade in Services and the Trade-Related Aspects of Intellectual Property Rights Agreement feature some discrepancies as compared to the text in the GATT, which causes confusion. Some free trade agreements have occasionally introduced security exception provisions as well, with notable distinctions compared to those of the World Trade Organization (WTO). The current world trading system has to deal with wholly different dimensions of national security such as cyber-security, terrorism, and energy security. This situation raises an imminent question on how to make those arcane security exception provisions effectively workable legal disciplines. This article examines legal developments in WTO and Free Trade Agreement security exceptions and diagnoses the systematic challenges to effectively apply the Security Exceptions. The WTO Members need to address this issue as early as possible to avoid an unnecessary and inappropriate burden for the dispute settlement system.",Not About Sufficiency
Application of computational algorithms for single-cell RNA-seq and ATAC-seq in neurodegenerative diseases,"Recent advancements in single-cell technologies, including single-cell RNA sequencing (scRNA-seq) and Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq), have greatly improved our insight into the epigenomic landscapes across various biological contexts and diseases. This paper reviews key computational tools and machine learning approaches that integrate scRNA-seq and scATAC-seq data to facilitate the alignment of transcriptomic data with chromatin accessibility profiles. Applying these integrated single-cell technologies in neurodegenerative diseases, such as Alzheimer's disease and Parkinson's disease, reveals how changes in chromatin accessibility and gene expression can illuminate pathogenic mechanisms and identify potential therapeutic targets. Despite facing challenges like data sparsity and computational demands, ongoing enhancements in scATAC-seq and scRNA-seq technologies, along with better analytical methods, continue to expand their applications. These advancements promise to revolutionize our approach to medical research and clinical diagnostics, offering a comprehensive view of cellular function and disease pathology.",Not About Sufficiency
Analysis of airport design for introducing infrastructure for autonomous drones,"PurposeConnecting autonomous drones to ground operations and services is a prerequisite for the adoption of scalable and sustainable drone services in the built environment. Despite the rapid advance in the field of autonomous drones, the development of ground infrastructure has received less attention. Contemporary airport design offers potential solutions for the infrastructure serving autonomous drone services. To that end, this paper aims to construct a framework for connecting air and ground operations for autonomous drone services. Furthermore, the paper defines the minimum facilities needed to support unmanned aerial vehicles for autonomous logistics and the collection of aerial data. Design/methodology/approachThe paper reviews the state-of-the-art in airport design literature as the basis for analysing the guidelines of manned aviation applicable to the development of ground infrastructure for autonomous drone services. Socio-technical system analysis was used for identifying the service needs of drones. FindingsThe key findings are functional modularity based on the principles of airport design applies to micro-airports and modular service functions can be connected efficiently with an autonomous ground handling system in a sustainable manner addressing the concerns on maintenance, reliability and lifecycle. Research limitations/implicationsAs the study was limited to the airport design literature findings, the evolution of solutions may provide features supporting deviating approaches. The role of autonomy and cloud-based service processes are quintessentially different from the conventional airport design and are likely to impact real-life solutions as the area of future research. Practical implicationsThe findings of this study provided a framework for establishing the connection between the airside and the landside for the operations of autonomous aerial services. The lack of such framework and ground infrastructure has hindered the large-scale adoption and easy-to-use solutions for sustainable logistics and aerial data collection for decision-making in the built environment. Social implicationsThe evolution of future autonomous aerial services should be accessible to all users, ""democratising"" the use of drones. The data collected by drones should comply with the privacy-preserving use of the data. The proposed ground infrastructure can contribute to offloading, storing and handling aerial data to support drone services' acceptability. Originality/valueTo the best of the authors' knowledge, the paper describes the first design framework for creating a design concept for a modular and autonomous micro-airport system for unmanned aviation based on the applied functions of full-size conventional airports.",Not About Sufficiency
Opportunities of integrated care to improve equity for adults with complex needs: a qualitative study of case management in primary care,"BackgroundPeople living in precarious socio-economic conditions are at greater risk of developing mental and physical health disorders, and of having complex needs. This places them at risk of health inequity. Addressing social determinants of health (SDH) can contribute to reducing this inequity. Case management in primary care is an integrated care approach which could be an opportunity to better address SDH. The aim of this study is to better understand how case management in primary care may address the SDH of people with complex needs.MethodsA case management program (CMP) for people with complex needs was implemented in four urban primary care clinics. A qualitative study was conducted with semi-structured interviews and a focus group with key informants (n = 24). An inductive thematic analysis was carried out to identify emerging themes.ResultsPrimary care case managers were well-positioned to provide a holistic evaluation of the person's situation, to develop trust with them, and to act as their advocates. These actions helped case managers to better address individuals' unmet social needs (e.g., poor housing, social isolation, difficulty affording transportation, food, medication, etc.). Creating partnerships with the community (e.g., streetworkers) improved the capacity in assisting people with housing relocation, access to transportation, and access to care. Assuming people provide their consent, involving a significant relative or member of their community in an individualized services plan could support people in addressing their social needs.ConclusionsCase management in primary care may better address SDH and improve health equity by developing a trusting relationship with people with complex needs, improving interdisciplinary and intersectoral collaboration and social support. Future research should explore ways to enhance partnerships between primary care and community organizations.",Not About Sufficiency
Approach and application to transfer heterogeneous simulation data from finite element analysis to neural networks,"The simulation of product behavior is a vital part of current virtual product development. It can be expected that soon there will be more product simulations due to the availability of easy-to-use finite element analysis software and computational power. Consequently, the amount of accessible new simulation data adds up to the already existing amount. However, even when using easy-to-use finite element software tools, errors can occur during the setup of finite element simulations, and users should be warned about certain mistakes by automatic algorithms. To use the vast amount of available finite element simulations for a data-driven finite element support tool, in this paper, a methodology will be presented to transform different finite element simulations to unified matrices. The procedure is based on the projection of nodes onto a detector sphere, which is converted into a matrix in the next step. The generated matrices represent the simulation and can be described as the DNA of a finite element simulation. They can be used as an input for any machine learning model, such as convolutional neural networks. The essential steps of preprocessing the data and an application with a large dataset are part of this contribution. The trained network can then be used for an automatic plausibility check for new simulations, based on the previous simulation data from the past. This can result in a tool for automatic plausibility checks and can be the backbone for a feedback system for less experienced users.",Not About Sufficiency
Estimation and Mapping of Forest Structure Parameters from Open Access Satellite Images: Development of a Generic Method with a Study Case on Coniferous Plantation,"Temperate forests are under climatic and economic pressures. Public bodies, NGOs and the wood industry are looking for accurate, current and affordable data driven solutions to intensify wood production while maintaining or improving long term sustainability of the production, biodiversity, and carbon sequestration. Free tools and open access data have already been exploited to produce accurate quantitative forest parameters maps suitable for policy and operational purposes. These efforts have relied on different data sources, tools, and methods that are tailored for specific forest types and climatic conditions. We hypothesized we could build on these efforts in order to produce a generic method suitable to perform as well or better in a larger range of settings. In this study we focus on building a generic approach to create forest parameters maps and confirm its performance on a test site: a maritime pine (Pinus pinaster) forest located in south west of France. We investigated and assessed options related with the integration of multiple data sources (SAR L- and C-band, optical indexes and spatial texture indexes from Sentinel-1, Sentinel-2 and ALOS-PALSAR-2), feature extraction, feature selection and machine learning techniques. On our test case, we found that the combination of multiple open access data sources has synergistic benefits on the forest parameters estimates. The sensibility analysis shows that all the data participate to the improvements, that reach up to 13.7% when compared to single source estimates. Accuracy of the estimates is as follows: aboveground biomass (AGB) 28% relative RMSE, basal area (BA) 27%, diameter at breast height (DBH) 20%, age 17%, tree density 24%, and height 13%. Forward feature selection and SVR provided the best estimates. Future work will focus on validating this generic approach in different settings. It may prove beneficial to package the method, the tools, and the integration of open access data in order to make spatially accurate and regularly updated forest structure parameters maps effortlessly available to national bodies and forest organizations.",Not About Sufficiency
Facile Fabrication of Fe2O3-Decorated Carbon Matrixes with a Multidimensional Structure as Anodes for Lithium-Ion Batteries,"Rational design of the dimension and structure for electrode materials is an efficient strategy to boost the electrochemical properties. Herein, Fe2O3 nanoparticles are integrated with carbon substrates of different dimensions including a one-dimensional carbon nanotube (CNT), two-dimensional reduced graphene oxide (rGO), and a three-dimensional carbon framework composed of CNT and rGO via facile heterogeneous nucleation under hydrothermal conditions. These materials demonstrate a strong structure-dependent electrochemical performance. Among the three composites constructed, the rGO/CNT-Fe2O3 composite possesses an interconnected network with Fe2O3 uniformly distributed in the three-dimensional carbon skeleton composed of CNT and rGO. The two-dimensional conductive rGO could not only confine the Fe2O3 nanoparticles within the graphene layers to prevent the pulverization and agglomeration of Fe2O3 but also offer accessible active sites for the electrochemical reaction. The one-dimensional CNT interspersed within the interlayer space between the rGO nanosheet could impede the folding of the rGO sheet to enhance the ion/electron transport as well as maintain the multistructure of the composite during the charge and discharge process. Therefore, rGO/CNT-Fe2O3 can achieve a superior initial reversible capacity of 1306.9 mAh g(-1) at 500 mA g(-1) with a high capacity retention of 760.3 mAh g(-1) after 400 cycles and a remarkable rate performance of 424.2 mAh g(-1) at 5 A g(-1). This work provides insight into the effect of carbon dimension on the energy storage capacity and develops an efficient strategy to construct multidimensional transition-metal oxide-based composites as anode materials for lithium-ion batteries.",Not About Sufficiency
A Proof-of-Concept IoT System for Remote Healthcare Based on Interoperability Standards,"The Internet of Things paradigm in healthcare has boosted the design of new solutions for the promotion of healthy lifestyles and the remote care. Thanks to the effort of academia and industry, there is a wide variety of platforms, systems and commercial products enabling the real-time information exchange of environmental data and people's health status. However, one of the problems of these type of prototypes and solutions is the lack of interoperability and the compromised scalability in large scenarios, which limits its potential to be deployed in real cases of application. In this paper, we propose a health monitoring system based on the integration of rapid prototyping hardware and interoperable software to build system capable of transmitting biomedical data to healthcare professionals. The proposed system involves Internet of Things technologies and interoperablility standards for health information exchange such as the Fast Healthcare Interoperability Resources and a reference framework architecture for Ambient Assisted Living UniversAAL.",Not About Sufficiency
Recovery of ecosystem productivity in China due to the Clean Air Action plan,"Severe air pollution reduces ecosystem carbon assimilation through the vegetation damaging effects of ozone and by altering the climate through aerosol effects, exacerbating global warming. In response, China implemented the Clean Air Action plan in 2013 to reduce anthropogenic emissions. Here we assess the impact of air pollution reductions due to the Clean Air Action plan on net primary productivity (NPP) in China during the period 2014-2020 using multiple measurements, process-based models and machine learning algorithms. The Clean Air Action plan led to a national NPP increase of 26.3 +/- 27.9 TgC yr-1, of which 20.1 +/- 10.9 TgC yr-1 is attributed to aerosol reductions, driven by both the enhanced light availability as a result of decreased black carbon concentrations and the increased precipitation caused by weakened aerosol climatic effects. The impact of ozone amelioration became more important over time, surpassing the effects of aerosol reduction by 2020, and is expected to drive future NPP recovery. Two machine learning models simulated similar NPP recoveries of 42.8 +/- 26.8 TgC yr-1 and 43.4 +/- 30.1 TgC yr-1. Our study highlights substantial carbon gains from controlling aerosols and surface ozone, underscoring the co-benefits of regulating air pollution for public health and carbon neutrality in China. A quantitative assessment suggests that the reductions in aerosol and ozone levels from 2014 to 2020 due to the clean air action in China led to a substantial increase in the national net primary productivity due to the weakened aerosol climatic effects, alleviated ozone vegetation damage and enhanced light availability.",Not About Sufficiency
The phenomenon of walking: diverse and dynamic,"Everyday walking is a far-reaching activity with the potential to increase health and well-being in the general public. From a phenomenological perspective, walking can be seen as a function of being-in-the-world, where the landscape, a sense of place, and the moment are closely entwined with the walker's own lived experiences. Using interviews with 73 walkers in a medium-sized town in Norway, this article explores the phenomenon of everyday walking. The data illustrate the multiple ways in which people emphasise well-being and ascribe meaning to their walking experiences, and how these ways may vary significantly during a life course, from day to day, and even within a single walk. Insights from this study may prove useful to policy-makers and administrative bodies in acknowledging people's various needs and gains related to everyday walking, and hence for promoting a diversified management of walking within the field of health policy, as well as in urban planning for walkable cities.",Not About Sufficiency
Protocol for a systematic review of reviews on training primary care providers in dermoscopy to detect skin cancers,"Introduction Globally, incidence, prevalence and mortality rates of skin cancers are escalating. Earlier detection by well-trained primary care providers in techniques such as dermoscopy could reduce unnecessary referrals and improve longer term outcomes. A review of reviews is planned to compare and contrast the conduct, quality, findings and conclusions of multiple systematic and scoping reviews addressing the effectiveness of training primary care providers in dermoscopy, which will provide a critique and synthesis of the current body of review evidence. Methods and analysis Four databases (Cochrane, CINAHL, EMBASE and MEDLINE Complete) will be comprehensively searched from database inception to identify published, peer-reviewed English-language articles describing scoping and systematic reviews of the effectiveness of training primary care providers in the use of dermoscopy to detect skin cancers. Two researchers will independently conduct the searches and screen the results for potentially eligible studies using 'Research Screener' (a semi-automated machine learning tool). Backwards and forwards citation tracing will be conducted to supplement the search. A narrative summary of included reviews will be conducted. Study characteristics, for example, population; type of educational programme, including content, delivery method, duration and assessment; and outcomes for dermoscopy will be extracted into a standardised table. Data extraction will be checked by the second reviewer. Methodological quality will be evaluated by two reviewers independently using the Critical Appraisal Tool for Health Promotion and Prevention Reviews. Results of the assessments will be considered by the two reviewers and any discrepancies will be resolved by team consensus. Ethics and dissemination Ethics approval is not required to conduct the planned systematic review of peer-reviewed, published articles because the research does not involve human participants. Findings will be published in a peer-reviewed journal, presented at leading public health, cancer and primary care conferences, and disseminated via website postings and social media channels.",Not About Sufficiency
Genomic cluster formation among invasive group A streptococcal infections in the USA: a whole-genome sequencing and population-based surveillance study,"Background Clusters of invasive group A streptococcal (iGAS) infection, linked to genomically closely related group A streptococcal (GAS) isolates (referred to as genomic clusters), pose public health threats, and are increasingly identified through whole-genome sequencing (WGS) analysis. In this study, we aimed to assess the risk of genomic cluster formation among iGAS cases not already part of existing genomic clusters. Methods In this WGS and population-based surveillance study, we analysed iGAS case isolates from the Active Bacterial Core surveillance (ABCs), which is part of the US Centers for Disease Control and Prevention's Emerging Infections Program, in ten US states from Jan 1, 2015, to Dec 31, 2019. We included all residents in ABCs sites with iGAS infections meeting the case definition and excluded non-conforming GAS infections and cases with whole-genome assemblies of the isolate containing fewer than 1.5 million total bases or more than 150 contigs. For iGAS cases we collected basic demographics, underlying conditions, and risk factors for infection from medical records, and for isolates we included emm types, antimicrobial resistance, and presence of virulence- related genes. Two iGAS cases were defined as genomically clustered if their isolates differed by three or less single-nucleotide variants. An iGAS case not clustered with any previous cases at the time of detection, with a minimum trace-back time of 1 year, was defined as being at risk of cluster formation. We monitored each iGAS case at risk for a minimum of 1 year to identify any cluster formation event, defined as the detection of a subsequent iGAS case clustered with the case at risk. We used the Kaplan-Meier method to estimate the cumulative incidence of cluster formation events over time. We used Cox regression to assess associations between features of cases at risk upon detection and subsequent cluster formation. We developed a random survival forest machine-learning model based on a derivation cohort (random selection of 50% of cases at risk) to predict cluster formation risk. This model was validated using a validation cohort consisting of the remaining 50% of cases at risk. Findings We identified 2764 iGAS cases at risk from 2016 to 2018, of which 656 (24%) formed genomic clusters by the end of 2019. Overall, the cumulative incidence of cluster formation was 0.057 (95% CI 0.048-0.066) at 30 days after detection, 0.12 (0.11-0.13) at 90 days after detection, and 0.16 (0.15-0.18) at 180 days after detection. A higher risk of cluster formation was associated with emm type (adjusted hazard ratio as compared with emm 89 was 2.37 [95% CI 1.71-3.30] for emm 1, 2.72 [1.82-4.06] for emm 3, 2.28 [1.49-3.51] for emm 6, 1.47 [1.05-2.06] for emm 12, and 2.21 [1.38-3.56] for emm 92), homelessness (1.42 [1.01-1.99]), injection drug use (2.08 [1.59-2.72]), residence in a long-term care facility (1.78 [1.29-2.45]), and the autumn-winter season (1.34 [1.14-1.57]) in multivariable Cox regression analysis. The machine-learning model stratified the validation cohort (n=1382) into groups at low (n=370), moderate (n=738), and high (n=274) risk. The 90-day risk of cluster formation was 0.03 (95% CI 0.01-0.05) for the group at low risk, 0.10 (0.08-0.13) for the group at moderate risk, and 0.21 (0.17-0.25) for the group at high risk. These results were consistent with the cross-validation outcomes in the derivation cohort. Interpretation Using population-based surveillance data, we found that pathogen, host, and environment factors of iGAS cases were associated with increased likelihood of subsequent genomic cluster formation. Groups at high risk were consistently identified by a predictive model which could inform prevention strategies, although future work to refine the model, incorporating other potential risk factors such as host contact patterns and immunity to GAS, is needed to improve its predictive performance.  Copyright Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",Not About Sufficiency
Transformative Aspects of Agricultural Modernization and Its Land Use Requirements: Insights from Chinese Case Studies,"Sustainable agriculture has been proposed by the United Nations as a key indicator of the Sustainable Development Goals (SDGs). It presents diverse features and rich connotations in the transformation towards modernization. However, for a long time in China, transformations of agricultural modernization have not been the concern of spatial planning, nor the emerging land use requirements of agricultural modernization under the trends of urban-rural integration, such as the application of modern technologies for the mechanization of agricultural production, the coexistence of multiple forms of business entities with agricultural enterprises as the main body, the extension of the industrial chain from the primary to the secondary and the tertiary, and the concentration of industrial spaces, as well as specific land use due to those transformations. This paper constructs an analytical framework of ""business entity, business model, production technology, and production space"" based on the literature studies and selects eight representative agricultural cases from Beijing, Zhejiang, and Yunnan to conduct field investigations and case studies to reveal the transformative aspects of agricultural modernization in China and its land use requirements, enriching the understanding of modern agriculture from the perspective of spatial planning. This study finds that the transformation of agricultural modernization has generated new requirements for the construction of necessary production facilities, but these requirements cannot be met in terms of land use due to the constraints imposed by China's current land use regulations. The paper advocates for the development of agricultural parks, the optimization of land use regulations, and the allocation of agricultural land use in spatial planning in line with the trends of agricultural modernization, thus supporting the sustainable development of agriculture.",Not About Sufficiency
The case for tracking misinformation the way we track disease,"While public health organizations can detect disease spread, few can monitor and respond to real-time misinformation. Misinformation risks the public's health, the credibility of institutions, and the safety of experts and front-line workers. Big Data, and specifically publicly available media data, can play a significant role in understanding and responding to misinformation. The Public Good Projects uses supervised machine learning to aggregate and code millions of conversations relating to vaccines and the COVID-19 pandemic broadly, in real-time. Public health researchers supervise this process daily, and provide insights to practitioners across a range of disciplines. Through this work, we have gleaned three lessons to address misinformation. (1) Sources of vaccine misinformation are known; there is a need to operationalize learnings and engage the pro-vaccination majority in debunking vaccine-related misinformation. (2) Existing systems can identify and track threats against health experts and institutions, which have been subject to unprecedented harassment. This supports their safety and helps prevent the further erosion of trust in public institutions. (3) Responses to misinformation should draw from cross-sector crisis management best practices and address coordination gaps. Real-time monitoring and addressing misinformation should be a core function of public health, and public health should be a core use case for data scientists developing monitoring tools. The tools to accomplish these tasks are available; it remains up to us to prioritize them.",Not About Sufficiency
Pragmatic Distance in IoT Devices,"Stand-alone devices perform their functions without human intervention. These types of devices would benefit from capabilities that allow collaboration among the devices in order to maximize service availability, as well as dynamic adaptation of the devices to user requirements in order to minimize resource usage. For example, when detecting a malfunction in one of the devices, the system should find devices with similar functionality to recover the overall service by functionality substitution. Additionally, for example, in the context of a user with hearing loss, the system should utilize devices with visual feedback functionality and disable audio feedback functionality to conserve resources. In this paper, we describe the implementation of a method to detect similar functionalities between heterogeneous devices, denoted Lightweight Machine to Machine Resource Pragmatic Distance (LwRPD). We define that devices are pragmatically close if they can be substituted with each other in order to perform a specific function in a particular context. We implement the method for enabling the aforementioned capabilities in specific use cases, using resources from real-life devices, to demonstrate the benefits. We also compare LwRPD with a multi-agent pragmatic similarity metric, Fuzzy logic and Levenshtein distance techniques, demonstrating LwRPD superiority over those techniques.",Not About Sufficiency
Reinforcement-Enhanced Autoregressive Feature Transformation: Gradient-steered Search in Continuous Space for Postfix Expressions,"Feature transformation aims to generate new pattern-discriminative feature space from original features to improve downstream machine learning (ML) task performances. However, the discrete search space for the optimal feature explosively grows on the basis of combinations of features and operations from low-order forms to high-order forms. Existing methods, such as exhaustive search, expansion reduction, evolutionary algorithms, reinforcement learning, and iterative greedy, suffer from large search space. Overly emphasizing efficiency in algorithm design usually sacrifices stability or robustness. To fundamentally fill this gap, we reformulate discrete feature transformation as a continuous space optimization task and develop an embedding-optimization-reconstruction framework. This framework includes four steps: 1) reinforcement-enhanced data preparation, aiming to prepare high-quality transformation-accuracy training data; 2) feature transformation operation sequence embedding, intending to encapsulate the knowledge of prepared training data within a continuous space; 3) gradient-steered optimal embedding search, dedicating to uncover potentially superior embeddings within the learned space; 4) transformation operation sequence reconstruction, striving to reproduce the feature transformation solution to pinpoint the optimal feature space. Finally, extensive experiments and case studies are performed to demonstrate the effectiveness and robustness of the proposed method. The code and data are publicly accessible https://www.dropbox.com/sh/imh8ckui7va3k5u/AACulQegVx0MuywYyoCqSdVPa?dl=0.",Not About Sufficiency
Dutch design manual for lightweight pavements with expanded polystyrene geofoam,"The use of expanded polystyrene (EPS) geofoam instead of traditional ""heavy"" sand for pavement subbases can reduce or even eliminate the additional load on the subsoil, thus decreasing or eliminating the settlement of pavement structures on a compressible subsoil. Experience with EPS geofoam is very positive, but a uniform design procedure does not yet exist for this type of structure. Optimization of the existing EPS pavement design guidelines and their improvement has demanded materials research on FPS, the use or three-dimensional finite-element pavement models, and in situ full-scale measurement. Extensive materials research provided data for the stress-strain response of EPS under representative loading and environmental conditions. Three-dimensional modeling enabled critical evaluation of existing design methodologies by analyzing pavements with different road bases, different EPS types, and different asphalt thicknesses. In situ measurements by means of built-in strain transducers in asphalt provided data for verification of the three-dimensional modeling. Furthermore, evaluation and monitoring of Dutch projects with EPS geofoam were carried out to define problematic aspects from the practical point of view. On the basis of the research findings, the current Dutch design guidelines have been revised and optimized. In order to make this knowledge easily accessible, an official Dutch design manual for lightweight pavements with EPS geofoam was published under the auspices of the Center for Research and Contract Standardization in Civil and Traffic Engineering. An outline of the manual Is presented.",Not About Sufficiency
Standardization: colorfull or dull?,"After mentioning the necessity of standardization in general, this paper explains how human factors, or ergonomics standardization by ISO and the deployment of information technology were linked. Visual display standardization is the main topic; the present as well as the future situation in this field are treated, mainly from an ISO viewpoint. Some observations are made about the necessary and interesting co-operation between physicists and psychologists, of different nationality, who both may be employed by either private enterprise or governmental institutions, in determining visual display requirements. The display standard that is to succeed the present ISO standards in this area: ISO 9241-3, -7, -8 and ISO 13406-1, -2, will have a scope that is not restricted to office tasks. This means a large extension of the contexts for which display requirements have to be investigated and specified - especially if mobile use of displays, under outdoor lighting conditions, is included. ne new standard will be structured in such a way that it is better accessible than the present ones for different categories of standards users. The subject color in the new standard is elaborated here. A number of questions are asked as to which requirements on color rendering should be made, taking new research results into account, and how far the new standard should go in making recommendations to the display user.",Not About Sufficiency
Investigation of seismic damage to existing buildings by using remotely observed images,"A prolonged and highly destructive seismic sequence struck Central Italy, spanning from August 24, 2016, to January 2017. This series of earthquakes resulted in widespread structural failures and extensive damage across four regions along the Apennines: Lazio, Umbria, Marche, and Abruzzo. In the context of this study, based on post-earthquake remotely observed damages data, we conducted an in-depth empirical examination of the vulnerability of various types of buildings, including reinforced concrete, masonry, and steel structures. These valuable data, accessible through a Google Street View application, furnished critical insights into building locations, characteristics, and spatially observed patterns of damage before and after the seismic events. We undertook a meticulous review and comparison of this extensive dataset to ensure its consistency with prior earthquake observations, fragility functions, and relevant scientific literature. Our research involved a comprehensive examination of selected structures by combining postearthquake inspections and visual assessments using Google Street View. This comprehensive study allowed us to compare similar buildings with and without damage, generating significant statistical insights into the types of structural pathologies that lead to localized and widespread damage, as well as instances of structural collapses or domino effects. Furthermore, we propose a preliminary glimpse at the power of Machine Learning techniques into earthquake risk assessment. We suggest that this approach could enhance the robustness and accuracy of modeling procedures, which are essential when addressing such complex challenges.",Not About Sufficiency
Designing Learning for Sustainable Development: Digital Practices as Boundary Crossers and Predictors of Sustainable Lifestyles,"Sustainable development (SD) is a multidimensional issue. However, research findings report a divide between students' awareness and behavior. It is identified that study programs are designed more for awareness outcomes, and not so much for behavioral outcomes. For higher-order learning outcomes manifested in a sustainable development behavior, the authors argue for a model based on an understanding of learning as boundary crossing. Based on this model, learning for sustainable development occurs in relating social practices, lifestyles, academic practices, professional practices, and students' digital practices. To inform teachers' approaches to teaching as an important driver of institutional change, we conducted a survey among students of urban and spatial planning in Slovenia. Examined factors included personal, academic, and digital predictors for sustainable development awareness, lifestyle, and behavioral intention. We hypothesized that a significant predictor for sustainable development behavior, which was measured as sustainable lifestyle and sustainable development behavioral intention, would be learning in social practices, and that learning in social practices would predict preferred teaching methods. The findings of hierarchical regression analysis indicated personal factors as the most important predictors of SD behavioral intention, and academic predictors as the most important factors for SD awareness. Digital practices were found to be the most important predictors of a sustainable lifestyle. Social practices of sustainable lifestyle, digital practices, and perceived teaching methods predicted students' preferred teaching methods. We discuss the future directions of sustainable development education, considering digital social media practices as essential boundary crossers.",Not About Sufficiency
From capture to detection: A critical review of passive sampling techniques for pathogen surveillance in water and wastewater,"Water quality, critical for human survival and well-being, necessitates rigorous control to mitigate contamination risks, particularly from pathogens amid expanding urbanization. Consequently, the necessity to maintain the microbiological safety of water supplies demands effective surveillance strategies, reliant on the collection of representative samples and precise measurement of contaminants. This review critically examines the advancements of passive sampling techniques for monitoring pathogens in various water systems, including wastewater, freshwater, and seawater. We explore the evolution from conventional materials to innovative adsorbents for pathogen capture and the shift from culture-based to molecular detection methods, underscoring the adaptation of this field to global health challenges. The comparison highlights passive sampling's efficacy over conventional techniques like grab sampling and its potential to overcome existing sampling challenges through the use of innovative materials such as granular activated carbon, thermoplastics, and polymer membranes. By critically evaluating the literature, this work identifies standardization gaps and proposes future research directions to augment passive sampling's efficiency, specificity, and utility in environmental and public health surveillance.",Not About Sufficiency
Deep learning approach for diabetes prediction using PIMA Indian dataset,"Purpose International Diabetes Federation (IDF) stated that 382 million people are living with diabetes worldwide. Over the last few years, the impact of diabetes has been increased drastically, which makes it a global threat. At present, Diabetes has steadily been listed in the top position as a major cause of death. The number of affected people will reach up to 629 million i.e. 48% increase by 2045. However, diabetes is largely preventable and can be avoided by making lifestyle changes. These changes can also lower the chances of developing heart disease and cancer. So, there is a dire need for a prognosis tool that can help the doctors with early detection of the disease and hence can recommend the lifestyle changes required to stop the progression of the deadly disease. Method Diabetes if untreated may turn into fatal and directly or indirectly invites lot of other diseases such as heart attack, heart failure, brain stroke and many more. Therefore, early detection of diabetes is very significant so that timely action can be taken and the progression of the disease may be prevented to avoid further complications. Healthcare organizations accumulate huge amount of data including Electronic health records, images, omics data, and text but gaining knowledge and insight into the data remains a key challenge. The latest advances in Machine learning technologies can be applied for obtaining hidden patterns, which may diagnose diabetes at an early phase. This research paper presents a methodology for diabetes prediction using a diverse machine learning algorithm using the PIMA dataset. Results The accuracy achieved by functional classifiers Artificial Neural Network (ANN), Naive Bayes (NB), Decision Tree (DT) and Deep Learning (DL) lies within the range of 90-98%. Among the four of them, DL provides the best results for diabetes onset with an accuracy rate of 98.07% on the PIMA dataset. Hence, this proposed system provides an effective prognostic tool for healthcare officials. The results obtained can be used to develop a novel automatic prognosis tool that can be helpful in early detection of the disease. Conclusion The outcome of the study confirms that DL provides the best results with the most promising extracted features. DL achieves the accuracy of 98.07% which can be used for further development of the automatic prognosis tool. The accuracy of the DL approach can further be enhanced by including the omics data for prediction of the onset of the disease.",Not About Sufficiency
Comparison and integration of computational methods for deleterious synonymous mutation prediction,"Synonymous mutations do not change the encoded amino acids but may alter the structure or function of an mRNA in ways that impact gene function. Advances in next generation sequencing technologies have detected numerous synonymous mutations in the human genome. Several computational models have been proposed to predict deleterious synonymous mutations, which have greatly facilitated the development of this important field. Consequently, there is an urgent need to assess the state-of-the-art computational methods for deleterious synonymous mutation prediction to further advance the existing methodologies and to improve performance. In this regard, we systematically compared a total of 10 computational methods (including specific method for deleterious synonymous mutation and general method for single nucleotide mutation) in terms of the algorithms used, calculated features, performance evaluation and software usability. In addition, we constructed two carefully curated independent test datasets and accordingly assessed the robustness and scalability of these different computational methods for the identification of deleterious synonymous mutations. In an effort to improve predictive performance, we established an ensemble model, named Prediction of Deleterious Synonymous Mutation (PrDSM), which averages the ratings generated by the three most accurate predictors. Our benchmark tests demonstrated that the ensemble model PrDSM outperformed the reviewed tools for the prediction of deleterious synonymous mutations. Using the ensemble model, we developed an accessible online predictor, PrDSM, available at http://bioinfo.ahu.edu.cn:8080/PrDSM/. We hope that this comprehensive survey and the proposed strategy for building more accurate models can serve as a useful guide for inspiring future developments of computational methods for deleterious synonymous mutation prediction.",Not About Sufficiency
Quantitative ultrasound augmented with machine learning to assess tissue microstructure during wound healing,"Objective: Wound healing follows a complex process which is impaired in the case of chronic wounds. Therefore, there is a need to early identify healing deficiencies to adapt the treatments and reduce the significant burden of wound management on healthcare. Here, we propose an analysis method based on features extraction from B-mode ultrasound imaging, a non-invasive and clinically usable modality to quantitatively describe the wound healing process through its entire course. Methods: Fifteen C57BL/6 mice received a 1-cm full thickness wound on the dorsum and were imaged with a 15 MHz transducer during the first 3 weeks post-injury. Image features based on the texture and brightness of pixels were computed and compared to an automatic processing method. A machine learning algorithm was also applied to detect the healing status from images. Results: The histogram peak value from the surface region of interest (ROI) remained increased from day 8, while echogenicity of the underlying tissue was temporarily higher from day 2 to day 10. These two metrics respectively correlate with collagen and nuclei expressions from histology. An accuracy of 89.7 % was reached in determining the wound healing phases using a machine learning framework. Conclusion: Ultrasound imaging of open wounds provide time-dependent features which can predict the wound healing stage when coupled with machine learning. Significance: This study indicates that conventional ultrasound imaging can be a useful tool for an early identification of abnormalities during the wound healing process by processing B-mode images, which are accessible from any clinical ultrasound imaging system.",Not About Sufficiency
Assessing governance implications of city digital twin technology: A maturity model approach,"Digital twin technology has great potential to transform urban planning. However, the governance aspects of city-scale digital twins (CDTs)- a virtual representation of urban environments -are understudied. This study bridges this knowledge gap by adopting a framework that scrutinizes the maturity stages of technology. We introduce the CITYSTEPS Maturity Model, a pioneering maturity framework tailored for CDTs, to assess all development stages of CDTs, including those utilizing artificial intelligence, and analyze the technology's role in urban governance. We highlight the promise of CDTs in enhancing public participation in urban planning and addressing key smart city concerns, such as accountability and transparency. However, significant challenges remain, including public participation, public trust in privacy protection, and technical impediments like inadequate data integration, systems integration, and interoperability. There's also the pressing issue of social inclusion: the potential exclusion of marginalized groups, including those often overlooked in data collection, like the hidden homeless and informal sector workers. We propose CDTs should be designed with a humancentric approach, transparent and unbiased data collection and algorithm development, and be led by an adaptive regulatory framework. The CITYSTEPS Maturity Model lays out a framework to assess CDTs' present state, forecast their future, and understand their governance implications, promoting more inclusive technology adoption.",Not About Sufficiency
Health Effects of the Relocation of Patients With Dementia: A Scoping Review to Inform Medical and Policy Decision-Making,"Background and Objectives: Research into the relocation (including international relocation) of people with dementia is increasingly important due to the aging population and latest developments in the international politics (including globalization and concerns over international migration). There is need for an overview of the health effects of relocation to facilitate and inform decision- and policy-making regarding these relocations. The aim of this literature review was to provide insight into the physical, psychological, and social consequences of varied types of relocations of older adults suffering from dementia. Research Design and Methods: A scoping literature review with a systematic search was performed in PubMed, Web of Science, PsychInfo, JSTOR, and ScienceDirect. The articles dealing with subject of relocation of older adults from 1994 to 2017 were included and analyzed. Methodological quality assessment was performed for all articles. Results: Final list included 13 articles. The effects of relocation were discussed in terms of mortality and morbidity. In most studies, the health effects of the relocation of older adults suffering from dementia were negative. A decline in physical, mental, behavioral, and functional well-being was reported. The most recurring effect was a higher level of stress, which is more problematic for patients with dementia. In general, unless it is carefully planned, it is best to avoid changing lives of people with dementia and it is recommended to actively work to reduce their exposure to stress. Discussion and Implications: The outcomes of the study suggest definite evidence for the negative effects of relocation of the older adults. This research aims to be used as the support of the legal and medical decisions of relocation of patients with dementia.",Not About Sufficiency
"The urge to count. Objectification, measurement and standardization as social practice",,Not About Sufficiency
A Machine Learning Model To Predict CO2 Reduction Reactivity and Products Transferred from Metal-Zeolites,"Various reactive intermediates and Cn products from carbon dioxide reduction reaction (CO2RR) play critical roles in the chemical and fuel industry. Developing easily accessible activity descriptors to predict possible intermediates and products of CO2RR is of great importance. The free energy changes (Delta G) for all possible reaction intermediate and product probability (P) of CO2 reduction to methanol, methane, and formaldehyde on 26 single-atom catalysts (SACs) in zeolites were predicted by density functional theory (DFT) calculations and machine learning (ML) models. The adsorption free energies of Delta G*OH and Delta G*O*CH2 were highly correlated with catalytic activity. Producing methanol was favorable for metal-zeolites with early transition metals and main group elements. Methane production was more feasible for some systems such as Co-zeolite, due to the low free energy and high selectivity against the hydrogen evolution reaction. Both XGBoost and ExtraTrees algorithms could give satisfactory predictions of Delta G and P in CO2RR using descriptors of reaction pathways, metal, charge transfer (CT) between the metal and reaction intermediate, hydrogen bond interaction between the intermediate and zeolite framework, and geometry. The global electronegativity difference (delta chi T) and average ionization energy difference (delta IE) between the metal-zeolite and intermediate were introduced as features (along with the valence electron number of metals and the atomic number of reaction species) for prediction of CT values without the need of DFT calculations. The CT feature could be replaced by some additional descriptors such as the band gap (Eg) or coordination number of metals to intermediates in training ML models for free energy prediction. ML models on an external test such as MOFs, 2D materials, and molecular complex materials indicate that the proposed descriptors are general for the reaction free energy change and product prediction of SACs in CO2RR.",Not About Sufficiency
Physical Activity through Sustainable Transport Approaches (PASTA): a study protocol for a multicentre project,"Introduction: Only one-third of the European population meets the minimum recommended levels of physical activity (PA). Physical inactivity is a major risk factor for non-communicable diseases. Walking and cycling for transport (active mobility, AM) are well suited to provide regular PA. The European research project Physical Activity through Sustainable Transport Approaches (PASTA) pursues the following aims: (1) to investigate correlates and interrelations of AM, PA, air pollution and crash risk; (2) to evaluate the effectiveness of selected interventions to promote AM; (3) to improve health impact assessment (HIA) of AM; (4) to foster the exchange between the disciplines of public health and transport planning, and between research and practice. Methods and analysis: PASTA pursues a mixed-method and multilevel approach that is consistently applied in seven case study cities. Determinants of AM and the evaluation of measures to increase AM are investigated through a large scale longitudinal survey, with overall 14 000 respondents participating in Antwerp, Barcelona, London, Orebro, Rome, Vienna and Zurich. Contextual factors are systematically gathered in each city. PASTA generates empirical findings to improve HIA for AM, for example, with estimates of crash risks, factors on AM-PA substitution and carbon emissions savings from mode shifts. Findings from PASTA will inform WHO's online Health Economic Assessment Tool on the health benefits from cycling and/or walking. The study's wide scope, the combination of qualitative and quantitative methods and health and transport methods, the innovative survey design, the general and city-specific analyses, and the transdisciplinary composition of the consortium and the wider network of partners promise highly relevant insights for research and practice. Ethics and dissemination: Ethics approval has been obtained by the local ethics committees in the countries where the work is being conducted, and sent to the European Commission before the start of the survey. The PASTA website (http://www.pastaproject.eu) is at the core of all communication and dissemination activities.",Not About Sufficiency
Strengthening Public Health Capacity to Address Infectious Diseases: Lessons From 3 Centers of Excellence in Public Health and Homelessness,"People experiencing homelessness are disproportionately affected by infectious diseases and often face barriers to receiving appropriate medical treatment. Responding to the needs of people experiencing homelessness requires state and local health departments to integrate information sources and coordinate multisector efforts. From 2021 to 2023, the CDC Foundation, in cooperation with the Centers for Disease Control and Prevention, established pilot Centers of Excellence in Public Health and Homelessness in Seattle, Washington; San Francisco, California; and the state of Minnesota. These centers strengthened their capacity to address the needs of people experiencing homelessness by supporting cross-sector partnerships, assessing the interoperability of data systems, prioritizing infectious disease needs, and identifying health disparities. These programs demonstrated that health departments are heterogeneous entities with differing resources and priorities. They also showed the importance of employing dedicated public health staff focused on homelessness, establishing diverse partnerships and the need for support from local leaders to address homelessness.",Not About Sufficiency
Machine Learning Approaches for Epidemiological Investigations of Food-Borne Disease Outbreaks,"Foodborne diseases (FBDs) are infections of the gastrointestinal tract caused by foodborne pathogens (FBPs) such as bacteria [Salmonella, Listeria monocytogenes and Shiga toxin-producing E coli (STEC)] and several viruses, but also parasites and some fungi. Artificial intelligence (Al) and its sub-discipline machine learning (ML) are re-emerging and gaining an ever increasing popularity in the scientific community and industry, and could lead to actionable knowledge in diverse ranges of sectors including epidemiological investigations of FBD outbreaks and antimicrobial resistance (AMR). As genotyping using whole-genome sequencing (WGS) is becoming more accessible and affordable, it is increasingly used as a routine tool for the detection of pathogens, and has the potential to differentiate between outbreak strains that are closely related, identify virulence/resistance genes and provide improved understanding of transmission events within hours to days. In most cases, the computational pipeline of WGS data analysis can be divided into four (though, not necessarily consecutive) major steps: de novo genome assembly, genome characterization, comparative genomics, and inference of phylogeny or phylogenomics. In each step, ML could be used to increase the speed and potentially the accuracy (provided increasing amounts of high-quality input data) of identification of the source of ongoing outbreaks, leading to more efficient treatment and prevention of additional cases. In this review, we explore whether ML or any other form of Al algorithms have already been proposed for the respective tasks and compare those with mechanistic model-based approaches.",Not About Sufficiency
"Civil liberties or public health, or civil liberties and public health? Using surveillance technologies to tackle the spread of COVID-19","To help tackle the spread of COVID-19 a range of surveillance technologies - smartphone apps, facial recognition and thermal cameras, biometric wearables, smart helmets, drones, and predictive analytics - have been rapidly developed and deployed. Used for contact tracing, quarantine enforcement, travel permission, social distancing/movement monitoring, and symptom tracking, their rushed rollout has been justified by the argument that they are vital to suppressing the virus, and civil liberties have to be sacrificed for public health. I challenge these contentions, questioning the technical and practical efficacy of surveillance technologies, and examining their implications for civil liberties, governmentality, surveillance capitalism, and public health.",Not About Sufficiency
JudithJeyafreeda at SemEval-2023 Task 10: Machine Learning for Explainable Detection of Online Sexism,"The rise of the internet and social media platforms has brought about significant changes in how people interact with each another. For a lot of people, the internet have also become the only source of news and information about the world. Thus due to the increase in accessibility of information, online sexism has also increased. Efforts should be made to make the internet a safe space for everyone, irrespective of gender, both from a larger social norms perspective and legal or technical regulations to help alleviate online gender-based violence. As a part of this, this paper explores simple methods that can be easily deployed to automatically detect online sexism in textual statements.",Not About Sufficiency
"Urban projects in France: navigating neoliberalism, justice, and commons planning","This review paper examines the concept of urban projects within the evolving landscape of urban planning theory and practice, with a particular focus on the French context. Through a targeted review of French and English language literature, we trace the development of planning models from rational comprehensive approaches to more collaborative and strategic models. We argue that the urban project emerges as a multifaceted construct, embodying processes, outcomes, and actor-rules dynamics. The paper explores three key research subjects in contemporary urban projects: the interplay between neoliberal and the just city paradigms, the divergence between judicial and spatial understandings of justice, and the potential of commons and social innovation as an emerging paradigm and a response to neoliberal paradigm and spatial inequalities. This research synthesises existing knowledge and identifies key areas for future investigation, offering insights that can inform both theoretical discourse and practical applications in urban planning.",Not About Sufficiency
Research and Implementation of the Text Matching Algorithm in the Field of Housing Law and Policy Based on Deep Learning,"Machine learning enables machines to learn rules from a large amount of data input from the outside world through algorithms, so as to identify and judge. It is the main task of the government to further emphasize the importance of improving the housing security mechanism, expand the proportion of affordable housing, increase financial investment, improve the construction quality of affordable housing, and ensure fair distribution. It can be seen that the legal system of housing security is essentially a system to solve the social problems brought by housing marketization, and it is an important part of the whole national housing system. More and more attention has been paid to solving the housing difficulties of low- and middle-income people and establishing a housing security legal system suitable for China's national conditions and development stage. Aiming at the deep learning problem, a text matching algorithm suitable for the field of housing law and policy is proposed. Classifier based on matching algorithm is a promising classification technology. The research on the legal system of housing security is in the exploratory stage, involving various theoretical and practical research studies. Compare the improved depth learning algorithm with the general algorithm, so as to clearly understand the advantages and disadvantages of the improved depth learning algorithm and depth learning algorithm. This paper introduces the practical application of the deep learning model and fast learning algorithm in detail. Creatively put forward to transform it into an independent public law basis or into an independent savings system.",Not About Sufficiency
Equitable distribution of bikeshare stations: An optimization approach,"Bikeshare systems have attracted increased research interest ranging from bikeshare planning analyses to operational improvement studies (e.g., rebalancing, or station optimization). However, the interaction between bikeshare station spatial distribution and actual bikeshare activities when addressing equity issues has not been thoroughly considered. Moreover, there is a paucity of research helping governments develop incentive programs for equitable bikeshare services. To fill this research gap, we develop a model to estimate the potential demand (i. e., bikeshare trip production and attraction) and its distribution, and evaluate performance over a set of objectives (e.g., maximization of annual revenue, accessibility improvements) to find the most equitable distribution of stations. We build a genetic algorithm to solve this multi-objective optimization. The study uses the Divvy bikeshare system in Chicago as a case study, and compares the solutions of the model with the system's expansion (new stations added) in 2016, which targeted disadvantaged areas. When selecting accessibility as the main objective, the results indicate the need to provide more stations in disadvantaged areas and those results overlap with the system's expansion in 2016. On the contrary, the goal of revenue maximization results in a smaller network of stations and fewer accessibility improvements, especially in disadvantaged communities. A sensitivity analysis uncovers the greatest obstacle (i.e., station cost) to adding more stations in disadvantaged areas. More importantly, a Pareto frontier of this multi-objective optimization supports several policy suggestions for incentivizing private bikeshare companies to target more disadvantaged populations. Our results show the importance of considering accessibility and other equity constraints in developing a more inclusive, equitable and sustainable transportation system, and we provide several planning suggestions.",Not About Sufficiency
A Deliberative Rural Community Consultation to Assess Support for Flood Risk Management Policies to Strengthen Resilience in Malawi,"As disasters increase in frequency and magnitude with adverse effects on population health, governments will be forced to implement disaster risk management policies that may include forced relocation. Ineffective public consultation has been cited as one reason for failure of these policies. Using the deliberative polling method, this study assessed the capacity of rural communities to participate in flood risk management policy priority setting and the impact of providing accurate and balanced information on policies by comparing pre-and post -deliberation data. The study also assessed the level of trust on whether government and community would use the results of this study. Results indicated strong community support for policy options to reduce vulnerability in communities and strong resistance to relocation. As all the top five ranked policy options were concerned with population pressure, gender, and social service issues, which are all conceptually considered social determinants of a healthy community, this study concludes that public health considerations are central to flood risk policy development and implementation. The study revealed high levels of trust in government and the community relating to flood risk management, which policymakers in low-to-middle income countries can capitalise on for meaningful community consultation for effective disaster risk management.",Not About Sufficiency
Artificial Intelligence-Clinical Decision Support System in Infectious Disease Control: Combatting Multidrug-Resistant Klebsiella pneumoniae with Machine Learning,"Purpose: The World Health Organization has identified Klebsiella pneumoniae (KP) as a significant threat to global public health. The rising threat of carbapenem-resistant Klebsiella pneumoniae (CRKP) leads to prolonged hospital stays and higher medical costs, necessitating faster diagnostic methods. Traditional antibiotic susceptibility testing (AST) methods demand at least 4 days, requiring 3 days on average for culturing and isolating the bacteria and identifying the species using matrix-assisted laser desorption/ionization time-of-flight mass spectrometry (MALDI-TOF MS), plus an extra day for interpreting AST results. This lengthy process makes traditional methods too slow for urgent clinical situations requiring rapid decision-making, potentially hindering prompt treatment decisions, especially for fast-spreading infections such as those caused by CRKP. This research leverages a cutting-edge diagnostic method that utilizes an artificial intelligence-clinical decision support system (AI-CDSS). It incorporates machine learning algorithms for the swift and precise detection of carbapenem-resistant and colistin-resistant strains. Patients and Methods: We selected 4307 KP samples out of a total of 52,827 bacterial samples due to concerns about multi-drug resistance using MALDI-TOF MS and Vitek-2 systems for AST. It involved thorough data preprocessing, feature extraction, and machine learning model training fine-tuned with GridSearchCV and 5-fold cross-validation, resulting in high predictive accuracy, as demonstrated by the receiver operating characteristic and area under the curve (AUC) scores, laying the groundwork for our AI-CDSS. Results: MALDI-TOF MS analysis revealed distinct intensity profiles differentiating CRKP and susceptible strains, as well as colistin-resistant Klebsiella pneumoniae (CoRKP) and susceptible strains. The Random Forest Classifier demonstrated superior discriminatory power, with an AUC of 0.96 for detecting CRKP and 0.98 for detecting CoRKP. Conclusion: Integrating MALDI-TOF MS with machine learning in an AI-CDSS has greatly expedited the detection of KP resistance by approximately 1 day. This system offers timely guidance, potentially enhancing clinical decision-making and improving treatment outcomes for KP infections.",Not About Sufficiency
Behaviour associated with the presence of a school sports ground: Visual information for policy makers,"The planning and development of sports infrastructure is a complex process that has a broad and long-term impact on health and well-being in communities. It involves many different stake- holders and usually requires significant public or private investments. Its framework is outlined by policies that define the general social goals of such development. To ensure the maximum alignment between the goals and the development activities, it is important to support the policy making process by high-quality information based on real-world data and presented in a clear and focused way. This work introduces a new pipeline of methods for processing and interpretation of data on physical activity and lifestyle in adolescents. The data is extracted from the Health Behaviour in School-aged Children (HBSC) study and analyzed by modern machine learning methods. We identify behavioural patterns associated with the presence and absence of a school sports ground in different sex and age groups of adolescent in the Czech Republic. The patterns are presented by concise graphical models that ease their use by stake- holders without expert knowledge in sociology, statistics, mathematical modelling, etc. They enable intuitive visual assessment of situation in different regions and highlight the specific similarities and differences among them. Together, the proposed methods contribute towards objective evidence-based policy making in sports management and development.",Not About Sufficiency
Trajectory-level fog detection based on in-vehicle video camera with TensorFlow deep learning utilizing SHRP2 naturalistic driving data,"Providing drivers with real-time weather information and driving assistance during adverse weather, including fog, is crucial for safe driving. The primary focus of this study was to develop an affordable in-vehicle fog detection method, which will provide accurate trajectory-level weather information in real-time. The study used the SHRP2 Naturalistic Driving Study (NDS) video data and utilized several promising Deep Learning techniques, including Deep Neural Network (DNN), Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), and Convolutional Neural Network (CNN). Python programming on the TensorFlow Machine Learning library has been used for training the Deep Learning models. The analysis was done on a dataset consisted of three weather conditions, including clear, distant fog and near fog. During the training process, two optimizers, including Adam and Gradient Descent, have been used. While the overall prediction accuracy of the DNN, RNN, LSTM, and CNN using the Gradient Descent optimizer were found to be around 85 %, 77 %, 84 %, and 97 %, respectively; much improved overall prediction accuracy of 88 %, 91 %, 93 %, and 98 % for the DNN, RNN, LSTM, and CNN, respectively, were observed considering the Adam optimizer. The proposed fog detection method requires only a single video camera to detect weather conditions, and therefore, can be an inexpensive option to be fitted in maintenance vehicles to collect trajectory-level weather information in real-time for expanding as well as updating weather-based Variable Speed Limit (VSL) systems and Advanced Traveler Information Systems (ATIS).",Not About Sufficiency
Theoretical model for the sustainable city,"The objective of this study was framed in proposing a theoretical model for the sustainable city. For this, a bibliographic, descriptive and content analysis of the characteristics that, for this type of city, several authors propose, was used. The documentary research inquired about the city as satisfying, and about the theoretical statements issued by various authors, considered reliable because they are criteria of renowned professionals dedicated to the investigation of the subject of study, about the particularities that the sustainable city should have. Among the most revealing findings, it stands out that the sustainable city must not only include urban typologies and standards, but also the full exercise of human rights. The main conclusions establish that the sustainable city must be: a physical space with dense, diverse, integrating, efficient and green environments, environments that must be included in urban planning and regulations; and it must be considered as a space that satisfies justice, inclusion, production, culture, security, participation and education, which are considered basic human needs that constitute universal human rights.",Not About Sufficiency
Re-shaping urban mobility - Key to Europe<acute accent>s green transition,"This paper outlines the vision of EIT Urban Mobility towards sustainable urban mobility. EIT Urban Mobility is an initiative of the European Institute for Innovation and Technology (EIT), a body of the European Union. EIT Urban Mobility's ecosystem counts more than 260 organisations from cities, research & academia, and industry working to enable people and goods to move affordable, fast, comfortably, safely, and cleanly.In the context of climate emergency and extreme weather events that an increasing number of European cities are already facing, it is of utmost importance to develop, and scale decarbonised urban mobility solutions. Such solutions must address acute challenges faced by cities and their inhabitants, linking urban and mobility planning, while actively engaging with citizens at all stages of transformation processes, from design to implementation.Our vision aims to facilitate this process by channelling public and private efforts towards priority areas of transformation, which are technological and behavioural pathways leading to more sustainable urban mobility spanning from street experiments to connected and shared on-demand mobility. The paper also identifies structural enablers of change, which are key technological and regulatory innovations required to turn our vision into an everyday reality.",Not About Sufficiency
"SERS-based approaches in the investigation of bacterial metabolism, antibiotic resistance, and species identification","Surface-enhanced Raman scattering (SERS) is an inelastic scattering phenomenon that occurs when photons interact with substances, providing detailed molecular structure information. It exhibits various advantages including high sensitivity, specificity, and multiple-detection capabilities, which make it particularly effective in bacterial detection and antibiotic resistance research. In this review, we review the recent development of SERSbased approaches in the investigation of bacterial metabolism, antibiotic resistance, and species identification. Although the promising applications have been realized in clinical microbiology and diagnostics, several challenges still limit the further development, including signal variability, the complexity of spectral data interpretation, and the lack of standardized protocols. To overcome these obstacles, more reproducible and standardized methodologies, particularly in nanomaterial design and experimental condition optimization. Furthermore, the integration of SERS with machine learning and artificial intelligence can automate spectral analysis, improving the efficiency and accuracy of bacterial species identification, resistance marker detection, and metabolic monitoring. Combining SERS with other analytical techniques, such as mass spectrometry, fluorescence microscopy, or genomic sequencing, could provide a more comprehensive understanding of bac- terial physiology and resistance mechanisms. As SERS technology advances, its applications are expected to extend beyond traditional microbiology to areas like environmental monitoring, food safety, and personalized medicine. In particular, the potential for SERS to be integrated into point-of-care diagnostic devices offers sig- nificant promise for enhancing diagnostics in resource-limited settings, providing cost-effective, rapid, and accessible solutions for bacterial infection and resistance detection.",Not About Sufficiency
Facilitating interoperability in Semantic Web applications using ontologies,"Semantic Web applications often rely on the reuse of underlying data from legacy information systems. The motivation for our research is how to make this data accessible for use in such applications. We propose the use of ontologies and conceptual modelling in an overall approach for data interoperability The approach incorporates a process for ontology, alignment using our tool for visual ontology alignment. First results from our tool evaluation were promising and show that visualization is useful for locating additional candidate alignments by an end user.",Not About Sufficiency
Contextual factors predicting compliance behavior during the COVID-19 pandemic: A machine learning analysis on survey data from 16 countries,"Voluntary isolation is one of the most effective methods for individuals to help prevent the transmission of diseases such as COVID-19. Understanding why people leave their homes when advised not to do so and identifying what contextual factors predict this non-compliant behavior is essential for policymakers and public health officials. To provide insight on these factors, we collected data from 42,169 individuals across 16 countries. Participants responded to items inquiring about their socio-cultural environment, such as the adherence of fellow citizens, as well as their mental states, such as their level of loneliness and boredom. We trained random forest models to predict whether someone had left their home during a one week period during which they were asked to voluntarily isolate themselves. The analyses indicated that overall, an increase in the feeling of being caged leads to an increased probability of leaving home. In addition, an increased feeling of responsibility and an increased fear of getting infected decreased the probability of leaving home. The models predicted compliance behavior with between 54% and 91% accuracy within each country's sample. In addition, we modeled factors leading to risky behavior in the pandemic context. We observed an increased probability of visiting risky places as both the anticipated number of people and the importance of the activity increased. Conversely, the probability of visiting risky places increased as the perceived putative effectiveness of social distancing decreased. The variance explained in our models predicting risk ranged from < .01 to .54 by country. Together, our findings can inform behavioral interventions to increase adherence to lockdown recommendations in pandemic conditions.",Not About Sufficiency
Gaining knowledge about the complex human lifeworld,"Proponents of justificationism are adamant that knowledge claims have to be proven to be true by referring to an appropriate epistemological authority and indisputable foundation of knowledge. However, as such an assumption cannot be justified on rational grounds, it cannot be accepted as knowledge in terms of this criterion of justificationism. Furthermore, according to Cilliers, justificationism (""foundationalism"") represents the traditional (""modern') way of gathering knowledge. In the current era (whether regarded as postmodern or not), which is characterised by a complex lifeworld, such a strategy represents avoiding complexity instead of acknowledging and studying it. Cilliers explains that a complex, open system is characterised by an interwoven network of dynamic interactions among numerous heterogeneous components (which can also form subsystems) of the system and other relevant systems. As a result of these dynamic interactions the structure of the system changes and is reorganised, often by means of self-organisation, for new characteristics to emerge and contribute to its functioning. Whether an organism consists of one cell or many, for it to survive necessitates the conversion of suitable nutrients to energy. Such a process involves finding and internally absorbing requisite energy products, converting these to a universal source of energy for physiological processes, removal of waste products and the consumption of the energy to maintain this routine until the end of the life cycle of the organism. Maintaining this process is extremely complicated and demanding as inter alia sufficient quantities of nutrients for example sugars, fats and protein are required, the correct mixture of gasses such as oxygen and carbon dioxide and an optimal pH-balance need to be maintained. The process to maintain such optimally balanced conditions is called homeostasis and according to Damasio this capability has been transmitted genetically since the development of primitive organisms. Living organisms thrive under optimal homeostatic conditions. Specific mechanisms in the human brain determine the extent of deviations from homeostatic values through comparisons with set chemical parameters, to assess the intensity of a need. Subsequently a particular deviation from the homeostatic value enables other mechanisms in the brain to initiate corrective actions or, depending on the urgency of the response, may effect a ""reward"" or ""punishment"". Damasio explains that in the human brain, with a mind, consciousness and the ability to create neurological maps to reflect such internal states, the parameters associated with homeostatic values correspond with observations of pain and pleasure on a conscious level. Furthermore, such observations can be meaningfully associated in the mind with linguistic labels such as pleasure, wellbeing, discomfort and pain in brains that have mastered language. Mind develops by virtue of the activity of special cells, called neurons, which are crucial for the functioning of the brain. Neurons are not essential for the maintenance of basic life processes; this is evident from the existence of simple organisms without neurons. However, in more complex organisms with numerous cells, neurons assist with the control of life processes. Although the composition of neurons to some extent corresponds with that of other body cells, Damasio maintains that neurons differ functionally as well as strategically from other cells. Functionally, neurons are capable of generating electrochemical signals that can alter conditions in other cells, including other neurons. This influence on the condition in cells is the source of activities that cause and control behaviour and eventually also contribute to the creation of mind. Consequently the functional difference is also the base of the strategic difference: neurons exist to serve all other cells in the body and as such assist with the management of life. The human brain is a super system of systems comprising a complex network of billions of neurons, of which those in close proximity are richly interconnected, while a few connections over slightly longer distances also occur. Each system comprises a richly interconnected network of macroscopic, cortical regions and subcortical nuclei, which in turn consist of microscopic local networks of neurons interconnected via synapses. Activities in various small local networks in the brain are organised and coordinated over bigger, more extensive networks so that interactive neural patterns or ""maps"" are continuously and momentarily constructed and reconstructed These maps represent objects and events in the human body or brain, or in the external environment as experienced by the senses. They may be very detailed or less so, refer to concrete objects or abstractions and are consciously experienced as mental ""images"" which are not based on visual stimuli only, but also on stimuli from the other senses, emotions, memory and so on. The human mind and self are persistently constructed and reconstructed by means of continuous interactions in a complex network of systems, which includes the whole body with all receptors, senses and the brain, existing within a complex physical and social environment. Feelings, emotions, images, thoughts, behaviour and so on, emerge, enabling the human being to exist as a result of the dynamic interactions between the environment and the brain. With reference to research, Damasio demonstrates that a human being is in essence an integrated complex system with a body, brain, senses and a mind that develops thoughts and self-consciousness in order to live a meaningful life within a physical and social environment. Cilliers argues that the recognition of the complex nature of the human lifeworld, presupposes that such a world cannot be known completely. But, such a supposition does not imply that no knowledge of the lifeworld is possible, or that information that is accessible, is vague. Knowledge claims have to be meaningful in that their limits of meaning are determined with reference to the relevant situation. Damasio demonstrates that the essential nature of humans determines the way in which they construct and understand their lifeworld. Through the creation of a self and a conscious mind, human beings gather knowledge and add meaning and value to their everyday lives. Likewise the human scientist as a member of a community of scientists is acculturated and trained in its paradigm to gather scientific knowledge about the lifeworld of humans.",Not About Sufficiency
Discovery of a Low Thermal Conductivity Oxide Guided by Probe Structure Prediction and Machine Learning,"We report the aperiodic titanate Ba10Y6Ti4O27 with a room-temperature thermal conductivity that equals the lowest reported for an oxide. The structure is characterised by discontinuous occupancy modulation of each of the sites and can be considered as a quasicrystal. The resulting localisation of lattice vibrations suppresses phonon transport of heat. This new lead material for low-thermal-conductivity oxides is metastable and located within a quaternary phase field that has been previously explored. Its isolation thus requires a precisely defined synthetic protocol. The necessary narrowing of the search space for experimental investigation was achieved by evaluation of titanate crystal chemistry, prediction of unexplored structural motifs that would favour synthetically accessible new compositions, and assessment of their properties with machine-learning models.",Not About Sufficiency
Urban heat governance: examining the role of urban planning,"Heat is an increasing climate risk for cities due to climate change and the urban heat island effect. Extreme heat has inequitable impacts across social, economic, and urban environmental systems. Despite increasing awareness of heat risk, the planning and governance structures for mitigating and managing heat are less understood than those for other climate risks. We studied five large, climatically-diverse U.S. cities to better understand urban heat governance with a focus on the field of urban planning. We first conducted a plan evaluation of these cities' comprehensive, climate action, and hazard mitigation plans (n = 14) and then interviewed urban planners, resilience professionals, hazard mitigation planners, emergency managers, and public health professionals (n = 22). We found that aspects of heat planning occur across a variety of municipal plans but only a small number of strategies were explicitly framed in terms of heat, suggesting an opportunity to better connect heat with other policy goals. Urban planners tended to play a backseat role relative to other professions, despite the field's importance for reducing heat-related inequity. Better understanding the role of urban planning within broader governance structures can help policymakers to best engage in heat mitigation and management.",Not About Sufficiency
DeepCSO: A Deep-Learning Network Approach to Predicting Cysteine S-Sulphenylation Sites,"Cysteine S-sulphenylation (CSO), as a novel post-translational modification (PTM), has emerged as a potential mechanism to regulate protein functions and affect signal networks. Because of its functional significance, several prediction approaches have been developed. Nevertheless, they are based on a limited dataset from Homo sapiens and there is a lack of prediction tools for the CSO sites of other species. Recently, this modification has been investigated at the proteomics scale for a few species and the number of identified CSO sites has significantly increased. Thus, it is essential to explore the characteristics of this modification across different species and construct prediction models with better performances based on the enlarged dataset. In this study, we constructed several classifiers and found that the long short-term memory model with the word-embedding encoding approach, dubbed LSTMWE, performs favorably to the traditional machine-learning models and other deep-learning models across different species, in terms of cross-validation and independent test. The area under the receiver operating characteristic (ROC) curve for LSTMWE ranged from 0.82 to 0.85 for different organisms, which was superior to the reported CSO predictors. Moreover, we developed the general model based on the integrated data from different species and it showed great universality and effectiveness. We provided the on-line prediction service called DeepCSO that included both species-specific and general models, which is accessible through .",Not About Sufficiency
"Evolution of The Metropolitan Area of Shenzhen, Analysis: From Theory to Selected Examples","A continuously expanding megacity, the city of Shenzhen is situated on the eastern bank of the Pearl River delta, in the Southeast of China After only 37 years of urban development, it is currently the fastest growing metropolitan area in the world. It is also a huge-scale experiment, in what is commonly referred to as a ""plan-led urban form"". Established in 1980, as a first Special Economic Zone (SEZ), and through the transformations of successive master plans, Shenzhen has now become one of the largest urban areas in China Specifically, ""The Master Plan of Shenzhen 1996-2010"" and its subsequent revision, ""The Comprehensive Plan of Shenzhen 2010-2020"" reconfigured and accelerated city growth. Since 2010 the metropolitan area expanded northwards, including the previous non-SEZ territories. The plan was focused to conclusive structural evolution on border and periphery zones. The current structural master plan introduced a hierarchy of urban development on a municipal level, district level, and cluster level. Another important aspect was to regulate and control land use policy to avoid speculations on land price and building density inside emerging new districts and clusters. This article will try to describe, via particular examples, how concepts result in real site situations. It will analyse the process of detailing, from spatial development of multi-scale theoretical plans into architectural plans of study cases and finally built projects. Architectural features such as a specific building function or scale always reflect on pre-proposed theoretical guidelines of urban planning Spatial hierarchy is imposed from macro-scale into micro-scale. Chosen insights are corresponding with recent periods of urban development and respond to infrastructural, economic and social needs of the time. The buildings are designed by European consultants, gmp International GmbH architects and engineers and they are implemented to Chinese frameworks. The analysis is based on comparison of theoretical surveys and observed in-field implementation. As a conclusion, the author tries critically to analyse the advantages and disadvantages of the fast and global growth of Shenzhen metropolitan area and transposition into architectural scale. What is making Shenzhen unique and innovative in city development?",Not About Sufficiency
"Exploring the Dynamic Relationship Between Energy Efficiency, Trade, Economic Growth, and CO2 Emissions: Evidence From Novel Fourier ARDL Approach","Climate change, energy security, and volatile energy prices have been emerging as eminent threats worldwide. To overcome these concerns, energy efficiency could play a positive role. Hence, this study probes whether energy efficiency curbs CO2 emissions in the US, while controlling for trade, economic growth, and population. We use the Fourier-ADF and Fourier-LM tests to discern the unit-roots. Moreover, to render reliable findings, we rely on the novel Fourier ARDL model. The study divulges that energy efficiency plunges emissions in the long- and short-run. In particular, a 1% upsurge in energy efficiency impedes emissions by about 0.37% and 1.07% during the long- and short-term, respectively. Moreover, population and economic growth escalate emissions whether it is long- or short-run. Next, we document that trade upsurges emissions in the long-run. Also, we perform two types of sensitivity analysis to test whether our key results remain the same across different models/methods. Finally, we suggest escalating energy efficiency through investment and technological advancement. Moreover, import tariffs on renewables should be plunged while there should be relatively high tariffs on non-renewables.",Not About Sufficiency
Reliability Analysis of Gravity Retaining Wall Using Hybrid ANFIS,"Gravity retaining walls are a vital structure in the area of geotechnical engineering, and academicians in earlier studies have conveyed substantial uncertainties involved in calculating the factor of safety against overturning, using a deterministic approach. Hence, to enhance the accuracy and eliminate the uncertainties involved, artificial intelligence (AI) was used in the present research. The main aim of this study is to propose a high-performance machine learning (ML) model to determine the factor of safety (FOS) of gravity retaining walls against overturning. The projected methodology included a novel hybrid machine learning model that merged with an adaptive neuro-fuzzy inference system (ANFIS) and meta-heuristic optimization techniques (particle swarm optimization (PSO), genetic algorithm (GA), firefly algorithm (FFA) and grey wolf optimization (GWO)). In this research, four hybrid models, namely ANFIS-PSO, ANFIS-FFA, ANFIS-GA and ANFIS-GWO, were created to estimate the factor of safety against overturning. The proposed hybrid models were evaluated on two distinct datasets (training 70% and testing 30%) with three input combinations, namely cohesion (c), unit weight of soil (?) and angle of shearing resistance (phi). To access the prediction power of different hybrid models, various statistical parameters such as R-2, AdjR(2), VAF, WI, LMI, a-20 index, PI, KGE, RMSE, SI, MAE, NMBE and MBE were computed for training (TR) and testing (TS) datasets. The overall performance of the models indicated that ANFIS-PSO provided better results among all four models. The reliability index was computed using the first-order second-moment (FOSM) method for all models, and the probability of failure was also computed. A Williams plot was drawn to check the applicability domain of the hybrid model and to check the influence of different input parameters on the prediction of the factor of safety, and the Gini index was also computed.",Not About Sufficiency
Boosting Fair Classifier Generalization through Adaptive Priority Reweighing,"With the increasing penetration of machine learning applications in critical decision-making areas, calls for algorithmic fairness are more prominent. Although there have been various modalities to improve algorithmic fairness through learning with fairness constraints, their performance does not generalize well in the test set. A performance-promising fair algorithm with better generalizability is needed. This article proposes a novel adaptive reweighing method to eliminate the impact of the distribution shifts between training and test data on model generalizability. Most previous reweighing methods propose to assign a unified weight for each (sub)group. Rather, our method granularly models the distance from the sample predictions to the decision boundary. Our adaptive reweighing method prioritizes samples closer to the decision boundary and assigns a higher weight to improve the generalizability of fair classifiers. Extensive experiments are performed to validate the generalizability of our adaptive priority reweighing method for accuracy and fairness measures (i.e., equal opportunity, equalized odds, and demographic parity) in tabular benchmarks. We also highlight the performance of our method in improving the fairness of language and vision models. The code is available at https://github.com/che2198/APW.",Not About Sufficiency
ALDELE: All-Purpose Deep Learning Toolkits for Predicting the Biocatalytic Activities of Enzymes,"Rapidly predicting enzyme properties for catalyzing specific substrates is essential for identifying potential enzymes for industrial transformations. The demand for sustainable production of valuable industry chemicals utilizing biological resources raised a pressing need to speed up biocatalyst screening using machine learning techniques. In this research, we developed an all-purpose deep-learning-based multiple-toolkit (ALDELE) workflow for screening enzyme catalysts. ALDELE incorporates both structural and sequence representations of proteins, alongside representations of ligands by subgraphs and overall physicochemical properties. Comprehensive evaluation demonstrated that ALDELE can predict the catalytic activities of enzymes, and particularly, it identifies residue-based hotspots to guide enzyme engineering and generates substrate heat maps to explore the substrate scope for a given biocatalyst. Moreover, our models notably match empirical data, reinforcing the practicality and reliability of our approach through the alignment with confirmed mutation sites. ALDELE offers a facile and comprehensive solution by integrating different toolkits tailored for different purposes at affordable computational cost and therefore would be valuable to speed up the discovery of new functional enzymes for their exploitation by the industry.",Not About Sufficiency
Is COVID-19 Related Anxiety an Accelerator for Responsible and Sustainable Investing ? A Sentiment Analysis,"The excessive volatility generated by the COVID-19 pandemic highlights that environmental and social issues are potential elements that businesses and governments must manage effectively and swiftly. This study seeks to test whether the rising anxiety over this pandemic has affected the attitudes and choices towards environmentally and socially responsible investing. To this end, we first use machine learning tools to examine tweets related to this unprecedented and wild shock. Second, we compare the impact of these sentiments on the stock performance of companies from the S&P500 that meet environmental and social sustainability criteria for three COVID-19 phases with varying levels of anxiety, which we label incubation, fever and the increasing risk of second wave pandemic (in the absence of vaccine). Our findings reveal that the increasing uncertainty and worries over COVID-19 and its consequences has not distracted investors' attention away from environmental and social issues, but companies with responsible strategies on environmental issues that specifically address climate responsibility are likely to be more responsive to sentiments at the current situation of emergency.",Not About Sufficiency
"Public Health Resources, Religion, and Freedom as Predictors of COVID-19 Vaccination Rates: A Global Study of 89 Countries","The COVID-19 global pandemic necessitated adequate compliance to safe and effective vaccinations developed against the disease. Vaccination reduces high infection, morbidity, and mortality rates, develops herd immunity, and alleviates overburdened healthcare systems and massive economic costs. COVID-19 also raised awareness about the importance of robust health systems, notably public health competence and the number and training of community health workers. Using the World Health Organization, Global Development of Applied Community Studies project, and other available cross-sectional secondary data on 89 countries, we found that the strength of community health training and research (CHTR), the importance of religion, and freedom score (political rights, civil liberties) are associated with COVID-19 full-vaccination rate. Significant bivariate correlations included per-capita-GDP, number of nurses, per-capita health spending, aged population, noncommunicable disease rate, life satisfaction, government response stringency, nonviolent activism, education, and strength of community development, urban planning, and liberation theology fields. Our assessment of CHTR contribution to the COVID-19 response revealed a great resource for effectively targeting vaccine-hesitant individuals and increasing vaccination rates. The results suggest that to motivate vaccine adherence countries need adequate community health workforce training and research, a population not hesitant to adhere to public health measures based on religion, and societal-level freedoms.",Not About Sufficiency
A Retrospective and Multicenter Study on COVID-19 in Inner Mongolia: Evaluating the Influence of Sampling Locations on Nucleic Acid Test and the Dynamics of Clinical and Prognostic Indexes,"COVID-19 is spreading widely, and the pandemic is seriously threatening public health throughout the world. A comprehensive study on the optimal sampling types and timing for an efficient SARS-CoV-2 test has not been reported. We collected clinical information and the values of 55 biochemical indices for 237 COVID-19 patients, with 37 matched non-COVID-19 pneumonia patients and 131 healthy people in Inner Mongolia as control. In addition, the results of dynamic detection of SARS-CoV-2 using oropharynx swab, pharynx swab, and feces were collected from 197 COVID-19 patients. SARS-CoV-2 RNA positive in feces specimen was present in approximately one-third of COVID-19 patients. The positive detection rate of SARS-CoV-2 RNA in feces was significantly higher than both in the oropharynx and nasopharynx swab (P < 0.05) in the late period of the disease, which is not the case in the early period of the disease. There were statistically significant differences in the levels of blood LDH, CRP, platelet count, neutrophilic granulocyte count, white blood cell number, and lymphocyte count between COVID-19 and non-COVID-19 pneumonia patients. Finally, we developed and compared five machine-learning models to predict the prognosis of COVID-19 patients based on biochemical indices at disease onset and demographic characteristics. The best model achieved an area under the curve of 0.853 in the 10-fold cross-validation.",Not About Sufficiency
Challenges and directions for digital twin implementation in otorhinolaryngology,"BackgroundDigital twin technology heralds a transformative era in Otorhinolaryngology (ORL), merging the physical and digital worlds to offer dynamic, virtual models of physical entities or processes.PurposeThese models, capable of simulating, predicting, and optimizing real-world counterparts, are evolving from static replicas to intelligent, adaptive systems.MethodsFueled by advancements in communication, sensor technology, big data analytics, Internet of Things (IoT), and simulation technologies, artificial intelligence (AI), digital twins in ORL promise personalized treatment planning, virtual experimentation, and therapeutic intervention optimization. Despite their potential, the integration of digital twins in ORL faces challenges including data privacy and security, data integration and interoperability, computational demands, model validation and accuracy, ethical and regulatory considerations, patient engagement, and cost and accessibility issues.ResultsOvercoming these challenges requires robust data protection measures, seamless data integration, substantial computational resources, rigorous validation studies, ethical transparency, patient education, and making the technology accessible and affordable. Looking ahead, the future of digital twins in ORL is bright, with advancements in AI and machine learning, omics data integration, real-time monitoring, virtual clinical trials, patient empowerment, seamless healthcare integration, longitudinal data analysis, and collaborative research.ConclusionThese developments promise to refine diagnostic and treatment strategies, enhance patient care, and facilitate more efficient and tailored ORL research, ultimately leading to more effective and personalized ORL management.",Not About Sufficiency
Stacked machine learning models for non-technical loss detection in smart grid: A comparative analysis,"The growing prominence and emphasis of renewable energy to decrease carbonization in the power system and reduce the dependability of fossil fuel for energy needs play an important role in the development of smart grids. Many technological advancements are integrated into smart grid to optimize the power system and renewable energy sources. Smart grid leverages electricity and energy consumption data exchange to establish a significantly advanced, automated, and decentralized electricity network. However, this brings numerous vulnerabilities to the power system, including cyber-attacks, grid blackouts, and electricity theft. While the most significant concern is energy theft, where some culprit's consumers manipulate their energy meters to reduce their readings. This destabilizes the country's electricity utility and economic development and causes a high tariff on energy for consumers who pay the bill. Therefore, developing an advanced framework for electricity theft detection is necessary. To address this problem, we propose a machine learning-based stacked framework to detect malicious activity in the smart grid. The proposed data-based stacked ensemble model detects honest and anomalous consumers in two stages. In the first stage, the model employs four individual classifiers at the base level to analyze data and a single classifier at the meta-level to classify the results of the base learners for the second stage classification. Furthermore, the Borderline SMOTE and Principle Component Analysis techniques are employed to address the class imbalance and curse of dimensionality issues respectively. Through experimental analysis, we proved the effectiveness of the proposed framework in detecting suspicious activity in four different experiments, including preprocessed data, feature extracted data, balanced data, and lastly, both feature engineering and data balancing. The simulation outcomes demonstrate that our proposed framework enhanced energy security and overcomes the impact of theft attacks on the smart grid.",Not About Sufficiency
Urban heat island mitigation by green infrastructure in European Functional Urban Areas,"The Urban Heat Island (UHI) effect is one of the most harmful environmental hazards for urban dwellers. Climate change is expected to increase the intensity of the UHI effect. In this context, the implementation of Urban Green Infrastructure (UGI) can partially reduce UHI intensity, promoting a resilient urban environment and contributing to climate change adaptation and mitigation. In order to achieve this result, there is a need to systematically integrate UGI into urban planning and legislation, but this process is subject to the availability of widely applicable, easily accessible and quantitative evidence. To offer a big picture of urban heat intensity and opportunities to mitigate high temperatures, we developed a model that reports the Ecosystem Service (ES) of microclimate regulation of UGI in 601 European cities. The model simulates the temperature difference between a baseline and a no-vegetation scenario, extrapolating the role of UGI in mitigating UHI in different urban contexts. Finally, a practical, quantitative indicator that can be applied by policymakers and city administrations has been elaborated, allowing to estimate the amount of urban vegetation that is needed to cool summer temperatures by a certain degree. UGI is found to cool European cities by 1.07 degrees C on average, and up to 2.9 degrees C, but in order to achieve a 1 degrees C drop in urban temperatures, a tree cover of at least 16% is required. The microclimate regulation ES is mostly dependent on the amount of vegetation inside a city and by transpiration and canopy evaporation. Furthermore, in almost 40% of the countries, more than half of the residing population does not benefit from the microclimate regulation service provided by urban vegetation. Widespread implementation of UGI, in particular in arid regions and cities with insufficient tree cover, is key to ensure healthy urban living conditions for citizens.",Not About Sufficiency
"ARBUR, a machine learning-based analysis system for relating behaviors and ultrasonic vocalizations of rats","Deciphering how different behaviors and ultrasonic vocalizations (USVs) of rats interact can yield insights into the neural basis of social interaction. However, the behavior -vocalization interplay of rats remains elusive because of the challenges of relating the two communication media in complex social contexts. Here, we propose a machine learning -based analysis system (ARBUR) that can cluster without bias both non -step (continuous) and step USVs, hierarchically detect eight types of behavior of two freely behaving rats with high accuracy, and locate the vocal rat in 3-D space. ARBUR reveals that rats communicate via distinct USVs during different behaviors. Moreover, we show that ARBUR can indicate findings that are long neglected by former manual analysis, especially regarding the non -continuous USVs during easyto -confuse social behaviors. This work could help mechanistically understand the behavior -vocalization interplay of rats and highlights the potential of machine learning algorithms in automatic animal behavioral and acoustic analysis.",Not About Sufficiency
Land-use problem and controling for sustainable coastal development in South Bali,"Land-use development in the coastline areas due to tourism activities leads to indications of the spatial significant discrepancy because it is not in line with the spatial planning and does not pay attention to aspects of disaster mitigation. Therefore, it is essential to maintain the coastline area that used as protection and public space. Although several previous studies have been conducted, there are still many spatial violations. This study analyses the level of land-use suitability in the coastline area and formulates strategies for land-use control following the spatial plan. The study employs Geographic Information System (GIS) by overlaying the land use map in 2022 with the regional spatial plan of Gianyar Regency. Here, the data analysis used a matrix of the Internal Factor Evaluation (IFE), the External Factor Evaluation matrix (EFE) and the Internal External Matrix (IE). The results show that 171,427.62 m2 (92.08%) of the existing coastal land use is in conformity with the spatial plan. Meanwhile, there is 14,750.09 m2 (7.92%) of the total study area that is not conforming with the spatial plan. Strategies that can be carried out in efforts to control land-use are intensifying the socialization and supervision of land-use control, preparing detailed the spatial plans, implementing incentive and disincentive programs to relevant stakeholders, and strengthened in enforcing regional regulations regarding the spatial plan of the Gianyar Regency.",Not About Sufficiency
Sepsis 2019: What Surgeons Need to Know,"The definition of sepsis continues to be as dynamic as the management strategies used to treat this. Sepsis-3 has replaced the earlier systemic inflammatory response syndrome (SIRS)-based diagnoses with the rapid Sequential Organ Failure Assessment (SOFA) score assisting in predicting overall prognosis with regards to mortality. Surgeons have an important role in ensuring adequate source control while recognizing the threat of carbapenem-resistance in gram-negative organisms. Rapid diagnostic tests are being used increasingly for the early identification of multi-drug-resistant organisms (MDROs), with a key emphasis on the multidisciplinary alert of results. Novel, higher generation antibiotic agents have been developed for resistance in ESKCAPE (Enterococcus faecium, Staphylococcus aureus, Klebsiella pneumoniae, Acinetobacter baumannii, Pseudomonas aeruginosa, and Enterobacter species) organisms while surgeons have an important role in the prevention of spread. The Study to Optimize Peritoneal Infection Therapy (STOP-IT) trial has challenged the previous paradigm of length of antibiotic treatment whereas biomarkers such as procalcitonin are playing a prominent role in individualizing therapy. Several novel therapies for refractory septic shock, while still investigational, are gaining prominence rapidly (such as vitamin C) whereas others await further clinical trials. Management strategies presented as care bundles continue to be updated by the Surviving Sepsis Campaign, yet still remain controversial in its global adoption. We have broadened our temporal and epidemiologic perspective of sepsis by understanding it both as an acute, time-sensitive, life-threatening illness to a chronic condition that increases the risk of mortality up to five years post-discharge. Artificial intelligence, machine learning, and bedside scoring systems can assist the clinician in predicting post-operative sepsis. The public health role of the surgeon is key. This includes collaboration and multi-disciplinary antibiotic stewardship at a hospital level. It also requires controlling pharmaceutical sales and the unregulated dispensing of antibiotic agents globally through policy initiatives to control emerging resistance through prevention.",Not About Sufficiency
Turkish sentiment analysis: A comprehensive review,"Sentiment analysis (SA) is a very popular research topic in the text mining field. SA is the process of textual mining in which the meaning of a text is detected and extracted. One of the key aspects of SA is to analyze the body of a text to determine its polarity to understand the opinions it expresses. Substantial amounts of data are produced by online resources such as social media sites, blogs, news sites, etc. Due to this reason, it is impossible to process all of this data without automated systems, which has contributed to the rise in popularity of SA in recent years. SA is considered to be extremely essential, mostly due to its ability to analyze mass opinions. SA, and Natural Language Processing (NLP) in particular, has become an overwhelmingly popular topic as social media usage has increased. The data collected from social media has sourced numerous different SA studies due to being versatile and accessible to the masses. This survey presents a comprehensive study categorizing past and present studies by their employed methodologies and levels of sentiment. In this survey, Turkish SA studies were categorized under three sections. These are Dictionary-based, Machine Learning-based, and Hybrid-based. Researchers can discover, compare, and analyze properties of different Turkish SA studies reviewed in this survey, as well as obtain information on the public dataset and the dictionaries used in the studies. The main purpose of this study is to combine Turkish SA approaches and methods while briefly explaining its concepts. This survey uniquely categorizes a large number of related articles and visualizes their properties. To the best of our knowledge, there is no such comprehensive and up-to-date survey that strictly covers Turkish SA which mainly concerns analysis of sentiment levels. Furthermore, this survey contributes to the literature due to its unique property of being the first of its kind.",Not About Sufficiency
Distinctive features of spatial planning nearby estuaries-An exploratory analysis of water-related rules in municipal master plans in Portugal,"Land-use types and related intensities are often associated with pressures and disturbances on estuarine environmental values and ecosystem services provided by water. Although with varied legal frameworks across countries, broadly, spatial planning has been expected to contribute to the protection of environmentally sensitive areas, such as estuaries. Among the various planning tools are the plan's land-use control rules. This article studies the incorporation of water-related terms in the regulations of municipal master plans to assess if land-use rules established on estuarine areas are significantly different from others, such as in upstream areas. It does so by developing a content analysis of a set of plans' regulations located in estuarine and upstream areas of two river basins of Mainland Portugal. The results show greater incorporation of water-related terms in plans' regulations located in estuarine areas. Moreover, they show a greater diversity of water-related topics, types, and focus of rules on estuarine areas, whereas on upstream areas the regulatory approaches look poorer. Although the incorporation of water-related terms is globally higher in younger plans, and to a certain extent, in more artificialized and dense territories, a clear distinctiveness of water-related concerns in land-use regulations of municipal plans on estuarine areas remains visible. Surprisingly, the results bring to the fore fragilities of landuse regulations on upstream areas worthy of attention in future studies. The methodology used for content analysis disclosed a valuable path for future research as it is easily expandable to take into consideration different land-uses or to be applied to different regions, to further refine if the distinctive features are explicitly related with estuarine areas or with other types of water problems.",Not About Sufficiency
"Banking on chinas WTO commitments: Same bed, different dreams in Chinas financial services sector","Foreign banks and the Chinese Government have different dreams about the business opportunities and obligations that arise under Chinas World Trade Organization (WTO) commitments on financial services. This article provides an overview of Chinas banking sector reforms and its gradual opening to foreign participation in the context of General Agreement on Trade in Services (GATS) rules governing international trade in financial services and the obligations that apply since Chinas WTO accession in December 2001. The article highlights the contradictory interpretations that China and other Members have issued regarding Chinas GATS commitments and provides a framework for assessing the WTO consistency of Chinas banking measures. An analysis is conducted under this framework to evaluate whether China has fully implemented its GATS commitments on (i) the acquisition of Chinese banks by foreign financial institutions, (ii) legitimate prudential regulation in the banking sector, and (iii) full market access for credit card and electronic payment services. Notwithstanding the apparent complexity of GATS rules, the article concludes that the WTO legal framework supports the case for increased access to Chinas financial services market consistent with its GATS commitments, and fully consistent with Chinas plans for continuing domestic growth and its medium-term financial services export interests.",Not About Sufficiency
Modelling future impacts of urban development in Kuwait with the use of ABM and GIS,"During the last six decades, Kuwait has experienced rapid and unprecedented population growth with only a small increase in the urban areas. The alarming rise in urban density in Kuwait has caused issues for the residents' lifestyles, the economy and the environment. These issues have been aggravated by urban planning which perpetuated a city-centric urban form without modelling the impacts of current patterns of urban growth. A spatial model using Agent Based Modelling (ABM) and Geographical Information Systems (GIS) is proposed to model disaggregate future changes in land-use patterns given forecast population estimates and planning policies. The two main impacts considered are housing shortage and traffic congestion, as these are the two most significant social impacts for Kuwaitis. This article discusses the design methodology and parameterization of the ABM and the agent groups. It characterizes urban growth by rules for different citizen groups, historical growth patterns and the influence of decision-makers. The model is validated against data for the period 1995-2015 and simulations run to 2050; the results predict that continued city-centric growth will aggravate the problems, with more than 50% increase in housing shortage and congestion unless the government intervenes to rectify the situation.",Not About Sufficiency
Rest tremor quantification based on fuzzy inference systems and wearable sensors,"Background: Currently the most consistent, widely accepted and detailed instrument to rate Parkinson's disease (PD) is the Movement Disorder Society sponsored Unified Parkinson Disease Rating Scale (MDS-UPDRS). However, the motor examination is based upon subjective human interpretation trying to capture a snapshot of PD status. Wearable sensors and machine learning have been broadly used to analyze PD motor disorder, but still most ratings and examinations lay outside MDS-UPDRS standards. Moreover, logical connections between features and output ratings are not clear and complex to derive from the model, thus limiting the understanding of the structure in the data. Methods: Fifty-seven PD patients underwent a full motor examination in accordance to the MDS-UPDRS on twelve different sessions, gathering 123 measurements. Overall, 446 different combinations of limb features correlated to rest tremors amplitude are extracted from gyroscopes, accelerometers, and magnetometers and feed into a fuzzy inference system to yield severity estimations. Results: A method to perform rest tremor quantification fully adhered to the MDS-UPDRS based on wearable sensors and fuzzy inference system is proposed, which enables a reliable and repeatable assessment while still computing features suggested by clinicians in the scale. This quantification is straightforward and scalable allowing clinicians to improve inference by means of new linguistic statements. In addition, the method is immediately accessible to clinical environments and provides rest tremor amplitude data with respect to the timeline. A better resolution is also achieved in tremors rating by adding a continuous range.",Not About Sufficiency
Raman spectroscopy combined with machine learning for rapid detection of food-borne pathogens at the single-cell level,"Rapid detection of food-borne pathogens in early food contamination is a permanent topic to ensure food safety and prevent public health problems. Raman spectroscopy, a label-free, highly sensitive and dependable technology has attracted more and more attention in the field of diagnosing food-borne pathogens in recent years. In the research, 15,890 single-cell Raman spectra of 23 common strains from 7 genera were obtained at the single cell level. Then, the nonlinear features of raw data were extracted by kernel principal component analysis, and the individual bacterial cell was evaluated and discriminated at the serotype level through the decision tree algorithm. The results demonstrated that the average correct rate of prediction on independent test set was 86.23 +/- 0.92% when all strains were recognized by only one model, but there were high misjudgment rates for certain strains. Therefore, the four-level classification models were introduced, and the different hierarchies of the identification models achieved accuracies in the range of 87.1%-95.8%, which realized the efficient prediction of strains at the serotype level. In summary, Raman spectroscopy combined with machine learning based on fingerprint difference was a prospective strategy for the rapid diagnosis of pathogenic bacteria.",Not About Sufficiency
Community-Based Planning and the New Public Health,"Social planners have begun to recognize that communities are an important resource for solving many problems. Understanding local norms and values is thought to provide insight into how issues are defined and what interventions might be considered practical. Communities in this framework are not just the physical locations at which programs are targeted, but are actively constructed spaces that must be properly understood. In many ways, the field of public health has been sensitive to this understanding and has elevated the community in importance, thus emphasizing that significant features are variable and locally defined. Even in the new public health, however, empirical indicators are still often relied on, thereby leading to the increasing standardization of communities, rather than to openness to neighborhood particularities. For this reason, a community-based focus should guide future public health endeavors. Most important, this strategy underscores the importance of getting to know the communities in which symptoms, illness and care are defined, and allows for full participation by community members that make interventions both relevant and sustainable.",Not About Sufficiency
Measuring the Sustainability of Transport System in Amman,"Urban governments show great interest in formulating policies for the sustainable transport sector. Many tend to integrate environmental, economic, and social impacts into transport decision-making, and develop a system capable of achieving all transport objectives which are: accessibility, safety, affordability, and environmentally friendly. The purpose of this study is to measure the sustainability of Amman transportation system based on United States environmental protection agency guidelines, using four performance measures: transit accessibility, vehicle miles traveled (VMT) per capita, carbon intensity, and transit productivity. Wadi Al-sair and Al Abdali Districts were selected to represent Amman. Accessibility was evaluated using GIS software; estimation of VMT per capita, carbon intensity, and transit productivity were mainly based on data obtained from field survey. The results reveal that accessibility is good in the study area, while VMT per capita was found to be lower than in the United States, carbon intensity was higher than the world average, while transit productivity was higher than in developed countries. The study concluded that the system needs improvement in all measured areas to go towards sustainability, in addition to a significant reliance on private transit at the expense of public transport. Public transport must be improved to encourage people to move towards it. As well as the adoption of intelligent land-use growth, such as comprehensive urban and regional planning, including compact cities, mixed land use, and transit oriented development.",Not About Sufficiency
Digital Pathology in Europe: Coordinating Patient Care and Research Efforts,"The COST Action IC0604 ""Telepathology Network in Europe"" (EURO-TELEPATH) is an initiative of the COST (European Cooperation in the field of Scientific and Technical Research) framework, supported by the Seventh Framework Programme for research and technological development (FP7), of the European Union will be running from 2007 to 2011 and is aimed to coordinate research efforts to develop the most adequate technological framework for the management of multimedia electronic healthcare records (data and images) applied to Anatomic Pathology. Sixteen countries are participating in EURO-TELEPATH. Activities are organized in four Working Groups (WGs): WG1 - Pathology Business Modeling, WG2 - Informatics Standards in Pathology, WG3 - Images: Analysis, Processing, Retrieval and Management, and WG4 - Technology and Automation in Pathology. During the first year of work, the collaboration between software engineers, computer scientists, pathologists and other clinicians has been essential to detect three main areas of interest in digital pathology research: virtual microscopy scanning solutions, health informatics standards, and image processing and analysis. Research in these areas is essential to a correct approach to telepathology, including primary diagnosis, and secondary or teleconsultation services. Managing microscopic pathology images (virtual slides) is a challenge to existing information systems, mainly due to its large size, large number, and complex interpretation. Regarding interoperability, the integration of pathology reports and images into eHealth records is an essential objective that research groups should consider. Promoting participation in standards bodies (DICOM, IHE, HL7, IHTSDO) is an essential part of the project work. Understanding the business process of pathology departments in daily practice, including healthcare, education, research, and quality control activities, is the starting point to be sure that standardization efforts converge with user needs. Following a recent IHE proposal, coordination with public health services like national or regional tumor registries must also be supported. Virtual or digital slides are fostering the use of image processing and analysis in pathology not only for research purposes, but also in daily practice. Nowadays, further discussion is needed on the adequacy of current existing technical solutions, including for instance quality of images obtained by scanners, or the efficiency of image analysis applications.",Not About Sufficiency
COVID-19 and Your Smartphone: BLE-Based Smart Contact Tracing,"While contact tracing is of paramount importance in preventing the spreading of infectious diseases, manual contact tracing is inefficient and time consuming as those in close contact with infected individuals are informed hours, if not days, later. This article proposes a smart contact tracing (SCT) system utilizing the smartphone's Bluetooth low energy signals and machine learning classifiers to automatically detect those possible contacts to infectious individuals. SCT's contribution is two-fold: a) classification of the user's contact as high/low-risk using precise proximity sensing, and b) user anonymity using a privacy-preserving communication protocol. To protect the user's privacy, both broadcasted and observed signatures are stored in the user's smartphone locally and only disseminate the stored signatures through a secure database when a user is confirmed by public health authorities to be infected. Using received signal strength each smartphone estimates its distance from other user's phones and issues real-time alerts when social distancing rules are violated. Extensive experimentation utilizing real-life smartphone positions and a comparative evaluation of five machine learning classifiers indicate that a decision tree classifier outperforms other state-of-the-art classification methods with an accuracy of about 90% when two users carry their smartphone in a similar manner. Finally, to facilitate research in this area while contributing to the timely development, the dataset of six experiments with about 123 000 data points is made publicly available.",Not About Sufficiency
Accelerated PALM for Nonconvex Low-Rank Matrix Recovery With Theoretical Analysis,"Low-rank matrix recovery is a major challenge in machine learning and computer vision, particularly for large-scale data matrices, as popular methods involving nuclear norm and singular value decomposition (SVD) are associated with high computational costs and biased estimators. To overcome this challenge, we propose a novel approach to learning low-rank matrices based on the matrix volume and a nonconvex logarithmic function. The matrix volume is the product of all the nonzero singular values of a matrix and has unique geometric properties and connections with other convex and nonconvex functions. We establish a generalized nonconvex regularization problem using the penalty function strategy and introduce an accelerated proximal alternating linearized minimization (AccPALM) algorithm with double acceleration, which combines Nesterov's acceleration and power strategy. The algorithm reduces computational costs and has provable convergence results under the Kurdyka- ojasiewicz (K) inequality with mild conditions. Our approach shows superior accuracy, efficiency, and convergence behavior compared to other low-rank matrix learning methods on robust matrix completion (RMC) and low-rank representation (LRR) tasks. We analyze the impact of algorithm parameters on convergence and performance and present visually appealing results to further demonstrate the effectiveness of our approach. The proposed methodology represents a promising advance in the field of low-rank matrix recovery, and its effectiveness has been validated via extensive numerical experiments. The source code for the proposed algorithms is accessible at https://github.com/ZhangHengMin/AccPALMcodes.",Not About Sufficiency
Auxiliary identification of depression patients using interpretable machine learning models based on heart rate variability: a retrospective study,"ObjectiveDepression has emerged as a global public health concern with high incidence and disability rates, which are timely imperative to identify and intervene in clinical practice. The objective of this study was to explore the association between heart rate variability (HRV) and depression, with the aim of establishing and validating machine learning models for the auxiliary diagnosis of depression.MethodsThe data of 465 outpatients from the Affiliated Hospital of Southwest Medical University were selected for the study. The study population was then randomly divided into training and test sets in a 7:3 ratio. Logistic regression (LR), support vector machine (SVM), random forest (RF) and eXtreme gradient boosting (XGBoost) algorithm models were used to construct risk prediction models in the training set, and the model performance was verified in the test set. The four models were evaluated by the area under the receiver operating characteristic curve (ROC), calibration curve and the decision curve analysis (DCA). Furthermore, we employed the SHapley Additive exPlanations (SHAP) method to illustrate the effects of the features attributed to the model.ResultsThere were 237 people in the depressed group and 228 in the non-depressed group. In the training set (n = 325) and test set (n = 140), the area under of the curve(AUC) values of the XGBoost model are 0.92 [95% confidence interval (CI) 0.888,0.95] and 0.82 (95% CI 0.754,0.892)] respectively, which are higher than the other three models. The XGBoost model has excellent predictive efficacy and clinical utility. The SHAP method was ranked according to the importance of the degree of influence on the model, with age, heart rate, Standard deviation of the NN intervals (SDNN), two nonlinear parameters of HRV and sex considered to be the top 6 predictors.ConclusionWe provided a feasibility study of HRV as a potential biomarker for depression. The proposed model based on HRV provides clinicians with a quantitative auxiliary diagnostic tool, which is assist to improving the accuracy and efficiency of depression diagnosis, and can also be utilized for the monitoring and prevention of depression.",Not About Sufficiency
Dynamic Electronic Structure Fluctuations in the De Novo Peptide ACC-Dimer Revealed by First-Principles Theory and Machine Learning,"Recent studies have reported long-range charge transport in peptide- and protein-based fibers and wires, rendering this class of materials as promising charge-conducting interfaces between biological systems and electronic devices. In the complex molecular environment of biomolecular building blocks, however, it is unclear which chemical and structural dynamic features support electronic conductivity. Here, we investigate the role of finite temperature fluctuations on the electronic structure and its implications for conductivity in a peptide-based fiber material composed of an antiparallel coiled coil hexamer, ACC-Hex, building block. All-atom classical molecular dynamics (MD) and first-principles density functional theory (DFT) are combined with interpretable machine learning (ML) to understand the relationship between physical and electronic structure of the peptide dimer subunit of ACC-Hex. For 1101 unique MD ""snapshots"" of the ACC peptide dimer, hybrid DFT calculations predict a significant variation of near-gap orbital energies among snapshots, with an increase in the predicted number of nearly degenerate states near the highest occupied molecular orbital (HOMO), which suggests improved conductivity. Interpretable ML is then used to investigate which nuclear conformations increase the number of nearly degenerate states. We find that molecular conformation descriptors of interphenylalanine distance and orientation are, as expected, highly correlated with increased state density near the HOMO. Unexpectedly, we also find that descriptors of tightly coiled peptide backbones, as well as those describing the change in the electrostatic environment around the peptide dimer, are important for predicting the number of hole-accessible states near the HOMO. Our study illustrates the utility of interpretable ML as a tool for understanding complex trends in large-scale ab initio simulations.",Not About Sufficiency
Aromatic Fingerprints: VOC Analysis with E-Nose and GC-MS for Rapid Detection of Adulteration in Sesame Oil,"Food quality assurance is an important field that directly affects public health. The organoleptic aroma of food is of crucial significance to evaluate and confirm food quality and origin. The volatile organic compound (VOC) emissions (detectable aroma) from foods are unique and provide a basis to predict and evaluate food quality. Soybean and corn oils were added to sesame oil (to simulate adulteration) at four different mixture percentages (25-100%) and then chemically analyzed using an experimental 9-sensor metal oxide semiconducting (MOS) electronic nose (e-nose) and gas chromatography-mass spectroscopy (GC-MS) for comparisons in detecting unadulterated sesame oil controls. GC-MS analysis revealed eleven major VOC components identified within 82-91% of oil samples. Principle component analysis (PCA) and linear detection analysis (LDA) were employed to visualize different levels of adulteration detected by the e-nose. Artificial neural networks (ANNs) and support vector machines (SVMs) were also used for statistical modeling. The sensitivity and specificity obtained for SVM were 0.987 and 0.977, respectively, while these values for the ANN method were 0.949 and 0.953, respectively. E-nose-based technology is a quick and effective method for the detection of sesame oil adulteration due to its simplicity (ease of application), rapid analysis, and accuracy. GC-MS data provided corroborative chemical evidence to show differences in volatile emissions from virgin and adulterated sesame oil samples and the precise VOCs explaining differences in e-nose signature patterns derived from each sample type.",Not About Sufficiency
"Estimating sewage flow rate in Jefferson County, Kentucky using machine learning for wastewater-based epidemiology applications","Direct measurement of the flow rate in sanitary sewer lines is not always feasible and is an important parameter for the normalization of data used in wastewater-based epidemiology applications. Machine learning to estimate past wastewater influent flow rates supporting public health applications has not been studied. The aim of this study was to assess wastewater treatment plant influent flow rates when compared with weather data and to retrospectively estimate flow rates in Louisville, Kentucky (USA), based on other data types using machine learning. A random forest model was trained using a range of variables, such as feces-related indicators, weather data that could be associated with dilution in sewage systems, and area demographics. The developed algorithm successfully estimated the flow rate with an accuracy of 91.7%, although it did not perform as well with short-term (1-day) high flow rates. This study suggests using variables such as precipitation (mm/day) and population size are more important for wastewater flow estimation. The fecal indicator concentration (cross-assembly phage and pepper mild mottle virus) was less important. Our study challenges currently accepted opinions by showing the important public health potential application of artificial intelligence in wastewater treatment plant flow rate estimation for wastewater-based epidemiological applications.",Not About Sufficiency
Intra-urban inequalities during rapid development: space egalitarianism in Tokyo between 1955-1975,"This paper demonstrates empirically that Tokyo's rapid post-war growth coincided with decreasing intra-urban inequalities in the special ward area, both in terms of private and public living standards. This phenomenon has not received much attention to date because Japan's income inequalities were generally very low during this period. However, megacity growth of this kind is normally associated with growing segregation. This paper develops the narrative of 'spatial egalitarianism'. It attributes low intra-urban inequalities to Tokyo's homogenous urban form, equal economic structure of its neighbourhoods, and a redistributive intermediate layer of government that took a hands-off approach to urban planning. The implications are of relevance to today's developing megacities in Asia and beyond.",Not About Sufficiency
"Biomedical engineering in low- and middle-income settings: analysis of current state, challenges and best practices","Supporting the expansion of best practices in Biomedical Engineering (BME) can facilitate pathway toward the providing universal health coverage and more equitable and accessible healthcare technologies, especially in low- and middle-income (LMI) settings. These best practices can act as drivers of change and may involve scientific-technological issues, human intervention during technology development, educational aspects, social performance management for improved interactions along the medical technology life cycle, methods for managing resources and approaches for the establishment of regulatory frameworks. The aim of our study was to identify weaknesses and strengths of the scientific, technological, socio-political, regulatory and educational landscape in BME in LMI resource settings. We thus analysed the current state-of-the-art through six dimensions considered fundamental for advancing quality and equity in healthcare: 1) relevant and 2) emergent technologies, 3) new paradigms in medical technology development, 4) innovative BME education, 5) regulation and standardization for novel approaches, and 6) policy making. In order to evaluate and compare their relevance, maturity and implementation challenges, they were assessed through a questionnaire to which 100 professionals from 35 countries with recognized experience in the field of BME and its application to LMI settings responded. The results are presented and discussed, highlighting the main challenges and pinpointing relevant areas where intervention, including local lobbying and international promotion of best practices is necessary. We were also able to identify areas where minimal effort is required to make big changes in global health.",Not About Sufficiency
Problematic blue growth: a thematic synthesis of social sustainability problems related to growth in the marine and coastal tourism,"Marine and coastal tourism constitutes one of the largest and fastest-growing segments in tourism. Growth in marine tourism is now furthered through the 'blue growth' imperative, which this article problematises. The paper argues that there are already existing sustainability issues related to the marine tourism sector. These problems could be exacerbated if growth is additionally boosted. Since the social sustainability consequences of the growth of marine tourism are less known in the sustainability science literature, this paper thematically synthesises these types of sustainability problems in particular, as presented in the tourism studies, and brings them closer to sustainability science readers. The cases of cruise tourism, ecotourism, and tourism in marine protected areas, and community-based tourism studies are examined, wherein the latter represents a critical case for social sustainability matters. The paper reports several social and environmental injustices, produced through structural forces, and a manipulated access to natural resources, health services, and healthy environments. Social sustainability issues are most obvious in cruise tourism; however, also tourism in marine protected areas, ecotourism, and community-based tourism are not unproblematic. Thus, blue growth initiatives should be carefully examined and questioned.",Not About Sufficiency
Exploring the Relationship between Ridesharing and Public Transit Use in the United States,"Car travel accounts for the largest share of transportation-related greenhouse gas emissions in the United States (U.S.), leading to serious air pollution and negative health effects; approximately 76.3% of car trips are single-occupant. To reduce the negative externalities of cars, ridesharing and public transit are advocated as cost-effective and more environmentally sustainable alternatives. A better understanding of individuals' uses of these two transport modes and their relationship is important for transport operators and policymakers; however, it is not well understood how ridesharing use is associated with public transit use. The objective of this study is to examine the relationships between the frequency and probability of ridesharing use and the frequency of public transit use in the U.S. Zero-inflated negative binomial regression models were employed to investigate the associations between these two modes, utilizing individual-level travel frequency data from the 2017 National Household Travel Survey. The survey data report the number of times the respondent had used ridesharing and public transit in the past 30 days. The results show that, generally, a one-unit increase in public transit use is significantly positively related to a 1.2% increase in the monthly frequency of ridesharing use and a 5.7% increase in the probability of ridesharing use. Additionally, the positive relationship between ridesharing and public transit use was more pronounced for people who live in areas with a high population density or in households with fewer vehicles. These findings highlight the potential for integrating public transit and ridesharing systems to provide easier multimodal transportation, promote the use of both modes, and enhance sustainable mobility, which are beneficial for the environment and public health.",Not About Sufficiency
Lethality risk markers by sex and age-group for COVID-19 in Mexico: a cross-sectional study based on machine learning approach,"Background Mexico ranks fifth worldwide in the number of deaths due to COVID-19. Identifying risk markers through easily accessible clinical data could help in the initial triage of COVID-19 patients and anticipate a fatal outcome, especially in the most socioeconomically disadvantaged regions. This study aims to identify markers that increase lethality risk in patients diagnosed with COVID-19, based on machine learning (ML) methods. Markers were differentiated by sex and age-group. Methods A total of 11,564 cases of COVID-19 in Mexico were extracted from the Epidemiological Surveillance System for Viral Respiratory Disease. Four ML classification methods were trained to predict lethality, and an interpretability approach was used to identify those markers. Results Models based on Extreme Gradient Boosting (XGBoost) yielded the best performance in a test set. This model achieved a sensitivity of 0.91, a specificity of 0.69, a positive predictive value of 0.344, and a negative predictive value of 0.965. For female patients, the leading markers are diabetes and arthralgia. For males, the main markers are chronic kidney disease (CKD) and chest pain. Dyspnea, hypertension, and polypnea increased the risk of death in both sexes. Conclusions ML-based models using an interpretability approach successfully identified risk markers for lethality by sex and age. Our results indicate that age is the strongest demographic factor for a fatal outcome, while all other markers were consistent with previous clinical trials conducted in a Mexican population. The markers identified here could be used as an initial triage, especially in geographic areas with limited resources.",Not About Sufficiency
Breast Cancer Detection and Classification Empowered With Transfer Learning,"Cancer is a major public health issue in the modern world. Breast cancer is a type of cancer that starts in the breast and spreads to other parts of the body. One of the most common types of cancer that kill women is breast cancer. When cells become uncontrollably large, cancer develops. There are various types of breast cancer. The proposed model discussed benign and malignant breast cancer. In computer-aided diagnosis systems, the identification and classification of breast cancer using histopathology and ultrasound images are critical steps. Investigators have demonstrated the ability to automate the initial level identification and classification of the tumor throughout the last few decades. Breast cancer can be detected early, allowing patients to obtain proper therapy and thereby increase their chances of survival. Deep learning (DL), machine learning (ML), and transfer learning (TL) techniques are used to solve many medical issues. There are several scientific studies in the previous literature on the categorization and identification of cancer tumors using various types of models but with some limitations. However, research is hampered by the lack of a dataset. The proposed methodology is created to help with the automatic identification and diagnosis of breast cancer. Our main contribution is that the proposed model used the transfer learning technique on three datasets, A, B, C, and A2, A2 is the dataset A with two classes. In this study, ultrasound images and histopathology images are used. The model used in this work is a customized CNN-AlexNet, which was trained according to the requirements of the datasets. This is also one of the contributions of this work. The results have shown that the proposed system empowered with transfer learning achieved the highest accuracy than the existing models on datasets A, B, C, and A2.",Not About Sufficiency
Smart Home Supporting Integrated Health and Care Services for Older Adults in the Community: Literature review and research agenda,"Due to the ageing of European populations, a growing number of older adults are exposed to risks of losing their autonomy and independence. Technological innovations can support older adults to live independently in their communities and improve their autonomy and overall quality of life. Ambient Assisted Living Technologies embedded in Smart Home have the potential to enable older adults to live safer and, therefore, longer in their communities and postpone or even prevent the relocation to a nursing home. Smart homes with embedded wireless sensors networks, connected to cyber-physical systems through smart devices using internet of things as the infrastructure supporting real-time monitoring and control of activities of residents which is facilitated by big data and machine learning can support older adults to live autonomously and independently in their communities and mitigate the risks leading to ill health and disability. Literature review found some gaps why there is not sufficient uptake of assistive technologies by older residents. Digital transformation of health and social infrastructure on a neighbourhood level is required that will enable connection of smart homes to community health care and social care services to provide timely services to the residents of smart homes and, therefore, mitigate the risks of events leading to ill health and disability.",Not About Sufficiency
GCFC: Graph Convolutional Fusion CNN Network for Cross-Domain Zero-Shot Extraction of Winter Wheat Map,"Accurate extraction of winter wheat and its planted area holds significance for agricultural research and government real-time food monitoring. Traditional machine learning methods often demand extensive data and corresponding labels for training in cross-domain classification problems. The heterogeneity of land cover types causes an uneven distribution of samples, leading to unsatisfactory results when these methods are applied directly to other regions. This article introduces a two-branch Graph Convolutional Fusion CNN network incorporating dynamic weighted stratified loss to address these challenges. To reduce the weight of losses generated by easily classified pixels, this loss function adds task masks and category dynamic weights to the cross-entropy loss. Dual branching merges global insights from graph convolutional networks with local emphasis from convolutional neural networks. It enhances the handling of cross-domain classification problems. The first branch introduces an adaptive mechanism and applies it to the graph's adjacency matrix to enhance the model's adaptability to different domain graph structures. The second branch alleviates the oversmoothing problem of edge clustering caused by graph convolution and handles multiscale and spectral information more efficiently. The experimental results showed that the proposed method achieved 99.98% accuracy and good classification results on the Zhoukou dataset. The zero-shot cross-domain prediction on the Suixian dataset achieved 96.11% accuracy. Ultimately, the entire winter wheat planting area of Shangqiu City was extracted with an accuracy of 91.92%. Numerous experiments and practical applications confirm that the proposed method is feasible and effective for winter wheat cross-domain extraction.",Not About Sufficiency
EARLY CLASSIFICATION OF GRAM-NEGATIVE BACTERIA WITH COLONY IMAGING AND DEEP LEARNING WITHOUT CODING EXPERIENCE,"Bacterial colony morphology is the first step in classifying bacterial species during the microbial identification process. It is very important to assess the morphology of bacterial colonies in a preliminary screening process to largely reduce the scope of possible bacteria species and increase work productivity in clinical bacteriology by making later identification more specific. However, making a decision about this topic requires sufficient clinical laboratory expertise. Teachable Machine (R) is a rapid, easy -to -use, web -based tool accessible to everyone that is used to create machine learning models. In this study, the performance of Teachable Machine (R) was assessed for cheap, rapid and practical identification of enteric and non -fermenting bacteria frequently isolated in microbiology laboratories. A total of 1202 colony images were used to train and validate the network's diagnostic performance. Additionally, 80 representative test images were used to assess performance. Level 1 was defined as E. coliK. pneumoniae , Level 2 was defined as P. aeruginosa-A. baumannii , Level 3 was defined as enteric bacteria -non -fermenting bacteria and Level 4 was defined as differentiating these four pathogens from each other. Mean accuracy of Teachable Machine (R) for the defined classes was 96.7%, 94.1%, 94.3%, and 90.3% for Levels 1, 2, 3, and 4, respectively. General accuracy for classification of the 80 representative colonies was 82.5% and the hit rates were 85.0%, 100%, 75.0%, and 70.0% for E. coli , K. pneumoniae , P. aeruginosa and A. baumannii , respectively. This cost-effective bacterial identification system, supported by deep learning, will be an important pioneer for a variety of applications in clinical microbiology by reducing the identification process by a significant degree and automating identification of colonies without requiring a specialist.",Not About Sufficiency
"Multihead Text Mining from COVID-19 Feedback Using Machine Learning, Deep Learning, and Hybrid Deep Learning Approaches","This study examines the impact of the COVID-19 epidemic on students in Bangladesh through text classification using various machine learning (ML) algorithms and deep learning (DL) models. The pandemic led to emergency crisis protocols in the country, including self-quarantine and the closure of educational and governmental institutions, resulting in significant negative impacts on individuals' physical and mental health, including anxiety, sadness, and terror. To better understand the psychological effects of the epidemic, the authors collected survey data from 400 students in various divisions of Bangladesh using self-administered questionnaires through Google Forms. Preprocessing techniques such as tokenization, filtering, and n-gram modeling were used in the analysis. The study deployed eight different ML algorithms and DL models, including LSTM, BiLSTM, and CNN, to classify the effects on students' academic, mental, and social lives. The results show that the ML classifier algorithms were highly effective, achieving accuracies of 95.00%, 93.75%, and 95.00% for academic, mental, and social life impact, respectively. Furthermore, hybrid DL models, such as CNN-LSTM and CNN-BiLSTM, produced good scores in predicting the impacts on students' lives. Overall, this study provides valuable insights into the impacts of the COVID-19 epidemic on students' academic, mental, and social well-being in Bangladesh.",Not About Sufficiency
"Advancements and Challenges in Video-Based Deception Detection: A Systematic Literature Review of Datasets, Modalities, and Methods","Video-based deception detection has emerged as a promising field that leverages advances in computer vision, machine learning, and multimodal analysis to capture a wealth of nonverbal cues for identifying deceptive behavior. However, the field faces significant challenges related to dataset development, methodological approaches, and ethical considerations. This systematic literature review (SLR) aims to provide a comprehensive analysis of video-based deception detection research, with five distinct contributions: 1) an unprecedented analysis of 21 datasets, revealing critical gaps and opportunities in data resources; 2) a novel evaluation framework for assessing dataset quality and ecological validity; 3) a systematic comparison of multimodal integration approaches, identifying optimal strategies for combining visual, audio, and textual cues; 4) a critical examination of temporal modeling techniques for capturing the dynamic nature of deceptive behavior; and 5) a roadmap for addressing ethical challenges in deployment. Following the PRISMA guidelines, we reviewed studies published between 2019 and 2024 in major databases, including IEEE Xplore, ACM Digital Library, ScienceDirect, and Springer Link. The review process involved a rigorous two-stage screening, which resulted in the inclusion of 42 primary research papers. Our analysis revealed several key findings: 1) only 52.4% of identified datasets are publicly accessible, highlighting a critical gap in research reproducibility; 2) multimodal approaches consistently outperform unimodal methods, with accuracy improvements of 10-15%; 3) deep learning architectures, particularly LSTM variants and attention mechanisms, demonstrate superior performance in capturing temporal aspects of deception; 4) the Real-Life Trial Dataset emerged as the most frequently used dataset (65% of studies), indicating a preference for high-stakes ecologically valid data; and 5) significant ethical challenges remain unaddressed, particularly regarding privacy, bias, and cross-cultural validity. This review makes several novel contributions to advance the field: 1) provides a comprehensive framework for dataset evaluation and development; 2) identifies optimal strategies for multimodal integration and temporal modeling; 3) presents a structured approach to addressing ethical considerations; and 4) offers a detailed roadmap for future research priorities. These contributions will guide researchers in developing more robust, ethical, and generalizable deception detection systems, while addressing critical gaps in current methodologies and datasets.",Not About Sufficiency
Resilience benefit assessment for multi-scale urban flood control programs,"Urban flood frequencies have increased and will continuously increase due to global climate change and rapid urbanization, causing enormous economic problems and social impact. Resistant strategies are no longer the best option for disaster mitigation. Improving cities' resilience to external disruptions is becoming a more important method for mitigating the impact of urban flooding. However, there are relatively little research has been conducted on the resilience of urban flood control systems. The introduction of the concept of resilience can provide a new way of thinking for urban flood control research, which can help to understand the coping process of the system during heavy rainfall, emphasizing the absorption, resistance, and recovery stages to improve the system's adaptability to the changing environment and effectively alleviate the pressure of urban flooding. This study discusses the impact of the design and configuration of urban flood control programs on the resilience of the system under heavy rainfall. Moreover, the resilience benefit is presented to quantify the phased process of system resilience capacity for measuring the quantitative relationship between programs and system resilience. Finally, the unit annual average cost of the program is combined with the resilience benefit to make decisions and ensure that cities are effectively reducing the risk of urban flooding with the optimal alternatives. The proposed framework was applied to Zhangjiagang City, a highly urbanized and densely populated city in China. The results showed that (1) during the same rainfall event, the hydrological indicators characterizing the system's resilience process (absorption, resistance, and recovery) have distinct weights; (2) rainfall has an effect on the index weight, and the weight fluctuates when the system responds to rainfall in distinct return periods; (3) rainfall return periods can have an impact on a program's effectiveness in improving the system's resilience at various stages, and the extent to which different programs are affected by the return period varies; and (4) combining large-scale and micro-scale programs improve the resilience of the system more effectively, whereas concentrating on expanding storage volume without raising the water surface rate improves the resilience of the system less effective. This framework can be used to assess the improvements in resilience acquired from various urban flood control program configurations and to assist city planners in selecting the optimal configuration, so assisting in the decision-making process for urban planning and disaster mitigation.",Not About Sufficiency
"Biomethane Production from the Mixture of Sugarcane Vinasse, Solid Waste and Spent Tea Waste: A Bayesian Approach for Hyperparameter Optimization for Gaussian Process Regression","In this work, sugarcane vinasse combined with organic waste (food and wasted tea) was demonstrated to be an excellent source of biomethane synthesis from carbon-rich biowaste. The discarded tea trash might be successfully used to generate bioenergy. The uncertainties and costs associated with experimental testing were recommended to be decreased by the effective use of contemporary machine learning methods such as Gaussian process regression. The training hyperparameters are crucial in the construction of a robust ML-based model. To make the process autoregressive, the training hyperparameters were fine-tuned by employing the Bayesian approach. The value of R-2 was found to be greater during the model test phase by 0.72%, assisting in the avoidance of model overtraining. The mean squared error was 36.243 during the model training phase and 21.145 during the model testing phase. The mean absolute percentage error was found to be under 0.1%, which decreased to 0.085% throughout the model's testing phase. The research demonstrated that a combination of wasted tea trash, sugarcane vinasse and food waste may be a viable source for biomethane generation. The contemporary methodology of the Bayesian approach for hyperparameters tuning for Gaussian process regression is an efficient method of model prediction despite the low correlation across data columns. It is possible to enhance the sustainability paradigm in the direction of energy security via the efficient usage of food and agroforestry waste.",Not About Sufficiency
scEVOLVE: cell-type incremental annotation without forgetting for single-cell RNA-seq data,"The evolution in single-cell RNA sequencing (scRNA-seq) technology has opened a new avenue for researchers to inspect cellular heterogeneity with single-cell precision. One crucial aspect of this technology is cell-type annotation, which is fundamental for any subsequent analysis in single-cell data mining. Recently, the scientific community has seen a surge in the development of automatic annotation methods aimed at this task. However, these methods generally operate at a steady-state total cell-type capacity, significantly restricting the cell annotation systems'capacity for continuous knowledge acquisition. Furthermore, creating a unified scRNA-seq annotation system remains challenged by the need to progressively expand its understanding of ever-increasing cell-type concepts derived from a continuous data stream. In response to these challenges, this paper presents a novel and challenging setting for annotation, namely cell-type incremental annotation. This concept is designed to perpetually enhance cell-type knowledge, gleaned from continuously incoming data. This task encounters difficulty with data stream samples that can only be observed once, leading to catastrophic forgetting. To address this problem, we introduce our breakthrough methodology termed scEVOLVE, an incremental annotation method. This innovative approach is built upon the methodology of contrastive sample replay combined with the fundamental principle of partition confidence maximization. Specifically, we initially retain and replay sections of the old data in each subsequent training phase, then establish a unique prototypical learning objective to mitigate the cell-type imbalance problem, as an alternative to using cross-entropy. To effectively emulate a model that trains concurrently with complete data, we introduce a cell-type decorrelation strategy that efficiently scatters feature representations of each cell type uniformly. We constructed the scEVOLVE framework with simplicity and ease of integration into most deep softmax-based single-cell annotation methods. Thorough experiments conducted on a range of meticulously constructed benchmarks consistently prove that our methodology can incrementally learn numerous cell types over an extended period, outperforming other strategies that fail quickly. As far as our knowledge extends, this is the first attempt to propose and formulate an end-to-end algorithm framework to address this new, practical task. Additionally, scEVOLVE, coded in Python using the Pytorch machine-learning library, is freely accessible at https://github.com/aimeeyaoyao/scEVOLVE.",Not About Sufficiency
Health equity in Lebanon: a microeconomic analysis,"Background: The health sector in Lebanon suffers from high levels of spending and is acknowledged to be a source of fiscal waste. Lebanon initiated a series of health sector reforms which aim at containing the fiscal waste caused by high and inefficient public health expenditures. Yet these reforms do not address the issues of health equity in use and coverage of healthcare services, which appear to be acute. This paper takes a closer look at the micro-level inequities in the use of healthcare, in access, in ability to pay, and in some health outcomes. Methods: We use data from the 2004/2005 Multi Purpose Survey of Households in Lebanon to conduct health equity analysis, including equity in need, access and outcomes. We briefly describe the data and explain some of its limitations. We examine, in turn, and using standardization techniques, the equity in health care utilization, the impact of catastrophic health payments on household wellbeing, the effect of health payment on household impoverishment, the equity implications of existing health financing methods, and health characteristics by geographical region. Results: We find that the incidence of disability decreases steadily across expenditure quintiles, whereas the incidence of chronic disease shows the opposite pattern, which may be an indication of better diagnostics for higher quintiles. The presence of any health-related expenditure is regressive while the magnitude of out-of-pocket expenditures on health is progressive. Spending on health is found to be ""normal"" and income-elastic. Catastrophic health payments are likelier among disadvantaged groups (in terms of income, geography and gender). However, the cash amounts of catastrophic payments are progressive. Poverty is associated with lower insurance coverage for both private and public insurance. While the insured seem to spend an average of almost LL93,000 ($62) on health a year in excess of the uninsured, they devote a smaller proportion of their expenditures to health. Conclusions: The lowest quintiles of expenditures per adult have less of an ability to pay out-of-pocket for healthcare, and yet incur healthcare expenditures more often than the wealthy. They have lower rates of insurance coverage, causing them to spend a larger proportion of their expenditures on health, and further confirming our results on the vulnerability of the bottom quintiles.",Not About Sufficiency
Spatiotemporal Modeling of the Smart City Residents' Activity with Multi-Agent Systems,"The article proposes the concept of modeling that uses multi-agent systems of mutual interactions between city residents as well as interactions between residents and spatial objects. Adopting this perspective means treating residents, as well as buildings or other spatial objects, as distinct agents that exchange multifaceted packages of information in a dynamic and non-linear way. The exchanged information may be reinforced or diminished during the process, which may result in changing the social activity of the residents. Utilizing Latour's actor-network theory, the authors developed a model for studying the relationship between demographic and social factors, and the diversified spatial arrangement and the structure of a city. This concept was used to model the level of residents' trust spatiotemporally and, indirectly, to study the level of social (geo)participation in a smart city. The devised system, whose test implementation as an agent-based system was done in the GAMA: agent-based, spatially explicit, modeling and simulation platform, was tested on both model and real data. The results obtained for the model city and the capital of Poland, Warsaw, indicate the significant and interdisciplinary analytical and scientific potential of the authorial methodology in the domain of geospatial science, geospatial data models with multi-agent systems, spatial planning, and applied social sciences.",Not About Sufficiency
"Effects of fine particulate matter from wildfire and non-wildfire sources on emergency-department visits in people who were housed and unhoused in San Diego County (CA, USA) during 2012-20: atime-stratified case-crossover study","Background Being unhoused can increase vulnerability to adverse health effects due to air pollution. We aimed to quantify changes in emergency-department visits during and after exposure to wildfire-specific and non-wildfire particulate matter 2<middle dot>5 mu m or less in diameter (PM2<middle dot>5) in San Diego County (CA, USA) in people who were both unhoused and housed. Methods For this time-stratified case-crossover study, we used data on exposure to wildfire-specific PM 2<middle dot>5 in California and individual-level data for people admitted to the emergency departments of two hospitals (UC San Diego Health emergency departments at La Jolla and Hillcrest, San Diego) in San Diego County between July 1, 2012, and Dec 31, 2020. People with a postcode outside of San Diego County were excluded. Demographic information was age group, race or ethnicity, and transport to the emergency department. Wildfire-specific PM 2<middle dot>5 concentration at the postcode level was previously estimated using an ensemble model that combined multiple machine-learning algorithms and explanatory variables obtained via data on 24-h mean PM 2<middle dot>5 concentrations from the US Environmental Protection Agency Air Quality System. Conditional logistic regression models were applied, adjusting for specific humidity, wind velocity, and maximum temperature extracted from the US Gridded Surface Meteorological Dataset. Housing status was established by registration staff or triage nurses on arrival at the emergency department. For people who were unhoused, exposure was defined based on the weighted mean PM 2<middle dot>5 concentration at the city level proportional to the number of people who were unhoused in each specific city across urban centres in San Diego County. For people who were housed, we used residence postcode to measure exposure. We assessed the association between PM 2<middle dot>5 from wildfire and non-wildfire sources and emergency-department visits in people who were housed and unhoused. Findings There were 587 562 emergency-department visits at the two hospitals, 76 407 (13<middle dot>0%) of which were by people who were unhoused. People who were housed had a higher exposure to overall PM 2<middle dot>5 (24-h mean over the study period of 9<middle dot>904 mg/m3, SD 3<middle dot>445) and non-wildfire PM 2<middle dot>5 (9<middle dot>663, 2<middle dot>977) than people who were unhoused (9<middle dot>863, 3<middle dot>221; 9<middle dot>557, 2<middle dot>599). However, people who were unhoused had a higher exposure to wildfire-specific PM 2<middle dot>5 (0<middle dot>305, 1<middle dot>797) than people who were housed (0<middle dot>240, 1<middle dot>690). Overall PM 2<middle dot>5 exposure was associated with increased odds of emergency-department visits for both people who were housed (odds ratio 1<middle dot>003, 95% CI 1<middle dot>001-1<middle dot>004 per 1 mu g/m3 PM 2<middle dot>5 for 0-3 days after exposure) and people who were unhoused (1<middle dot>004, 1<middle dot>000-1<middle dot>008 for 0-3 days after exposure). We found that non-wildfire PM 2<middle dot>5 was associated with emergency-department visits among people who were housed (1<middle dot>003, 1<middle dot>002-1<middle dot>005 for 0-3 days after exposure) and wildfire-specific PM 2<middle dot>5 was associated with emergency-department visits in people who were unhoused (1<middle dot>006, 1<middle dot>001-1<middle dot>011 for 0-3 days after exposure). Interpretation People who were unhoused in San Diego County were more likely to visit emergency departments after exposure to increased wildfire-specific PM2<middle dot>5. As the intensity and frequency of wildfires increase, understanding risk factors for vulnerable populations, such as people who are unhoused, is crucial to develop effective adaptation strategies.",Not About Sufficiency
Unveiling the impact of dataset size on machine learning models for anxiety and depression prediction amid the COVID-19 pandemic: determining optimal data collection thresholds,"Our emotional, psychological, and social well-being are all parts of our mental health. An individual's routine can be disrupted and their mental is health affected by stress, despair, and anxiety. Mental health preservation and restoration are essential for each person as well as for communities and society as a whole. The COVID-19 pandemic has triggered a strong emotional and psychological reaction in many people, in addition to triggering a global health emergency. The pandemic's uncertainty, disruptions, and social changes have amplified stress, fear, and depression, which are common responses to crises. Data collection for the COVID-19-related depression and anxiety assessment was limited to online methods because of the ongoing pandemic. In the field of mental health evaluation, the application of machine learning techniques has emerged as a promising strategy for identifying and grasping anxiety and depression symptoms. This paper employed K-Nearest Neighbors (kNN), Random Forest (RF), Decision Tree (DT), and Support Vector Machine (SVM) techniques on the prevalence of anxiety and depression among Bangladeshi university students during the COVID-19 pandemic. This paper addresses how the accuracy of predictions made by various machine learning models is affected by the size of the datasets. The findings of this study illuminate the scalability and generalizability of different machine-learning methods. The findings validate that how accuracy of the models has consistently and significantly improved as the dataset size varies. The performance of classification models is further assessed using the F1 score, precision, and recall.",Not About Sufficiency
Thermodynamics and dielectric response of BaTiO3 by data-driven modeling,"Modeling ferroelectric materials from first principles is one of the successes of density-functional theory and the driver of much development effort, requiring an accurate description of the electronic processes and the thermodynamic equilibrium that drive the spontaneous symmetry breaking and the emergence of macroscopic polarization. We demonstrate the development and application of an integrated machine learning model that describes on the same footing structural, energetic, and functional properties of barium titanate (BaTiO3), a prototypical ferroelectric. The model uses ab initio calculations as a reference and achieves accurate yet inexpensive predictions of energy and polarization on time and length scales that are not accessible to direct ab initio modeling. These predictions allow us to assess the microscopic mechanism of the ferroelectric transition. The presence of an order-disorder transition for the Ti off-centered states is the main driver of the ferroelectric transition, even though the coupling between symmetry breaking and cell distortions determines the presence of intermediate, partly-ordered phases. Moreover, we thoroughly probe the static and dynamical behavior of BaTiO3 across its phase diagram without the need to introduce a coarse-grained description of the ferroelectric transition. Finally, we apply the polarization model to calculate the dielectric response properties of the material in a full ab initio manner, again reproducing the correct qualitative experimental behavior.",Not About Sufficiency
A deep reinforcement learning-based maintenance optimization for vacuum packaging machines considering product quality degradation,"Vacuum loss in packaged meats can lead to product defects resulting in significant economic losses and negative public health issues. Therefore, it is crucial to study the degradation of components that are critical for the provision of vacuum and package sealing to enhance system availability and process safety. Accordingly, this article proposes a condition-based maintenance policy that integrates quality information considering meat cuts that lack proper vacuum as defective items. A deep reinforcement learning algorithm is used to learn a set of adequate maintenance actions to be performed at each maintenance inspection while maximizing the system availability and/or minimizing the total maintenance cost including the cost of producing defectives items. A numerical case study and benchmarking were performed, demonstrating that the proposed model surpasses the corrective maintenance policy. It leads to a 2.2% increase in system reliability, a 91% reduction in maintenance costs, a 93% reduction in defects identified in production, and a 90% reduction in defects identified on supermarket shelves. Such results demonstrate that the model can (i) prescribe maintenance actions at each inspection according to critical degradation states; (ii) exploit maintenance opportunities that lead to economic savings; and (iii) reduce product reprocessing and propagation of defects to shelves.Practical applicationsA new machine learning-based maintenance model promises to revolutionize the vacuum packaging industry by enhancing system reliability, reducing costs, and improving product quality. The model enables managers to make dynamic decisions based on the system state, avoiding inefficient maintenance planning and ensuring maximum productivity. By predicting quality performance through vacuum condition, the model allows for timely decision-making and reduces the need for costly laboratory analysis. The free-model's estimation of structural and economic relations of components enables managers to retrain and adapt maintenance policies for optimal system performance. With improved system reliability and reduced production of non-conforming items, the industry can reduce reprocessing costs, contamination risks, and protect brand image by ensuring better control over meat hygiene. This new approach to maintenance optimization has significant implications for process safety, efficiency of operations, and profits of the vacuum packaging industry, making it a potential game-changer in the field.",Not About Sufficiency
"Machine Learning-Based Models for Assessing Physical and Social Impacts Before, During and After Hurricane Michael","Multi-modal approach machine learning techniques have been used to examine Hurricane Michael's physical sensor data of cloud cover temperature and social media data from Twitter to help stakeholders and government agencies consider the societal implications of hurricane impacts more thoroughly and understand how to plan for mitigating future storms as these disasters become more frequent. Data were obtained from Twitter and NOAA on Hurricane Michael and used to evaluate the relationship between the social sentiment and the physical data during severe weather events. Of all the classification methods employed in this study to evaluate sentiment, the naive Bayes classifier results showed the highest accuracy. Models of natural language processing have been developed to explain sentiment data. Future events prediction models have been tested to improve extreme weather events emergency management. The findings demonstrate that natural language processing and machine learning techniques, using Twitter data, are practical methods of sentiment analysis. This research carried out a social media sentiment analysis that could be used by emergency managers, government officials and decision-makers to make informed emergency response decisions.",Not About Sufficiency
BM-DDPG: An Integrated Dispatching Framework for Ride-Hailing Systems,"This paper proposes an integrated dispatching framework for matching drivers with riders in ride-hailing systems. The goal is to compute matching solutions that maximize social welfare and benefit both sides of the market, such that the sustainable growth of the ride-hailing system is ensured. The proposed framework integrates data-driven proactive guidance strategies with batched matching optimization to increase social welfare, improve matching rate and reduce rider wait time. Proactive guidance strategies are computed by leveraging short-term demand forecasts based on historical data. Taken the resulting guidance strategies as inputs, the batched matching algorithm computes optimal bipartite matching between drivers and riders in a batch. Using New York City taxi data from 2016 March 1st to March 31st as input, we conduct a numerical study to evaluate the performance of the proposed framework and compare it with existing approaches in the literature. Our results show that the proposed framework improves social welfare for up to 50%. It also increases the matching rate by an average of 20% and reduces the average rider wait time by over 15%. This implies a strong potential for the proposed dispatching framework to improve service quality in ride-hailing systems.",Not About Sufficiency
Design and Development of Wireless Sensor Network based data logger with ESP-NOW protocol,"This paper presents Wireless Sensor Network (WSN) based data logger system for collecting data accurately from agricultural field. Agriculture is the backbone of India. Though many technological advances have been made in various fields, not much of it is implemented in the field of agriculture. One of the fundamental aspects that we can improve in agricultural field is by getting more data from soil, atmosphere, plants, etc accurately. By collecting more data we can apply machine learning and prediction algorithms to figure out the amount of fertilizer, water and other manures required for crops to get better yield. But in developing countries like India, cost of the technology is quite not affordable for implementing in large scale. Hence an effective and low cost real time monitoring system is needed, to monitor and collect data accurately. Wireless Sensor Network (WSN) based data logger system with ESP-NOW protocol developed by us aims at doing just that and the project is discussed here.",Not About Sufficiency
Pooled CRISPR screens with imaging on microraft arrays reveals stress granule-regulatory factors,"CRISPR-based microraft followed by guide RNA identification (CRaft-ID) combines microraft arrays, microscopy and CRISPR-Cas9 technology for high-content image-based phenotyping. CRaft-ID was used to identify proteins involved in stress granule formation. Genetic screens using pooled CRISPR-based approaches are scalable and inexpensive, but restricted to standard readouts, including survival, proliferation and sortable markers. However, many biologically relevant cell states involve cellular and subcellular changes that are only accessible by microscopic visualization, and are currently impossible to screen with pooled methods. Here we combine pooled CRISPR-Cas9 screening with microraft array technology and high-content imaging to screen image-based phenotypes (CRaft-ID; CRISPR-based microRaft followed by guide RNA identification). By isolating microrafts that contain genetic clones harboring individual guide RNAs (gRNA), we identify RNA-binding proteins (RBPs) that influence the formation of stress granules, the punctate protein-RNA assemblies that form during stress. To automate hit identification, we developed a machine-learning model trained on nuclear morphology to remove unhealthy cells or imaging artifacts. In doing so, we identified and validated previously uncharacterized RBPs that modulate stress granule abundance, highlighting the applicability of our approach to facilitate image-based pooled CRISPR screens.",Not About Sufficiency
"Urban health: an example of a ""health in all policies"" approach in the context of SDGs implementation","Background: Cities are an important driving force to implement the Sustainable Development Goals (SDGs) and the New Urban Agenda. The SDGs provide an operational framework to consider urbanization globally, while providing local mechanisms for action and careful attention to closing the gaps in the distribution of health gains. While health and well-being are explicitly addressed in SDG 3, health is also present as a pre condition of SDG 11, that aims at inclusive, safe, resilient and sustainable cities. Health in All Policies (HiAP) is an approach to public policy across sectors that systematically takes into account the health implications of decisions, seeks synergies, and avoids harmful health impacts in order to improve population health and health equity. HiAP is key for local decision-making processes in the context of urban policies to promote public health interventions aimed at achieving SDG targets. HiAPs relies heavily on the use of scientific evidence and evaluation tools, such as health impact assessments (HIAs). HIAs may include city-level quantitative burden of disease, health economic assessments, and citizen and other stakeholders' involvement to inform the integration of health recommendations in urban policies. The Barcelona Institute for Global Health (ISGlobal)'s Urban Planning, Environment and Health Initiative provides an example of a successful model of translating scientific evidence into policy and practice with regards to sustainable and healthy urban development. The experiences collected through ISGlobal's participation implementing HIAs in several cities worldwide as a way to promote HiAP are the basis for this analysis. Aim: The aim of this article is threefold: to understand the links between social determinants of health, environmental exposures, behaviour, health outcomes and urban policies within the SDGs, following a HiAP rationale; to review and analyze the key elements of a HiAP approach as an accelerator of the SDGs in the context of urban and transport planning; and to describe lessons learnt from practical implementation of HIAs in cities across Europe, Africa and Latin-America. Methods: We create a comprehensive, urban health related SDGs conceptual framework, by linking already described urban health dimensions to existing SDGs, targets and indicators. We discuss, taking into account the necessary conditions and steps to conduct HiAP, the main barriers and opportunities within the SDGs framework. We conclude by reviewing HIAs in a number of cities worldwide (based on the experiences collected by co-authors of this publication), including city-level quantitative burden of disease and health economic assessments, as practical tools to inform the integration of health recommendations in urban policies. Results: A conceptual framework linking SDGs and urban and transportplanning, environmental exposures, behaviour and health outcomes, following a HiAP rationale, is designed. We found at least 38 SDG targets relevant to urban health, corresponding to 15 SDGs, while 4 important aspects contained in our proposed framework were not present in the SDGs (physical activity, noise, quality of life or social capital). Thus, a more comprehensive HiAP vision within the SDGs could be beneficial. Our analysis confirmed that the SDGs framework provides an opportunity to formulate and implement policies with a HiAP approach. Three important aspects are highlighted: 1) the importance of the intersectoral work and health equity as a cross-cutting issue in sustainable development endeavors; 2) policy coherence, health governance, and stakeholders' participation as key issues; and 3) the need for high quality data. HIAs are a practical tool to implement HiAP. Opportunities and barriers related to the political, legal and health governance context, the capacity to inform policies in other sectors, the involvement of different stakeholders, and the availability of quality data are discussed based on our experience. Quantitative assessments can provide powerful data such as: estimates of annual preventable morbidity and disability-adjusted life-years (DALYs) under compliance with international exposure recommendations for physical activity, exposure to air pollution, noise, heat, and access to green spaces; the associated economic impacts in health care costs per year; and the number of preventable premature deaths when improvements in urban and transport planning are implemented. This information has been used to support the design of policies that promote cycling, walking, public, zero and low-emitting modes of transport, and the provision of urban greening or healthy public open spaces in Barcelona (e.g. Urban Mobility, Green Infrastructure and Biodiversity Plans, or the Superblocks's model), the Bus Rapid Transit and Open Streets initiatives in several Latin American cities or targeted SDGs assessments in Morocco. Conclusions: By applying tools such as HIA, HiAP can be implemented to inform and improve transport and urban planning to achieve the 2030 SDG Agenda. Such a framework could be potentially used in cities worldwide, including those of less developed regions or countries. Data availability, taking into account equity issues, strenghtening the communication between experts, decision makers and citizens, and the involvement of all major stakeholders are crucial elements for the HiAP approach to translate knowledge into SDG implementation.",Not About Sufficiency
Unsupervised Training of Neural Cellular Automata on Edge Devices,"The disparity in access to machine learning tools for medical imaging across different regions significantly limits the potential for universal healthcare innovation, particularly in remote areas. Our research addresses this issue by implementing Neural Cellular Automata (NCA) training directly on smartphones for accessible X-ray lung segmentation. We confirm the practicality and feasibility of deploying and training these advanced models on five Android devices, improving medical diagnostics accessibility and bridging the tech divide to extend machine learning benefits in medical imaging to low- and middle-income countries (LMICs). We further enhance this approach with an unsupervised adaptation method using the novel Variance-Weighted Segmentation Loss (VWSL), which efficiently learns from unlabeled data by minimizing the variance from multiple NCA predictions. This strategy notably improves model adaptability and performance across diverse medical imaging contexts without the need for extensive computational resources or labeled datasets, effectively lowering the participation threshold. Our methodology, tested on three multisite X-ray datasets-Padchest, ChestX-ray8, and MIMIC-III-demonstrates improvements in segmentation Dice accuracy by 0.7 to 2.8%, compared to the classic Med-NCA. Additionally, in extreme cases where no digital copy is available and images must be captured by a phone from an X-ray lightbox or monitor, VWSL enhances Dice accuracy by 5-20%, demonstrating the method's robustness even with suboptimal image sources.",Not About Sufficiency
Fusion-Based Body-Worn IoT Sensor Platform for Gesture Recognition of Autism Spectrum Disorder Children,"The last decade's developments in sensor technologies and artificial intelligence applications have received extensive attention for daily life activity recognition. Autism spectrum disorder (ASD) in children is a neurological development disorder that causes significant impairments in social interaction, communication, and sensory action deficiency. Children with ASD have deficits in memory, emotion, cognition, and social skills. ASD affects children's communication skills and speaking abilities. ASD children have restricted interests and repetitive behavior. They can communicate in sign language but have difficulties communicating with others as not everyone knows sign language. This paper proposes a body-worn multi-sensor-based Internet of Things (IoT) platform using machine learning to recognize the complex sign language of speech-impaired children. Optimal sensor location is essential in extracting the features, as variations in placement result in an interpretation of recognition accuracy. We acquire the time-series data of sensors, extract various time-domain and frequency-domain features, and evaluate different classifiers for recognizing ASD children's gestures. We compare in terms of accuracy the decision tree (DT), random forest, artificial neural network (ANN), and k-nearest neighbour (KNN) classifiers to recognize ASD children's gestures, and the results showed more than 96% recognition accuracy.",Not About Sufficiency
A Systematic Review of Beef Meat Quantitative Microbial Risk Assessment Models,"Each year in Europe, meat is associated with 2.3 million foodborne illnesses, with a high contribution from beef meat. Many of these illnesses are attributed to pathogenic bacterial contamination and inadequate operations leading to growth and/or insufficient inactivation occurring along the whole farm-to-fork chain. To ensure consumer health, decision-making processes in food safety rely on Quantitative Microbiological Risk Assessment (QMRA) with many applications in recent decades. The present study aims to conduct a critical analysis of beef QMRAs and to identify future challenges. A systematic approach, the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, was used to collate beef QMRA models, identify steps of the farm-to-fork chain considered, and analyze inputs and outputs included as well as modelling methods. A total of 2343 articles were collected and 67 were selected. These studies focused mainly on western countries and considered Escherichia coli (EHEC) and Salmonella spp. pathogens. Future challenges were identified and included the need of whole-chain assessments, centralization of data collection processes, and improvement of model interoperability through harmonization. The present analysis can serve as a source of data and information to inform QMRA framework for beef meat and will help the scientific community and food safety authorities to identify specific monitoring and research needs.",Not About Sufficiency
Interactive Machine Learning Tool for Clustering in Visual Analytics,"Clustering is an important task in knowledge discovery with the goal of finding groups of similar data points in a dataset. Today there are many different approaches to clustering, including methods to incorporate user decisions into the clustering process. Some of these interactive approaches fall into the category of visual analytics and emphasize the power of visualizations to help find clusters manually in various types of datasets or to verify the results of clustering algorithms. The interactive projection-based clustering (IPBC) is an open-source and parameter-free method using user input on interactive visualizations to cluster high-dimensional data. This work introduces the IPBC approach and compares it to the results of accessible visual analytics approaches for clustering, showing that IPBC can outperform them.",Not About Sufficiency
Fiscal data in text: Information extraction from audit reports using Natural Language Processing,"Supreme audit institutions (SAIs) are touted as an integral component to anticorruption efforts in developing nations. SAIs review governmental budgets and report fiscal discrepancies in publicly available audit reports. These documents contain valuable information on budgetary discrepancies, missing resources, or may even report fraud and corruption. Existing research on anticorruption efforts relies on information published by national-level SAIs while mostly ignoring audits from subnational SAIs because their information is not published in accessible formats. I collect publicly available audit reports published by a subnational SAI in Mexico, the Auditoria Superior del Estado de Sinaloa, and build a pipeline for extracting the monetary value of discrepancies detected in municipal budgets. I systematically convert scanned documents into machine-readable text using optical character recognition, and I then train a classification model to identify paragraphs with relevant information. From the relevant paragraphs, I extract the monetary values of budgetary discrepancies by developing a named entity recognizer that automates the identification of this information. In this paper, I explain the steps for building the pipeline and detail the procedures for replicating it in different contexts. The resulting dataset contains the official amounts of discrepancies in municipal budgets for the state of Sinaloa. This information is useful to anticorruption policymakers because it quantifies discrepancies in municipal spending potentially motivating reforms that mitigate misappropriation. Although I focus on a single state in Mexico, this method can be extended to any context where audit reports are publicly available.Policy Significance Statement Annual audits by supreme audit institutions produce important information on the health and accuracy of govern-mental budgets. These reports include the monetary value of discrepancies, missing funds, and corrupt actions. This paper offers a strategy for collecting that information from historical audit reports and creating a database on budgetary discrepancies. It uses machine learning and natural language processing to accelerate and scale the collection of data to thousands of paragraphs. The granularity of the budgetary data obtained through this approach is useful to reformers and policymakers who require detailed data on municipal finances. This approach can also be applied to other countries that publish audit reports in PDF documents across different languages and contexts.",Not About Sufficiency
Exploring the influence of relative attractiveness in green spaces on urban movements: A potential to kinetic energy framework,"Urban green spaces are increasingly recognized for their positive impact on well-being, quality of life, and public health. Researchers have explored how green space characteristics, such as size, theme, and convenience, influence urban movement. However, an important aspect that has received limited attention is the relative attractiveness of green spaces. When residents have access to similar green spaces nearby, their inclination to visit more distant ones decreases. This leads to the central research question: How does the relative attractiveness of urban green spaces affect urban movements? The existing literature lacks a comprehensive theory to address this question. To bridge this gap, this paper introduces the Potential to Kinetic Energy (PKE) framework, inspired by principles from physics. In this framework, potential energy represents the relative attractiveness of green spaces between the travel origin and destination, while kinetic energy signifies the urban movement resulting from the this attractiveness. To demonstrate the PKE framework's applicability, this paper analyzes green space data and smart card subway ridership records from Beijing. The results show that a unit increase in size, cultural, attraction, proximity, and location potential energy is associated with a 0.22%, 0.27%, 0.14%, 0.19%, and 0.02% increase in urban movement. The findings provide compelling evidence for the significant role of relative attractiveness in shaping urban movements and offer valuable insights for urban planning and policy development. The proposal of the PKE framework encourages interdisciplinary discussions among urban planners, environmental scientists, and transportation experts, fostering collaborative efforts to enhance the sustainability of cities.",Not About Sufficiency
A meta-level active multidatabase system architecture for heterogeneous information resources,"With the rapid progress of global network and database technology, various information resources, such as relational databases, document databases and web contents, have become accessible over networks. The integration among those information resources adds new values to themselves. An active multidatabase system is essential to provide dynamic access and integration functions for those information resources. This paper presents a principled architecture of a meta-level active multidatabase system that provides active database functionalities for accessing and integrating heterogeneous information resources. The meta-level active multidatabase system realizes active rules that check and react to changes of contents in different information resources on the global-area network. The essence of our method is that active database functionalities and database integration mechanisms are realized in the meta-level system over local information resources. Our method applies the active database functionalities to heterogeneous information resources to access and integrate them in a dynamic way. In this paper, we clarify feasibility and effectiveness of our system architecture by showing several experimental results.",Not About Sufficiency
Farming in Motown: competing narratives for urban development and urban agriculture in Detroit,"Detroit used to be the core of the US car industry, the engine of American industrial production. In the 1950s, Detroiters had apparently achieved the American dream, but when the industrial system started to collapse, the city rapidly followed. For many years local administrators have tried to revive the past glory, until the 2008 crisis made clear that that path is closed. Urban gardens started to appear in the last decade as a grassroots response to the urban decay, spurred by the lack of access to fresh food that makes Detroit one of the largest urban food deserts in the US. In the last five years the phenomenon has caught the eyes of the media, and recently the local government has started to recognise it. According to local sources, the number of gardens is steadily growing. Five structures that call themselves farms are now operating in Detroit, along with more than 300 community gardens. Until recently, institutional support for these activities has been limited. Some institutional actors are now investing in the sector. Wayne State University sponsors a farmers market and lets the students grow gardens on campus; the Eastern Market Corporation started an experimental farm on its campus; the City planning department is working on a plan that would permit commercial farming within the city boundaries. Moore's work on sustainable cities could help frame what's happening in Detroit. I argue that urban farmers in Detroit follow a story line that describes the future of Detroit as a city of gardens. As many Detroit gardeners are the heirs of the activists who fought in the past for social justice and equal rights, their quest for locally produced food is informed by the concept of food justice. The arrival of a new, well-capitalised farm on the stage might change this storyline towards a new direction.",Not About Sufficiency
A global review of ocean ecosystem accounts and their data: Lessons learned and implications for marine policy,"Given the ocean ecosystem's contribution to the environment, economy and human wellbeing, its condition and value need to be accounted for in decision making for a healthy and sustainable society. Ocean accounting aims to incorporate the ocean environment into national accounts by tracking natural assets, the services they provide, and their links to our economies, and social wellbeing. Around the world, adoption of National Ocean Accounts is gathering momentum, taking national accounts ""Beyond GDP"" to produce more holistic statistics to inform ocean policy. We conducted the first review of publicly available ocean ecosystem accounts, including ten case studies to identify key opportunities and challenges. The spatial scale, scope, methods, and reporting quality of data within the case studies varied greatly and poor data documentation generally hindered the review. Case studies typically focussed on ecosystem assets that were inherently easy to map (e.g., nearshore or intertidal areas) and excluded deeper continental shelf waters, pelagic and offshore deep-sea ecosystems. The focus on easy to map ecosystems suggests that ocean accounts will have difficulty representing national marine jurisdictions with large extents of deeper continental shelf waters, pelagic and offshore deep-sea ecosystems. Ecosystem extent accounts consistently had limited transparency of methods and absent reporting on uncertainty or data variance. We recommend future ocean ecosystem account case studies should consider 1) open science 2) FAIR (Findability, Accessibility, Interoperability and Reusability) data principles 3) reporting error around data. By addressing these points, ocean ecosystem accounts will be more robust, and comparability of case studies through space and time will be improved, making future case studies better tools for decision makers.",Not About Sufficiency
Design and Optimization of a Soft Magnetic Tactile Sensor,"This paper introduces SoftMag, an affordable and efficient soft magnetic sensor capable of detecting contact and normal force. The sensor incorporates a three-axis Hall sensor and four counter-symmetric deployed NdFeB magnets embedded within a deformable porous body. An analytical model for computing the flux density is established as the foundation for design and optimization, which are validated via a multi-physics simulation framework. To calibrate the sensor, we trained a K-nearest-neighbor classifier for position and a feed-forward neural network for force measurement. Experimental evaluation demonstrates a global full-scale output of 7.12 N with a static error of 9.52%, and a global accuracy of 100% for position estimation with a resolution of 10 mm, suggesting that SoftMag has a promising potential for providing tactile feedback in soft grippers.",Not About Sufficiency
What drives tourists' sustainable mobility at city destinations? Insights from ten European capital cities,"The growth in tourism has brought benefits but also pressured urban transportation systems, particularly in capitals where it intersects with business and local life. While urban sustainable mobility development typically centers on the transportation needs of the residents, it frequently neglects the unsustainable travel habits of tourists. This study focuses on the key elements influencing sustainable tourist transportation, stressing the imperative for city administrations to tackle this concern. Based on a study of ten European capital cities and 5220 individual tourists, unique perspectives on urban tourist transportation are provided. Initially, a DCSM index (Destination City' Sustainable Mobility Index) was crafted to mirror the sustainability of urban transportation systems, integrating data on cities' transportation modes and factors promoting sustainable transportation while deterring unsustainable options. This index was then embedded in an ordered logit model, with the sustainable transportation choices of tourists as the dependent variable. The findings show that the traits of the destination city and the transportation preferences from the tourist's home location, combined with accommodation and trip details, considerably influence their transportation decisions at the destination. These insights underline the potential to mold sustainable tourist behaviors by advancing environmentally friendly transportation systems, enhancing urban planning, and optimally positioning hotels. However, it is vital to acknowledge that without influencing tourists' routine behaviors at home through education, their travel choices might persist. This research underscores the need for holistic strategies that integrate both destination-specific actions and pre-trip educational initiatives to truly promote tourists' sustainable transportation behavior.",Not About Sufficiency
"Barriers to participation in sports activities: a study with caregivers and professionals linked to the Association of Parents, Friends, and People with Disabilities of Banco do Brasil Employees (APABB)","The aim of this study was to identify the barriers that prevent people with disabilities (PWD) from accessing the sports activities at a branch of the Association of Parents, Friends, and People with Disabilities of Banco do Brasil Employees (APABB), situated in a capital in the south of Brazil. The focus was on the perspectives of the PWD parents and some institution's professionals. The study was qualitative and exploratory and involved individual semi-structured interviews with eleven parents and two APABB professionals. We conducted a thematic analysis of the data, looking for the main themes addressed. According to the participants, the main barriers are: (1) a scarcity of opportunities to access sports activities due to financial difficulties and a lack of public policies that facilitate access; (2) attitudinal barriers such as stereotypes, prejudice, and bullying usually associated with PWD; (3) the absence of unisex bathrooms, which does not allow opposite-sex caregivers to assist their children and prevent potential situations of sexual abuse, embarrassment, and violence by third parties; (4) architectural problems (e.g., lack of adapted bathrooms and absence of ramps), urban planning (accessible sidewalks), and transport; (5) lack of categories according to skill level in competitions; (6) unpreparedness of professionals to work with PWD; (7) difficulty in reconciling the time of practice with other daily activities. The study provides support for the development of public policies and the search for strategies aimed at minimizing the barriers encountered and expanding access to sports for people with disabilities.",Not About Sufficiency
On the Environmental and Social Sustainability of Technological Innovations in Urban Bus Transport: The EU Case,"Logistics in urban areas are currently suffering a radical transformation due to increasing population concentration and the massive use of cars as the preferred transport mode. These issues have resulted in higher pollution levels in urban environments and traffic congestion, impacting the world globally. Facilitating the use of sustainable transport modes is widely regarded as a necessity to cope with these adverse effects on citizens' life quality. Hence, some regions, such as the European Union, are encouraging bus transport firms to make their business models more environmentally and socially sustainable. The aim of this research is thus to explore how technological innovations adopted by urban bus companies can improve cities' sustainability. With this in mind, a combined Importance Performance Analysis (IPA)-Analytic Hierarchy Process (AHP) method was applied. In this way, their environmental and social sustainability effects were separately represented through hierarchical structures. Subsequently, the importance and performance ratings of technological innovations in each sustainability dimension were estimated, and thus two IPA grids were generated. These grids support managers in the establishment of more effective action plans to improve logistics sustainability in cities. The findings also provide guidance to governments on the technological innovations that should be promoted in future urban mobility plans.",Not About Sufficiency
Model for integration of information and communication technologies in resource sharing practices for enhanced service delivery in academic libraries in southeast Nigeria,"PurposeThis study aims to develop a model for integration of information and communication technologies (ICTs) in resource sharing practices for enhanced service delivery in academic libraries in Southeast Nigeria. Seven objectives guided the study; ICT-based resource sharing practices in academic libraries in Southeast Nigeria; ICT infrastructure for resource sharing; ICT needs of librarians for resource sharing practices; stages of ICT integration in resource sharing; perception of librarians towards ICT-based resource sharing; challenges to integration of ICT in resource sharing practices; design a model for the integration of ICT into resource sharing; all in academic libraries in Southeast Nigeria.Design/methodology/approachThe research design adopted for this study is the mixed research design containing the ""descriptive survey"" research design and the ""research and development, R&D."" The population of the study is 164, comprising all of the librarians in federal universities in Southeast Nigeria. All of the librarians of the five federal universities will be involved in the study. Hence, there was no sampling. The instrument for data collection is a structured questionnaire.FindingsThe findings of the study revealed that 88.8% of the librarians use ICT to boost the volume of resources; while 74% of the librarians were positive in all responses pertaining to ICT-based resource sharing practices in the libraries. The study also revealed that ICT infrastructures in the library for resource sharing are highly available and applicable; while computers and internet networks are the most needed gadgets for the operations. Libraries in Southeast Nigeria are in the applying and transforming stages of ICT adoption at a 69.7% response rate; while the librarians have a strong belief and understanding that a lot can be achieved in resource sharing through ICT; and would recommend such. Poor electrification, inadequate funding and unavailability of some ICT technologies were equally identified as challenges. The study conclusively developed a resource sharing model, the Southeast Federal University Library Connect; accessible at https://southeastfeduniconnect.njh.com.ngOriginality/valueThe research study is one of the few types of research that has developed a functional model for resource sharing in academic libraries in Southeast Nigeria.",Not About Sufficiency
Globalization and top income shares,"This paper documents empirically that access to global markets is associated with a higher executive-to-worker pay ratio within the firm. It then uses China's 2001 accession to the World Trade Organization as a trade shock to show that firms that exported to China prior to 2001 subsequently exported more, grew larger, and grew more unequal in terms of executive-to-worker pay. To evaluate analytically and quantitatively the impacts of globalization on top income inequality, this paper builds a model with heterogeneous firms, occupational choice, and executive compensation. In the model, executive compensation grows with the size of the firm, while the wage paid to ordinary workers is determined in a country-wide labor market. As a result, the extra profits earned in the foreign markets benefit the executives more than the average workers. We calibrate the model to the U.S. economy and match the income distribution closely in the data. Counterfactual exercises suggest that trade and FDI liberalizations can explain around 44% of the surge in top 0.1% income shares in the data between 1988 and 2008. (C) 2020 Elsevier B.V. All rights reserved.",Not About Sufficiency
Personal and social patterns predict influenza vaccination decision,"Background Seasonal influenza vaccination coverage remains suboptimal in most developed countries, despite longstanding recommendations of public health organizations. The individual's decision regarding vaccination is located at the core of non-adherence. We analyzed large-scale data to identify personal and social behavioral patterns for influenza vaccination uptake, and develop a model to predict vaccination decision of individuals in an upcoming influenza season. Methods We analyzed primary data from the electronic medical records of a retrospective cohort of 250,000 individuals between the years 2007 and 2017, collected from 137 clinics. Individuals were randomly sampled from the database of Maccabi Healthcare Services. Maccabi's clients are representative of the Israeli population, reflect all demographic, ethnic, and socioeconomic groups and levels. We used several machine-learning models to predict whether a patient would get vaccinated in the future. Models' performance was evaluated based on the area under the ROC curve. Results The vaccination decision of an individual can be explained in two dimensions, Personal and social. The personal dimension is strongly shaped by a ""default"" behavior, such as vaccination timing in previous seasons and general health consumption, but can also be affected by temporal factors such as respiratory illness in the prior year. In the social dimension, a patient is more likely to become vaccinated in a given season if at least one member of his family also became vaccinated in the same season. Vaccination uptake was highly assertive with age, socioeconomic score, and geographic location. An XGBoost-based predictive model achieved an ROC-AUC score of 0.91 with accuracy and recall rates of 90% on the test set. Prediction relied mainly on the patient's individual and household vaccination status in the past, age, number of encounters with the healthcare system, number of prescribed medications, and indicators of chronic illnesses. Conclusions Our ability to make an excellent prediction of the patient's decision sets a major step toward personalized influenza vaccination campaigns, and will help shape the next generation of targeted vaccination efforts.",Not About Sufficiency
Measuring Equity through Spatial Variability of Infrastructure Systems across the Urban-Rural Gradient,"Recent regional research has taken an 'infrastructure turn' where scholars have called for examining the transformative ability of different infrastructures in causing systemic inequities beyond the spatial conception of 'urban and the other'. This research examines the interconnected impact of infrastructure systems on existing spatial inequities through a study in metropolitan Philadelphia, Pennsylvania. This study investigates whether the urban-rural (U-R) gradient concept can enhance understanding of the spatial relationship between socioeconomic indicators and infrastructure systems. Indicators of spatial inequalities were regressed against infrastructure variables and imperviousness, as a proxy for the U-R gradient, using multivariate and spatial regression methods. The models show that imperviousness has a positive correlation with the concentration of racialized minorities and a negative correlation with access to health insurance. The study also shows that the predictive power of multiple infrastructures varies across space and does not adhere to urban boundaries or the U-R gradient. The complex interactions among different infrastructures shape inequities and require further inquiry in urban regions around the world.",Not About Sufficiency
Assessing Leadership Competencies Through Social Network Analysis,"Leadership competencies are regularly identified as some of the most in demand workplace competencies. However, the development of these competencies requires appropriate assessments that are often either highly subjective (e.g. manager appraisals) or prohibitively expensive (e.g. roleplays with trained actors). The increasing usage of workplace social networks and increasing prevalence of digital collaboration tools presents a continuous stream of social interactions that can contain evidence of leadership occurring in situ. In this paper we present initial research on the feasibility of Social Network Analysis in the workplace to assess leadership competencies. We examine the assessment in terms of content, construct, and criterion validity. We then present our hypotheses on how the assessment can be conducted including the algorithms necessary to extract relevant features from a social network graph model. Our initial research, to our surprise, shows a weak correlation between an individual's degree centrality and betweenness centrality and the leadership competency that is self-reported. However, experiments indicated a strong positive correlation between network structure based and social collaboration activities based features and the characteristics of the leadership competencies. Our initial machine learning experiments achieved an Area Under the Curve (AUC) score of 0.899 when social network and collaboration activity based features were leveraged to distinguish individuals with self-reported leadership competencies from others. Finally we discuss our findings on the practicality of the approach, and future work on validating and improving the results obtained using parallel conventional assessments for leadership competencies.",Not About Sufficiency
Ontology-based feature engineering in machine learning workflows for heterogeneous epilepsy patient records,"Biomedical ontologies are widely used to harmonize heterogeneous data and integrate large volumes of clinical data from multiple sources. This study analyzed the utility of ontologies beyond their traditional roles, that is, in addressing a challenging and currently underserved field of feature engineering in machine learning workflows. Machine learning workflows are being increasingly used to analyze medical records with heterogeneous phenotypic, genotypic, and related medical terms to improve patient care. We performed a retrospective study using neuropathology reports from the German Neuropathology Reference Center for Epilepsy Surgery at Erlangen, Germany. This cohort included 312 patients who underwent epilepsy surgery and were labeled with one or more diagnoses, including dual pathology, hippocampal sclerosis, malformation of cortical dysplasia, tumor, encephalitis, and gliosis. We modeled the diagnosis terms together with their microscopy, immunohistochemistry, anatomy, etiologies, and imaging findings using the description logic-based Web Ontology Language (OWL) in the Epilepsy and Seizure Ontology (EpSO). Three tree-based machine learning models were used to classify the neuropathology reports into one or more diagnosis classes with and without ontology-based feature engineering. We used five-fold cross validation to avoid overfitting with a fixed number of repetitions while leaving out one subset of data for testing, and we used recall, balanced accuracy, and hamming loss as performance metrics for the multi-label classification task. The epilepsy ontology-based feature engineering approach improved the performance of all the three learning models with an improvement of 35.7%, 54.5%, and 33.3% in logistics regression, random forest, and gradient tree boosting models respectively. The run time performance of all three models improved significantly with ontology-based feature engineering with gradient tree boosting model showing a 93.8% reduction in the time required for training and testing of the model. Although, all three models showed an overall improved performance across the three-performance metrics using ontology-based feature engineering, the rate of improvement was not consistent across all input features. To analyze this variation in performance, we computed feature importance scores and found that microscopy had the highest importance score across the three models, followed by imaging, immunohistochemistry, and anatomy in a decreasing order of importance scores. This study showed that ontologies have an important role in feature engineering to make heterogeneous clinical data accessible to machine learning models and also improve the performance of machine learning models in multilabel multiclass classification tasks.",Not About Sufficiency
Unusual water-assisted NO adsorption over Pd/FER calcined at high temperatures: The effect of cation migration,"Moisture contained in vehicle exhaust gas normally degrades the capacity and efficiency of Pd ion-exchanged zeolites as NOx adsorbents by competitive adsorption on active sites. Here, we report a counterexample to this general proposition, in which moisture facilitates the storage of NO as a nitrosyl complex on hydrated Pd ions in high temperature calcined FER-type zeolites. Divalent Pd2+ cations upon elevated temperature (>800 degrees C) calcination occupy sterically constrained cationic position in the zeolite framework, and become inactive for the adsorption of probe molecules such as NO. These 'hidden' Pd ions, however, are accessible by NO when the zeolite is hydrated, but readily release NO as dehydration proceeds. Combined systematic in situ infra-red data with X-ray diffraction Rietveld analyses reveal that the high temperature-induced relocation of Pd ions to more stable cationic positions located near 6-membered ring of the ferrierite cage is responsible for this behavior.",Not About Sufficiency
Enhancing biomass conversion to bioenergy with machine learning: Gains and problems,"The growing concerns about environmental sustainability and energy security, such as exhaustion of traditional fossil fuels and global carbon footprint growth have led to an increasing interest in alternative energy sources, especially bioenergy. Recently, numerous scenarios have been proposed regarding the use of bioenergy from different sources in the future energy systems. In this regard, one of the biggest challenges for scientists is managing, modeling, decision -making, and future forecasting of bioenergy systems. The development of machine learning (ML) techniques can provide new opportunities for modeling, optimizing and managing the production, consumption and environmental effects of bioenergy. However, researchers in bioenergy fields have not widely utilized the ML concepts and practices. Therefore, a comparative review of the current ML techniques used for bioenergy productions is presented in this paper. This review summarizes the common issues and difficulties existing in integrating ML with bioenergy studies, and discusses and proposes the possible solutions. Additionally, a detailed discussion of the appropriate ML application scenarios is also conducted in every sector of the entire bioenergy chain. This indicates the modernized conversion processes supported by ML techniques are imperative to accurately capture process -level subtleties, and thus improving techno-economic resilience and socio-ecological integrity of bioenergy production. All the efforts are believed to help in sustainable bioenergy production with ML technologies for the future.",Not About Sufficiency
A Machine Learning Approach for Atrial Fibrillation Detection in Telemonitored Patients,"Atrial fibrillation (AF) is the most common type of cardiac arrhythmia. As it is typically asymptomatic, it often goes undiagnosed until major complications arise, such as stroke. Therefore, the development of rapid, economical, and widely accessible diagnostic tools for detectingAF at an early stage is crucial. Telemonitoring with machine learning-assisted devices shows promise in achieving this goal. This paper presents an algorithm that automatically detects AF in signals obtained by portable electrocardiographs connected to a telemonitoring platform via smartphones. The algorithm consists of three stages: a noise detection, ectopic beat removal and an AF detection. The noise detection involves analyzing the ECG signals using 5-s windows with a 1-s shift. A K-nearest neighbors (KNN) classifier predicts the presence or absence of noise in each window, allowing for the detection of noisy and non-noisy segments of the signal. The non-noisy segments are processed using a Pan-Tompkins algorithm to find the R peaks of the signal, and the corresponding RR interval series. Then ectopic beats are removed using an XGBoost classifier, generating the NN series. In the AF detection stage, X features are obtained from this series, which serve as input features of an XGBoost classifier that predicts the presence or absence of AF in the ECG signal. The algorithm was trained and tested using the Physionet Short Single-Lead AF Database (SSLAFDB) and achieved an accuracy of 90.87% and an F1-score of 90.91%. Further validation was performed by an external partner using two other databases, reporting an accuracy of 90.41% and 89.61% respectively.",Not About Sufficiency
Big Data COVID-19 Systematic Literature Review: Pandemic Crisis,"The COVID-19 pandemic has frightened people worldwide, and coronavirus has become the most commonly used phrase in recent years. Therefore, there is a need for a systematic literature review (SLR) related to Big Data applications in the COVID-19 pandemic crisis. The objective is to highlight recent technological advancements. Many studies emphasize the area of the COVID-19 pandemic crisis. Our study categorizes the many applications used to manage and control the pandemic. There is a very limited SLR prospective of COVID-19 with Big Data. Our SLR study picked five databases: Science direct, IEEE Xplore, Springer, ACM, and MDPI. Before the screening, following the recommendation, Preferred Reporting Items for Systematic Reviews and Meta Analyses (PRISMA) were reported for 893 studies from 2019, 2020 and until September 2021. After screening, 60 studies met the inclusion criteria through COVID-19 data statistics, and Big Data analysis was used as the search string. Our research's findings successfully dealt with COVID-19 healthcare with risk diagnosis, estimation or prevention, decision making, and drug Big Data applications problems. We believe that this review study will motivate the research community to perform expandable and transparent research against the pandemic crisis of COVID-19.",Not About Sufficiency
"Incidence and Mortality Patterns of Acute Myeloid Leukemia in Belgrade, Serbia (1999-2013)","Introduction: To assess incidence and mortality trends of acute myeloid leukemia (AML) in Belgrade (Serbia) in a 15-year period (from 1999 to 2013). Material and Methods: Data were obtained from the Cancer Registry of Serbia, Institute of Public Health of Serbia. Standardized incidence and mortality rates per 100,000 inhabitants were calculated by direct standardization method using World Standard Population. Analysis of raw data indicated single-digit numbers per year and per 5-year age cohorts. Therefore, we merged years of diagnosis to three-year intervals, creating so-called ""moving averages"". We also merged study population to 10-year age cohorts. Results: Both incidence and mortality rates increased with age, i.e., the lowest rates were observed in the youngest age groups and the highest rates were observed in oldest age groups. In all age groups, except the youngest (15-24 years), AML incidence was statistically significantly higher in men compared with women. Average age-adjusted incidence was 2.73/100,000 (95% confidence interval (CI) 2.28-3.71). Average age-adjusted mortality was 1.81/100,000 (95% CI 1.30-2.26). Overall, there were no significant changes in incidence trend. Age-adjusted incidence rates had increasing tendency among men aged 65-74 years (B = 0.80, standard error (SE) = 0.11; p = 0.005) and in total population aged 65-74 years (B = 0.41, SE = 0.09; p = 0.023). Increasing tendency in incidence of AML among women was observed in age group >75 years (B = 0.63, SE = 0.14; p = 0.019). No changes of mortality trend were observed. Conclusion: There was no significant change in trends of AML from 1999 to 2013 in the population of Belgrade.",Not About Sufficiency
Accessibility of the elderly in metropolitan environments: Metropolitan Area of Guadalajara (Mexico),"The increase of the urban population and population aging in Latin America give rise to focusing the urban analysis on the range of the elderly and their accessibility in a metropolitan context. The objective of this article is to explore the accessibility in the appropriation of the fundamentals and to distinguish the basic resources and activities that make up the structure of access opportunities. Documentary research has determined two housing contexts to compare in the Metropolitan Area of Guadalajara, Mexico. Conducting 18 in-depth interviews has revealed the influence of mobility capacity on the daily appropriation of fundamental skills. The results show processes of social exclusion and show the need to place the elderly at the center of urban planning that contributes to active aging.",Not About Sufficiency
The Evolution of Smart Mobility Strategies and Behaviors to Build the Smart City,"From an urban planner perspective, a city is smart when is able to respond to the needs of its inhabitants in a more efficient and sustainable way, mainly by properly using information and communication technologies (ICTs). This definition of smart city highlights the importance of two main factors for the development of smarter cities: citizens and technology. Human and social capital, indeed, play a key role, and new technologies have to be integrated into citizens' habits in order to effectively improve urban sustainability. Such considerations apply especially to the smart mobility context, where social dynamics very much influence how ICTs applied to transport Intelligent Transportation Systems (ITS) reduce pollution and congestion, increase safety and improve the management and promotion of public transport demand. Therefore, investigating the evolution of mobility habits and policy strategies within the Italian context during the last years may be of great interest in order to evaluate the effectiveness of smart mobility interventions. In this context, this paper aims to measure and compare mobility habits and policies in thirteen Italian metropolitan cities at 2006 and 2014, by using a principal component analysis. This analysis shows that metropolitan cities located in the northern part of the country have achieved several improvements between 2006 and 2014, with a reduction in the use of private transport and an increase in the share of sustainable means of transport, such as public transportation, cycling and car sharing. However, southern cities have not succeeded in improving their mobility habits and, thus the North South gap has widened.",Not About Sufficiency
"Evaluation of the impacts of land use/cover changes on water balance of Bilate watershed, Rift valley basin, Ethiopia","Land use/cover change is one of the responsible factors for changing the water balance of the watershed by altering the magnitude of surface runoff, interflow, base flow, and evapotranspiration. This study was aimed at evaluating the impacts of land use/cover change on the water balance of Bilate watershed between 1989, 2002, and 2015. The water balance simulation model (WaSiM) was used to access the impacts of land use/cover change on water balance. The model was calibrated (1989-2003) and validated (2007-2015) using the streamflow of at Bilate Tena gauging station. The result of land-use dynamics showed land use/cover change has a significant impact on the water balance of the watershed like on runoff production, base flow, interflow, evapotranspiration, and total simulation flow. In the study watershed, the change in total simulated flow increased by 77.83%, and surface runoff, interflow, and base flow increased by 80.23%, 75.69%, and 87.79% respectively and evapotranspiration decreased by 6% throughout the study period (1989-2015). The results obtained from this study revealed that the watershed is under the land/cover change that shows its impacts on hydrological processes and water balance. Thus, effective information regarding the environmental response of land use/cover, change is important to hydrologists, land-use planners, watershed management, and decision-makers for sustainable water resource projects and ecosystem services. Therefore, the policy-makers, planners, and stakeholders should design strategies to ensure the sustainability of the watershed hydrology for the sake of protecting agricultural activities, and urban planning and management systems within the watershed area. HIGHLIGHTS The research was done on the title: evaluating the impacts of land use/cover change on the water balance of bilate water shed, rift valley basin, Ethiopia. Hence, To understand the LULC change impact on water balance. The impacts on hydrological cycle processes. It is possible to use the WaSiM model for any watersheds. Used for watershed managers. Used as input data construction of hydraulic structures.",Not About Sufficiency
An assessment of urban parks distribution from multiple dimensions at the community level: A case study of Beijing,"Urban parks improve the quality of urban ecological environment and residents' physical and mental health. Existing studies have measured their spatial distribution in terms of accessibility or quality, however, this is not effective. Here, this study developed a spatial distribution assessment framework for urban parks, which included four dimensions: convenient degree (D-1), crowded degree (D-2), diversity of choices (D-3), and service quality (D-4). Taking Beijing as an example, we analyzed the difference in park distribution and found that park distribution is relatively equitable except for convenient degree. However, there are significant differences from the city center to the periphery. The inner-city area has better convenience, while the middle areas have more choices, and the outer ring performs better in terms of D-2 and D-4. There are significant gaps among communities with different housing prices. We found a gentrification trend within the Low, Middle-Low, and High price groups when analyzing the intra-group differences. This study can help urban planners and policymakers to better identify the gaps in park distribution and provide directions for optimizing the layout.",Not About Sufficiency
"Dynamic Spatial Modelling of Land Use Change in South Kuta, Bali","South Kuta District is one of the National Priority areas for the tourism sector. Development of urban areas in South Kuta was carried out quickly for support the program which is characterized by the process of built areas and will high consumption of land This study purpose to predict land use change in 2017 - 2037. The approach of this study is quantitative raster-based Analysis was executed based on trend of land use change in 2017-2037. The simulation model uses Cellular Automata (CA) concept and algorithm with driving factors (physic, socio-economic, facilities, accessibility, environment, and policy) that were shown into 14 variables and used assumptions and rules in land use changes simulation. The results showed that changes in residential area increased by 37%, commercial and services 1.050%, accommodation for tourism 472%, and public and social facilities 1.811%. On the other hand, the direction of development for residential area spreads to the north, quite close to protected area and disaster area. This is extremely different from detailed spatial plan document of South Kuta District.",Not About Sufficiency
Land Use as a Motivation for Railway Trespassing: Experience from the Czech Republic,"Railway trespassing is a very risky but common behaviour, resulting in about 200 casualties annually in the Czech Republic. This study describes the formation of 27 selected risk localities with frequent occurrence of trespassing in the regions of southern, central and northern Moravia. To be able to describe the process, an evaluation of the development of land use was conducted within a wide spatial context of each spot. The evaluation was focused on functional use of built-up areas (collective and individual housing, industrial areas, shopping and services, recreational areas, etc.). In the sample of investigated localities were places of two kinds: (1) localities where the railway intersected existing settlement structures, and relationships and links within the area were radically disturbed and severed. A lack of legal possibilities for crossing restricts the movement of inhabitants at these localities; (2) localities where the railway originally passed through open landscape and was later surrounded by built-up areas with various land-use functions. Here, trespassing is the consequence of gaps in the urban-planning process, wherein the needs of pedestrians and cyclists were not sufficiently considered. The analysis of the development of land use since 1836 showed how the motivations of trespassing were gradually intensified with more and more complex structures of functional division of areas. The percentage of built-up areas increased in all monitored localities overall from 6.28% during 1836-1852 to 52.15% during 2014-2015.",Not About Sufficiency
A Comprehensive Machine and Deep Learning Approach for Aerosol Optical Depth Forecasting: New Evidence from the Arabian Peninsula,"Accurate forecasting of environmental pollution indicators holds significant importance in diverse fields, including climate modeling, environmental monitoring, and public health. In this study, we investigate a wide range of machine learning and deep learning models to enhance Aerosol Optical Depth (AOD) predictions for the Arabian Peninsula (AP) region, one of the world's main dust source regions. Additionally, we explore the impact of feature extraction and their different types on the forecasting performance of each of the proposed models. Preprocessing of the data involves inputting missing values, data deseasonalization, and data normalization. Subsequently, hyperparameter optimization is performed on each model using grid search. The empirical results of the basic, hybrid and combined models revealed that the convolutional long short-term memory and Bayesian ridge models significantly outperformed the other basic models. Moreover, for the combined models, specifically the weighted averaging scheme, exhibit remarkable predictive accuracy, outperforming individual models and demonstrating superior performance in longer-term forecasts. Our findings emphasize the efficacy of combining distinct models and highlight the potential of the convolutional long short-term memory and Bayesian ridge models for univariate time series forecasting, particularly in the context of AOD predictions. These accurate daily forecasts bear practical implications for policymakers in various areas such as tourism, transportation, and public health, enabling better planning and resource allocation.",Not About Sufficiency
"Educational Case Studies: Creating a Digital Twin of the Production Line in TIA Portal, Unity, and Game4Automation Framework","In today's industry, the fourth industrial revolution is underway, characterized by the integration of advanced technologies such as artificial intelligence, the Internet of Things, and big data. One of the key pillars of this revolution is the technology of digital twin, which is rapidly gaining importance in various industries. However, the concept of digital twins is often misunderstood or misused as a buzzword, leading to confusion in its definition and applications. This observation inspired the authors of this paper to create their own demonstration applications that allow the control of both the real and virtual systems through automatic two-way communication and mutual influence in context of digital twins. The paper aims to demonstrate the use of digital twin technology aimed at discrete manufacturing events in two case studies. In order to create the digital twins for these case studies, the authors used technologies as Unity, Game4Automation, Siemens TIA portal, and Fishertechnik models. The first case study involves the creation of a digital twin for a production line model, while the second case study involves the virtual extension of a warehouse stacker using a digital twin. These case studies will form the basis for the creation of pilot courses for Industry 4.0 education and can be further modified for the development of Industry 4.0 educational materials and technical practice. In conclusion, selected technologies are affordable, which makes the presented methodologies and educational studies accessible to a wide range of researchers and solution developers tackling the issue of digital twins, with a focus on discrete manufacturing events.",Not About Sufficiency
An end-to-end real-time pollutants spilling recognition in wastewater based on the IoT-ready SENSIPLUS platform,"The problem of detecting illegal pollutants in wastewater is of fundamental importance for public health and security. The availability of distributed, low-cost and low-power monitoring systems, particularly enforced by IoT communication mechanisms and low-complexity machine learning algorithms, would make it feasible and easy to manage in a widespread manner. Accordingly, an End-to-End IoT-ready node for the sensing, local processing, and transmission of the data collected on the pollutants in the wastewater is presented here. The proposed system, organized in sensing and data processing modules, can recognize and distinguish contaminants from unknown substances typically present in wastewater. This is particularly important in the classification stage since distinguishing between background (not of interest) and foreground (of interest) substances drastically improves the classification performance, especially in terms of false positive rates. The measurement system, i.e., the sensing part, is represented by the so-called Smart Cable Water based on the SENSIPLUS chip, which integrates an array of sensors detecting various water-soluble substances through impedance spectroscopy. The data processing is based on a commercial Micro Control Unit (MCU), including an anomaly detection module, a classification module, and a false positive reduction module, all based on machine learning algorithms that have a computational complexity suitable for low-cost hardware implementation.An extensive experimental campaign on different contaminants has been carried out to train machinelearning algorithms suitable for low-cost and low-power MCU. The corresponding dataset has been made publicly available for download. The obtained results demonstrate an excellent classification ability, achieving an accuracy of more than 95% on average, and are a reliable ""proof of concept"" of a pervasive IoT system for distributed monitoring.& COPY; 2022 The Author(s). Published by Elsevier B.V. on behalf of King Saud University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",Not About Sufficiency
Precision medicine for traumatic cervical spinal cord injuries: accessible and interpretable machine learning models to predict individualized in-hospital outcomes,"BACKGROUND CONTEXT: A traumatic spinal cord injury (SCI) can cause temporary or per-manent motor and sensory impairment, leading to serious short and long-term consequences that can result in significant morbidity and mortality. The cervical spine is the most commonly affected area, accounting for about 60% of all traumatic SCI cases. PURPOSE: This study aims to employ machine learning (ML) algorithms to predict various out-comes, such as in-hospital mortality, nonhome discharges, extended length of stay (LOS), extended length of intensive care unit stay (ICU-LOS), and major complications in patients diagnosed with cervical SCI (cSCI). STUDY DESIGN: Our study was a retrospective machine learning classification study aiming to predict the outcomes of interest, which were binary categorical variables, in patients diagnosed with cSCI. PATIENT SAMPLE: The data for this study were obtained from the American College of Sur-geons (ACS) Trauma Quality Program (TQP) database, which was queried to identify patients who suffered from cSCI between 2019 and 2021. OUTCOME MEASURES: The outcomes of interest of our study were in-hospital mortality, non -home discharges, prolonged LOS, prolonged ICU-LOS, and major complications. The study evalu-ated the models' performance using both graphical and numerical methods. The receiver operating characteristic (ROC) and precision-recall curves (PRC) were used to assess model performance graphically. Numerical evaluation metrics included AUROC, balanced accuracy, weighted area under PRC (AUPRC), weighted precision, and weighted recall. METHODS: The study employed data from the American College of Surgeons (ACS) Trauma Quality Program (TQP) database to identify patients with cSCI. Four ML algorithms, namely XGBoost, LightGBM, CatBoost, and Random Forest, were utilized to develop predictive models. The most effective models were then incorporated into a publicly available web application designed to forecast the outcomes of interest. RESULTS: There were 71,661 patients included in the analysis for the outcome mortality, 67,331 for the outcome nonhome discharges, 76,782 for the outcome prolonged LOS, 26,615 for the outcome prolonged ICU-LOS, and 72,132 for the outcome major complications. The algorithms exhibited an AUROC value range of 0.78 to 0.839 for in-hospital mortality, 0.806 to 0.815 for nonhome discharges, 0.679 to 0.742 for prolonged LOS, 0.666 to 0.682 for prolonged ICU-LOS, and 0.637 to 0.704 for major complications. An open access web application was developed as part of the study, which can generate predictions for individual patients based on their characteristics. CONCLUSIONS: Our study suggests that ML models can be valuable in assessing risk for patients with cervical cSCI and may have considerable potential for predicting outcomes during hospitalization. ML models demonstrated good predictive ability for in-hospital mortality and non -home discharges, fair predictive ability for prolonged LOS, but poor predictive ability for pro-longed ICU-LOS and major complications. Along with these promising results, the development of a user-friendly web application that facilitates the integration of these models into clinical practice is a significant contribution of this study. The product of this study may have significant implica-tions in clinical settings to personalize care, anticipate outcomes, facilitate shared decision making and informed consent processes for cSCI patients. (c) 2023 Elsevier Inc. All rights reserved.",Not About Sufficiency
Strategic Planning for Setting Up Base Stations in Emergency Medical Systems,"Emergency Medical Systems (EMSs) are an important component of public health-care services. Improving infrastructure for EMS and specifically the construction of base stations at the ""right"" locations to reduce response times is the main focus of this paper. This is a computationally challenging task because of the: (a) exponentially large action space arising from having to consider combinations of potential base locations, which themselves can be significant; and (b) direct impact on the performance of the ambulance allocation problem, where we decide allocation of ambulances to bases. We present an incremental greedy approach to discover the placement of bases that maximises the service level of EMS. Using the properties of submodular optimisation we show that our greedy algorithm provides quality guaranteed solutions for one of the objectives employed in real EMSs. Furthermore, we validate our derived policy by employing a real-life event driven simulator that incorporates the real dynamics of EMS. Finally, we show the utility of our approaches on a real-world dataset from a large asian city and demonstrate significant improvement over the best known approaches from literature.",Not About Sufficiency
TOP-GAN: Stain-free cancer cell classification using deep learning with a small training set,"We propose a new deep learning approach for medical imaging that copes with the problem of a small training set, the main bottleneck of deep learning, and apply it for classification of healthy and cancer cell lines acquired by quantitative phase imaging. The proposed method, called transferring of pre-trained generative adversarial network (TOP-GAN), is hybridization between transfer learning and generative adversarial networks (GANs). Healthy cells and cancer cells of different metastatic potential have been imaged by low-coherence off-axis holography. After the acquisition, the optical path delay maps of the cells are extracted and directly used as inputs to the networks. In order to cope with the small number of classified images, we use GANs to train a large number of unclassified images from another cell type (sperm cells). After this preliminary training, we change the last layers of the network and design automatic classifiers for the correct cell type (healthy/primary cancer/metastatic cancer) with 90-99% accuracies, although small training sets of down to several images are used. These results are better in comparison to other classic methods that aim at coping with the same problem of a small training set. We believe that our approach makes the combination of holographic microscopy and deep learning networks more accessible to the medical field by enabling a rapid, automatic and accurate classification in stain-free imaging flow cytometry. Furthermore, our approach is expected to be applicable to many other medical image classification tasks, suffering from a small training set. (C) 2019 Published by Elsevier B.V.",Not About Sufficiency
Integration of Simulated and Experimentally Determined Thermophysical Properties of Aqueous Mixtures by ThermoML,"In order to make thermophysical properties of complex liquid mixtures available to a comprehensive analysis, we developed a data management and analysis platform based on the standard data exchange format ThermoML. The practicability of integrating thermophysical data from experiments and simulations was demonstrated for two binary mixtures, methanol-water and glycerol-water, by systematically studying the dependence of densities and diffusion coefficients from water content over the whole composition range and temperatures between 278.15 and 318.15 K. Experimental data were extracted manually from the literature. The same parameter space was explored by comprehensive molecular dynamics simulations, whose results were directly transferred to the analysis platform. The benefit of data integration was illustrated by assessing the transferability of the force fields, which had been developed for pure compounds to different compositions and temperatures, and by analyzing the excess mixing properties as a measure of nonideality of methanol-water and glycerol-water mixtures. The core of the data management and analysis platform is the newly developed Python library pyThermoML, which represents metadata, the parameters, and the experimentally determined or simulated properties as Python data classes. The feasibility of a seamless data flow from data acquisition to a comprehensive data analysis was demonstrated. PyThermoML enables interoperability and reusability of the datasets. The publication of ThermoML documents on the Dataverse installation of the University of Stuttgart (DaRUS) makes thermophysical data findable and accessible and thus FAIR.",Not About Sufficiency
Auxiliary Classifier Generative Adversarial Network for Interictal Epileptiform Discharge Modeling and EEG Data Augmentation,"Interictal epileptiform discharges (IEDs), or ""spikes"", are indicative of seizures and are useful in the diagnosis of epilepsy. Automated algorithms like machine learning models have shown promise in detecting spikes without requiring human supervision. However, there is still a lack of comprehensive, high-quality, and accessible data that can be used to train these algorithms. The goal of this work is to assess whether Generative Adversarial Networks (GANs) can create synthetic spikes that are both realistic and that can improve the performance of machine learning classification. Here we developed an Auxiliary Classifier GAN (AC-GAN) to generate synthetic spikes and trained it on expert-annotated real spike events from intracranial EEG recordings. We found that the AC-GAN was capable of successfully generating realistic synthetic spikes, demonstrated by qualitative and quantitative metrics. Furthermore, the synthetic spikes improved classification accuracy when augmented to the training data. Our findings exemplify the utility of GANs in assisting with epilepsy diagnosis through synthesis of realistic neurophysiological signal waveforms and through data augmentation in classification.",Not About Sufficiency
"Computational Analysis of Affect, Personality, and Engagement in Human-Robot Interactions","This chapter focuses on recent advances in social robots that are capable of sensing their users, and support their users through social interactions, with the ultimate goal of fostering their cognitive and socio-emotional wellbeing. Designing social robots with socio-emotional skills is a challenging research topic still in its infancy. These skills are important for robots to be able to provide physical and social support to human users, and to engage in and sustain long-term interactions with them in a variety of application domains that require human-robot interaction, including healthcare, education, entertainment, manufacturing, and many others. The availability of commercial robotic platforms and developments in collaborative academic research provide us with a positive outlook; however, the capabilities of current social robots are quite limited. The main challenge is understanding the underlying mechanisms of humans in responding to and interacting with real life situations, and how to model these mechanisms for the embodiment of naturalistic, human-inspired behavior in robots. Addressing this challenge successfully requires an understanding of the essential components of social interaction, including nonverbal behavioral cues such as interpersonal distance, body position, body posture, arm and hand gestures, head and facial gestures, gaze, silences, vocal outbursts, and their dynamics. To create truly intelligent social robots, these nonverbal cues need to be interpreted to form an understanding of the higher level phenomena including first-impression formation, social roles, interpersonal relationships, focus of attention, synchrony, affective states, emotions, personality, and engagement, and in turn defining optimal protocols and behaviors to express these phenomena through robotic platforms in an appropriate and timely manner. This chapter sets out to explore the automatic analysis of social phenomena that are commonly studied in the fields of affective computing and social signal processing, together with an overview of recent vision-based approaches used by social robots. The chapter then describes two case studies to demonstrate how emotions and personality, two key phenomena for enabling effective and engaging interactions with robots, can be automatically predicted from visual cues during human-robot interactions. The chapter concludes by summarizing the open problems in the field and discussing potential future directions.",Not About Sufficiency
Characterizing the phase diagram of finite-size dipolar Bose-Hubbard systems,"We use state-of-the-art density matrix renormalization group calculations in the canonical ensemble to determine the phase diagram of the dipolar Bose-Hubbard model on a finite cylinder. We consider several observables that are accessible in typical optical lattice setups and assess how well these quantities perform as order parameters. We find that, especially for small systems, the occupation imbalance is less susceptible to boundary effects than the structure factor in uncovering the presence of a periodic density modulation. By analyzing the nonlocal correlations, we find that the appearance of supersolid order is very sensitive to boundary effects, which may render it difficult to observe in quantum gas lattice experiments with a few tens of particles. Finally, we show that density measurements readily obtainable on a quantum gas microscope allow distinguishing between superfluid and solid phases using unsupervised machine-learning techniques.",Not About Sufficiency
"A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms","A key challenge in intelligent robotics is creating robots that are capable of directly interacting with the world around them to achieve their goals. The last decade has seen substantial growth in research on the problem of robot manipulation, which aims to exploit the increasing availability of affordable robot arms and grippers to create robots capable of directly interacting with the world to achieve their goals. Learning will be central to such autonomous systems, as the real world contains too much variation for a robot to expect to have an accurate model of its environment, the objects in it, or the skills required to manipulate them, in advance. We aim to survey a representative subset of that research which uses machine learning for manipulation. We describe a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework and highlight the many remaining research opportunities and challenges.",Not About Sufficiency
Deep learning the slow modes for rare events sampling,"The development of enhanced sampling methods has greatly extended the scope of atomistic simulations, allowing longtime phenomena to be studied with accessible computational resources. Many such methods rely on the identification of an appropriate set of collective variables. These are meant to describe the system's modes that most slowly approach equilibrium under the action of the sampling algorithm. Once identified, the equilibration of these modes is accelerated by the enhanced sampling method of choice. An attractive way of determining the collective variables is to relate them to the eigenfunctions and eigenvalues of the transfer operator. Unfortunately, this requires knowing the long-term dynamics of the system beforehand, which is generally not available. However, we have recently shown that it is indeed possible to determine efficient collective variables starting from biased simulations. In this paper, we bring the power of machine learning and the efficiency of the recently developed on the fly probability-enhanced sampling method to bear on this approach. The result is a powerful and robust algorithm that, given an initial enhanced sampling simulation performed with trial collective variables or generalized ensembles, extracts transfer operator eigenfunctions using a neural network ansatz and then accelerates them to promote sampling of rare events. To illustrate the generality of this approach, we apply it to several systems, ranging from the conformational transition of a small molecule to the folding of a miniprotein and the study of materials crystallization.",Not About Sufficiency
The conflict between environmental justice and culture,"During a study of Taiwan's Indigenous Tayal people and their symbiotic relationship with the environment, a conflict between environmental justice and culture was identified. The conflict is driven by environmental laws developed to protect society and the environment. Although Indigenous people such as the Tayal are part of a larger society, they are a distinct people with their own culture, language, and laws that are protective of the people and environment. By simply following their culture, sustainable environmental practices, and protecting the environment, they violate the laws developed to protect all people. Every culture on the planet has cultural practices that probably violate an environmental law. Indigenous people are generally not included in the law-making process and their inclusion developing environmental law is essential to respect and protect a people's culture and to manage the planet's environmental resources in a responsible manner without reprisal.",Not About Sufficiency
Circulating CCN6/WISP3 in type 2 diabetes mellitus patients and its correlation with insulin resistance and inflammation: statistical and machine learning analyses,"Introduction Cellular Communication Network Factor 6 (CCN6) is an adipokine whose production undergoes significant alterations in metabolic disorders. Given the well-established link between obesity-induced adipokine dysfunction and the development of insulin resistance and type 2 diabetes mellitus (T2DM), this study investigates the potential role of CCN6 as a biomarker for T2DM. The present study aimed to investigate the association between serum CCN6 levels and T2DM, as well as its risk factors, for the first time. Methods In this case-control study, a total of 80 individuals diagnosed with T2DM and 80 healthy control individuals, who referred to Shariati hospital (Tehran, Iran), were included in the study. Biochemical parameters including fasting blood glucose (FBG), aspartate transaminase (AST), alanine transaminase (ALT), triglycerides (TG), total cholesterol (TC), high-density lipoprotein (HDL), and low-density lipoprotein (LDL) were determined using the AutoAnalyzer instrument. The circulating levels of CCN6, adiponectin, Tumor necrosis factor-alpha (TNF)-alpha, Interleukin 6 (IL-6), and insulin were quantified using ELISA. The Student t-test was applied to data that presented as mean +/- standard deviations (SD). Moreover, the Gini Index was utilized to determine the weight of each factor in T2DM classification. Additionally, various machine learning models were employed to develop classifiers for predicting T2DM. Results T2DM patients demonstrated significantly lower levels of CCN6 (1259.76 +/- 395.02 pg/ml) compared to controls (1979.17 +/- 471.99 pg/ml, P < 0.001), as well as lower levels of adiponectin (P < 0.001) and higher levels of TNF-alpha and IL-6 (P < 0.001) compared to non-T2DM individuals. In the T2DM group, CCN6 exhibited negative correlations with insulin, Homeostatic Model Assessment for Insulin Resistance (HOMA-IR), body mass index (BMI), IL-6, and TNF-alpha. Logistic regression analysis indicated an increased risk of T2DM, with a CCN6 cutoff value of 1527.95 pg/mL distinguishing T2DM patients with 86.3% sensitivity and 73.8% specificity. The Gini Index highlighted that HOMA-IR, IL6, and CCN6 had the highest weighting on T2DM. Conclusion Our research identified a significant and negative association between serum CCN6 levels and the likelihood of T2DM, as well as inflammation biomarkers (IL-6 and TNF-alpha). CCN6 shows promise as a potential biomarker for T2DM; however, further investigations are necessary to validate this finding and assess its clinical utility.",Not About Sufficiency
Diverse experts' perspectives on ethical issues of using machine learning to predict HIV/AIDS risk in sub-Saharan Africa: a modified Delphi study,"Objective To better understand diverse experts' views about the ethical implications of ongoing research funded by the National Institutes of Health that uses machine learning to predict HIV/AIDS risk in sub-Saharan Africa (SSA) based on publicly available Demographic and Health Surveys data. Design Three rounds of semi-structured surveys in an online expert panel using a modified Delphi approach. Participants Experts in informatics, African public health and HIV/AIDS and bioethics were invited to participate. Measures Perceived importance of or agreement about relevance of ethical issues on 5-point unipolar Likert scales. Qualitative data analysis identified emergent themes related to ethical issues and development of an ethical framework and recommendations for open-ended questions. Results Of the 35 invited experts, 22 participated in the online expert panel (63%). Emergent themes were the inclusion of African researchers in all aspects of study design, analysis and dissemination to identify and address local contextual issues, as well as engagement of communities. Experts focused on engagement with health and science professionals to address risks, benefits and communication of findings. Respondents prioritised the mitigation of stigma to research participants but recognised trade-offs between privacy and the need to disseminate findings to realise public health benefits. Strategies for responsible communication of results were suggested, including careful word choice in presentation of results and limited dissemination to need-to-know stakeholders such as public health planners. Conclusion Experts identified ethical issues specific to the African context and to research on sensitive, publicly available data and strategies for addressing these issues. These findings can be used to inform an ethical implementation framework with research stage-specific recommendations on how to use publicly available data for machine learning-based predictive analytics to predict HIV/AIDS risk in SSA.",Not About Sufficiency
"Space, Sport, Society. The practice of sport in the design of contemporary public space","In the current socio-cultural scenario, the practice of sport represents one of the main drivers of development, given the inclusive connotation it incorporates and the functional and spatial qualification potential that it expresses. Literature on this subject, and the many experiments in the field, serve to highlight how sports activities today are a central tool in the promotion of an ""open city"" ethic, namely one that is liveable and safe. Equally, in modern times, the city's public infrastructure system represents an increasingly important factor for urban and social quality, requiring programmes and strategies capable of redefining places and their modes of use according to the themes of health and environmental quality. On the basis of these premises, this paper aims to analyse the recent evolution of the methods of planning and design of public space in relation to sports practices understood as a ""comprehensive social reality"", as areas where urban and social regeneration policies based on the desire to promote health education actions, social inclusion and programmes for the physical qualification of the built environment are applied.",Not About Sufficiency
Reducing particulate matter and oxides of nitrogen emissions from heavy-duty vehicles - The urban bus case,"The 1990 Clean Air Act Amendments mandated stricter emissions standards for heavy-duty vehicles. One category of heavy-duty vehicles, urban transit buses, constitutes a highly visible source of pollutant emissions and must meet even more stringent standards. In response, engine manufacturers have produced diesel engines that produce virtually no black smoke and emit several times less particulate matter (PM) than older engines. The tighter PM emissions standards that apply to urban transit buses only were found to be cost-effective ($4,600 to $6,300 per Mg of PM reduced) compared with other control strategies. The Urban Bus Retrofrt/Rebuild Program was found to be somewhat less cost-effective ($6,900 to $42,000 per Mg), Both of these programs could be extended to other heavy-duty vehicles. However, the use of compressed natural gas (CNG) in transit buses was found to have a cost-effectiveness of $0.9 million to $1.8 million per Mg of PM. Replacing older diesel engines with low-emission diesels is the most cost-effective way to reduce transit bus emissions. By purchasing more clean diesels instead of fewer cleaner CNG buses, transit agencies can produce greater reductions in fleetwide emissions. In fact, the transit agency in this study with the most alternative-fuel buses has the highest fleetwide PM emissions rate. It is premature to expand the alternative-fuel transit bus fleet beyond the experimental level. Regulatory policy should be reassessed as the understanding of the link between diesel emissions and health effects improves.",Not About Sufficiency
A Machine-Learning-Based Approach to Predict the Health Impacts of Commuting in Large Cities: Case Study of London,"The daily commute represents a source of chronic stress that is positively correlated with physiological consequences, including increased blood pressure, heart rate, fatigue, and other negative mental and physical health effects. The purpose of this research is to investigate and predict the physiological effects of commuting in Greater London on the human body based on machine-learning approaches. For each participant, the data were collected for five consecutive working days, before and after the commute, using non-invasive wearable biosensor technology. Multimodal behaviour, analysis and synthesis are the subjects of major efforts in computing field to realise the successful human-human and human-agent interactions, especially for developing future intuitive technologies. Current analysis approaches still focus on individuals, while we are considering methodologies addressing groups as a whole. This research paper employs a pool of machine-learning approaches to predict and analyse the effect of commuting objectively. Comprehensive experimentation has been carried out to choose the best algorithmic structure that suit the problem in question. The results from this study suggest that whether the commuting period was short or long, all objective bio-signals (heat rate and blood pressure) were higher post-commute than pre-commute. In addition, the results match both the subjective evaluation obtained from the Positive and Negative Affect Schedule and the proposed objective evaluation of this study in relation to the correlation between the effect of commuting on bio-signals. Our findings provide further support for shorter commutes and using the healthier or active modes of transportation.",Not About Sufficiency
Prednisolone versus antihistamine for allergic rhinitis: No significant difference found in randomized trial,"BackgroundSeasonal allergic rhinitis (AR) impacts public health by affecting work productivity and quality of life. The Swedish tree pollen season starts in February with alder and hazel pollination, followed by birch and ends with oak in May. Systemic corticosteroids are often prescribed when topical treatments fail, despite limited evidence supporting their efficacy.ObjectiveTo compare the effectiveness of prednisolone tablets versus antihistamine tablets in reducing symptoms and medication usage in patients with moderate to severe tree pollen-induced AR.MethodsThis interventional single-center, double-blinded randomized trial included 34 patients. Treatment was initiated, and symptoms were registered during the tree pollen season. The two groups received either prednisolone tablets (20 mg) or ebastine tablets (20 mg) for 7 days. Treatment effects were evaluated by comparing daily symptom scores, use of topical medication, and a combined symptom-medical score between the groups. Quality of life was recorded at the start and after 3 weeks.ResultsBoth interventions demonstrated efficacy in enhancing quality of life metrics. The area under the curve (AUC) for the combined symptom severity and medication usage score averaged 34.0 (SD = 19.1, 95% CI = 24.5-43.4) in the group treated with prednisolone. This was marginally lower than the control group, with an AUC of 32.6 (SD = 13.2, 95% CI = 25.6-39.7). The difference was not statistically significant (p = 0.80). Both groups exhibited only mild adverse events, which were statistically comparable in frequency and severity.ConclusionsPrednisolone tablets did not show superior efficacy over antihistamine tablets in reducing symptoms or medication usage in tree pollen-induced AR. These results suggest that systemic corticosteroids may not provide additional benefits over antihistamines, and clinicians should prioritize individualized treatment based on patient preferences and tolerability.",Not About Sufficiency
Public Health Aspects of Climate Change Adaptation in Three Cities: A Qualitative Study,"Climate change presents an unprecedented public health challenge as it has a great impact on population health outcomes across the global population. The key to addressing these health challenges is adaptation carried out in cities through collaboration between institutions, including public health ones. Through semi-structured interviews (n = 16), this study investigated experiences and perceptions of what public health aspects are considered by urban and public health planners and researchers when planning climate change adaptation in the coastal cities of Soderhamn (Sweden), Porto (Portugal) and Navotas (the Philippines). Results of the thematic analysis indicated that participating stakeholders were aware of the main climate risks threatening their cities (rising water levels and flooding, extreme temperatures, and air pollution). In addition, the interviewees talked about collaboration with other sectors, including the public health sector, in implementing climate change adaptation plans. However, the inclusion of the public health sector as a partner in the process was identified in only two cities, Navotas and Porto. Furthermore, the study found that there were few aspects pertaining to public health (water and sanitation, prevention of heat-related and water-borne diseases, and prevention of the consequences associated with heat waves in vulnerable groups such as children and elderly persons) in the latest climate change adaptation plans posted on each city's website. Moreover, participants pointed to different difficulties: insufficient financial resources, limited intersectoral collaboration for climate change adaptation, and lack of involvement of the public health sector in the adaptation processes, especially in one of the cities in which climate change adaptation was solely the responsibility of the urban planners. Studies using larger samples of stakeholders in larger cities are needed to better understand why the public health sector is still almost absent in efforts to adapt to climate change.",Not About Sufficiency
Parental Attitudes toward Artificial Intelligence-Driven Precision Medicine Technologies in Pediatric Healthcare,"Precision medicine relies upon artificial intelligence (AI)-driven technologies that raise ethical and practical concerns. In this study, we developed and validated a measure of parental openness and concerns with AI-driven technologies in their child's healthcare. In this cross-sectional survey, we enrolled parents of children <18 years in 2 rounds for exploratory (n= 418) and confirmatory (n= 386) factor analysis. We developed a 12-item measure of parental openness to AI-driven technologies, and a 33-item measure identifying concerns that parents found important when considering these technologies. We also evaluated associations between openness and attitudes, beliefs, personality traits, and demographics. Parents (N= 804) reported mean openness to AI-driven technologies of M = 3.4/5, SD = 0.9. We identified seven concerns that parents considered important when evaluating these technologies: quality/accuracy, privacy, shared decision making, convenience, cost, human element of care, and social justice. In multivariable linear regression, parental openness was positively associated with quality (beta = 0.23), convenience (beta = 0.16), and cost (beta = 0.11), as well as faith in technology (beta = 0.23) and trust in health information systems (beta = 0.12). Parental openness was negatively associated with the perceived importance of shared decision making (beta = -0.16) and being female (beta = -0.12). Developers might support parental openness by addressing these concerns during the development and implementation of novel AI-driven technologies.",Not About Sufficiency
"A World of False Promises: International Labour Organization, World Health Organization, and the Plea of Workers Under Neoliberalism","Occupational health and safety is poorly served by United Nations agencies designated to protect workers: the World Health Organization (WHO) and the International Labor Organization (ILO). The neoliberal programs initially adopted by the United Nations supported institutions of social protection and regulation and expanded worker protections and union growth. Neoliberalism later became synonymous with globalism and shared in its international success. The fundamental change under neoliberalism was the exchange and accumulation of capital. The major beneficiaries of neoliberalism, at the expense of workers, were large transnational corporations and wealthy investors. During this period, WHO and ILO activities in support of workers declined. As neoliberalism ultimately became neoconservatism, occupational health and safety was purposely ignored, and labor was treated with hostility. Neoliberalism had evolved into a harsh economic system detrimental to labor and labor rights. The United Nations is now in decline, taking with it the trivial WHO and ILO programs. Replacements for the WHO and ILO programs must be developed. It is not enough to call for renewed funding, given the United Nations' failure to direct the global effort to protect workers. A new direction must be found.",Not About Sufficiency
SmartGrid: Future networks for New Zealand power systems incorporating distributed generation,"The concept of intelligent electricity grids, which primarily involves the integration of new information and communication technologies with power transmission lines and distribution cables, is being actively explored in the European Union and the United States. Both developments share common technological developmental goals but also differ distinctly towards the role of distributed generation for their future electrical energy security. This paper looks at options that could find relevance to New Zealand (NZ), in the context of its aspiration of achieving 90% renewable energy electricity generation portfolio by 2025. It also identifies developments in technical standardization and industry investments that facilitate a pathway towards an intelligent or smart grid development for NZ. Some areas where policy can support research in NZ being a ""fast adapter"" to future grid development are also listed. This paper will help policy makers quickly review developments surrounding SmartGrid and also identify its potential to support NZ Energy Strategy in the electricity infrastructure. This paper will also help researchers and power system stakeholders for identifying international standardization, projects and potential partners in the area of future grid technologies. (C) 2009 Elsevier Ltd. All rights reserved.",Not About Sufficiency
Dilated residual grooming kernel model for breast cancer detection,"Breast cancer is the most significant cause of mortality among women. When detected and treated early, it saves lives. Breast cancer detection is becoming more accessible and accurate thanks to machine learning and deep learning models. This research aims to enhance medical science and technology by employing a deep learning model to detect small cancer cells with pinpoint accuracy. The proposed model uses datasets from the Breast Cancer Histopathological Image Classification (BreakHis) and Breast Cancer Histopathological Annotation and Diagnosis (BreCaHAD). Next, the image processing procedure employs strain normalization to rectify color divergence caused by using different slide scanners, staining processes, and biopsy materials. The data augmentation with nineteen parameters such as scaling, rotation, flip, resize, and gamma value tackles the overfitting problems. The augmented images are then processed using the dilated residual (DR) model; the DR model combines the proposed dilated spatial convolution unit, fully connected dilation unit, and dilated channel convolution. The first unit is the dilated spatial convolution, which handles all channels equally and amplifies the valuable aspects. The second unit is a fully connected dilation unit; it displays low-level properties such as edges, contours, and color. The third unit is dilated channel convolution, which detects tiny objects and thin boundaries without adding complexity. The proposed dilated residual grooming kernel (DRGK) model is a 14-layer deep learning model that stretches the receptive field while retaining feature information, using the proposed DR unit and ghost model, as well as convolution, pooling, downsampling, and dilated convolution. Dilated convolutions are extensively used in the proposed model to extract features. Accuracy, the area under the curve, average precision score, precision, sensitivity, and f1 score all improve with a learning rate of 0.001. With 96.33% and 93.35% marks, the proposed approach surpasses several state-of-the-art methods. (C) 2022 Elsevier B.V. All rights reserved.",Not About Sufficiency
"A comparison of the prevalence of and modifiable risk factors for cognitive impairment among community-dwelling Canadian seniors over two decades, 1991-2009","Background The prevalence of cognitive impairment or dementia is of public health concern globally. Accurate estimates of this debilitating condition are needed for future public health policy planning. In this study, we estimate prevalence and modifiable risk factors for cognitive impairment by sex over approximately 16 years. Methods Canadian Study of Health and Aging (CSHA) baseline data conducted between 1991-1992 were used to measure the prevalence of cognitive impairment and dementia among adults aged 65+ years. The standard Modified Mini-Mental State Examination (3MS) was used for the screening test for cognitive impairment. We compared the CSHA data with Canadian Community Health Survey-Healthy Aging (CCHS-HA) conducted between 2008-2009. The CCHS-HA used a four-dimension cognitive module to screen for cognitive impairment. Only survey community-dwelling respondents were included in the final sample. After applying exclusion criteria, final samples of (N = 8504) respondents in the CSHA sample and (N = 7764) respondents for CCHS-HA sample were analyzed. To account for changes in the age structure of the Canadian population, prevalence estimates were calculated using age-sex standardization to the 2001 population census of Canada. Logistic regression analyses were used to examine predictors of cognitive impairment. A sex stratified analysis was used to examine risk factors for cognitive impairment in the survey samples. Results We found that prevalence of cognitive impairment among respondents in CSHA sample was 15.5% in 1991 while a prevalence of 10.8% was reported in the CCHS-HA sample in 2009, a 4.7% reduction [15.5% (CI = 14.8-16.3), CSHA vs 10.8% (CI = 10.1-11.5), CCHS-HA]. Men reported higher prevalence of cognitive impairment in CSHA study (16.0%) while women reported higher prevalence of cognitive impairment in CCHS-HA (11.6%). In the multivariable analyses, risk factors such as age, poor self-rated health, stroke, Parkinson's disease, and hearing problems were common to both cohorts. Sex differences in risk factors were also noted. Conclusions This study provides suggestive evidence of a potential reduction in the occurrence of cognitive impairment among community-dwelling Canadian seniors despite the aging of the Canadian population. The moderating roles of improved prevention and treatment of vascular morbidity and improvements in the levels of education of the Canadian population are possible explanations for this decrease in the cognitive impairment.",Not About Sufficiency
Impact assessment of rainfall scenarios and land-use change on hydrologic response using synthetic Area IDF curves,"In combination with land use change, climate change is increasingly leading to extreme weather conditions and consequently novel hydrologic conditions. Rainfall Area intensity-duration-frequency (IDF) curves, commonly used tools for modeling hydrology and managing flood risk can be used to assess hydrologic response under extreme rainfall conditions. We explore the influence of land use change on hydrologic response under designed extreme rainfall over the period 1976 to 2006 in the Kamo River basin. Run-off for all six designed rainfall shapes under 2006 land use is higher than that under 1976 land use, but the timing of peak discharge under 2006 land use occurs at roughly the same time as that under 1976 land use. Results indicate that run-off under 2006 land use yielded higher discharge than under 1976 land use, and rainfall shape six leads to the most extreme hydrologic response and most dangerous conditions from the perspective of urban planning and flood risk management. Future hydrologic response will differ from present due both to changes in land cover and changes in extreme rainfall patterns requiring modification to Area IDF curves for catchments.",Not About Sufficiency
"Assessment of street space quality and subjective well-being mismatch and its impact, using multi-source big data","This study makes initial efforts by delineating the distribution map of the mismatch between street space quality and SWB in central Qingdao through machine learning approaches, then creatively combines ordered logistic regression and restrictive cubic spline to examine the nonlinear influence of urban variables on the mismatch based on multi -source big data. The study primarily found that low -quality spaces are concentrated in the old city area; The SWB scores of the internal space in central Qingdao are generally good and evenly distributed, while the SWB scores of the peripheral space have significant differences; Road network accessibility, green space, living convenience, and housing prices are positively correlated with SWB significantly higher than street space quality, however, land mixed use, night lighting index, and population density are negatively correlated with it. When the green space agglomeration value reaches 2.9 or exceeds 7.8, the living convenience value exceeds 12.2, and the housing price value reaches 26.6 thousand yuan/m2, improving the street space quality is most likely to enhance residents' SWB. These findings link urban spatial quality with SWB and provide support for urban further planning and regeneration to improve public SWB through targeted interventions.",Not About Sufficiency
"Intelligent Noise Mapping for Smart Cities: Solutions, Trends, and Research Opportunities","Noise pollution is an often-overlooked environmental threat that poses a concealed but severe risk to public health and urban ecological environment. Despite being invisible, noise has become the second-largest environmental factor contributing to public health issues. Furthermore, noise pollution hinders the sustainable development of cities. To manage noise pollution in urban areas, intelligent noise mapping is expected to be an essential tool that can transform invisible noise problems into visible ones and provide an efficient governance basis. However, this technological approach is still in its infancy, and there is a significant gap in implementing large-scale dynamic urban noise mapping. In this article, we provide a critical review of recent advances in intelligent noise mapping, analyzing their technical principles, fundamental research topics, advantages, and limitations. We also present our insights on the development trends of intelligent noise mapping for smart cities. Finally, we identify four critical open issues and discuss our insights to address these gaps for future research. This study's findings can contribute to the development of more effective solutions to address noise pollution and promote sustainable smart cities.",Not About Sufficiency
Lithofacies classification integrating conventional approaches and machine learning technique,"This study introduces an integrated approach for lithofacies classification utilizing core samples, wireline logs, and a machine learning technique. It is specialized for the Eagle Ford shale and the Austin Chalk where operators often face difficulties to define geological heterogeneity due to relatively consistent and low reservoir quality compared to conventional reservoirs. A set of cored slabs, thin sections, scanning electron microscope (SEM) images were utilized for lithofacies classification. Four lithofacies were defined with depositional texture, fabric, mineralogy, pore type, diagenesis, and biological features: (1) Organic-matter-rich mudstone, (2) Organic-matter-lean calcareous marl, (3) Heterogeneous argillaceous wackestone and marl, (4) Massive marly chalk. These four lithofacies are not easily classifiable through conventional wireline log cross-plotting. A convolutional neural network (CNN) model was trained to classify the lithofacies using five wireline logs (Gamma ray, bulk density, neutron porosity, deep resistivity, and compressional sonic logs) which are commonly accessible at field sites. In order to validate and test the CNN model, k-fold cross-validation and blind well test were conducted. This data-driven approach is expected to provide technical insights to operators seeking practical approaches to identifying prospective drilling locations and optimal completion strategies based on in-depth reservoir characterization.",Not About Sufficiency
Assessing land-use and -cover changes in relation to geographic factors and urban planning in the metropolitan area of Concepcion (Chile). Implications for biodiversity conservation,"The conservation of biodiversity in Latin American metropolitan areas is threatened by the intense land-use and -cover change. Assessing the overall biodiversity changes in entire regions faces with the traditional lack of consistent biodiversity data. This work aims at contributing to this assessment through a set of major pressures to biodiversity defined from land-use and -cover changes, and evaluating their extent, distribution and correlations with geographical variables. The study was performed in the framework of the Metropolitan Urban Plan of Concepcion (MUPC, Chile). Land-use and -cover maps were obtained through image classification for the years 2000 and 2010, before and after the MUPC approval, and combined in a land-use and -cover change (LUCC) map. A set of pressures to biodiversity (natural and artificial forestation, deforestation, agricultural abandonment and expansion, and urbanization) was obtained from reclassifying the LUCC map. The correlations of these pressures with a set of geographical variables were assessed using canonical ordination methods. Finally, a preliminary forecast analysis of the effects of the MUPC was performed by combining the land-use and -cover map of 2010 with the urban-extension areas of the plan. Results showed that, in only 10 years, 57% of the Concepcion Metropolitan Area (CMA) was affected by land-use and -cover changes, and 48% was affected by the pressures to biodiversity. Artificial forestation and deforestation were the dominant pressures, followed by agricultural abandonment and urbanization. The geographical distribution of pressures during the 2000-2010 period also contributed to affect the conservation of biodiversity and the sustainable management of the CMA. Indeed, natural forestation occurred close to urbanization, thus threatening the ecological integrity of native forests, while artificial forestation, deforestation and agricultural abandonment took place in steeply areas thus increasing landslide risk. Despite urbanization was not the most relevant pressure in the short studied period, urban development planned in the MUPC would determine an overall increase of 60% in the built-up area of the CMA, mostly affecting brushwood and forest plantations but also native forest and wetlands. Implications of these results for the strategic environmental assessment (SEA) and the sustainable management of Latin American metropolis are finally discussed. (C) 2013 Elsevier Ltd. All rights reserved.",Not About Sufficiency
"The influence of increasing tree cover on mean radiant temperature across a mixed development suburb in Adelaide, Australia","The implementation of trees in urban environments can mitigate outdoor thermal stress. Growing global urban population and the risk of heatwaves, compounded by development driven urban warmth (the urban heat island), means more people are at risk of heat stress in our cities. Effective planning of urban environments must minimise heat-health risks through a variety of active and passive design measures at an affordable cost. Using the Solar and Longwave Environmental Irradiance Geometry (SOLWEIG) model and working within the bounds of current urban design, this study aimed to quantify changes in mean radiant temperature (T-mrt) from increased tree cover at five different 200 x 200 m urban forms (including compact mid-rise development, residential and open grassy areas) within a suburb of Adelaide, Australia during summer. Following a successful validation of SOLWEIG, street trees were strategically distributed throughout each of the five urban forms and the model run over five warm sunny days (13-17 February 2011). Results showed spatially averaged daytime (7:30-20:00) T-mrt reduced by between 1.7 C and 5.1 degrees C at each site, while under peak heating conditions (16 February, 14:00) T-mrt reduced by between 2.0 degrees C and 7.1 degrees C. The largest reduction in T,,t under peak warming conditions was at the residential site, despite having the fewest number of trees added. Directly below clusters of trees, Tmrt could be reduced by between 14.1 degrees C and 18.7 degrees C. SOLWEIG also highlighted that more built-up sites showed higher Tmrt under peak warming conditions due to increased radiation loading from 3D urban surfaces, but over the course of the day, open sites were exposed to greater and more uniform Tmrt. This study clearly demonstrates the capacity of street trees to mitigate outdoor thermal stress and provides guidance for urban planners on strategic street tree implementation. (C) 2016 Elsevier GmbH. All rights reserved.",Not About Sufficiency
Trans-Governance and Food Systems (Tr-GaF) for food policy integration: A case study of the Australian food policy landscape,"Food policy integration is critical for sustainability as urban food policies become a new norm in the policy agenda. Yet, little research demonstrates how food policies are integrated with food systems and across governance to achieve urban sustainability. This paper presents the Trans-Governance and Food Systems (Tr-GaF) framework for food policy integration developed to evaluate the Australian food policy landscape. The analysis of 102 food-relevant policy documents sourced from multiple government institutions in Queensland, Western Australia, and Victoria identifies limited food policy integration in Australia, with little attention paid to foodrelated urban planning. Policy misalignments arise in the agrifood economy, health and well-being, and environment and ecology domains, with horizontal and vertical fragmentation across governance levels and sectors. Inadequate integration with food systems further jeopardises the efficiency of urban food supply chains and consumption activities. These three prominent issues collectively stifle urban sustainability by economically restraining food supply chain actors' ability to innovate and compete. While environmental imbalance worsens, urbanites inevitably face barriers to maintaining health and social well-being. Tr-GaF provides an evidencedbased framework for policymakers to evaluate and integrate food policies anchored in food systems governed by collaborative multi-scalar government agencies, institutions, food industries, and civil society. In doing so, urban development can sustainably evolve economically, socially, and environmentally.",Not About Sufficiency
A REAL-TIME SHEEP COUNTING DETECTION SYSTEM BASED ON MACHINE LEARNING,"With the development of modern breeding industry, it is very important to count sheep accurately. In the past, herdsmen used manual statistics to count and manage sheep, which was time-consuming, laborious and often had large errors. In recent years, machine learning methods are widely used in automatic target recognition, which can replace manual labor. This system is based on YOLOv5 algorithm for sheep counting management. The counting of sheep was controlled by two -way counting. This improves the accuracy of counting, saves a lot of manpower and material resources for herdsmen, and greatly promotes the management of animal husbandry.",Not About Sufficiency
A Practical Roadmap to Implementing Deep Learning Segmentation in the Clinical Neuroimaging Research Workflow,"BACKGROUND: Thanks to the proliferation of open- source tools, we are seeing an exponential growth of machine-learning applications, and its integration has become more accessible, particularly for segmentation tools in neuroimaging.  METHODS: This article explores a generalized methodology that harnesses these tools and aims/enables to expedite and enhance the reproducibility of clinical research. Herein, critical considerations include hardware, software, neural network training strategies, and data labeling guidelines. More specifically, we advocate an iterative approach to model training and transfer learning, focusing on internal validation and outlier handling early in the labeling process and fine-tuning later.  RESULTS: The iterative refinement process allows experts to intervene and improve model reliability while cutting down on their time spent in manual work. A seamless integration of the final model's predictions into clinical research is proposed to ensure standardized and reproducible results.  CONCLUSIONS: In short, this article provides a comprehensive framework for accelerating research using machine-learning techniques for image segmentation.",Not About Sufficiency
Optimal Feature Selection-Based Dental Caries Prediction Model Using Machine Learning for Decision Support System,"The high frequency of dental caries is a major public health concern worldwide. The condition is common, particularly in developing countries. Because there are no evident early-stage signs, dental caries frequently goes untreated. Meanwhile, early detection and timely clinical intervention are required to slow disease development. Machine learning (ML) models can benefit clinicians in the early detection of dental cavities through efficient and cost-effective computer-aided diagnoses. This study proposed a more effective method for diagnosing dental caries by integrating the GINI and mRMR algorithms with the GBDT classifier. Because just a few clinical test features are required for the diagnosis, this strategy could save time and money when screening for dental caries. The proposed method was compared to recently proposed dental procedures. Among these classifiers, the suggested GBDT trained with a reduced feature set achieved the best classification performance, with accuracy, F1-score, precision, and recall values of 95%, 93%, 99%, and 88%, respectively. Furthermore, the experimental results suggest that feature selection improved the performance of the various classifiers. The suggested method yielded a good predictive model for dental caries diagnosis, which might be used in more imbalanced medical datasets to identify disease more effectively.",Not About Sufficiency
Landscape and climatic factors shaping mosquito abundance and species composition in southern Spain: A machine learning approach to the study of vector ecology,"Vector-borne diseases pose significant challenges to public health, with mosquitoes acting as crucial vectors for pathogens globally. This study explores the interaction between environmental and climate factors, investigating their influence on the abundance and species composition of mosquitoes in southwestern Spain, a region endemic to several mosquito-borne diseases. Using comprehensive field data from 2020, we analysed mosquito abundance and species richness alongside remote sensing variables and modeling techniques, including the machine learning Random Forest. We collected 5859 female mosquitoes representing 13 species. Non-linear correlations were observed between mosquito abundance and climatic variables, notably temperature and rainfall. Extremely high temperatures correlated with a decrease in mosquito abundance, while accumulated rainfall in the three weeks preceding sampling positively impacted mosquito abundance by providing breeding habitats. A positive correlation between Normalized Difference Vegetation Index (NDVI) and mosquito metrics was also found, aligning with prior studies highlighting vegetation's role shaping mosquito habitats. Interestingly, a negative relationship was observed between mosquito species richness and autumn NDVI. Additionally, wind speed negatively affected mosquito species richness. This research provides valuable insights into the ecological determinants of mosquito abundance and species composition in a Mediterranean climate. These findings are crucial for understanding disease transmission dynamics and improving vector control strategies. By integrating climatic characteristics into public health interventions, management measures can become more targeted and efficient, especially during periods of heightened temperature.",Not About Sufficiency
Research Progress on Metallic Heterostructure Materials,"Heterostructured materials (HS) are a new type of novel materials that overcome the strength-conductivity trade-off due to significant differences in mechanical and physical properties between heterogeneous regions, resulting in a synergistic strengthening mechanism. This review article starts with the classification of heterostructured materials, which are divided into gradient nanograined materials, layered heterogeneous materials, and bimodal structural materials. The strengthening mechanisms of heterostructured materials are analyzed, including gradient plasticity mechanism, heterogeneous deformation-induced (HDI) strengthening and hardening mechanism, and the ""shear band"" strengthening mechanism. The HDI mechanism is attributed to the accumulation of geometrically necessary dislocations in the soft region at the grain boundary. The fatigue properties and formability of HS have been emphatically reviewed. Additionally, the combination of simulation and machine learning with materials genomics has promoted the rapid development of heterostructured materials, providing guidance for the prediction and design of high-strength and high-toughness heterostructured materials.",Not About Sufficiency
"Active Mobility: Bringing Together Transport Planning, Urban Planning, and Public Health","Active mobility is related to various positive effects and is promoted in urban planning, transport planning, and in public health. The goals of these three disciplines differ in many respects but have a strong overlap in the ambition to foster activemobility. Until now, efforts for strengthening activemobility have typically not been combined, but rather promoted separately within each discipline. This paper presents a review of research on determinants and impacts of active mobility and of policy measures for supporting active mobility, including the three disciplines of transport planning, urban planning, and public health. The paper further shows the different perspectives and ambitions of the three disciplines and, simultaneously, the substantial synergies that can be gained from an interdisciplinary collaboration in research and practice.",Not About Sufficiency
A methodology for planning a new urban car sharing system with fully automated personal vehicles,"Purpose The paper concerns a transport system for pedestrian areas, based on a fleet of fully-automated Personal Intelligent Accessible Vehicles. The following services are provided: instant access, open ended reservation and one way trips. All these features provide users with high flexibility, but create a problem of uneven distribution of vehicles among the stations. A fully vehicle based relocation strategy is proposed: when a relocation is required vehicles automatically move among stations. The paper focuses on a methodology that allows to plan the proposed transport system for wide pedestrian areas. The methodology aims to determine the fleet dimension and the relocation strategy parameters which minimize the system cost. The system cost takes into account the level of service and the efficiency. Relocation strategy parameters define when and among which stations relocations should be performed. Methods The problem faced is an optimisation problem where the search space is defined by all the possible fleet dimensions and relocation strategy parameters. As this cost function could be a multipeak function and since the search space is discrete and extremely large, a random search algorithm has been adopted. Because of the characteristics of the problem, a parallel optimization technique was required. Given a fleet dimension and relocation strategy parameters, a microsimulator models the activity of each user, as well as the activity of each vehicle over time with the aim of finding the level of service and the system efficiency. Results, conclusions and application The methodology has been applied to planning the proposed transport system for the centre of Barreiro, Portugal.",Not About Sufficiency
Exposing Public Health Surveillance Data Using Existing Standards,"With the growing use of information technologies, an increased volume of data is produced in Public Health Surveillance, enabling utilization of new data sources and analysis methods. Public health and research will benefit from the use of data standards promoting harmonization and data description through metadata. No data standard has yet been universally accepted for exchanging public health data. In this work, we implemented two existing standards eligible to expose public health data: Statistical Data and Metadata Exchange Health Domain (SDMX-HD) proposed by the World Health Organization and Open Data Protocol (OData) proposed by Microsoft Corp. SDMX-HD promotes harmonization through controlled vocabulary and predefined data structure suitable for public health but requires important investment, while OData, a generic purpose standard, proposes a simple way to expose data with minimal documentation and end-user integration tools. The two solutions were implemented and are publicly available at http://sdmx.sentiweb.fr and http://odata.sentiweb.fr. These solutions show that data sharing and interoperability are already possible in Public Health Surveillance.",Not About Sufficiency
Individual identity information persists in learned calls of introduced parrot populations,"Author summaryIn some avian and mammalian lineages, vocal communication partially depends on social learning. Learned vocalizations may carry information important to communicate to others, including individual identity or group membership. The information encoded in learned vocalizations may change under different social conditions, such as the number of individuals available for social interactions. We used populations of monk parakeets introduced to the United States of America as a natural experiment of social disruption. We tested the ideas that the type of identity information encoded in learned vocalizations could either remain the same or change in introduced populations compared to native range populations in Uruguay. Using computational approaches, we quantified patterns of acoustic variation linked to identity information in learned vocalizations of native and introduced range populations. We found that individual identity information was more pronounced than group membership in learned vocalizations in each of the native and introduced ranges. The type of identity information important for monk parakeets to communicate appears to have remained the same despite social disruption that occurred over the last 50 years. While socially learned traits are considered very flexible, our findings suggest that the type of identity information encoded in learned vocalizations can be robust to population disruption over cultural timescales. Animals can actively encode different types of identity information in learned communication signals, such as group membership or individual identity. The social environments in which animals interact may favor different types of information, but whether identity information conveyed in learned signals is robust or responsive to social disruption over short evolutionary timescales is not well understood. We inferred the type of identity information that was most salient in vocal signals by combining computational tools, including supervised machine learning, with a conceptual framework of ""hierarchical mapping"", or patterns of relative acoustic convergence across social scales. We used populations of a vocal learning species as a natural experiment to test whether the type of identity information emphasized in learned vocalizations changed in populations that experienced the social disruption of introduction into new parts of the world. We compared the social scales with the most salient identity information among native and introduced range monk parakeet (Myiopsitta monachus) calls recorded in Uruguay and the United States, respectively. We also evaluated whether the identity information emphasized in introduced range calls changed over time. To place our findings in an evolutionary context, we compared our results with another parrot species that exhibits well-established and distinctive regional vocal dialects that are consistent with signaling group identity. We found that both native and introduced range monk parakeet calls displayed the strongest convergence at the individual scale and minimal convergence within sites. We did not identify changes in the strength of acoustic convergence within sites over time in the introduced range calls. These results indicate that the individual identity information in learned vocalizations did not change over short evolutionary timescales in populations that experienced the social disruption of introduction. Our findings point to exciting new research directions about the robustness or responsiveness of communication systems over different evolutionary timescales.",Not About Sufficiency
Analysis of surface urban heat islands based on local climate zones via spatiotemporally enhanced land surface temperature,"Surface Urban heat island (SUHI) is a major adverse environmental consequence of urbanization. Many algo-rithms measuring SUHI across varying spatial or temporal scales are developed rapidly with the availability of thermal infrared (TIR) remote sensing data from satellites. However, the trade-off between the spatial and temporal resolution of TIR sensors limits the study of SUHI on both spatial and temporal domains. Therefore, this study aims to improve the characterization of SUHI using spatiotemporally enhanced land surface temperature (LST) derived from the synergistic use of coarse-and fine-spatial-resolution TIR data. Combining the spatial downscaling and temporal interpolation techniques, we generated daily 100 m-resolution LST in both daytime and nighttime to analyze the SUHI in different local climate zones (LCZs) in Beijing. LCZ is a manifestation of the urban form on the thermal environment, covering hundreds of meters to several kilometers in horizontal scale. The results indicate the spatiotemporally enhanced LST is reliable in capturing the LCZ-based SUHI magnitude compared with original observations, and providing a more accurate time range when the SUHI reaches to its maximum compared with those time-discontinuous original observations. Compared with temporally interpo-lated coarse-resolution LST, the spatiotemporally enhanced LST shows a larger annual variation of SUHI (especially in LCZ 2 with a mean absolute SUHI difference of 0.8 K and 1.3 K for daytime and nighttime, respectively) and provides larger SUHI difference between compact building and open building (especially when there is a significant SUHI effect). The superiority of the spatiotemporally enhanced LSTs in analyzing LCZ-based SUHI is more evident in daily and monthly SUHI analysis than in single-day analysis or annual analysis, espe-cially in compact building types (LCZ 1 and 2). These findings are valuable information for better and healthier urban planning for SUHI mitigation and public health care.",Not About Sufficiency
"The Role of Community Resilience in Adaptation to Climate Change: The Urban Poor in Jakarta, Indonesia","Megacities are particularly vulnerable to the future challenges of population growth and climate change. The majority of people living in megacities are poor and reside within slums. Rather than seeing slums as a problem, though, this paper suggests that they be considered as a solution to the challenges arising from mega-urban complexity. Results based on a case study of Jakarta show that the urban poor have developed forms of resilience that are mainly based on collective action, or so-called social capital. At the same time, empirical data show that technocratic approaches to urban planning are unable to keep up with the dynamic nature of urban sprawl. Instead of increasing the transfer of expert knowledge and technology to megacities, this paper suggests that urban planning should start from solutions already developed on the ground. Promoting a citizen perspective might also be more efficient in economic terms.",Not About Sufficiency
QUATgo: Protein quaternary structural attributes predicted by two-stage machine learning approaches with heterogeneous feature encoding,"Many proteins exist in natures as oligomers with various quaternary structural attributes rather than as single chains. Predicting these attributes is an essential task in computational biology for the advancement of proteomics. However, the existing methods do not consider the integration of heterogeneous coding and the accuracy of subunit categories with limited data. To this end, we proposed a tool that can predict more than 12 subunit protein oligomers, QUATgo. Meanwhile, three kinds of sequence coding were used, including dipeptide composition, which was used for the first time to predict protein quaternary structural attributes, and protein half-life characteristics, and we modified the coding method of the functional domain composition proposed by predecessors to solve the problem of large feature vectors. QUATgo solves the problem of insufficient data for a single subunit using a two-stage architecture and uses 10-fold cross-validation to test the predictive accuracy of the classifier. QUATgo has 49.0% cross-validation accuracy and 31.1% independent test accuracy. In the case study, the accuracy of QUATgo can reach 61.5% for predicting the quaternary structure of influenza virus hemagglutinin proteins. Finally, QUATgo is freely accessible to the public as a web server via the site http://predictor.nchu.edu.tw/QUATgo.",Not About Sufficiency
Harnessing heterogeneity in space with statistically guided meta-learning,"Spatial data are ubiquitous, massively collected, and widely used to support critical decision-making in many societal domains, including public health (e.g., COVID-19 pandemic control), agricultural crop monitoring, transportation, etc. While recent advances in machine learning and deep learning offer new promising ways to mine such rich datasets (e.g., satellite imagery, COVID statistics), spatial heterogeneity-an intrinsic characteristic embedded in spatial data-poses a major challenge as data distributions or generative processes often vary across space at different scales, with their spatial extents unknown. Recent studies (e.g., SVANN, spatial ensemble) targeting this difficult problem either require a known space-partitioning as the input, or can only support very limited number of partitions or classes (e.g., two) due to the decrease in training data size and the complexity of analysis. To address these limitations, we propose a model-agnostic framework to automatically transform a deep learning model into a spatial-heterogeneity-aware architecture, where the learning of arbitrary space partitionings is guided by a learning-engaged generalization of multivariate scan statistic and parameters are shared based on spatial relationships. Moreover, we propose a spatial moderator to generalize learned space partitionings to new test regions. Finally, we extend the framework by integrating meta-learning-based training strategies into both spatial transformation and moderation to enhance knowledge sharing and adaptation among different processes. Experiment results on real-world datasets show that the framework can effectively capture flexibly shaped heterogeneous footprints and substantially improve prediction performances.",Not About Sufficiency
Differential Impact of a Plan-Led Standardized Complex Care Management Intervention on Subgroups of High-Cost High-Need Medicaid Patients,"Interventions to better coordinate care for high-need high-cost (HNHC) Medicaid patients frequently fail to demonstrate changes in hospitalizations or emergency department (ED) use. Many of these interventions are modeled after practice-level complex care management (CCM) programs. The authors hypothesized that a national CCM program may be effective for some subgroups of HNHC patients, and the overall null effect may obfuscate subgroup-level impact. They used a previously published typology defining 6 subgroups of high-cost Medicaid patients and evaluated program impact by subgroup. The analysis used an individual-level interrupted time series with a comparison group. Intervention subjects were high-cost adult Medicaid patients who enrolled in 1 of 2 national CCM programs implemented by UnitedHealthcare (UHC) (n = 39,687). The comparators were patients who met CCM program criteria but were ineligible due to current enrollment in another UHC/Optum led program (N = 26,359). The intervention was a CCM program developed by UHC/Optum to provide ""whole person care"" delivering standardized interventions to address medical, behavioral, and social needs for HNHC Medicaid patients, and the outcome was probability of hospitalization or ED use in a given month, estimated at 12 months postenrollment. A reduction in risk of ED utilization for 4 of 6 subgroups was found. A reduction in risk of hospitalization for 1 of 6 subgroups was also found. The authors conclude that standardized health plan led CCM programs demonstrate effectiveness for certain subgroups of HNHC patients in Medicaid. This effectiveness is principally in reducing ED risk and may extend to the risk of hospitalization for a small number of patients.",Not About Sufficiency
Applications of machine learning techniques to predict filariasis using socio-economic factors,"Filariasis is one of the major public health concerns in India. Approximately 600 million people spread across 250 districts of India are at risk of filariasis. To predict this disease, a pilot scale study was carried out in 30 villages of Karimnagar district of Telangana from 2004 to 2007 to collect epidemiological and socio-economic data. The collected data are analysed by employing various machine learning techniques such as Naive Bayes (NB), logistic model tree, probabilistic neural network, J48 (C4.5), classification and regression tree, JRip and gradient boosting machine. The performances of these algorithms are reported using sensitivity, specificity, accuracy and area under ROC curve (AUC). Among all employed classification methods, NB yielded the best AUC of 64% and was equally statistically significant with the rest of the classifiers. Similarly, the J48 algorithm generated 23 decision rules that help in developing an early warning system to implement better prevention and control efforts in the management of filariasis.",Not About Sufficiency
"Development of an Unified Food Composition Database for the European Project ""Stance4Health""","The European Commission funded project Stance4Health (S4H) aims to develop a complete personalised nutrition service. In order to succeed, sources of information on nutritional composition and other characteristics of foods need to be as comprehensive as possible. Food composition tables or databases (FCT/FCDB) are the most commonly used tools for this purpose. The aim of this study is to describe the harmonisation efforts carried out to obtain the Stance4Health FCDB. A total of 10 FCT/FCDB were selected from different countries and organizations. Data were classified using FoodEx2 and INFOODS tagnames to harmonise the information. Hazard analysis and critical control points analysis was applied as the quality control method. Data were processed by spreadsheets and MySQL. S4H's FCDB is composed of 880 elements, including nutrients and bioactive compounds. A total of 2648 unified foods were used to complete the missing values of the national FCDB used. Recipes and dishes were estimated following EuroFIR standards via linked tables. S4H's FCDB will be part of the smartphone app developed in the framework of the Stance4Health European project, which will be used in different personalized nutrition intervention studies. S4H FCDB has great perspectives, being one of the most complete in terms of number of harmonized foods, nutrients and bioactive compounds included.",Not About Sufficiency
The Enigma Unveiled: How AI Compromises Free Will in Decision-Making,"This paper philosophically explores the impact of artificial intelligence (AI) on free will. As organizations increasingly integrate AI into various aspects of their operations and individuals rely on AI-powered tools and services, the question arises: has AI taken over our free will? This paper examines the concept of free will and its determining factors from philosophical perspectives. It explores how the introduction of AI compromises free will based on these factors. The paper discusses how AI influences decision-making processes, manipulates emotions, and lacks the responsiveness to reasons that humans possess. It also considers the potential consequences of AI on societal inequalities and injustices. The research acknowledges the ongoing debate and does not provide definitive solutions but presents arguments supporting the notion that AI compromises free will. Understanding these implications is crucial for safeguarding autonomy, ethical considerations, democratic principles, and societal well-being.",Not About Sufficiency
Evaluating a treatment selection approach for online single-session interventions for adolescent depression,"BackgroundThe question 'what works for whom' is essential to mental health research, as matching individuals to the treatment best suited to their needs has the potential to maximize the effectiveness of existing approaches. Digitally administered single-session interventions (SSIs) are effective means of reducing depressive symptoms in adolescence, with potential for rapid, large-scale implementation. However, little is known about which SSIs work best for different adolescents. ObjectiveWe created and tested a treatment selection algorithm for use with two SSIs targeting depression in high-symptom adolescents from across the United States. MethodsUsing data from a large-scale RCT comparing two evidence-based SSIs (N = 996; ClinicalTrials.gov: NCT04634903), we utilized a Personalized Advantage Index approach to create and evaluate a treatment-matching algorithm for these interventions. The two interventions were Project Personality (PP; N = 482), an intervention teaching that traits and symptoms are malleable (a 'growth mindset'), and the Action Brings Change Project (ABC; N = 514), a behavioral activation intervention. ResultsResults indicated no significant difference in 3-month depression outcomes between participants assigned to their matched intervention and those assigned to their nonmatched intervention. The relationship between predicted response to intervention (RTI) and observed RTI was weak for both interventions (r = .39 for PP, r = .24 for ABC). Moreover, the correlation between a participants' predicted RTI for PP and their predicted RTI for ABC was very high (r = .79). ConclusionsThe utility of treatment selection approaches for SSIs targeting adolescent depression appears limited. Results suggest that both (a) predicting RTI for SSIs is relatively challenging, and (b) the factors that predict RTI for SSIs are similar regardless of the content of the intervention. Given their overall effectiveness and their low-intensity, low-cost nature, increasing youths' access to both digital SSIs may carry more public health utility than additional treatment-matching efforts.",Not About Sufficiency
"What explains variation in neighborhood evictions? Investigation of neighborhood characteristics and federal rental assistance: Case study of Kansas City, Missouri","Housing evictions occur throughout the United States but are unevenly distributed within cities and counties. Evictions are more common in some neighborhoods than in others. This study explores why evictions are more prevalent in certain neighborhoods than in others. First, it explores what neighborhood attributes explain the varying prevalence of evictions and eviction filings. Second, it investigates whether federal rental housing assistance reduces neighborhood evictions or eviction filings. Using the Kernel-Based Regularized Least Squares method suitable for exploratory analysis, this study examines the variation in evictions and eviction filings across Kansas City, Missouri, neighborhoods, between 2010 and 2016. This study finds that neighborhood racial composition (percentage of Black rental households) and family composition (percentage of rental households with children) are the strongest predictors of neighborhood evictions and eviction filings. This study finds that neighborhood housing insecurity and gentrification do not significantly affect the prevalence of evictions and eviction filings when other neighborhood characteristics are held constant. Variations in evictions and eviction filings are due to several neighborhood characteristics, which have nonlinear and heterogeneous effects on evictions and eviction filings. It also finds that federal rental housing assistance significantly reduces neighborhood evictions and eviction filings.",Not About Sufficiency
"Digital Twins for Engineering Asset Management: Synthesis, Analytical Framework, and Future Directions","Effective engineering asset management (EAM) is critical to economic development and improving livability in society, but its complexity often impedes optimal asset functionalities. Digital twins (DTs) could revolutionize the EAM paradigm by bidirectionally linking the physical and digital worlds in real time. There is great industrial and academic interest in DTs for EAM. However, previous review studies have predominately focused on technical aspects using limited life-cycle perspectives, failing to holistically synthesize DTs for EAM from the managerial point of view. Based on a systematic literature review, we introduce an analytical framework for describing DTs for EAM, which encompasses three levels: DT 1.0 for technical EAM, DT 2.0 for technical-human EAM, and DT 3.0 for technical-environmental EAM. Using this framework, we identify what is known, what is unknown, and future directions at each level. DT 1.0 addresses issues of asset quality, progress, and cost management, generating technical value. It lacks multi-objective self-adaptive EAM, however, and suffers from high application cost. It is imperative to enable closed-loop EAM in order to provide various functional services with affordable DT 1.0. DT 2.0 accommodates issues of human-machine symbiosis, safety, and flexibility management, generating managerial value beyond the technical performance improvement of engineering assets. However, DT 2.0 currently lacks the automation and security of human-machine interactions and the managerial value related to humans is not prominent enough. Future research needs to align technical and managerial value with highly automated and secure DT 2.0. DT 3.0 covers issues of participatory governance, organization management, sustainable development, and resilience enhancement, generating macro social value. Yet it suffers from organizational fragmentation and can only address limited social governance issues. Numerous research opportunities exist to coordinate different stakeholders. Similarly, future research opportunities exist to develop DT 3.0 in a more open and complex system. (c) 2024 THE AUTHORS. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",Not About Sufficiency
ViraLM: empowering virus discovery through the genome foundation model,"Motivation Viruses, with their ubiquitous presence and high diversity, play pivotal roles in ecological systems and public health. Accurate identification of viruses in various ecosystems is essential for comprehending their variety and assessing their ecological influence. Metagenomic sequencing has become a major strategy to survey the viruses in various ecosystems. However, accurate and comprehensive virus detection in metagenomic data remains difficult. Limited reference sequences prevent alignment-based methods from identifying novel viruses. Machine learning-based tools are more promising in novel virus detection but often miss short viral contigs, which are abundant in typical metagenomic data. The inconsistency in virus search results produced by available tools further highlights the urgent need for a more robust tool for virus identification.Results In this work, we develop ViraLM for identifying novel viral contigs in metagenomic data. By using the latest genome foundation model as the backbone and training on a rigorously constructed dataset, the model is able to distinguish viruses from other organisms based on the learned genomic characteristics. We thoroughly tested ViraLM on multiple datasets and the experimental results show that ViraLM outperforms available tools in different scenarios. In particular, ViraLM improves the F1-score on short contigs by 22%.Availability and implementation The source code of ViraLM is available via: https://github.com/ChengPENG-wolf/ViraLM.",Not About Sufficiency
A comprehensive review of standards and best practices for utility grid integration with electric vehicle charging stations,"The present work presents a comprehensive state-of-the-art bibliographical review of standards related to utility grid integration and best practices of the electric vehicle (EV) charging stations. The presence of a robust tuning method is essential for successful utility grid integration with the charging stations. The lack of system standardization may hamper the EV uptake as well as successful utility grid integration with the charging stations. The distributed energy resources (DER) and vehicle to grid (V2G) are going to play a vital role in the power system operation and control. The applicability of criterions within the utility grid integration with charging station area is important to the grid operators, charging service providers, manufacturers, fleet operators, and so forth to ensure safety, dependability, and interoperability. Hence, this work tries to deliver a comprehensive and systematic review of standards and best practices for utility grid interaction with charging stations. It will help the specialists of power as well as transport sectors to track down every one of the norms and best practices, which are accessible at one stage to compare different guidelines. This article is categorized under: Energy Infrastructure > Systems and Infrastructure Wind Power > Systems and Infrastructure Fossil Fuels > Economics and Policy Fossil Fuels > Systems and Infrastructure",Not About Sufficiency
"Telemedicine adoption before, during, and after COVID-19: The role of socioeconomic and built environment variables","The COVID-19 pandemic has led to a significant shift in healthcare delivery, with telemedicine emerging as an important additional service provision channel. This study introduces a novel methodological framework, combining a multiperiod multivariate binary probit (MBP) system and a cross-sectional MBP system, to investigate telemedicine adoption trends, as well as the facilitators and deterrents of adoption. The analysis utilizes data from a three-wave COVID Future Survey (April 2020-November 2021), supplemented by population density and healthcare-related establishment data. The results reveal a generational digital divide, with older adults exhibiting lower adoption rates due to technological barriers and preferences for traditional healthcare interactions. The study also highlights the role of the presence of children, income, transportation access, employment status, and residential location characteristics in telemedicine adoption. Notably, individuals without vehicle access or living in areas with lower geographic accessibility to healthcare providers are more likely to adopt telemedicine, suggesting its potential to reduce healthcare access disparities. The analysis of telemedicine facilitators and deterrents underscores the importance of accessibility, lifestyle preferences, privacy and security issues, technological confidence, and mobility constraints. The study provides valuable insights into policy implications across the public health, telecommunication, transportation, and urban planning sectors.",Not About Sufficiency
Text Reader for Visually Impaired People: ANY READER,"Smartphones have been the most appreciated utility device by humanity as it continues to ease our day to day problems with innovative technological solutions. However, there has been a steady growth in using smartphones substantially to address health issues. Visually impaired often find it difficult to cope up with the modern society in many spheres of life. Even though most of the research labs and a few start-ups have come up with advanced devices to assist the visually impaired people but only a handful are affordable to most of the people due to higher prices and lower availability widely. Thus, to address the problem, this paper provides a cost effective solution to help the visually impaired people in reading out texts easily with full depth analysis of the textual content through an android application - ANY READER. The application notifies the user about the headings, subheadings, and paragraphs in the text through voice notifications. Moreover, the application is optimally designed to run from mid-range to high-end android phones which make it readily usable and accessible to various kinds of users irrespective of their possessed smartphones.",Not About Sufficiency
Analysis and enhanced prediction of the Spanish Electricity Network through Big Data and Machine Learning techniques,"Electricity demand is shown to steadily increase in the last few years, and it is one of the key aspects of living standards and quantifying welfare effects. However, the irregularity of electricity demand is one of the main problems in this field. Therefore, it is important to accurately anticipate future expenditures in order to optimize energy generation and to avoid unexpected wastes. As a result, we developed Machine Learning models to predict electricity demand. In particular, our study has been performed using data of the Spanish Electricity Network from 2007 to 2019. To this end, we propose the implementation of a set of Machine Learning techniques using various frameworks. In particular, we implemented six different prediction models: Linear Regression, Regression Trees, Gradient Boosting Regression, Random Forests, Multi-layer Perceptron, and three types of recurrent neural networks. Our experimentation shows promising results in all cases, since our models provides better prediction than the one estimated by the Spanish Electricity Network with an improvement of 12% in the worst case and up to 37% for the best predictor, which turned out to be the Gated Recurrent Unit neural network. (C) 2021 Elsevier Inc. All rights reserved.",Not About Sufficiency
Assumptions of the Energy Policy of the Country versus Investment Outlays Related to the Purchase of Alternative Fuels: Poland as a Case Study,"Nowadays, the importance of activities for the protection of the environment is growing. This approach has a major impact on the current energy and mining policy in Poland. On the one hand, the energy policy has imposed several restrictions to which the Polish economy will have to adapt; on the other, however, it raises great social opposition from professional groups that will be at risk of changing or losing their jobs and income, which implies extensive restructuring processes. These processes involve the decarbonisation of the economy and include, among others: sustainable production and consumption, sustainable municipal management and high quality of life in the city, waste management, sustainable transport, and energy management. The aim of the article is to indicate the importance of investment outlays and costs incurred when purchasing alternative fuels that would replace hard coal in Poland. It is part of the process of adjusting to the requirements of the new energy policy adopted by Poland as an EU member. In order to ensure energy security by abandoning coal mining, disproportionately high investment outlays for such a transformation would have to be incurred, as well as significant resources that would have to be allocated in the future to the purchase of alternative fuels. The result of the scenario methods used is the proposition of the proprietary RCAES index, which is to facilitate the transition from fossil fuels to alternative fuels, which the authors will fill in the gap existing in this area.",Not About Sufficiency
Prediction of absenteeism in public schools teachers with machine learning,"OBJECTIVE: To predict the risk of absence from work due to morbidities of teachers working in early childhood education in the municipal public schools, using machine learning algorithms. METHODS: This is a cross-sectional study using secondary, public and anonymous data from the Relacao Anual de Informacoes Sociais, selecting early childhood education teachers who worked in the municipal public schools of the state of Sao Paulo between 2014 and 2018 (n = 174,294). Data on the average number of students per class and number of inhabitants in the municipality were also linked. The data were separated into training and testing, using records from 2014 to 2016 (n = 103,357) to train five predictive models, and data from 2017 to 2018 (n = 70,937) to test their performance in new data. The predictive performance of the algorithms was evaluated using the value of the area under the ROC curve (AUROC). RESULTS: All five algorithms tested showed an area under the curve above 0.76. The algorithm with the best predictive performance (artificial neural networks) achieved 0.79 of area under the curve, with accuracy of 71.52%, sensitivity of 72.86%, specificity of 70.52%, and kappa of 0.427 in the test data. CONCLUSION: It is possible to predict cases of sickness absence in teachers of public schools with machine learning using public data. The best algorithm showed a better result of the area under the curve when compared with the reference model (logistic regression). The algorithms can contribute to more assertive predictions in the public health and worker health areas, allowing to monitor and help prevent the absence of these workers due to morbidity.",Not About Sufficiency
Systematic SLA Data Management,"The cloud computing paradigm emerged with service oriented principles. In the cloud setting, organizations out source their IT equipment and manage their business processes through virtual services that are typically exchanged over HTTP. Service Level Agreements (SLAs) depict the status of running services. SLAs represent operational contracts that allow providers to estimate their service availability according to their resource capacity. The SLA data schema and content are operationally defined by the type, volume and relations of service elements that organizations operate on their physical resources. Current lack of a uniform SLA standardization leads to semantic and operational differences between SLAs, that are produced and consumed by different organizations. Such differences prohibit common business SLA practices in the cloud computing domain. Our research introduces systematic SLA data management to describe the formalization, storage and processing of SLAs over distributed computing environments. Services in scope are framed within the cloud computing context.",Not About Sufficiency
Role model advocacy for sustainable transport,"Individual aspirations of associating with role models are routinely harnessed by marketers, who for instance, use celebrity endorsement in selling brands and products. It appears there has been no research to date, however, on the potential for celebrity activism, or role model advocacy beyond celebrities, such as from politicians, to form effective interventions for encouraging sustainable transport behavior. This is despite studies suggesting that celebrity endorsement is a potential gateway for transforming public opinion on carbon intensive transport modes. The present paper consequently offers a critical review of the literature on role model advocacy and celebrity activism, and how these concepts have been harnessed to address environmental issues, in order to conceptually assess the potential for extending these intervention techniques to the context of sustainable transport. The scope of the paper includes the potential that high profile politicians/celebrities might play as role models in exercising referent power to influence social norms surrounding sustainable transport, given that the success of social marketing interventions are closely tied to the need for changes in the policy landscape. Key dimensions of role model endorsement in transport are identified and applied to a series of examples of how celebrity and political role models have influenced transport cultures. In addition to offering an original application of a theoretical framework to a new context, in order to help address the increasingly important societal issue of transport's growing contribution to climate change, the paper discusses the challenges associated with the neoliberal framing of this approach.",Not About Sufficiency
Individualized prediction of chronic kidney disease for the elderly in longevity areas in China: Machine learning approaches,"Background: Chronic kidney disease (CKD) has become a major public health problem worldwide and has caused a huge social and economic burden, especially in developing countries. No previous study has used machine learning (ML) methods combined with longitudinal data to predict the risk of CKD development in 2 years amongst the elderly in China. Methods: This study was based on the panel data of 925 elderly individuals in the 2012 baseline survey and 2014 follow-up survey of the Healthy Aging and Biomarkers Cohort Study (HABCS) database. Six ML models, logistic regression (LR), lasso regression, random forests (RF), gradient-boosted decision tree (GBDT), support vector machine (SVM), and deep neural network (DNN), were developed to predict the probability of CKD amongst the elderly in 2 years (the year of 2014). The decision curve analysis (DCA) provided a range of threshold probability of the outcome and the net benefit of each ML model. Results: Amongst the 925 elderly in the HABCS 2014 survey, 289 (18.8%) had CKD. Compared with the other models, LR, lasso regression, RF, GBDT, and DNN had no statistical significance of the area under the receiver operating curve (AUC) value (>0.7), and SVM exhibited the lowest predictive performance (AUC = 0.633, p-value = 0.057). DNN had the highest positive predictive value (PPV) (0.328), whereas LR had the lowest (0.287). DCA results indicated that within the threshold ranges of similar to 0-0.03 and 0.37-0.40, the net benefit of GBDT was the largest. Within the threshold ranges of similar to 0.03-0.10 and 0.26-0.30, the net benefit of RF was the largest. Age was the most important predictor variable in the RF and GBDT models. Blood urea nitrogen, serum albumin, uric acid, body mass index (BMI), marital status, activities of daily living (ADO/instrumental activities of daily living (IADL) and gender were crucial in predicting CKD in the elderly. Conclusion: The ML model could successfully capture the linear and nonlinear relationships of risk factors for CKD in the elderly. The decision support system based on the predictive model in this research can help medical staff detect and intervene in the health of the elderly early.",Not About Sufficiency
A Systematic Review of Factors Influencing the Vitality of Public Open Spaces: A Novel Perspective Using Social-Ecological Model (SEM),"A number of studies address the spatial planning, architectural design, and management of public open spaces (POSs) to curb the overuse of spaces to create high-quality spaces. Little attention has been paid to the problem of underutilization of POSs. Therefore, this paper undertakes a comprehensive analysis of the literature on the factors that influence the vitality of POSs, proposing Bronfenbrenner's social-ecological model (SEM) as a conceptual framework. In this work, we conducted a systematic literature search using the PRISMA method to screen and select articles from three major databases (Science Web, Elsevier, and Scopus). Thirty-four journal articles from 2000 to 2022 were selected for the final review. This study systematically identifies and classifies a set of variables related to the vitality of POSs and develops an SEM-based framework of factors that influence the vitality of POSs. The framework examines the influence of individual user characteristics, the social environment, the physical environment, and the political environment on the vitality of POSs. Finally, strategies to improve the vitality of POSs are proposed: (1) POSs' optimization and promotion strategies should be developed gradually, starting from the most basic needs, stage by stage; (2) To improve the vitality of POSs, we should consider both the general public and special groups; (3) Through the synergistic effect between social, material, and policy environments, the comprehensive improvement of POSs' vitality is achieved. This study provides the latest insights into the vitality of POSs and makes a theoretical contribution by conceptualizing the SEM framework and summarizing the influencing factors at different levels. The study of these factors should also have practical implications, as the results will ultimately provide improvement strategies to help policy-makers and local communities to effectively and sustainably improve the vitality of POSs.",Not About Sufficiency
"How do Twitter users feel about telehealth? A mixed-methods analysis of experiences, perceptions and expectations","BackgroundTelehealth use has increased considerably in the last years and evidence suggests an overall positive sentiment towards telehealth. Twitter has a wide userbase and can enrich our understanding of telehealth use by users expressing their personal opinions in an unprompted way. This study aimed to explore Twitter users' experiences, perceptions and expectations about telehealth over the last 5 years.MethodsMixed-methods study with sequential complementary quantitative and qualitative phases was used for analysis stages comprising (1) a quantitative semiautomated analysis and (2) a qualitative research-led thematic analysis. A machine learning model was used to establish the data set with relevant English language tweets from 1 September 2017 to 1 September 2022 relating to telehealth using predefined search words. Results were integrated at the end.ResultsFrom the initial 237,671 downloaded tweets, 6469 had a relevancy score above 0.8 and were input into Leximancer and 595 were manually analysed. Experiences, perceptions and expectations were categorised into three domains: experience with telehealth consultation, telehealth changes over time and the purpose of the appointment. The most tweeted experience was expectations for telehealth consultation in comparison to in-person consultations. Users mostly mentioned the hope that waiting times for the consultations to start to be less than in-person, more telehealth appointments to be available and telehealth to be cheaper. Perceptions around the use of telehealth in relation to healthcare delivery changes brought about by the COVID-19 pandemic were also expressed. General practitioners were mentioned six times more than other healthcare professionals.Conclusion/ImplicationsThis study found that Twitter users expect telehealth services to be better, more affordable and more available than in-person consultations. Users acknowledged the convenience of not having to travel for appointments and the challenges to adapt to telehealth.Patient or Public ContributionAn open data set with 237,671 tweets expressing users' opinions in an unprompted way was used as a source for telehealth service users, caregivers and members of the public experiences, perceptions and expectations of telehealth.",Not About Sufficiency
Smart peer-to-peer and transactive energy sharing architecture considering incentive-based demand response programming under joint uncertainty and line outage contingency,"Due to the widespread deployment of distributed energy resources, renewable energies and battery energy storage, the peer to peer (P2P) energy trading schematic has gained the staple attention for improving the energy efficiency and energy flexibility of power grids. This is while, smart demand response programming (DRP) is considered as the bridge between these two indicators of smart grid. Moreover, the subtle point of proliferating P2P schematics is the regulation towards the maximization of social welfare leading to economic profitability of customers and owner's of microgrid and, eventually, reduction of pollutant emission of fossil fuels. Also, uncertainty, aroused by electrical consumption and renewable energy resources, is the core of every considerations, which has to be dealt with intelligent algorithms for strengthening the stability of transactions. On the other hand, compatibility with upper grid's regulations, i.e. power loss and voltage deviation, along with determining fair price of energy trading are the subjects of P2P-based tactics. Therefore, this paper proposes a P2P-based transactive energy sharing architecture, as two stage mixed integer non-linear programming, using smart DRP integrated with machine learning approach, i.e. radial basis neural network. Firstly, the uncertainty of electrical demand and renewable energies are relaxed through short term forecasting. Doing so, the day ahead transactions of peers are obtained based on their energy management objective, targeting the energy reliability of customers, which energy not supplied criterion has to be equal to zero. Then, participation of customers in DRP, cost of customers, revenue of microgrid's owner and transactions of real time programming are optimally acquired based on Pareto front technique. Also, the simulations are conducted on IEEE 85 bus test system to realize the considerations. The results convey that the profitability of customers and owners is tied with the implementation of smart DRP and accurate forecasting of uncertain variables. In addition, the maximum improvements towards maximizing the revenue of owners and minimizing the cost of customers take place at hours which the electrical consumption is shifted from peaks to off-peaks and mid-peaks, certifying the performance of proposed methodology.",Not About Sufficiency
A data-driven approach to forecasting ground-level ozone concentration,"The ability to forecast the concentration of air pollutants in an urban region is crucial for decision-makers wishing to reduce the impact of pollution on public health through active measures (e.g. temporary traffic closures). In this study, we present a machine learning approach applied to forecasts of the day-ahead maximum value of ozone concentration for several geographical locations in southern Switzerland. Due to the low density of measurement stations and to the complex orography of the use-case terrain, we adopted feature selection methods instead of explicitly restricting relevant features to a neighborhood of the prediction sites, as common in spatio-temporal forecasting methods. We then used Shapley values to assess the explainability of the learned models in terms of feature importance and feature interactions in relation to ozone predictions. Our analysis suggests that the trained models effectively learned explanatory cross-dependencies among atmospheric variables. Finally, we show how weighting observations helps to increase the accuracy of the forecasts for specific ranges of ozone's daily peak values. (C) 2021 The Author(s). Published by Elsevier B.V. on behalf of International Institute of Forecasters.",Not About Sufficiency
Good results from sensor data: Performance of machine learning algorithms for regression problems in chemical sensors,"Accurately predicting unseen data, instead of mere memorization of training examples, is a critical goal of machine learning. This generalization is particularly important in the field of chemical sensors, where the ability to accurately predict the chemical properties or concentration levels of unknown samples is crucial. The paper presents a comprehensive yet accessible introduction to various machine learning concepts, highlighting the importance of model interpretability and generalization in ensuring reliable and accurate results in this context. Nonlinear sensor array data are utilized to introduce key concepts (e.g., bias-variance tradeoff) and techniques (linear models, partial least squares regression, support vector machines, k-nearest neighbors, decision trees, ensemble methods, automated machine learning, symbolic regression, and artificial neural networks), providing a solid foundation to make informed decisions when selecting machine learning techniques for sensor-specific regression applications. The results clearly indicate a number of conclusions. First, overparameterized deep feedforward neural networks show great accuracy and generalization when trained on a sufficiently large dataset. Second, symbolic regression models proved to be more accurate than deep feedforward neural networks and classical machine learning techniques on smaller datasets. Third, the performance of various machine learning models was dataset-dependent, showing the importance of comparative studies to determine the most suitable approach. It is clear that the optimal model cannot be known a priori. This paper aims to provide a starting point for investigations on the performance of different machine learning techniques in chemical sensor applications.",Not About Sufficiency
Sustainability and the Built Environment: The Role of Durability,"A sustainable city combines stable long-term economic growth with a resilient ecological system. It is also a region of social sustainability with low levels of spatial segregation of different socio-economic groups. Spatial inclusion primarily involves provision of equalized city-wide access to territorial public goods. High durability of physical networks and buildings facilitates economic, environmental and social sustainability. This study shows that durability varies considerably between Asian, European and North American cities, with mean life expectancies of buildings that range from below 20 years in Chinese cities to over 100 years in European cities such as Paris. Urban planning principles that focus on the slow and steady expansion of accessibility and density within a durable built environment are consistent with general economic equilibria, while avoiding the pitfalls of political planning of the markets for private goods.",Not About Sufficiency
Infant Low Birth Weight Prediction Using Graph Embedding Features,"Low Birth weight (LBW) infants pose a serious public health concern worldwide in both the short and long term for infants and their mothers. Infant weight prediction prior to birth can help to identify risk factors and reduce the risk of infant morbidity and mortality. Although many Machine Learning (ML) algorithms have been proposed for LBW prediction using maternal features and produced considerable model performance, their performance needs to be improved so that they can be adapted in real-world clinical settings. Existing algorithms used for LBW classification often fail to capture structural information from the tabular dataset of patients with different complications. Therefore, to improve the LBW classification performance, we propose a solution by transforming the tabular data into a knowledge graph with the aim that patients from the same class (normal or LBW) exhibit similar patterns in the graphs. To achieve this, several features related to each node are extracted such as node embedding using node2vec algorithm, node degree, node similarity, nearest neighbors, etc. Our method is evaluated on a real-life dataset obtained from a large cohort study in the United Arab Emirates which contains data from 3453 patients. Multiple experiments were performed using the seven most commonly used ML models on the original dataset, graph features, and a combination of features, respectively. Experimental results show that our proposed method achieved the best performance with an area under the curve of 0.834 which is over 6% improvement compared to using the original risk factors without transforming them into knowledge graphs. Furthermore, we provide the clinical relevance of the proposed model that are important for the model to be adapted in clinical settings.",Not About Sufficiency
Application of pulsed-field gel electrophoresis for molecular identification of pathogenic Leptospira species in Iran: a rapid and reliable method,"Background and Objectives: Leptospirosis is a zoonotic disease caused by pathogenic Leptospira serovars. The genus Leptospira cannot differentiated by conventional techniques. However, identity determination of pathogenic serovar is precious of public health problems and epidemiological studies. Pulsed-field gel electrophoresis facilitates rapid identification of Leptospires to the serovar levels. Materials and Methods: In this study, we employed PFGE to evaluate 28 Leptospira isolates, with animal, human and environmental origin, obtained from Razi Vaccine and Serum Research Institute of Karaj, Iran. PFGE patterns of 28 Leptospira serovars were generated using the Not I restriction enzyme in comparison with the lambda ladder. Results: Out of 28 serovars evaluated, we identified 22 different pulsed types, designated P1- P22. Out of 22 pulse groups, 3 were found to be a common type, but others were a single Type. Groups consisting of the common type were P3, P9, P14, and P16. The results showed that the discriminatory index of PFGE by Not I enzyme was 0.99, demonstrating heterogeneous differentiation among serovar members. Conclusion: The PFGE methodology used in this study showed excellent interlaboratory report usability, rapid, reliable, enabling standardization and data sharing between laboratories.",Not About Sufficiency
Affordable Oxygen Microscopy-Assisted Biofabrication of Multicellular Spheroids,"Multicellular spheroids are important tools for studying tissue and cancer physiology in 3D and are frequently used in tissue engineering as tissue assembling units for biofabrication. While the main power of the spheroid model is in mimicking physical-chemical gradients at the tissue microscale, the real physiological environment (including dynamics of metabolic activity, oxygenation, cell death, and proliferation) inside the spheroids is generally ignored. At the same time, the effects of the growth medium composition and the formation method on the resulting spheroid phenotype are well documented. Thus, characterization and standardization of spheroid phenotype are required to ensure the reproducibility and transparency of the research results. The analysis of average spheroid oxygenation and the value of O-2 gradients in three dimensions (3D) can be a simple and universal way for spheroid phenotype characterization, pointing at their metabolic activity, overall viability, and potential to recapitulate in vivo tissue microenvironment. The visualization of 3D oxygenation can be easily combined with multiparametric analysis of additional physiological parameters (such as cell death, proliferation, and cell composition) and applied for continuous oxygenation monitoring and/or end-point measurements. The loading of the O-2 probe is performed during the stage of spheroid formation and is compatible with various protocols of spheroid generation. The protocol includes a high-throughput method of spheroid generation with introduced red and near-infrared emitting ratiometric fluorescent O-2 nanosensors and the description of multiparameter assessment of spheroid oxygenation and cell death before and after bioprinting. The experimental examples show comparative O-2 gradients analysis in homo- and hetero-cellular spheroids as well as spheroid-based bioprinted constructs.",Not About Sufficiency
Identification of efficient algorithms for web search through implementation of learning-to-rank algorithms,"Today, amount of information on the web such as number of publicly accessible web pages, hosts and web data is increasing rapidly and exhibiting an enormous growth at an exponential rate. Thus, information retrieval on web is becoming more difficult. Conventional methods of information retrieval are not very effective in ranking since they rank the results without automatically learning the model. Machine learning domain called learning-to-rank comes to the aid to rank the obtained results. Different state-of-the-art methodologies have been developed for learning-to-rank to date. This paper focuses on finding out the best algorithm for web search by implementation of different state-of-the-art algorithms for learning-to-rank. Our work in this paper marks the implementation of learning-to-rank algorithms and analyses effect of topmost performing algorithms on respective datasets. It presents an overall review on the approaches designed under learning-to-rank and their evaluation strategies.",Not About Sufficiency
Exploratory Study of Some Machine Learning Techniques to Classify the Patient Treatment,"Numerous studies have been carried out on computation and its applications to medical data with proven benefits for improving the quality of public health. However, not all research results or practical applications can be applied to all conditions but must be in accordance with the various contexts such as community culture, geographical, or citizen behaviors. Unfortunately, the use of digital data in Indonesia is still very limited. The study objective is to assess various data mining techniques to utilize data from laboratory test results collected from a private hospital in Indonesia in predicting the next patient treatment. Furthermore, various machine learning classification techniques were explored for the purpose. Based on the experiments, it was concluded that XGBoost with hyperparameter tuning produced the best accuracy level at 0.7579, compared to other classifiers. A better level of accuracy can be obtained by enriching the type of dataset used, such as the patient's medical record history.",Not About Sufficiency
"Revolutionizing Personalized Medicine: Synergy with Multi-Omics Data Generation, Main Hurdles, and Future Perspectives","The field of personalized medicine is undergoing a transformative shift through the integration of multi-omics data, which mainly encompasses genomics, transcriptomics, proteomics, and metabolomics. This synergy allows for a comprehensive understanding of individual health by analyzing genetic, molecular, and biochemical profiles. The generation and integration of multi-omics data enable more precise and tailored therapeutic strategies, improving the efficacy of treatments and reducing adverse effects. However, several challenges hinder the full realization of personalized medicine. Key hurdles include the complexity of data integration across different omics layers, the need for advanced computational tools, and the high cost of comprehensive data generation. Additionally, issues related to data privacy, standardization, and the need for robust validation in diverse populations remain significant obstacles. Looking ahead, the future of personalized medicine promises advancements in technology and methodologies that will address these challenges. Emerging innovations in data analytics, machine learning, and high-throughput sequencing are expected to enhance the integration of multi-omics data, making personalized medicine more accessible and effective. Collaborative efforts among researchers, clinicians, and industry stakeholders are crucial to overcoming these hurdles and fully harnessing the potential of multi-omics for individualized healthcare.",Not About Sufficiency
Machine learning based models for pressure drop estimation of two-phase adiabatic air-water flow in micro-finned tubes: Determination of the most promising dimensionless feature set,"The present study is focused on determining the most promising set of dimensionless features and the optimal machine learning algorithm that can be employed for data-driven frictional pressure drop estimation of water (single-phase) and air-water mixture (twophase) flow in micro-finned horizontal tubes. Accordingly, an experimental activity is first conducted, in which the frictional pressure drop of both water and air-water flows, at various flow conditions, is measured. Next, machine learning based pipelines are developed, in which dimensionless parameters are provided as features and the friction factor (for the single-phase case) and the two-phase flow multipliers (for the two-phase case) are considered as the targets. Next, the feature selection procedure is performed, in which the most promising set of features, while employing a benchmark algorithm, is determined. An algorithm optimization procedure is then performed in order to choose the most suitable algorithm (and the corresponding tuning parameters) that lead to the highest possible accuracy. Moreover, the state-of-the-art physical models are implemented and the corresponding accuracy, while being applied to the experimentally obtained dataset, is determined. It is demonstrated that only 5 dimensionless features are selected (among 23 provided features) in the obtained pipeline developed for the estimation of the two-phase gas multiplier (in the extraction procedure of which, the single-phase friction factors are determined only using the Reynolds number and two geometrical parameters). Therefore, the latter procedures notably reduce the complexity of the model, while providing a higher accuracy (MARD of 6.72% and 7.05% on the training and test sets respectively) compared to the one achieved using the most promising available physical model (MARD of 15.21%). Finally, through implementing the forward feature combination strategy on the optimal pipeline, the contribution of each feature to the achieved accuracy is shown and the trade-off between the model's complexity (number of features) and the obtained accuracy is presented. Thus, the latter step provides the possibility of utilizing an even inferior number of features, while achieving an acceptable accuracy. Moreover, since these pipelines will be made publicly accessible, the implemented models also offer a higher reproducibility and ease of use. (c) 2021 Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved.",Not About Sufficiency
"Understanding Sustainable Transport Acceptance Behavior: A Case Study of Klang Valley, Malaysia","The objective of this study is to investigate the factors that influence public acceptance of sustainable transport in developing countries, specifically in the Klang Valley region, Malaysia. Based on the theory of planned behavior, a sustainable transport system acceptance model is developed using the structural equation modeling approach. The model reveals that the contributing factors towards public acceptance of sustainable transport include awareness, government actions, service availability, and willingness to pay. The influences of these factors vary significantly across different genders and educational groups. The findings presented in this paper provide useful insights for future sustainable transport policies in Malaysia.",Not About Sufficiency
Association between built environment factors and collective walking behavior in peri-urban area: Evidence from Chengdu,"Optimizing collective walking has been a pivotal strategy for alleviating the excessive reliance on private cars. As an important criterion for residents' social interaction and mobility vigor, collective walking describes the results of walking activities in streets and positively correlates with the built environments. However, limited attention has been given to the contribution of built environment factors in peri-urban areas towards promoting collective walking. Therefore, this study employed multisource data to assess the impact of these factors on collective walking in peri-urban areas. Our analysis delved into the impact of these factors on collective walking across various urban spaces and explored intervention effects through cross-classification studies that prioritize planning strategies. These results revealed that (1) The impacts of the built environment on collective walking behavior differs in peri-urban areas. (2) Diversity and distance only show significant impacts in peri-urban areas. (3) A distinct spatial mismatch between urban spatial form and collective walking behavior is observed. Subsequent urban design interventions should consider these spatial distribution differences for prioritized zoning planning. Our findings provided convincing evidence that delineated the impact of built environments in periurban areas that can guide government and planners to precisely design pedestrian environments through targeted intervention measures.",Not About Sufficiency
Impacts of 5G on cyber-physical risks for interdependent connected smart critical infrastructure systems,"5 G technology promises a wide range of benefits for critical infrastructure (CI), including improved reliability, increased efficiency, cost savings, and increased worker safety. However, it also brings many new risks that CI owners and operators must be prepared for to facilitate effective risk mitigation and response. These risks, however, have not been systematically assessed for CI systems. This paper investigates how the cyber-physical risk landscape will be impacted by 5 G for four major CI sectors in detail: smart transportation, smart water, smart power, and smart oil and gas networks. Compared to prior work only examining a single CI network, the authors present a comprehensive assessment of the types of threats that these sectors can expect based on past incidents, the new vulnerabilities introduced by 5 G and existing vulnerabilities exacerbated by the introduction of more connected devices, along with mitigation recommendations for each risk. Risks associated with the rollout of and transition to 5 G, risks from 5 G network disruptions, cyberattack risks, and privacy risks are included. While each of the sectors has a unique risk profile, general themes also emerged across multiple CI networks. Notably, there will be an increased number of threat vectors from smart devices reliant on the telecommunications network to provide monitoring and control of infrastructure services. Because many of these devices are accessible by the public, the risk of social engineering attacks and vulnerability to physical hacking are exacerbated. Successful risk mitigation requires collaboration among CI's many stakeholders to implement security measures at the interfaces between connected devices to limit the access to assets in case one security measure is successfully bypassed. Due to the increased interdependencies between CI networks, operators must create backup plans to keep the most essential services running on a smaller bandwidth in case of a 5 G outage or similar failure. As 5 G capabilities continue to develop and the risk landscape evolves, ongoing research is needed and CI owners and operators should be prepared to update security measures to remain ahead of identified risks and threats.",Not About Sufficiency
Attention-Based Node-Edge Graph Convolutional Networks for Identification of Autism Spectrum Disorder Using Multi-Modal MRI Data,"Autism Spectrum Disorder (ASD) is a widely prevalent neurodevelopmental disorder with symptoms of social interaction and communication problems and restrictive and repetitive behavior. In recent years, there has been increasing interest in identifying individuals with ASD patients from typical developing (TD) ones based on brain imaging data such as MRI. Although both traditional machine learning and recent deep learning methodologies have achieved promising performance, the classification accuracy is still far from satisfying due to large individual differences and/or heterogeneity among data from different sites. To help resolve the problem, we proposed a novel Attention-based Node-Edge Graph Convolutional Network (ANEGCN) to identify ASD from TD individuals. Specifically, it simultaneously models the features of nodes and edges in the graphs and combines multi-modalMRI data including structural MRI and resting state functional fMRI in order to utilize both structural and functional information for feature extraction and classification. Moreover, an adversarial learning strategy was used to enhance the model generalizability. A gradient-based model interpretability method was also applied to identify those brain regions and connections contributing to the classification. Using the worldwide Autism Brain Imaging Data Exchange I (ABIDE I) dataset with 1007 subjects from 17 sites, the proposed ANEGCN achieved superior classification accuracy (72.7%) and generalizability than other state-of-the-art models. This study provided a powerful tool for ASD identification.",Not About Sufficiency
Transforming Points of Single Contact Data into Linked Data,"Open data portals contain valuable information for citizens and business. However, searching for information can prove to be tiresome even in portals tackling domains similar information. A typical case is the information residing in the European Commission's portals supported by Member States aiming to facilitate service provision activities for EU citizens and businesses. The current work followed the FAIR principles (Findability, Accessibility, Interoperability, and Reuse of digital assets) as well as the GO-FAIR principles and tried to transform raw data into fair data. The innovative part of this work is the mapping of information residing in various governmental portals (Points of Single Contacts) by transforming information appearing in them in RDF format (i.e., as Linked data), in order to make them easily accessible, exchangeable, interoperable and publishable as linked open data. Mapping was performed using the semantic model of a single portal, i.e., the enriched Greek e-GIF ontology and by retrieving and analyzing raw, i.e., non-FAIR data, by defining the semantic model and by making data linkable. The Data mapping process proved to require a significant manual effort and revealed that data value remains unexplored due to poor data representation. It also highlighted the need for appropriately designing and implementing horizontal actions addressing an important number of recipients in an interoperable way.",Not About Sufficiency
Assessing the Policy Environment for Active Mobility in Cities-Development and Feasibility of the PASTA Cycling and Walking Policy Environment Score,"The importance of setting a policy focus on promoting cycling and walking as sustainable and healthy modes of transport is increasingly recognized. However, to date a science-driven scoring system to assess the policy environment for cycling and walking is lacking. In this study, spreadsheet-based scoring systems for cycling and walking were developed, including six dimensions (cycling/walking culture, social acceptance, perception of traffic safety, advocacy, politics and urban planning). Feasibility was tested using qualitative data from pre-specified sections of semi-standardized interview and workshop reports from a European research project in seven cities, assessed independently by two experts. Disagreements were resolved by discussions of no more than 75 minutes per city. On the dimension ""perception of traffic safety"", quantitative panel data were used. While the interrater agreement was fair, feasibility was confirmed in general. Validity testing against social norms towards active travel, modal split and network length was encouraging for the policy area of cycling. Rating the policy friendliness for cycling and walking separately was found to be appropriate, as different cities received the highest scores for each. Replicating this approach in a more standardized way would pave the way towards a transparent, evidence-based system for benchmarking policy approaches of cities towards cycling and walking.",Not About Sufficiency
"Sense of place, subjective well-being, and the influence of housing and neighbourhood: A comparative study of two marginalised districts in Hong Kong","This paper investigates the associations between the multiple dimensions of individuals' sense of place and subjective well-being. The impact of objective and subjective housing and neighbourhood attributes on sense of place was also examined. Using a questionnaire and government spatial datasets, data were collected from residents of two marginalised communities in Hong Kong, Sham Shui Po and Tin Shui Wai. The results reveal positive relationships between various facets of sense of place and subjective well-being that vary in strength in different urban forms. Among the attributes of housing and neighbourhood analysed, housing satisfaction is found to be the strongest predictor of sense of place. The study further verifies the use of a synthesised three-dimensional scale to measure sense of place. It also has important implications for urban planning policies and practices for high-density cities.",Not About Sufficiency
Integrating machine learning and artificial intelligence in life-course epidemiology: pathways to innovative public health solutions,"The integration of machine learning (ML) and artificial intelligence (AI) techniques in life-course epidemiology offers remarkable opportunities to advance our understanding of the complex interplay between biological, social, and environmental factors that shape health trajectories across the lifespan. This perspective summarizes the current applications, discusses future potential and challenges, and provides recommendations for harnessing ML and AI technologies to develop innovative public health solutions. ML and AI have been increasingly applied in epidemiological studies, demonstrating their ability to handle large, complex datasets, identify intricate patterns and associations, integrate multiple and multimodal data types, improve predictive accuracy, and enhance causal inference methods. In life-course epidemiology, these techniques can help identify sensitive periods and critical windows for intervention, model complex interactions between risk factors, predict individual and population-level disease risk trajectories, and strengthen causal inference in observational studies. By leveraging the five principles of life-course research proposed by Elder and Shanahan-lifespan development, agency, time and place, timing, and linked lives-we discuss a framework for applying ML and AI to uncover novel insights and inform targeted interventions. However, the successful integration of these technologies faces challenges related to data quality, model interpretability, bias, privacy, and equity. To fully realize the potential of ML and AI in life-course epidemiology, fostering interdisciplinary collaborations, developing standardized guidelines, advocating for their integration in public health decision-making, prioritizing fairness, and investing in training and capacity building are essential. By responsibly harnessing the power of ML and AI, we can take significant steps towards creating healthier and more equitable futures across the life course.",Not About Sufficiency
Is there an equality in the spatial distribution of urban vitality: A case study of Wuhan in China,"Urban vitality is a spatial phenomenon and a public service. Previous studies often did not measure the fairness of the accessibility of urban vitality. This research analyzed the spatial distribution of urban vitality inWuhan as a case study area using big data from multiple sources. The study used the two-step floating catchment area method (2SFCA) to measure the accessibility level of each residential district to the vibrant zones (VZs). Furthermore, the inequality in the level of accessibility of residential areas with different housing prices was assessed on the basis of the Gini coefficient. The main conclusions are as follows: (1) the Gini coefficient of reaching the accessibility level of high-grade VZs is 0.426, and the corresponding sub-high-grade Gini coefficient is 0.274. This shows that the inequality in accessibility of different residential areas is more obvious as the level of vitality is higher and(2) residential areas with high housing prices have greater accessibility than those areas with low housing prices. It was also noticed that the level of accessibility and fairness in vitality is generally higher in central urban areas than in suburban areas. The Gini coefficient of high-grade vitality is generally higher than that of sub-high-grade. The results of this study provide a reference for researching the accessibility level of urban vitality, which considers the needs of population, and can also provide guidelines in urban planning regarding the allocation of services and resources.",Not About Sufficiency
Using Blockchain Technology to Mitigate Challenges in Service Access for the Homeless and Data Exchange Between Providers: Qualitative Study,"Background: In the homeless population, barriers to housing and supportive services include a lack of control or access to data. Disparate data formats and storage across multiple organizations hinder up-to-date intersystem access to records and a unified view of an individual's health and documentation history. The utility of blockchain to solve interoperability in health care is supported in recent literature, but the technology has yet to be tested in real-life conditions encompassing the complex regulatory standards in the health sector. Objective: This study aimed to test the feasibility and performance of a blockchain system in a homeless community to securely store and share data across a system of providers in the health care ecosystem. Methods: We performed a series of platform demonstrations and open-ended qualitative feedback interviews to determine the key needs and barriers to user and stakeholder adoption. Account creation and data transactions promoting organizational efficiency and improved health outcomes in this population were tested with homeless users and service providers. Results: Persons experiencing homelessness and care organizations could successfully create accounts, grant and revoke data sharing permissions, and transmit documents across a distributed network of providers. However, there were issues regarding the security of shared data, user experience and adoption, and organizational preparedness for service providers as end users. We tested a set of assumptions related to these problems within the project time frame and contractual obligations with an existing blockchain-based platform. Conclusions: Blockchain technology provides decentralized data sharing, validation, immutability, traceability, and integration. These core features enable a secure system for the management and distribution of sensitive information. This study presents a concrete evaluation of the effectiveness of blockchain through an existing platform while revealing limitations from the perspectives of user adoption, cost-effectiveness, scalability, and regulatory frameworks.",Not About Sufficiency
e-Health: A Framework Proposal for Interoperability and Health Data Sharing. A Brazilian Case,"Interoperability among systems is a challenge that requires several regards and infrastructure often complex. The best worldwide reports and frameworks say that this can also improve health care and bring the best outcomes for stakeholders. Implementing Interoperability in developing countries is less affordable even it can also promote quality care and save lives. The best models and guidelines could offer protocols for sharing health data allowing to build a system that can deliver at the same time quality, transparency, and social value. This paper addresses an interoperability problem providing the steps built in a pilot to enable a conceptual framework for exchange healthcare data through EHR, and presents the first step and overview of a platform build using rules of PDCA. The experiment was built in a small Brazilian town intends to be a standard for deliver interaction between local government and citizens and also to offer to patients to control own medical data records through a mobile application.",Not About Sufficiency
Identification of hospitalized mortality of patients with COVID-19 by machine learning models based on blood inflammatory cytokines,"Coronavirus disease 2019 (COVID-19) spread worldwide and presented a significant threat to people's health. Inappropriate disease assessment and treatment strategies bring a heavy burden on healthcare systems. Our study aimed to construct predictive models to assess patients with COVID-19 who may have poor prognoses early and accurately. This research performed a retrospective analysis on two cohorts of patients with COVID-19. Data from the Barcelona cohort were used as the training set, and data from the Rotterdam cohort were used as the validation set. Cox regression, logistic regression, and different machine learning methods including random forest (RF), support vector machine (SVM), and decision tree (DT) were performed to construct COVID-19 death prognostic models. Based on multiple clinical characteristics and blood inflammatory cytokines during the first day of hospitalization for the 138 patients with COVID-19, we constructed various models to predict the in-hospital mortality of patients with COVID-19. All the models showed outstanding performance in identifying high-risk patients with COVID-19. The accuracy of the logistic regression, RF, and DT models is 86.96, 80.43, and 85.51%, respectively. Advanced age and the abnormal expression of some inflammatory cytokines including IFN-alpha, IL-8, and IL-6 have been proven to be closely associated with the prognosis of patients with COVID-19. The models we developed can assist doctors in developing appropriate COVID-19 treatment strategies, including allocating limited medical resources more rationally and early intervention in high-risk groups.",Not About Sufficiency
Biomechanical modelling of the pelvic system: improving the accuracy of the location of neoplasms in MRI-TRUS fusion prostate biopsy,"Background An accurate knowledge of the relocation of prostate neoplasms during biopsy is of great importance to reduce the number of false negative results. Prostate neoplasms are visible in magnetic resonance images (MRI) but it is difficult for the practitioner to locate them at the time of performing a transrectal ultrasound (TRUS) guided biopsy. In this study, we present a new methodology, based on simulation, that predicts both prostate deformation and lesion migration during the biopsy. Methods A three-dimensional (3-D) anatomy model of the pelvic region, based on medical images, is constructed. A finite element (FE) numerical simulation of the organs motion and deformation as a result of the pressure exerted by the TRUS probe is carried out using the Code-Aster open-source computer software. Initial positions of potential prostate lesions prior to biopsy are taken into consideration and the final location of each lesion is targeted in the FE simulation output. Results Our 3-D FE simulations show that the effect of the pressure exerted by the TRUS probe is twofold as the prostate experiences both a motion and a deformation of its original shape. We targeted the relocation of five small prostate lesions when the TRUS probe exerts a force of 30 N on the rectum inner wall. The distance travelled by these lesions ranged between 5.6 and 13.9 mm. Conclusions Our new methodology can help to predict the location of neoplasms during a prostate biopsy but further studies are needed to validate our results. Moreover, the new methodology is completely developed on open-source software, which means that its implementation would be affordable to all healthcare providers.",Not About Sufficiency
Introduction to Explainable AI,"As Artificial Intelligence technologies are increasingly used to make important decisions and perform autonomous tasks, providing explanations to allow users and stakeholders to understand the AI has become a ubiquitous concern. Recently, a number of open-source toolkits are making the growing collection of Explainable AI (XAI) techniques accessible for researchers and practitioners to incorporate explanation features in AI systems. This course is open to anyone interested in implementing, designing and researching on the topic, aiming to provide an overview on the technical and design methods for XAI, as well as hands-on experience with an XAI toolkit.",Not About Sufficiency
Durability and usability evaluation of a tilt-in-space manual wheelchair for children in India,"Around 8 million children with functional disabilities in India need postural support wheelchair designs. This study tests the durability and usability of a new tilt-in-space, postural support wheelchair design for children. The International Organization for Standardization (ISO) wheelchair durability testing was followed by a mixed-method observational field with a purposively selected, diverse sample of children aged 3-17 with varying diagnoses needing a tilt-in-space wheelchair in the rural and urban community settings in South India. Children were fitted with appropriate size wheelchairs. Demographic information was collected at baseline. Customized rapid surveys and interviews evaluated usability, use satisfaction, and willingness to buy and use the wheelchair device at the 2- and 12-month follow-up visits. The wheelchair passed ISO durability testing without part failures. Twelve participants (n = 7 boys, n = 5 girls), aged 10.25 +/- 2.67 years, reported high to moderate satisfaction of use. During follow-up, caregivers reported improvements in the child's physical function, social interaction, and time spent in the chair. No part failures were reported. Themes were found regarding the inappropriateness of previous wheelchair designs and the benefits for children's growth, function, and participation, as well as burden reduction for caregivers. The participants reported that they would buy the wheelchair for a price range of Rs. 15500-28751 ($186-346). The study results demonstrate the benefits of high-quality and usability of the postural support wheelchair design for children with functional disabilities. Such a design is necessary to promote growth, social skills, and reduced parental burden. Future studies should compare the design with wheelchairs currently prescribed in India.",Not About Sufficiency
A Bright Future: Innovation Transforming Public Health in Chicago,"Big cities continue to be centers for innovative solutions and services. Governments are quickly identifying opportunities to take advantage of this energy and revolutionize the means by which they deliver services to the public. The governmental public health sector is rapidly evolving in this respect, and Chicago is an emerging example of some of the changes to come. Governments are gradually adopting innovative informatics and big data tools and strategies, led by pioneering jurisdictions that are piecing together the standards, policy frameworks, and leadership structures fundamental to effective analytics use. They give an enticing glimpse of the technology's potential and a sense of the challenges that stand in the way. This is a rapidly evolving environment, and cities can work with partners to capitalize on the innovative energies of civic tech communities, health care systems, and emerging markets to introduce new methods to solve old problems.",Not About Sufficiency
Relocation or social welfare? Evaluation of accessibility conditions in eradications in the Metropolitan Area of Tucuman,"The Metropolitan Area of Tucuman (AMeT) is undergoing a process of gentrification linked to intense socio-spatial fragmentation, expressed through the unequal growth -regarding both area and population- of municipalities and communes (Gomez Lopez, Cuozzo and Boldrini, 2015). The Institute for Planning, Housing and Urban Development (IPVDU) is undertaking operations to regularize historic settlements in the ATS through relocations. In this context, this research aims to conduct a critical review of this process, based on a multicriterial, comparative analysis of cases, in relation to the strategies to ensure the accessibility of basic services and quality of life for the population.",Not About Sufficiency
Development of ML and IoT Enabled Disease Diagnosis Model for a Smart Healthcare System,"The current progression in the Internet of Things (IoT) and Machine Learning (ML) based technologies converted the traditional healthcare system into a smart healthcare system. The incorporation of IoT and ML has changed the way of treating patients and offers lots of opportunities in the healthcare domain. In this view, this research article presents a new IoT and ML-based disease diagnosis model for the diagnosis of different diseases. In the proposed model, vital signs are collected via IoT-based smart medical devices, and the analysis is done by using different data mining techniques for detecting the possibility of risk in people's health status. Recommendations are made based on the results generated by different data mining techniques, for high-risk patients, an emergency alert will be generated to healthcare service providers and family members. Implementation of this model is done on Anaconda Jupyter notebook by using different Python libraries in it. The result states that among all data mining techniques, SVM achieved the highest accuracy of 0.897 on the same dataset for classification of Parkinson's disease.",Not About Sufficiency
Can a nature walk change your brain? Investigating hippocampal brain plasticity after one hour in a forest,"In cities, the incidence of mental disorders is higher, while visits to nature have been reported to benefit mental health and brain function. However, there is a lack of knowledge about how exposure to natural and urban environments affects brain structure. To explore the causal relationship between exposure to these environments and the hippocampal formation, 60 participants were sent on a one hour walk in either a natural (forest) or an urban environment (busy street), and high-resolution hippocampal imaging was performed before and after the walks. We found that the participants who walked in the forest had an increase in subiculum volume, a hippocampal subfield involved in stress response inhibition, while no change was observed after the urban walk. However, this result did not withstand Bonferroni correction for multiple comparisons. Furthermore, the increase in subiculum volume after the forest walk was associated with a decrease in self-reported rumination. These results indicate that visits to nature can lead to observable alterations in brain structure, with potential benefits for mental health and implications for public health and urban planning policies.",Not About Sufficiency
IMGT/mAb-KG: the knowledge graph for therapeutic monoclonal antibodies,"Introduction Therapeutic monoclonal antibodies (mAbs) have demonstrated promising outcomes in diverse clinical indications, including but not limited to graft rejection, cancer, and autoimmune diseases lately.Recognizing the crucial need for the scientific community to quickly and easily access dependable information on monoclonal antibodies (mAbs), IMGT (R), the international ImMunoGeneTics information system (R), provides a unique and invaluable resource: IMGT/mAb-DB, a comprehensive database of therapeutic mAbs, accessible via a user-friendly web interface. However, this approach restricts more sophisticated queries and segregates information from other databases.Methods To connect IMGT/mAb-DB with the rest of the IMGT databases, we created IMGT/mAb-KG, a knowledge graph for therapeutic monoclonal antibodies connected to IMGT structures and genomics databases. IMGT/mAb-KG is developed using the most effective methodologies and standards of semantic web and acquires data from IMGT/mAb-DB. Concerning interoperability, IMGT/mAb-KG reuses terms from biomedical resources and is connected to related resources.Results and discussion In February 2024, IMGT/mAb-KG, encompassing a total of 139,629 triplets, provides access to 1,489 mAbs, approximately 500 targets, and over 500 clinical indications. It offers detailed insights into the mechanisms of action of mAbs, their construction, and their various products and associated studies. Linked to other resources such as Thera-SAbDab (Therapeutic Structural Antibody Database), PharmGKB (a comprehensive resource curating knowledge on the impact of genetic variation on drug response), PubMed, and HGNC (HUGO Gene Nomenclature Committee), IMGT/mAb-KG is an essential resource for mAb development. A user-friendly web interface facilitates the exploration and analyse of the content of IMGT/mAb-KG.",Not About Sufficiency
A Survey of Current Machine Learning Approaches to Student Free-Text Evaluation for Intelligent Tutoring,"Recent years have seen increased interests in applying the latest technological innovations, including artificial intelligence (AI) and machine learning (ML), to the field of education. One of the main areas of interest to researchers is the use of ML to assist teachers in assessing students' work on the one hand and to promote effective self-tutoring on the other hand. In this paper, we present a survey of the latest ML approaches to the automated evaluation of students' natural language free-text, including both short answers to questions and full essays. Existing systematic literature reviews on the subject often emphasise an exhaustive and methodical study selection process and do not provide much detail on individual studies or a technical background to the task. In contrast, we present an accessible survey of the current state-of-the-art in student free-text evaluation and target a wider audience that is not necessarily familiar with the task or with ML-based text analysis in natural language processing (NLP). We motivate and contextualise the task from an application perspective, illustrate popular feature-based and neural model architectures and present a selection of the latest work in the area. We also remark on trends and challenges in the field.",Not About Sufficiency
Peptaloid: A Comprehensive Database for Exploring Peptide Alkaloid,"Peptaloid is the first dedicated database for peptide alkaloid molecules, a unique class of naturally derived compounds known for their structural diversity and significant biological activities. Despite their promising potential in drug discovery and therapeutic development, research on peptide alkaloids has been limited by the absence of a comprehensive and centralized resource. Fragmented data across various sources have posed a significant challenge, underscoring the need for a specialized database to facilitate more efficient research and application. Peptaloid addresses this critical gap by providing a database with over 161,000 peptide alkaloid entries, each detailed with structural, physicochemical, and pharmacological properties. By leveraging advanced computational tools and machine learning, Peptaloid generates ADMET profiles, aiding in identifying and optimizing therapeutic candidates. Designed for versatility, the database supports various applications beyond drug discovery, including ecology and material sciences. Peptaloid (as a specialized database for peptide alkaloids) will play a crucial role in innovation and collaboration across scientific disciplines. Peptaloid is accessible at https://peptaloid.niser.ac.in.",Not About Sufficiency
A knowledge based approach on educational metadata use,"One of the most rapidly evolving e-services is e-Learning, that is, the creation of advanced educational resources that are accessible on-line and, potentially, offer numerous advantages over the traditional ones like intelligent access, interoperability between two or more educational resources and adaptation to the user. The driving force behind these approaches is the definition of the various standards about educational metadata, that is, data describing learning resources, the learner, assessment results, etc. The internal details of systems that utilize these metadata is an open issue since these efforts are primarily dealing with ""what"" and not ""how"". Under the light of these emerging efforts, we present CG-PerLS, a knowledge based approach for organizing and accessing educational resources. CG-PerLS is a model of a web portal for learning objects that encodes the educational metadata in the Conceptual Graph knowledge representation formalism, and uses related inference techniques to provide advanced functionality. The model allows learning resource creators to manifest their material and client-side learners to access these resources in a way tailored to their individual profile and educational needs.",Not About Sufficiency
A new insight into land use classification based on aggregated mobile phone data,"Land-use classification is essential for urban planning. Urban land-use types can be differentiated either by their physical characteristics (such as reflectivity and texture) or social functions. Remote sensing techniques have been recognized as a vital method for urban land-use classification because of their ability to capture the physical characteristics of land use. Although significant progress has been achieved in remote sensing methods designed for urban land-use classification, most techniques focus on physical characteristics, whereas knowledge of social functions is not adequately used. Owing to the wide usage of mobile phones, the activities of residents, which can be retrieved from the mobile phone data, can be determined in order to indicate the social function of land use. This could bring about the opportunity to derive land-use information from mobile phone data. To verify the application of this new data source to urban land-use classification, we first construct a vector of aggregated mobile phone data to characterize land-use types. This vector is composed of two aspects: the normalized hourly call volume and the total call volume. A semi-supervised fuzzy c-means clustering approach is then applied to infer the land-use types. The method is validated using mobile phone data collected in Singapore. Land use is determined with a detection rate of 58.03%. An analysis of the land-use classification results shows that the detection rate decreases as the heterogeneity of land use increases, and increases as the density of cell phone towers increases.",Not About Sufficiency
"FoodOn: a harmonized food ontology to increase global food traceability, quality control and data integration","The construction of high capacity data sharing networks to support increasing government and commercial data exchange has highlighted a key roadblock: the content of existing Internet-connected information remains siloed due to a multiplicity of local languages and data dictionaries. This lack of a digital lingua franca is obvious in the domain of human food as materials travel from their wild or farm origin, through processing and distribution chains, to consumers. Well defined, hierarchical vocabulary, connected with logical relationships-in other words, an ontology-is urgently needed to help tackle data harmonization problems that span the domains of food security, safety, quality, production, distribution, and consumer health and convenience. FoodOn () is a consortium-driven project to build a comprehensive and easily accessible global farm-to-fork ontology about food, that accurately and consistently describes foods commonly known in cultures from around the world. FoodOn addresses food product terminology gaps and supports food traceability. Focusing on human and domesticated animal food description, FoodOn contains animal and plant food sources, food categories and products, and other facets like preservation processes, contact surfaces, and packaging. Much of FoodOn's vocabulary comes from transforming LanguaL, a mature and popular food indexing thesaurus, into a World Wide Web Consortium (W3C) OWL Web Ontology Language-formatted vocabulary that provides system interoperability, quality control, and software-driven intelligence. FoodOn compliments other technologies facilitating food traceability, which is becoming critical in this age of increasing globalization of food networks.",Not About Sufficiency
Commute mode diversity and income inequality: an inter-urban analysis of 148 midsize US cities,"Little is known about the relationship between multi-modal transportation environments and aspects of sustainable urban development, such as reduced income inequality and affordable housing. This study, adapted from Molotch and Appelbaum's inter-city differential method, studied 148 semi-isolated mid-size cities. Using U.S. Census data from 2013, we found that increased diversity in commute modality is associated with less income inequality between white and African-American households, as well as between men and women. Commute mode diversity is also associated with higher earnings for white women and African-American men. Our study also shows that cities with more commuter mode diversity are associated with higher home property values and affordable rental markets compared to automobile dependent cities. These results undercut the notion that increasing automobile ownership is a reasonable policy response to urban poverty, and suggest that sustainable transportation policy can produce positive economic gains for cities. This work adds to the growing literature identifying fundamental differences between multimodal and automobile dependent cities.",Not About Sufficiency
Measuring the Coverage of the HL7� FHIR� Standard in Supporting Data Acquisition for 3 Public Health Registries,"With the increasing need for timely submission of data to state and national public health registries, current manual approaches to data acquisition and submission are insufficient. In clinical practice, federal regulations are now mandating the use of data messaging standards, i.e., the Health Level Seven (HL7 (R)) Fast Healthcare Interoperability Resources (FHIR (R)) standard, to facilitate the electronic exchange of clinical (patient) data. In both research and public health practice, we can also leverage FHIR (R) - and the infrastructure already in place for supporting exchange of clinical practice data - to enable seamless exchange between the electronic medical record and public health registries. That said, in order to understand the current utility of FHIR (R) for supporting the public health use case, we must first measure the extent to which the standard resources map to the required registry data elements. Thus, using a systematic mapping approach, we evaluated the level of completeness of the FHIR (R) standard to support data collection for three public health registries (Trauma, Stroke, and National Surgical Quality Improvement Program). On average, approximately 80% of data elements were available in FHIR (R) (71%, 77%, and 92%, respectively; inter-annotator agreement rates: 82%, 78%, and 72%, respectively). This tells us that there is the potential for significant automation to support EHR-to-Registry data exchange, which will reduce the amount of manual, error-prone processes and ensure higher data quality. Further, identification of the remaining 20% of data elements that are ""not mapped"" will enable us to improve the standard and develop profiles that will better fit the registry data model.",Not About Sufficiency
Mixed-Methods Approach to Land Use Renewal Strategies in and around Abandoned Airports: The Case of Beijing Nanyuan Airport,"Urban airports are typically large infrastructures with significant cultural, economic, and ecological impacts; meanwhile, abandoned airports are common worldwide. However, there is limited knowledge regarding transformation strategies for the renewal of abandoned airports and their surrounding regions in historically and culturally rich areas. We use Beijing's Nanyuan Airport as a case study, combining the historic urban landscape approach, land use and land cover change, and counterfactual simulations of land use patterns to construct a comprehensive analytical framework. Our framework was used to analyze the long-term land use patterns of the study area, determine its value, and improve perception from a macro- and multi-perspective. We discovered that the traditional knowledge and planning systems in the study area have largely disappeared, but Nanyuan Airport's impact on the surrounding land use patterns is unique and significant. By considering the characteristics and mechanisms of land use in the study area, we aimed to find a balance point between the historical context and future potential. As such, we propose optimized recommendations with the theme of connection and development engines. Our findings supplement the planning knowledge of relevant areas and provide a springboard for interdisciplinary research in landscape planning.",Not About Sufficiency
Systematic and ontology-based approach to interoperable cross-domain open government data services,"Purpose Open government data (OGD) has emerged as a radical paradigm shift and endeavor among government administrations across the world mainly due to its promises of transparency, accountability, public-private collaboration, civic participation, social innovation and data-driven value creation. Complexity, cross-cutting nature, diversity of data sets, interoperability and quality issues usually hamper unlocking the full potential value of data. To tackle these challenges, this paper aims to provide a novel solution using a top-down approach. Design/methodology/approach In this paper, the authors propose a systematic ontology-based approach combined with a novel architecture and its corresponding processes enabling organizations to carry out all the steps in the OGD value chain. In addition, an OGD Platform including a portal (www.iranopendata.ir) and a data management system (www.ogdms.iranopendata.ir) are developed to showcase the proposed solution. Findings The efficiency and the applicability of the solution are evaluated by a real-life use case on energy consumption of the buildings of the city of Tehran, Iran. Finally, a comparison was made with existing solutions, and the results show the proposed approach is able to address the existing gaps in the literature. Originality/value The results imply that modeling and designing the data model, as well as exploiting an ontology-based approach are critical pillars to create rich, relevant and well-described OGD data sets. Moreover, clarity on processes, roles and responsibilities are the key factors influencing the quality of the published data services. Thus, to the best of the knowledge, this is the first study that exploits and considers an ontology-based approach in a top-down manner to create OGD data sets.",Not About Sufficiency
The platform and the bricoleur-Improvisation and smart city initiatives in Indonesia,"This article investigates the design and evolution of smart city platforms in the global south using the Indonesian cities of Jakarta and Surabaya as case studies. While smart city projects are often framed in generic rhetoric of efficiency and modernization, the concept was originally formulated for cities in developed countries, and therefore requires adjustment for the local conditions in the developing world. While the former can rely on established institutions and well-developed infrastructures, the latter are characterized by rapid urbanization, weaker institutions, a lack of resources and public services. Unlike their highly regulated counterparts, cities in the global south are shaped by a dynamic informal economy and practices of improvisation. Using the lens of organizational improvisation, this article investigates how urban platforms emerge, and how they adapt to improvisational practices in the administration and the urban population. The article investigates different types of urban data platforms and their relationship with the social practices of their users. With Indonesian Smart City initiatives in Jakarta and Surabaya as a case study, this article aims to identify the local needs that motivated these cities to develop their respective projects. The second question asks how smart city implementations respond and adapt to the specific local conditions and the improvisational practices of their users. Distinguishing three types of urban data platforms, the article characterizes specific processes of bricolage and argues for stronger consideration of processes of improvisation in the design of urban data platforms. The contribution is threefold. The article provides a framework based on improvisation and bricolage that allows the social dynamics around smart city platforms and their impact on the system to be differentiated. It provides lessons on how traditional smart city models need to be adapted for cities in emerging economies. Finally, it offers a critique of the platform metaphors that are normally taken for granted.",Not About Sufficiency
Breast Cancer Risk Prediction with Stochastic Gradient Boosting,"Breast cancer, which is an important public health problem worldwide, is one of the deadliest cancers in women. This study aims to classify open-access breast cancer data and identify important risk factors with the Stochastic Gradient Boosting Method. The open-access breast cancer dataset was used to construct a classification model in the study. Stochastic Gradient Boosting was used to classify the disease. Balanced accuracy, accuracy, sensitivity, specificity, and positive/negative predictive values were evaluated for model performance. The accuracy, balanced accuracy, sensitivity, specificity, positive predictive value, negative predictive value, and F1 score metrics obtained with the Stochastic Gradient Boosting model were 100 %, 100 %, 100 %, 100 %, 100 %, and 100 %, and 100 % respectively. In addition, the importance of the variables obtained, the most important risk factors for breast cancer were a cave. points_mean, area_worst, and perimeter_worst, concave. points_worst respectively. According to the study results, with the machine-learning model Stochastic Gradient Boosting used, patients with and without breast cancer were classified with high accuracy, and the importance of the variables related to cancer status was determined. Factors with high variable importance can be considered potential risk factors associated with cancer status and can play an essential role in disease diagnosis.",Not About Sufficiency
Coherence analysis of research and education using topic modeling,"Research and education are organically connected in that lectures convey the results of research, which is frequently initiated by inspiring lectures. As a result, the contents of lecture materials and research publications and the research capabilities of universities should be considered in the investigations of the relationships between research and teaching. We examine the relationship between research and teaching using automatic text analysis. In particular, we scrutinize the relatedness of the content of research papers with the content of lecture materials to investigate the association between teaching and research. We adopt topic modeling for the correlation analysis of research capabilities and the reflectiveness of research topics in lecture materials. We select the field of machine learning as a case study because the field is contemporary and because data related to teaching and research are easily accessible via the Internet. The results reveal interesting characteristics of lecture materials and research publications in the field of machine learning. The research capability of an institute is independent of the lecture materials. However, for introductory courses, teaching and research measures showed a weak negative relationship, and there is little relationship between the measures for advanced courses.",Not About Sufficiency
Is it possible to determine antibiotic resistance of E. coli by analyzing laboratory data with machine learning?,"Objectives Microbial antibiotic resistance remains a serious public health problem worldwide. Conventional culture-based techniques are time-taking procedures; therefore, there is need for new approaches for detecting bacterial resistance. The aim of this study was to assess antibiotic resistance of Escherichia coli by analyzing biochemical parameters with machine learning systems without using antibiogram. Material and methods In this article, machine learning systems such as K-Nearest Neighbors, Artificial Neural Networks (ANN), Support Vector Machine and Decision Tree Learning were used to investigate whether E. coli is sensitive or resistant to antibiotics. The study was conducted based on the clinical records of 103 patients who were previously diagnosed with E. coli infection, including CBC and complete UA results, and CRP values. Results The accuracy rates of antibiotic resistance/susceptibility detected by ANN were as follows: Amikacin (96.0%), Ampicillin (77%), Ceftazidime (62%), Cefixime (63%), Cefotaxime (68%), Colistin (95%), Ciprofloxacin (76%), Cefepime (70%), Ertapenem (96%), Nitrofurantoin (90%), Phosphomycin (98%), Gentamicin (84%), Levofloxacin (98%), Piperacillin-Tazobactam (92%), and Trimethoprim-Sulfadiazine (79%). Conclusions The study determined the antibiotic resistance of E. coli with less time and cost compared to conventional culture-based methods machine learning based model contributes positively to artificial intelligence (AI) supported decision-making processes in laboratory medicine.",Not About Sufficiency
Informatics software for the ecologist's toolbox: A basic example,"Machine learning techniques for ecological applications or ""eco-informatics"" are becoming increasingly useful and accessible as software for these techniques becomes more readily available. Complex ecological data sets with a multitude of variables are also increasingly available. Ecologists, who do not necessarily have extensive backgrounds in machine-learning techniques, are facing decisions on new methods of data analysis. We evaluated the predictive ability of three commercially available (i.e. user-friendly) software packages for artificial neural networks (ANNs), evolutionary algorithms (EAs), and classificatiordregression trees (CART). To demonstrate their usage, we analyzed fish and habitat data from the mid-Atlantic region of the US, which was collected by the U.S. Environmental Protection Agency (EPA). These data, including over 200 environmental descriptors summarizing watershed, stream, and water chemistry, and physical habitat characteristics in addition to fish community metrics (i.e. richness, Index of Biotic Integrity (IBI) scores, % exotics), were collected as part of the EPA's Environmental Monitoring and Protection program. We predicted fish IBI scores as a function of these local and regional scale habitat variables. Predictive ability is evaluated with independent validation data. These approaches could prove especially useful for conservation or management applications where ecologists seek to utilize the most comprehensive data to make predictions at various scales. By employing ""user-friendly"" software we hope to show that ecologists, without extensive knowledge of computational science, can benefit from these techniques by extracting more information about complex ecosystems. We found that all models predicted better than chance (p < 0.05). Relative strengths and weaknesses of these three approaches are compared and recommendations for their use in ecological applications are presented. (c) 2006 Elsevier B.V. All rights reserved.",Not About Sufficiency
Validation of Whole-Genome Sequencing for Identification and Characterization of Shiga Toxin-Producing Escherichia coli To Produce Standardized Data To Enable Data Sharing,"Whole-genome sequencing (WGS) is rapidly becoming the method of choice for outbreak investigations and public health surveillance of microbial pathogens. The combination of improved cluster resolution and prediction of resistance and virulence phenotypes provided by a single tool is extremely advantageous. However, the data produced are complex, and standard bioinformatics pipelines are required to translate the output into easily interpreted epidemiologically relevant information for public health action. The main aim of this study was to validate the implementation of WGS at the Scottish Escherichia coli O157/STEC Reference Laboratory (SERL) using the Public Health England (PHE) bioinformatics pipeline to produce standardized data to enable interlaboratory comparison of results generated at two national reference laboratories. In addition, we evaluated the BioNumerics whole-genome multilocus sequence typing (wgMLST) and E. coli genotyping plug-in tools using the same data set. A panel of 150 well-characterized isolates of Shiga toxin-producing E. coli (STEC) that had been sequenced and analyzed at PHE using the PHE pipeline and database (SnapperDB) was assembled to provide identification and typing data, including serotype (O: H type), sequence type (ST), virulence genes (eae and Shiga toxin [stx] subtype), and a single-nucleotide polymorphism (SNP) address. To validate the implementation of sequencing at the SERL, DNA was reextracted from the isolates and sequenced and analyzed using the PHE pipeline, which had been installed at the SERL; the output was then compared with the PHE data. The results showed a very high correlation between the data, ranging from 93% to 100%, suggesting that the standardization of WGS between our reference laboratories is possible. We also found excellent correlation between the results obtained using the PHE pipeline and BioNumerics, except for the detection of stx(2a) and stx(2c) when these subtypes are both carried by strains.",Not About Sufficiency
Multiple Country Approach to Improve the Test-Day Prediction of Dairy Cows' Dry Matter Intake,"Simple Summary Dry matter intake, related to the number of nutrients available to an animal to meet its production and health needs, is crucial for the economic, environmental, and welfare management of dairy herds. Because the equipment required to weigh the ingested food at an individual level is not broadly available, we propose some new ways to approach the actual dry matter consumed by a dairy cow for a given day. To do so, we used regression models using parity (number of lactations), week of lactation, milk yield, milk mid-infrared spectrum, and prediction of bodyweight, fat, protein, lactose, and fatty acids content in milk. We chose these elements to predict individual dry matter intake because they are either easily accessible or routinely provided by regional dairy organizations (often called ""dairy herd improvement"" associations). We succeeded in producing a model whose dry matter intake predictions were moderately related to the actual values. We predicted dry matter intake of dairy cows using parity, week of lactation, milk yield, milk mid-infrared (MIR) spectrum, and MIR-based predictions of bodyweight, fat, protein, lactose, and fatty acids content in milk. The dataset comprised 10,711 samples of 534 dairy cows with a geographical diversity (Australia, Canada, Denmark, and Ireland). We set up partial least square (PLS) regressions with different constructs and a one-hidden-layer artificial neural network (ANN) using the highest contribution variables. In the ANN, we replaced the spectra with their projections to the 25 first PLS factors explaining 99% of the spectral variability to reduce the model complexity. Cow-independent 10 x 10-fold cross-validation (CV) achieved the best performance with root mean square errors (RMSECV) of 3.27 +/- 0.08 kg for the PLS regression and 3.25 +/- 0.13 kg for ANN. Although the available data were significantly different, we also performed a country-independent validation (CIV) to measure the models' performance fairly. We found RMSECIV varying from 3.73 to 6.03 kg for PLS and 3.69 to 5.08 kg for ANN. Ultimately, based on the country-independent validation, we discussed the developed models' performance with those achieved by the National Research Council's equation.",Not About Sufficiency
Object Detection to Assist Visually Impaired People: A Deep Neural Network Adventure,"Blindness or vision impairment, one of the top ten disabilities among men and women, targets more than 7 million Americans of all ages. Accessible visual information is of paramount importance to improve independence and safety of blind and visually impaired people, and there is a pressing need to develop smart automated systems to assist their navigation, specifically in unfamiliar healthcare environments, such as clinics, hospitals, and urgent cares. This contribution focused on developing computer vision algorithms composed with a deep neural network to assist visually impaired individual's mobility in clinical environments by accurately detecting doors, stairs, and signages, the most remarkable landmarks. Quantitative experiments demonstrate that with enough number of training samples, the network recognizes the objects of interest with an accuracy of over 98% within a fraction of a second.",Not About Sufficiency
Data-Driven Real-Time Pricing Strategy and Coordinated Optimization of Economic Load Dispatch in Electricity Market,"Compared to the step tariff, the real-time pricing (RTP) could be more stimulated for household consumers to change their electricity consumption behaviors. It can reduce the reserve capacity, peak load, and of course the electricity bill, which could achieve the purpose of saving energy. This paper proposes a coordinated optimization algorithm and data-driven RTP strategy in electricity market. First, the electricity price is divided into two parts, basic electricity price and fluctuating price. When the electricity consumption is equal to the average daily electricity consumption, the price is defined as the basic electricity price, which is the clearing electricity price. The consumer electricity data are analyzed. A random forest algorithm is adopted to predict the load data. Optimal adjustment parameters are obtained and the load fluctuation and the fluctuation of the electricity price are further quantified. Secondly, the appliances are modeled. The operation priority is established based on the preferences of customers and the Monte Carlo method is used to form the power load curve. Then, the smart energy planning unit is proposed to optimize the appliances on/off time and running time of residential electrical appliances. An incentive mechanism is used to further standardize the temporary electricity consumption. An improved multiobjective particle swarm optimization (IMOPSO) algorithm is adopted, which adopts the linear weighted evaluation function method to maximize the consumer's social welfare while minimizing the electricity bill. The simulation proves that the stability of the power grid is improved while obtaining the best power strategy.",Not About Sufficiency
A review on antimicrobial peptides databases and the computational tools,"Antimicrobial Peptides (AMPs) have been considered as potential alternatives for infection therapeutics since antibiotic resistance has been raised as a global problem. The AMPs are a group of natural peptides that play a crucial role in the immune system in various organisms AMPs have features such as a short length and efficiency against microbes. Importantly, they have represented low toxicity in mammals which makes them potential candidates for peptide-based drugs. Nevertheless, the discovery of AMPs is accompanied by several issues which are associated with labour-intensive and time-consuming wet-lab experiments. During the last decades, numerous studies have been conducted on the investigation of AMPs, either natural or synthetic type, and relevant data are recently available in many databases. Through the advancement of computational methods, a great number of AMP data are obtained from publicly accessible databanks, which are valuable resources for mining patterns to design new models for AMP prediction. However, due to the current flaws in assessing computational methods, more interrogations are warranted for accurate evaluation/analysis. Considering the diversity of AMPs and newly reported ones, an improvement in Machine Learning algorithms are crucial. In this review, we aim to provide valuable information about different types of AMPs, their mechanism of action and a landscape of current databases and computational tools as resources to collect AMPs and beneficial tools for the prediction and design of a computational model for new active AMPs.",Not About Sufficiency
Random forest evaluation using multi-key homomorphic encryption and lookup tables,"In recent years, machine learning (ML) has become increasingly popular in various fields of activity. Cloud platforms have also grown in popularity, as they offer services that are more secure and accessible worldwide. In this context, cloud-based technologies emerged to support ML, giving rise to the machine learning as a service (MLaaS) concept. However, the clients accessing ML services in order to obtain classification results on private data may be reluctant to upload sensitive information to cloud. The model owners may also prefer not to outsource their models in order to prevent model inversion attacks and to protect intellectual property. The privacy-preserving evaluation of ML models is possible through multi-key homomorphic encryption (MKHE), that allows both the client data and the model to be encrypted under different keys. In this paper, we propose an MKHE evaluation method for decision trees and we extend the proposed method for random forests. Each decision tree is evaluated as a single lookup table, and voting is performed at the level of groups of decision trees in the random forest. We provide both theoretical and experimental evaluations for the proposed method. The aim is to minimize the performance degradation introduced by the encrypted model compared to a plaintext model while also obtaining practical classification times. In our experiments with the proposed MKHE random forest evaluation method, we obtained minimal (less than 0.6%) impact on the main ML performance metrics considered for each scenario, while also achieving reasonable classification times (of the order of seconds).",Not About Sufficiency
"Identification, analysis and prediction of valid and false information related to vaccines from Romanian tweets","Introduction The online misinformation might undermine the vaccination efforts. Therefore, given the fact that no study specifically analyzed online vaccine related content written in Romanian, the main objective of the study was to detect and evaluate tweets related to vaccines and written in Romanian language.Methods 1,400 Romanian vaccine related tweets were manually classified in true, neutral and fake information and analyzed based on wordcloud representations, a correlation analysis between the three classes and specific tweet characteristics and the validation of several predictive machine learning algorithms.Results and discussion The tweets annotated as misinformation showed specific word patterns and were liked and reshared more often as compared to the true and neutral ones. The validation of the machine learning algorithms yielded enhanced results in terms of Area Under the Receiver Operating Characteristic Curve Score (0.744-0.843) when evaluating the Support Vector Classifier. The predictive model estimates in a well calibrated manner the probability that a specific Twitter post is true, neutral or fake. The current study offers important insights regarding vaccine related online content written in an Eastern European language. Future studies must aim at building an online platform for rapid identification of vaccine misinformation and raising awareness for the general population.",Not About Sufficiency
Systems to Democratize and Standardize Access to Web APIs,"Today, many web sites ofer third-party access to their data through web APIs. However, manually encoding URLs with arbitrary endpoints, parameters, authentication handshakes, and pagination, among other things, makes API use challenging and laborious for programmers, and untenable for novices. In addition, each API ofers its own idiosyncratic data model, properties, and methods that a new user must learn, even when the sites manage the same common types of information as many others. In my research, I show how working with web APIs can be dramatically simplifed by describing the APIs using a standardized, machine-readable ontology, and building systems that democratize and standardize access to these APIs. Specifcally, I focus on: 1) systems to enable users to query and retrieve data through APIs without programming and 2) systems that standardize access to APIs and simplify the work for users-even non-programmers-to create interactive web applications that operate on data accessible through arbitrary APIs.",Not About Sufficiency
Assessment of the Impact of Road Transport Change on the Security of the Urban Social Environment,"In the context of accelerating urbanisation, cities must ensure a viable economy, social well-being, and a healthy environment. Transport is one of the key conditions for economic development and meeting the needs of countries, regions, and cities. However, transport must meet not only the physiological but also the social needs of society, one of which is environmental security. Urban transport accounts for around 40% of CO2 emissions and 70% of other pollutants from road transport. Thus, one of the most difficult issues for any city to address when building bypasses is the growing number of cars in the city, traffic congestion, and the reduction of all greenhouse gas emissions. The documents adopted in July 2020 aim to revitalise the EU's economy by moving towards a green economy and sustainability. In addition to the systematic and comparative analysis of concepts published in the scientific literature, the article also presents an analysis of the concepts of the sustainable city and sustainable transport, as well as a study of the social impact of bypasses and the assessment of the security of the social environment in the Baltic capitals. The aim of the article is to assess the impact of the growing number of vehicles on the security of the city's social environment. Research results show that the number of pollutants and a direct dependence between the number of pollutants and the driving speed were established. Therefore, it needs to make investments in the transport sector: improving roads, the construction of bypasses, and the technical parameters of purchased cars.",Not About Sufficiency
Challenged Content Delivery Network: Eliminating the Digital Divide,"We present a complete system, called Challenged Content Delivery Network (CCDN), to efficiently deliver multimedia content to mobile users who live in developing countries, rural areas, or over-populated cities with no or weak network infrastructure. These mobile users do not have always-on Internet access. We demo our CCDN, implemented on a Linux server, Raspberry Pi proxies, and Android phones from three aspects: multimedia, networking, and machine learning tools. We propose multiple optimization algorithm modules that compute personalized distribution plans, and maximize the overall user experience. CCDN allows people living in area with challenged networks access to multimedia content, like news reports, using mobile devices, such as smartphones. This in turn will help in eliminating the digital divide, which refers to information inequality to persons with different Internet accessing abilities.",Not About Sufficiency
Low-Carbon Quick Wins: Integrating Short-Term Sustainable Transport Options in Climate Policy in Low-Income Countries,"In low income countries (LICs) in Africa and Asia per capita transport greenhouse gas emissions are relatively low but are expected to grow. Therefore, a substantial reduction in projected increases is required to bring emissions in line with long-term global climate objectives. Literature on how LICs are integrating climate change mitigation and sustainable transport strategies is limited. Key drivers of transport policy include improving accessibility, congestion, air quality, energy security, with reducing greenhouse gas emissions being of lower priority. This paper assesses the current status, feasibility and potential of selected low-carbon transport measures with high sustainable development benefits that can be implemented in the short to medium term, so- called 'quick wins'. It examines to what extent ten such quick wins are integrated in climate change strategies in nine low- and middle-income countries in Africa and South Asia. The research method comprises expert interviews, an online questionnaire survey of experts and policymakers in the focus countries, and a review of literature and government plans. Results indicate that sustainable urban transport policies and measures are considered high priority, with vehicle-related measures such as fuel quality and fuel economy standards and electric two- and three-wheelers being of key relevance. In existing national climate change strategies, these quick wins are integrated to a certain extent; however, with better coordination between transport and energy and environment agencies such strategies can be improved. A general conclusion of this paper is that for LICs, quick wins can connect a top-down' climate perspective with a 'bottom-up' transport sector perspective. A knowledge gap exists as to the mitigation potential and sustainable development benefits of these quick wins in the local context of LICs.",Not About Sufficiency
Wireless Sensor Network Based Real-Time Pedestrian Detection and Classification for Intelligent Transportation System,"Pedestrian safety has become a critical consideration in developing society especially road traffic, an intelligent transportation need of the hour is the solution left. India tops the world with 11% of global road accidents. With this data, we have moved in the direction of computer vision applications for efficient and accurate pedestrian detection for intelligent transportation systems (ITS). The important application of this research is robot development, traffic management and control, unmanned vehicle driving (UVD), intelligent monitoring and surveillance system, and automatic pedestrian detection system. Much research has focused on pedestrian detection, but sustainable solution-driven research must still be required to overcome road accidents. We have proposed a wireless sensor network-based pedestrian detection system that classifies the real-time set of pedestrian activity and samples the reciprocally received signal strength (RSS) from the sensor node. We applied a histogram of oriented gradient (HOG) descriptor algorithm K-nearest neighbor, decision tree and linear support vector machine to measure the performance and prediction of the target. Also, these algorithms have performed a comparative analysis under different aspects. The linear support vector machine algorithm was trained with 481 samples. The performance achieves the accuracy of 98.90%and has accomplished superior results with a maximum precision of 0.99, recall of 0.98, and F-score of 0.95 with 2% error rate. The model's prediction indicates that it can be used in the intelligent transportation system. Finally, the limitation and the challenges discussed to provide an outlook for future research direction to perform effective pedestrian detection.",Not About Sufficiency
A machine learning analysis of the relationship of demographics and social gathering attendance from 41 countries during pandemic,"Knowing who to target with certain messages is the prerequisite of efficient public health campaigns during pandemics. Using the COVID-19 pandemic situation, we explored which facets of the society-defined by age, gender, income, and education levels-are the most likely to visit social gatherings and aggravate the spread of a disease. Analyzing the reported behavior of 87,169 individuals from 41 countries, we found that in the majority of the countries, the proportion of social gathering-goers was higher in male than female, younger than older, lower-educated than higher educated, and low-income than high-income subgroups of the populations. However, the data showed noteworthy heterogeneity between the countries warranting against generalizing from one country to another. The analysis also revealed that relative to other demographic factors, income was the strongest predictor of avoidance of social gatherings followed by age, education, and gender. Although the observed strength of these associations was relatively small, we argue that incorporating demographic-based segmentation into public health campaigns can increase the efficiency of campaigns with an important caveat: the exploration of these associations needs to be done on a country level before using the information to target populations in behavior change interventions.",Not About Sufficiency
Support Vector Machine-based approach for Recognizing Bonsai Species using Leaf Image,"Recognition of Bonsai plant is one of the most challenging task. This is because most of the people have less knowledge about Bonsai especially for a beginner. For those who new to this field, it might be hard for them to recognize and identify the species of Bonsai because of its similarity in terms of shape, colour and etc. The incorrect identification of species, may resulting in damaging the Bonsai plant. Furthermore, different species of Bonsai may have different ways to take care of it. Therefore, the information about the Bonsai need to be accessible with the recognition of the species. As a solution, the aims of this project is to develop a system for recognising three species of Bonsai: 1) Adenium, 2) Red Japanese Maple and 3) Natal Plum by using its leaf. The project implemented a Rapid Application Development (RAD) Model as the methodology. There are four phases in RAD: 1) Planning, 2) Design, 3) Implementation and 4) Finalization. In pre-processed phase, feature extraction of the leaf is using colour moment and Gray-Level Co- occurrence Matrix (GLCM) were used for extracting the colour of the leaf. The species of Bonsai has been classified using Support Vector Machine-based approach and the system has been successfully recognize the species of Bonsai with accuracy of 98.2%.",Not About Sufficiency
"Reducing Barriers to Postabortion Contraception: The Role of Expanding Coverage of Postabortion Care in Dar es Salaam, Tanzania","Background: In Tanzania, limited access to postabortion care (PAC) contributes to high rates of maternal mortality. To address the issue, Pathfinder International and the Tanzania Ministry of Health, Community Development, Gender, Elderly and Children (MOHCDGEC) introduced and expanded coverage of PAC in 64 public health facilities in Dar es Salaam, Tanzania. Methods: During a 30-month period, we implemented a multifaceted approach to introduce and expand PAC, including clinical training and mentorship for health care providers; service reorganization, equipment provision, and an expanded method mix offering; standardization of PAC reporting tools; and community engagement and referral. We assessed outcomes using PAC service statistics from 64 public health facilities in 4 districts of Dar es Salaam and health care provider mentorship data from 385 observed PAC visits. Results: From January 2016 to June 2018, voluntary postabortion contraceptive uptake increased steadily. A total of 6,636 PAC clients, including 2,731 young people (ages 10-24), adopted a method post-procedure. Average semesterly client volume per facility increased from 27 to 52.4 manual vacuum aspiration clients and 17.6 to 43.9 postabortion contraceptors between the first and last periods. Overall postabortion contraceptive uptake was 80.6% (6,636/8,230), with a method mix of 58.3% implant, 18.9% intrauterine device, 13.7% pills, 8.6% injectables, and 0.5% permanent methods. Adults and young people had comparable method mix. Mentored providers showed improvements in service quality indicators. During the last period, 92% counseled the client on contraception, 93% considered the client's sexual and reproductive health intentions, 94% provided correct method information and supply, and 96% documented services on the client's family planning card. Different provider types (mid- and senior-level) performed comparably. Conclusions: Expanding PAC coverage to primary- and secondary-level facilities led to high uptake of voluntary contraception among postabortion clients. Key interventions included PAC clinical training and mentorship; service reorganization, equipment provision, and an expanded method mix offering; use of standardized PAC registers; and community engagement for awareness building and linkage to PAC.",Not About Sufficiency
The Impact of the Digital Economy on Carbon Emissions from Cultivated Land Use,"Is digitalization conducive to promoting carbon reduction in cultivated land use while empowering high-quality socio-economic development and intelligent territorial spatial planning? Derived from China's provincial panel data from the period 2011 to 2019, in this paper, we employ a fixed-effect model to study the impact of the digital economy on carbon emissions from cultivated land use and apply an intermediary-effect model to estimate the impact that the structure of the digital economy has on carbon emissions from cultivated land use. The results indicate the following: (1) The expansion of the digital economy can significantly decrease the carbon emissions caused by cultivated land use. This conclusion is still valid after considering endogenous issues and conducting a series of robustness tests. (2) Green technical renovation has played a significant intermediary role in the effect the digital economy has on the amount of carbon emissions from cultivated land use. (3) Digital economy development has significantly promoted innovation in green technology by increasing the size of green invention patent applications and authorizations, thus effectively curbing carbon emissions from cultivated land use and achieving the carbon emission reduction effect of the digital economy. However, some suggestions are put forward, including speeding up the deep integration of digital technology and cultivated land use planning, strengthening the application of green technical renovation achievements in the agricultural field, and enhancing the government's function in the institutional guarantee of the growth of the digital economy.",Not About Sufficiency
A Comprehensive Review of Cutting-Edge Flood Modelling Approaches for Urban Flood Resilience Enhancement,"Urban environments face a growing threat from floods, driven by factors like excessive rainfall, inadequate drainage, rapid urbanization, and climate-amplified extreme weather events. These floods inflict significant economic losses, endanger public health, and impede disaster response. This review tackles a critical issue by exploring the most effective flood modelling approaches for urban areas. Through a comprehensive analysis, it examines various hydrologic and hydraulic models such as SWAT, HEC-HMS, VIC, TOPMODEL, HBV, ANUGA, LISFLOOD, HEC-RAS, SWMM, MIKE URBAN, and others, highlighting their strengths and limitations, and offering a structured comparison based on criteria for selecting the appropriate model. The research revealed that integrating HEC-HMS and HEC-RAS produced the most favourable outcomes for urban flood modelling when open channels are key components, where the spatial extent of the flood, water surface profile, and velocity are easily determined by the HEC-RAS model. Conversely, SWMM integrates surface runoff and underground drainage, providing comprehensive urban water management. This nuanced understanding underscores the importance of selecting appropriate modelling approaches based on the unique characteristics of urban environments, ensuring effective flood management strategies tailored to the specific challenges each area presents. These urban flood modelling techniques serve as indispensable tools in forecasting flood patterns, evaluating vulnerability levels, and formulating effective strategies to mitigate risks. By empowering informed decision-making in urban planning and disaster management, these models are critical for minimizing the devastating effects of floods on communities and infrastructure. This review provides valuable insights for developing resilient urban areas prepared to navigate the complex challenges of urban flooding.",Not About Sufficiency
Interpretable Trend Analysis Neural Networks for Longitudinal Data Analysis,"Cohort study is one of the most commonly used study methods in medical and public health researches, which result in longitudinal data. Conventional statistical models and machine learning methods are not capable of modeling the evolution trend of the variables in longitudinal data. In this article, we propose a Trend Analysis Neural Networks (TANN), which models the evolution trend of the variables by adaptive feature learning. TANN was tested on dataset of Kaiuan research. The task was to predict occurrence of cardiovascular events within 2 and 5 years, with three repeated medical examinations during 2008 and 2013. For 2-year prediction, The AUC of the TANN is 0.7378, which is a significant improvement than that of conventional methods, while that of TRNS, RNN, DNN, GBDT, RF, and LR are 0.7222, 0.7034, 0.7054, 0.7136, 0.7160, and 0.7024, respectively. For 5-year prediction, TANN also shows improvement. The experimental results show that the proposed TANN achieves better prediction performance on cardiovascular events prediction than conventional models. Furthermore, by analyzing the weights of TANN, we could find out important trends of the indicators, which are ignored by conventional machine learning models. The trend discovery mechanism interprets the model well. TANN is an appropriate balance between high performance and interpretability.",Not About Sufficiency
Prediction of SARS-CoV-2 infection with a Symptoms-Based model to aid public health decision making in Latin America and other low and middle income settings,"Symptoms-based models for predicting SARS-CoV-2 infection may improve clinical decision-making and be an alternative to resource allocation in under-resourced settings. In this study we aimed to test a model based on symptoms to predict a positive test result for SARS-CoV-2 infection during the COVID-19 pandemic using logistic regression and a machine-learning approach, in Bogot & PRIME;a, Colombia. Participants from the CoVIDA project were included. A logistic regression using the model was chosen based on biological plausibility and the Akaike Information criterion. Also, we performed an analysis using machine learning with random forest, support vector machine, and extreme gradient boosting. The study included 58,577 participants with a positivity rate of 5.7%. The logistic regression showed that anosmia (aOR = 7.76, 95% CI [6.19, 9.73]), fever (aOR = 4.29, 95% CI [3.07, 6.02]), headache (aOR = 3.29, 95% CI [1.78, 6.07]), dry cough (aOR = 2.96, 95% CI [2.44, 3.58]), and fatigue (aOR = 1.93, 95% CI [1.57, 2.93]) were independently associated with SARS-CoV-2 infection. Our final model had an area under the curve of 0.73. The symptoms-based model correctly identified over 85% of participants. This model can be used to prioritize resource allocation related to COVID-19 diagnosis, to decide on early isolation, and contact-tracing strategies in individuals with a high probability of infection before receiving a confirmatory test result. This strategy has public health and clinical decision-making significance in low-and middle-income settings like Latin America.",Not About Sufficiency
Non-contingency in a Paraconsistent Setting,"We study an extension of first-degree entailment (FDE) by Dunn and Belnap with a non-contingency operator ?? which is construed as ""? has the same value in all accessible states' or ""all sources give the same information on the truth value of ?. We equip this logic dubbed K-FDEKFDE(?) with frame semantics and show how the bi-valued models can be interpreted as interconnected networks of Belnapian databases with the ? operator modelling search for inconsistencies in the provided information. We construct an analytic cut system for the logic and show its soundness and completeness. We prove that ? is not definable via the necessity modality ? of K-FDE?. Furthermore, we prove that in contrast to the classical non-contingency logic, reflexive, S4 and S5 (among others) frames are definable.",Not About Sufficiency
Machine learning approaches reveal highly heterogeneous air quality co-benefits of the energy transition,"Estimating health benefits of reducing fossil fuel use from improved air quality provides important rationales for carbon emissions abatement. Simulating pollution concentration is a crucial step of the estimation, but traditional approaches often rely on complicated chemical transport models that require extensive expertise and computational resources. In this study, we develop a machine learning framework that is able to provide precise and robust annual average fine particle (PM2.5) concentration estimations directly from a high-resolution fossil energy use dataset. Applications of the framework with Chinese data reveal highly heterogeneous health benefits of avoiding premature mortality by reducing fossil fuel use in different sectors and regions in China with a mean of $19/tCO(2) and a standard deviation of $38/tCO(2). Reducing rural and residential coal use offers the highest co-benefits with a mean of $151/ tCO(2). Our findings prompt careful policy designs to maximize cost-effectiveness in the transition toward a carbon-neutral energy system.",Not About Sufficiency
A Curve Relocation Approach for Robust Battery Open Circuit Voltage Reconstruction and Capacity Estimation Based on Partial Charging Data,"This article proposes a curve relocation approach for robust battery open circuit voltage (OCV) reconstruction and capacity estimation based on partial charging data. First, an electrode-level aging mechanism analysis is conducted to reveal the underlying reasons for battery OCV distortion and capacity decay, and three electrode aging parameters (EAPs) are proposed to account for those aging-induced relative position shifts of electrode OCV curves. Second, a deep long short-term memory recurrent neural network with a many-to-one structure is established to yield battery OCV estimations in high fidelity using accessible daily charging data. Then, a multicoupled optimization algorithm is designed to accurately estimate EAPs, which ensures that the reconstructed OCV curves match well with the estimated OCV segments while satisfying various OCV-related health features at a specific aging level. Obtaining the optimal EAPs contributes to 1) relocate the relative positions of electrode OCV curves for reliable battery OCV reconstruction; 2) determine battery usable capacity range and achieve capacity estimation in high accuracy; and 3) help understand electrode aging behaviors. The proposed method realizes a mean absolute error of less than 20 mV in battery OCV reconstruction and a mean absolute percentage error of less than 1.3% in capacity estimation given a short charging segment up to 1000 s over the whole battery lifetime.",Not About Sufficiency
Building communities to promote physical activity: A multi-scale geographical analysis,"The objective of this paper is to make explicit the linkages between specific characteristics in the urban built environment, moderate physical activity (in particular walking and cycling), and public health. The review will take place at three different scales - the region, the city and the city-block. At all three scales, the main interest is placed on accessibility, with the recognition that if distances are short enough and there is high connectivity within neighbourhoods, people might be encouraged to walk or cycle. The paper will draw on urban built environment characteristics from a number of Michigan municipalities, including Detroit, Ann Arbor, Birmingham, East Lansing and Okemos.",Not About Sufficiency
"Blockchain-based recommender systems: Applications, challenges and future opportunities","Recommender systems have been widely used in different application domains including energy preservation, e-commerce, healthcare, social media, etc. Such applications require the analysis and mining of massive amounts of various types of user data, including demographics, preferences, social interactions, etc. in order to develop accurate and precise recommender systems. Such datasets often include sensitive information, yet most recommender systems are focusing on the models' accuracy and ignore issues related to security and the users' privacy. Despite the efforts to overcome these problems using different risk reduction techniques, none of them has been completely successful in ensuring cryptographic security and protection of the users' private information. To bridge this gap, the blockchain technology is presented as a promising strategy to promote security and privacy preservation in recommender systems, not only because of its security and privacy salient features, but also due to its resilience, adaptability, fault tolerance and trust characteristics. This paper presents a holistic review of blockchain-based recommender systems covering challenges, open issues and solutions. Accordingly, a well-designed taxonomy is introduced to describe the security and privacy challenges, overview existing frameworks and discuss their applications and benefits when using blockchain before indicating opportunities for future research. (c) 2021 The Authors. Published by Elsevier Inc.",Not About Sufficiency
The Promoting Active Communities Program: Improvement of Michigan's Self-Assessment Tool,"Background: This project updated and improved the Promoting Active Communities Program (PAC), a Web-based assessment that enables communities to scrutinize their programs, policies, and environments related to physical activity, generating ideas and community commitment for improvements. Methods: A literature review, focus groups, and expert review guided PAC improvements. Results: Over 150 articles and audit measures in the fields of transportation, public health, and urban planning were reviewed. Indicators were identified, categorized, and evaluated for use in the PAC. Focus-group participants communicated motivations, processes, and obstacles for completing the PAC and developing an action plan. Participants requested technical information to guide them in achieving active-living environments. Conclusions: Information gathered was used to improve the PAC Web site. A technical assistance document, Design Guidelines for Active Michigan Communities, was created to aid communities in creating active-living environments. The new PAC and Design Guidelines are available for public use at www.mihealthtools.org/communities.",Not About Sufficiency
A Systematic and Formal Study of the Impact of Local Differential Privacy on Fairness: Preliminary Results,"Machine learning (ML) algorithms rely primarily on the availability of training data, and, depending on the domain, these data may include sensitive information about the data providers, thus leading to significant privacy issues. Differential privacy (DP) is the predominant solution for privacy-preserving ML, and the local model of DP is the preferred choice when the server or the data collector are not trusted. Recent experimental studies have shown that local DP can impact ML prediction for different subgroups of individuals, thus affecting fair decision-making. However, the results are conflicting in the sense that some studies show a positive impact of privacy on fairness while others show a negative one. In this work, we conduct a systematic and formal study of the effect of local DP on fairness. Specifically, we perform a quantitative study of how the fairness of the decisions made by the ML model changes under local DP for different levels of privacy and data distributions. In particular, we provide bounds in terms of the joint distributions and the privacy level, delimiting the extent to which local DP can impact the fairness of the model. We characterize the cases in which privacy reduces discrimination and those with the opposite effect. We validate our theoretical findings on synthetic and real-world datasets. Our results are preliminary in the sense that, for now, we study only the case of one sensitive attribute, and only statistical disparity, conditional statistical disparity, and equal opportunity difference.",Not About Sufficiency
Climate-Fit.City: Urban Climate Data and Services,"The Climate-fit.City service (https://www.climate-fit.city ) provides the best available scientific urban climate data and information for public and private end users operating in cities. Within the Climate-fit.City H2020 project, the benefits of urban climate information for end user communities was demonstrated, considering services in diverse domains (Climate and Health, Building Energy, Emergency Planning, Urban Planning, Active Mobility, Tourism and Cultural Heritage) to improve decision-making and to help end users to better address the consequences of climate change at the local scale. The socio-economic impact assessment performed in the Climate-fit.City project has demonstrated that, in all the cases, there are actual and potential added values in terms of public service effectiveness, economic impacts, policy innovation and social impacts. Further impact was also revealed in terms of raising awareness by end users, policymakers and the general public about climate change. These diversified impacts offer a variegated landscape of sub-areas and stakeholders that are touched upon by each climate service.",Not About Sufficiency
Graph representation learning and software homology matching based A study of JAVA code vulnerability detection techniques,"In nowadays using different tools and apps is a basic need of people's behavior in life, but the security issues arising from the existence of source code plagiarism among tools and apps are likely to bring huge losses to companies and even countries, so detecting the existence of vulnerabilities or malicious code in software becomes an important part of protecting information and detecting software security. This project is based on the aspect of JAVA code vulnerability detection based on graph representation learning and software homology comparison to carry out research. This project will be based on the content of deep learning, using a large number of vulnerable source code, extracting its features, and transforming it into a graph so that it can be tested source code for comparison and report the vulnerability content. The main work and results of this project are as follows: 1.By extracting the example dataset and generating json files to save the feature information of relevant java code; by generating vector files, bytecode files and dot files, and batch extracting nodes and edges based on the contents of the dot files for subsequent machine learning use, the before and after steps and operations form a logical self-consistency to ensure the integrity of the project. 2.Through the study of graph neural networks and graph convolutional neural networks, relevant models are selected for machine learning using predecessor files and manual model tuning is performed to ensure good learning results and feedback for the machine learning part of the project. 3.This project training dataset negative samples for sard above the shared dataset, which contains 46636 java vulnerability source code, and dataset support environment, test dataset negative samples dataset also from sard, positive samples dataset are generated from the relevant person in charge. 4.Based on Graph Neural Network (GNN) and Graph Convolutional Neural Network (GCN), this project will design and implement a whole set of automated vulnerability detection system for java code. 5. All the related contents of this project, after the human extensive search of domestic and foreign related papers and materials, there are not all projects or contents similar to this project, the same papers and materials appear, all the problems involved in this project and related ideas are for the project this group of people thinking, looking for solutions.",Not About Sufficiency
An analysis of COVID-19 vaccine hesitancy in the U.S.,"Reluctance or refusal to get vaccinated, commonly known as Vaccine Hesitancy (VH), poses a significant challenge to COVID-19 vaccination campaigns. Understanding the factors contributing to VH is essential for shaping effective public health strategies. This study proposes a novel framework for combining machine learning with publicly available data to generate a proxy metric that evaluates the dynamics of VH faster than the currently used survey methods. The metric is input to descriptive classification models that analyze a wide array of data, aiming to identify key factors associated with VH at the county level in the U.S. during the COVID-19 pandemic (i.e., January to October 2021). Both static and dynamic factors are considered. We use a Random Forest classifier that identifies political affiliation and Google search trends as the most significant factors influencing VH behavior. The model categorizes U.S. counties into five distinct clusters based on VH behavior. Cluster 1, with low VH, consists mainly of Democratic-leaning residents who, have the longest life expectancy, have a college degree, have the highest income per capita, and live in metropolitan areas. Cluster 5, with high VH, is predominantly Republican-leaning individuals in non-metropolitan areas. Individuals in Cluster 1 is more responsive to vaccination policies.",Not About Sufficiency
Surface-enhanced Raman spectroscopy: A novel diagnostic method for pathogenic organisms,"The enormous economic burden and increasing public health threats associated with pathogenic organisms have necessitated the development of rapid bacterial identification platforms. Surface-enhanced Raman spectroscopy (SERS) is considered one of the most promising pathogen diagnostic techniques. The method generates a unique and sensitive biomarker fingerprint that may be utilized for quick and multiplexed analysis on a portable platform. Herein, we have made a comprehensive review on the advances made in SERS. First, we briefly discussed the specific and non-specific capture of pathogens using the label-free method. Then, various label-based immunosensors and aptasensors that have been developed over the years for the detection of pathogenic organisms have been summarized. This review then focused on the combination of CRISPR/Cas assays and SERS techniques in the detection of pathogenic organisms. Next, we discussed recent applications of machine learning techniques in Raman signal analysis and their application in clinical medicine. Furthermore, various conditions that affect Raman signal effects and how to deal with such conditions were discussed. Finally, challenges, key points, and future outlooks for SERS application have been summarized.",Not About Sufficiency
Health system interventions and responses to anti-microbial resistance: A scoping review of evidence from 15 African countries,"The global rise in antimicrobial resistance (AMR) is claiming the lives of more than 1.2 million people each year. According to the World Health Organization (WHO) this global health crisis is particularly acute in Africa, largely due to fragile and underfunded health systems. Efforts to combat this public health threat have led to the implementation of health system interventions worldwide aimed at managing and containing the spread of AMR. However, the literature on the real time impacts and the barriers that hinder the implementation of these interventions in the African context is limited. The objective of this scoping review was to identify AMR interventions in African health systems, their impact, and the challenges of the implementation. Drawing on Muka and colleague's 24 step approach for scoping reviews, two major public health databases (PubMed and Global Health) were searched for articles in accordance with the PRISMA guidelines resulting in 4,783 records. Screening and retrieval of articles was done using Rayyan software based on specified inclusion criteria and 36 articles included in the final list. These articles were synthesized after extracting specific data on AMR interventions and their impact on African health systems. The review identified four broad impacts of AMR interventions including 1. Reduction in antibiotics use, 2. Increased adherence to guidelines and protocols, 3. Enhanced laboratory-based AMR surveillance, 4. Development of antimicrobial stewardship (AMS) Action Plans and Teams. However, challenges such as poor laboratory infrastructure, logistical challenges, poor financial commitment and inadequate education and training were identified as challenges impeding the successful implementation of AMR interventions in Africa. Our findings reveal a range of successful AMR interventions in African health systems although infrastructural and financial challenges remain. Better standardization and reporting of AMR diagnosis while leveraging the available information is needed to improve the optimization of treatment guidelines across Africa.",Not About Sufficiency
"Using Machine Learning to Create an Adaptable, Scalable, and Interpretable Behavioral Model","In this study, we introduce Adaptable Scalable Best Estimate and Sampling Tools (AS-BEAST), an interpretable model of human decision-making under uncertainty, that fuses the foundational principles of BEAST, a behavioral model grounded in psychological theory, with the capabilities of machine learning techniques. Our strategy involves mathematically formalizing BEAST as a differentiable function and representing it in a computational graph. This approach facilitates the learning of model parameters using automatic differentiation and gradient descent. AS-BEAST scales to larger data sets and adapts to new data more efficiently, while preserving the psychological interpretability of the original model. Evaluation of AS-BEAST on the largest publicly accessible data set of human choice under uncertainty shows that it predicts choice at state-of-the-art levels, similar to those of less interpretable deep neural networks and better than several benchmarks, including the original BEAST model. Importantly, AS-BEAST provides interpretable explanations for choice behavior, leading to the extraction of novel psychological insights from the data. This research demonstrates the potential of machine learning techniques to enhance the scalability and adaptability of models rooted in psychological theory, without compromising their interpretability or insight generation capabilities.",Not About Sufficiency
Organoids in high-throughput and high-content screenings,"Organoids are self-organized three-dimensional (3D) multicellular tissue cultures which derive from cancerous and healthy stem cells, sharing a highly similarity to the corresponding in vivo organs. Since their introduction in 2009, they have emerged as a valuable model for studying early embryogenesis, organ and tissue development, as well as tools in drug screening, disease modeling and personalized therapy. Organoids can now be established for various tissues, including brain, retina, thyroid, gastrointestinal, lung, liver, pancreas, and kidney. These micro-tissues resemble the native organ in terms of gene expression, protein expression, tissue architecture and cell-cell interactions. Despite the success of organoid-based research and the advances in patient-derived organoid culture, important challenges remain. In this review, we briefly showcase the evolution from the primary 3D systems to complex, multilayered 3D structures such as assembloids, gastruloids and ETiX embryoids. We discuss current developments in organoid research and highlight developments in organoid culturing systems and analysis tools which make organoids accessible for high-throughput and high-content screening. Finally, we summarize the potential of machine learning and computational modeling in conjunction with organoid systems.",Not About Sufficiency
Unlocking insights: Using machine learning to identify wasting and risk factors in Egyptian children under 5,"Introduction: Malnutrition, particularly wasting, continues to be a significant public health issue among children under five years in Egypt. Despite global advancements in child health, the prevalence of wasting remains a critical concern. This study employs machine learning techniques to identify and analyze the determinants of wasting in this population. Aim: To evaluate the prevalence of wasting among children under five years in Egypt and identify key factors associated with wasting using machine learning models. Methods: This study is based on secondary data sourced from the Demographic and Health Surveys (DHS), conducted in 2005, 2008, and 2014. Six machine learning classifiers (XGBoost, Logistic Regression, Random Forest, Gradient Boosting, K-Nearest Neighbor, and Decision Tree) were applied to the dataset. The study included children under five years of age, focusing on nutritional status, maternal health, and socio-economic factors. The dataset was cleaned, preprocessed, encoded using one-hot encoding, and split into training (70%) and test (30%) sets. Additionally, k-fold cross-validation and the StandardScaler function from Scikit-learn were used. Performance metrics such as accuracy, precision, recall, F1 score, and ROC-AUC were used to evaluate and compare the algorithms. Results: It was observed that 76.2% of the children in the dataset have normal nutritional status. Furthermore, 5.2% were found to be suffering from wasting (1.7% experiencing severe wasting and 3.5% moderate wasting), with notable regional disparities. The XGBoost model outperformed other models. Its efficiency metrics include an accuracy of 94.8%, precision of 94.7%, recall of 94.7%, F1 score of 94.7%, and an ROC-AUC of 99.4%. These results indicate that XGBoost was highly effective in predicting wasting. Conclusion: Machine learning techniques, particularly XGBoost, show significant potential for improving the classification of nutritional status and addressing wasting among children in Egypt. However, the limitations in simpler models highlight the need for further research to refine predictive tools and develop targeted interventions. Addressing the identified determinants of wasting can contribute to more effective public health strategies. (c) 2024 Elsevier Inc. All rights are reserved, including those for text and data mining, AI training, and similar technologies.",Not About Sufficiency
Development of a respiratory virus risk model with environmental data based on interpretable machine learning methods,"In recent years, numerous studies have explored the relationship between atmospheric conditions and respiratory viral infections. However, these investigations have faced certain limitations, such as the use of modestly sized datasets, a restricted geographical focus, and an emphasis on a limited number of respiratory pathogens. This study aimed to develop a nationwide respiratory virus infection risk prediction model through machine learning approach. We utilized the CRFC algorithm, a random forest-based method for multi-label classification, to predict the presence of various respiratory viruses. The model integrated binary classification outcomes for each virus category and incorporated air quality and meteorological data to enhance its accuracy. The data was collected from 31 regions in China between 2016 and 2021, encompassing pathogen detection, air quality indices, and meteorological measurements. The model's performance was evaluated using ROC curves, AUC scores, and precision-recall curves. Our model demonstrated robust performance across various metrics, with an average overall accuracy of 0.76, macro sensitivity of 0.75, macro precision of 0.77, and an average AUC score of 0.9. The SHAP framework was employed to interpret the model's predictions, revealing significant contributions from parameters such as age, NO2 levels, and meteorological conditions. Our model provides a reliable tool for predicting respiratory virus risks, with a comprehensive integration of environmental and clinical data. The model's performance metrics indicate its potential utility in clinical decision-making and public health planning. Future work will focus on refining the model and expanding its applicability to diverse populations and settings.",Not About Sufficiency
Development of a Smart DC Grid Model,"Smart grid and distributed generation should be the solution of the global climate change and the crisis energy of the main source of electrical power generation which is fossil fuel. In order to meet the rising electrical power demand and increasing service quality demands, as well as reduce pollution, the existing power grid infrastructure should be developed into a smart grid and distributed power generation which provide a great opportunity to address issues related to energy efficiency, energy security, power quality and aging infrastructure systems. The conventional of the existing distributed generation system is an AC grid while for a renewable resources requires a DC grid system. This paper explores the model of smart DC grid by introducing a model of smart DC grid with the stable power generation give a minimal and compressed circuitry that can be implemented very cost-effectively with simple components. The PC based application software for controlling was developed to show the condition of the grid and to control the grid become 'smart'. The model is then subjected to a severe system perturbation, such as incremental change in loads to test the performance of the system again stability. It is concluded that the system able to detect and controlled the voltage stability which indicating the ability of power system to maintain steady voltage within permissible rangers in normal condition.",Not About Sufficiency
Interoperability of Systems and Monitoring for Alert Itajai River Basin - SaDPreai v.3,"The project consists in developing a portal for public access to information for monitoring and alert levels of the rivers of the basin of Itajai, with data collected from telemetric stations, derived from existing systems and consolidated, mainly provided by civil defense each municipality and regulatory agencies, leveraging resources through the internet of things. All obtained data will be available and updated at intervals, serving compiler multiple data sources for integration of information to local arrangements, citizens and also to institutions interested in predicting floods in the valley of Itajai. All encompassed by a web application, centralizing, organizing, providing data for each station properly georeferenced maps accessible through the internet network. The proposed system aims to build a technology for collecting, interoperability and presentation of information freely, while preserving the security and privacy of people. Emphasis on free software contemplated in open code, using various other technologies like the i3geo, open layers, Google Maps and others. Also additionally allow the registration of people interested in receiving warnings by mobile phone text messages (SMS) or other forms of communication with the updated information to the levels of nearby rivers, reference points and the coverage of this project perimeters.",Not About Sufficiency
Choosing awareness over fear: Risk analysis and free trade support global food security,"Livestock production and global trade are key components to achieving food security, but are bedfellows with the risk for emergence and spread of infectious diseases. The World Trade Organization's Agreement on the Application of Sanitary and Phytosanitary Measures outlines provisions for member countries to protect animal, plant, and public health while promoting free trade. The capacity for risk analysis equips countries to increase access to export markets, improve local animal health and food safety regarding known hazards, and build the institutional capacity to respond to unexpected events. The COVID-19 pandemic has highlighted the need to detect, report, and implement effective response measures to emerging challenges on a local and global scale, and it is crucial that these measures are implemented in a way that supports food production and trade. The use of risk analysis coupled with sound understanding of underlying system dynamics will contribute to resilient and enduring food systems.",Not About Sufficiency
GFPrint�: A machine learning tool for transforming genetic data into clinical insights,"The increasing availability of massive genetic sequencing data in the clinical setting has triggered the need for appropriate tools to help fully exploit the wealth of information these data possess. GFPrint (TM) is a proprietary streaming algorithm designed to meet that need. By extracting the most relevant functional features, GFPrint (TM) transforms high-dimensional, noisy genetic sequencing data into an embedded representation, allowing unsupervised models to create data clusters that can be re-mapped to the original clinical information. Ultimately, this allows the identification of genes and pathways relevant to disease onset and progression. GFPrint (TM) has been tested and validated using two cancer genomic datasets publicly available. Analysis of the TCGA dataset has identified panels of genes whose mutations appear to negatively influence survival in non-metastatic colorectal cancer (15 genes), epidermoid non-small cell lung cancer (167 genes) and pheochromocytoma (313 genes) patients. Likewise, analysis of the Broad Institute dataset has identified 75 genes involved in pathways related to extracellular matrix reorganization whose mutations appear to dictate a worse prognosis for breast cancer patients. GFPrint (TM) is accessible through a secure web portal and can be used in any therapeutic area where the genetic profile of patients influences disease evolution.",Not About Sufficiency
"An integrated Bayesian networks and Geographic information system (BNs-GIS) approach for flood disaster risk assessment: A case study of Yinchuan, China","Flooding poses severe ecological and environmental challenges, requiring comprehensive risk assessments to inform the development of sustainable management practices. This study proposes a framework that integrates Bayesian networks (BNs) and Geographic information systems (GIS) for flood disaster risk assessment, based on an evaluation indicator system consisting of three dimensions: hazard, vulnerability, and exposure. The proposed BNs-GIS model synergistically combines the probabilistic reasoning capabilities of BNs to capture interdependencies among risk factors with the spatial analysis power of GIS. The model incorporates diverse data sources, including hydrological models, remote sensing, infrastructure details, land use patterns, and ecological indicators, to generate a comprehensive risk profile. The framework was applied Yinchuan, a city in northwest China prone to flash flooding events. The model incorporated sensitivity analyses to quantify uncertainties stemming from input data and model parametrizations, to enable robust risk estimation. Our findings demonstrate the efficacy of the BNs-GIS approach in identifying areas with elevated flood risk and assessing the reliability of risk estimates. This integrated methodology contributes to ecological assessment by providing a probabilistic yet spatially-explicit evaluation of flood hazards. The study contributes to the development of indicator-based monitoring and assessment programs, supporting informed decision-making for sustainable management practices in flood-prone urban ecosystems. By providing a comprehensive understanding of flood risks, this approach can help policymakers and urban planners develop more effective and sustainable flood management strategies. While the model shows promise, limitations include its reliance on the quality and availability of input data, and the need for local expertise in model parameterization. Future work should focus on incorporating real-time data and dynamic updating capabilities to enhance the model's applicability in rapidly changing urban environments.",Not About Sufficiency
DaST: Data-free Substitute Training for Adversarial Attacks,"Machine learning models are vulnerable to adversarial examples. For the black-box setting, current substitute attacks need pre-trained models to generate adversarial examples. However, pre-trained models are hard to obtain in real-world tasks. In this paper, we propose a data-free substitute training method (DaST) to obtain substitute models for adversarial black-box attacks without the requirement of any real data. To achieve this, DaST utilizes specially designed generative adversarial networks (GANs) to train the substitute models. In particular, we design a multi-branch architecture and label-control loss for the generative model to deal with the uneven distribution of synthetic samples. The substitute model is then trained by the synthetic samples generated by the generative model, which are labeled by the attacked model subsequently. The experiments demonstrate the substitute models produced by DaST can achieve competitive performance compared with the baseline models which are trained by the same train set with attacked models. Additionally, to evaluate the practicability of the proposed method on the real-world task, we attack an online machine learning model on the Microsoft Azure platform. The remote model misclassifies 98.35% of the adversarial examples crafted by our method. To the best of our knowledge, we are the first to train a substitute model for adversarial attacks without any real data. Our codes are publicly available(1).",Not About Sufficiency
An Efficient Machine Learning Method to Solve Imbalanced Data in Metabolic Disease Prediction,"The increase of obesity, its related diseases and the high incidence of metabolic diseases as a whole, constitute a major public health problem on a global scale. New strategies that allow for the discovery of novel metabolic disease-related genes are necessary to develop new treatments. In this paper, we proposed an efficient method to predict metabolic disease genes, solving the problem of imbalanced data. The method combined protein-protein interactions and miRNA-target interactions to construct integrated networks, whose topological properties can be used as features to train machine learning classifiers. We applied different strategies to optimize imbalanced class. The best model of gradient boosting achieved a significant F1-score of 0.82. When testing the model with non-disease genes, we predicted 549 candidates, out of which 123 were validated indirectly from literature to be related to metabolic diseases. The remaining genes' functions were investigated by gene enrichment analysis, revealing their association with diseases known to co-occur with metabolic diseases, such as cancer and cardiovascular conditions. These results indicated that this method contributed to the identification of novel metabolic disease-related genes.",Not About Sufficiency
Safety: A collective and embedded competency. An ethnographic study of safety practices at an industrial workplace in the Netherlands,"Introduction: Organizations place strong emphasis on the standardized occupational health and safety procedures to reduce work -related illnesses and workplace accidents. However, standardized procedures are not always followed up in daily work practices. Organizations must cope with the differences between standardized procedures and local adaptation by employees. Methods: This ethnographic field study at an industrial workplace in the Netherlands provides insights into employees ' everyday work practices, how these work practices are shaped, and how they relate to local occupational health and safety procedures. Acknowledging safety as a competency embedded in work practices, as introduced by Gherardi and Nicolini (2002), offers a theoretical point of view for looking beyond the dichotomy of standardization and local adaptations. Results: The results show that a standardized and noncontextualized occupational health and safety management system that focuses on accidentfree days and compliance actually leads to ignorance of practical and tacit competences of workers and no learning and improvement of safety procedures can take place. However, our findings also illustrate how employees in their informal everyday work practices reduce the risks produced by the safety system itself. Conclusion: Overall, the results indicate that social interactions among employees, leaders, and management within the organization play an important role in workplace safety. The analysis highlights the value of vulnerability and trust in relationships at work to be able to learn and develop safety procedures that align with local demands. Practical applications: This study emphasizes the need for participatory approaches in creating safer and healthier workplaces. The cocreation of occupational health and safety (OHS) rules and procedures, however, can only function if they are combined with a responsive leadership style.",Not About Sufficiency
On the Path to 6G: Embracing the Next Wave of Low Earth Orbit Satellite Access,"Offering space-based Internet services with mega-constellations of low Earth orbit (LEO) satellites is a promising solution to connecting the unconnected. It can complement the coverage of terrestrial networks to help bridge the digital divide. However, there are challenges from operational obstacles to technical hurdles facing the development of LEO satellite access. This article provides an overview of the state of the art in LEO satellite access, including the evolution of LEO satellite constellations and capabilities, critical technical challenges and solutions, standardization aspects from 5G evolution to 6G, and business considerations. We also identify several areas for future exploration to realize tight integration of LEO satellite access with terrestrial networks in 6G.",Not About Sufficiency
A novel machine-learning aided platform for rapid detection of urine ESBLs and carbapenemases: URECA-LAMP,"Pathogenic gram-negative bacteria frequently carry genes encoding extended-spectrum beta-lactamases (ESBL) and/or carbapenemases. Of great concern are carbapenem resistant Escherichia coli, Klebsiella pneumoniae, Pseudomonas aerugi nosa, and Acinetobacter baumannii. Despite the need for rapid AMR diagnostics globally, current molecular detection methods often require expensive equipment and trained personnel. Here, we present a novel machine-learning-aided platform for the rapid detection of ESBLs and carbapenemases using Loop-mediated isothermal Amplification (LAMP). The platform consists of (i) an affordable device for sample lysis, LAMP ampli fication, and visual fluorometric detection; (ii) a LAMP screening panel to detect the most common ESBL and carbapenemase genes; and (iii) a smartphone application for automated interpretation of results. Validation studies on clinical isolates and urine samples demonstrated percent positive and negative agreements above 95% for all targets. Accuracy, precision, and recall values of the machine learning model deployed in the smartphone application were all above 92%. Providing a simplified workflow , minimal operation training, and results in less than an hour, this study demonstrated the platform's feasibility for near-patient testing in resource-limited settings.",Not About Sufficiency
Comparison of feature learning methods for non-invasive interstitial glucose prediction using wearable sensors in healthy cohorts: a pilot study,"Background Alterations in glucose metabolism, especially the postprandial glucose response (PPGR), are crucial contributors to metabolic dysfunction, which underlies the pathogenesis of metabolic syndrome. Personalized low-glycemic diets have shown promise in reducing postprandial glucose spikes. However, current methods such as invasive continuous glucose monitoring (CGM) or multi-omics data integration to assess PPGR have limitations, including cost and invasiveness that hinder the widespread adoption of these methods in primary disease prevention. Our aim was to assess machine learning algorithms for predicting individual PPGR using non-invasive wearable devices, thereby, circumventing the limitations associated with the existing approaches. By identifying the most accurate model, we sought to provide a more accessible and efficient method for managing glucose metabolic dysfunction. Methods This data-driven analysis used the experimental dataset from the SENSE ( ""Systemische Ern & auml;hrungsmedizin "") study. Healthy participants used an Empatica E4 wristband and Abbott Freestyle Libre 3 CGM for 10 days. Blood volume pulse, electrodermal activity, heart rate, skin temperature, and the corresponding CGM values were measured. Subsequently, four data-driven deep learning (DL) models-convolutional neural network, lightweight transformer, long short-term memory with attention, and Bi-directional LSTM (BiLSTM) were implemented and compared to determine the potential of DL in predicting interstitial glucose levels without involving food and activity logs. Results The proposed BiLSTM achieved the best interstitial glucose prediction performance, with an average root mean squared error of 13.42 mg/dL, an average mean absolute percentage error of 0.12, and only 3.01% values falling within area D in Clarke error grid analysis, incorporating the leave-one-out cross-validation strategy for a five-minute prediction horizon. Conclusion The findings of this study may demonstrate the feasibility of transferring knowledge gained from invasive glucose monitoring devices to non-invasive approaches. Furthermore, it could emphasize the promising prospects of combining DL with wearable technologies to predict glucose levels in healthy individuals.",Not About Sufficiency
Coping strategies and trajectories of life satisfaction among households in a voluntary planned program of relocation from a flood-risk area,"Relocation from areas at risk is a highly effective, yet contested response to natural hazards. Affected households must deal with multiple long-term impacts on their livelihoods and communities. This study explores coping processes of households subjected to a voluntary home buyout scheme in the Danube floodplain in Austria. In a longitudinal study with three yearly waves of semi-structured qualitative interviews, 79 households were monitored over the decision-making, formation, implementation, and stabilization phases of the relocation process. Coping applies not just to those who leave but also to those who stay. Cognitive restructuring, opposition, problem solving, rumination, and escape/avoidance emerge as main coping strategies. These strategies take different characteristics depending on life circumstances and the phase of the relocation process. Most households follow a life satisfaction trajectory of resilience, recovery, or delayed recovery; they either maintain normal functioning or return to it within 5 years after announcement of the relocation program. The relocation stressor plays a minor role compared with family, job, health, and partnership, unless the relocation coincides with a personal crisis that already overstretches coping capacities. Few households exhibit a trajectory of chronic distress and remain deadlocked in social withdrawal or rumination of the lost former residence. Besides material impacts, households show a broad array of psychosocial reactions that need accounting for by cost-benefit assessments of relocation programs. Program managers should encourage positive coping strategies (e.g., support problem solving by formulation of early, structured and realistic plans) and discourage negative strategies (e.g., pre-empt opposition with citizen participation).",Not About Sufficiency
Valorization of cheese whey wastewater to achieve sustainable development goals,"While several studies have focused on managing cheese wastes for bioresource recovery in developing countries, assessing the techno-economic feasibility, carbon emissions, and sustainability performance of such systems is still to be determined. This research gap was addressed by designing a reliable cheese whey wastewater (CWW) management system that could recover biogas and biochar, followed by estimating the associated profits, net present values, and payback periods. Implementing this system to treat 1 m3 CWW with a chemical oxygen demand (COD) of 56.2 g/L could generate bio-CH4 (8.71 m3), biochar (1.21 kg), pyro-oil (1.99 kg), and syngas (0.31 kg) while self-consuming about half the energy produced. The revenues of carbon credit, COD shadow price, electricity, and biochar could maintain a 6.1-year payback period and a 10.15 % internal rate of return. The environmental benefits could recoup the annual expenses accompanied by biogas/biochar utilization in agricultural, energy, and field applications, demonstrating that the system's profitability criteria were less sensitive to variations in operational costs. Although the CO2-equivalent emissions represented significant environmental burdens, the green credit of biogenic CWW could endorse the life cycle assessment's mid-point and end-point categories. Ensuring clean water and affordable energy, recovering biochar as an enriched soil amendment, and enabling policies/finances by the proposed CWW valorization approach could fulfill sustainable development goals 6, 7, and 13 in low-income nations. Future studies are required to optimize the anaerobic digestion and pyrolysis operating parameters using machine learning techniques, reducing the running costs and improving the minimum selling prices of biofuel recovery from CWW.",Not About Sufficiency
Urban design and health: progress to date and future challenges,"Over the last 15 years, a growing body of Australian and international evidence has demonstrated that urban design attributes are associated with a range of health outcomes. For example, the location of employment, shops and services, provision of public and active transport infrastructure and access to open space and recreational opportunities are associated with chronic disease risk factors such as physical activity levels, access to healthy food, social connectedness, and air quality. Despite the growing knowledge base, this evidence is not being consistently translated into urban planning policy and practice in Australia. Low-density neighbourhoods with poor access to public transport, shops and services continue to be developed at a rapid rate in the sprawling outer suburbs of Australian cities. This paper provides an overview of the evidence of the association between the built environment and chronic diseases, highlighting progress and future challenges for health promotion. It argues that health promotion practitioners and researchers need to more closely engage with urban planning practitioners, policymakers and researchers to encourage the creation of healthy urban environments through integrated transport, land use and infrastructure planning. There is also a need for innovative research to evaluate the effectiveness of policy options. This would help evidence to be more effectively translated into policy and practice, making Australia a leader in planning healthy communities.",Not About Sufficiency
Downscaling satellite night-time lights imagery to support within-city applications using a spatially non-stationary model,"For mapping and monitoring socioeconomic activities in cities, night-time lights (NTL) satellite sensor images are used widely, measuring the light intensity during the night. However, the main challenge to mapping human activities in cities using such NTL satellite sensor images is their coarse spatial resolution. To address this drawback, spatial downscaling of satellite nocturnal images is a plausible solution. However, common approaches for spatial downscaling employ spatially stationary models that may not be optimal where the data are spatially heterogeneous. In this research, a geostatistical model termed Random Forest area-to-point regression Kriging (RFATPK) was employed to disaggregate coarse spatial scale VIIRS NTL images (450 m) to a fine spatial scale (100 m). The RF predicts at a coarse resolution from fine spatial resolution variables, such as a Population raster. ATPK then downscales the coarse residuals from the RF prediction. In numerical experiments, RFATPK was compared with three benchmark techniques, including the simple Allocation of pixel values from the coarse resolution NTL data, Machine Learning with Splines and Geographically Weighted Regression. The downscaled results were validated using fine resolution LuoJia 1-01 satellite sensor imagery. RFATPK produced more accurate disaggregated images than the three benchmark approaches, with mean root mean square errors (RMSE) for the year 2018 of 13.89 and 6.74 nWcm- 2 sr-1, for Mumbai and New Delhi, respectively. Also, the property of perfect coherence, measured by the Correlation Coefficient, was preserved consistently when applying RFATPK and was almost 1 for all years. The applicability of the disaggregated NTL data to monitor socioeconomic activities at the within-city scale against the reference NTL was illustrated by utilizing them as a proxy for the Gross National Income (GNI) per capita and the Night Light Development Index. The GNI estimation from the down scaled NTL outperformed the coarse resolution NTL when examining their coefficients of determination, with R2 of 0.67 and 0.47 for the GNI estimation using the fine and coarse resolution NTL data, respectively. For the Night Light Development Index (NLDI), the results of the index were compared by measuring their correlation with the Human Development Index (HDI). The NLDI from the downscaled NTL outperformed the coarse resolution NTL when measuring the correlation with the HDI, with Pearson's correlation coefficients of-0.48 and-0.35 for the NLDI using the fine and coarse resolution NTL data, respectively, for New Delhi. The outcomes indicate that RFATPK provides more accurate predictions than the three benchmark techniques and the downscaled NTL data are more suitable for fine scale socioeconomic applications, as demonstrated by the NLDI and GNI. This research, thus, shows that the RFATPK solution for NTL disaggregation can facilitate data enhancement for fine-scale sub national applications in social sciences and can be generalized worldwide by including other cities as well as other applications.",Not About Sufficiency
Secure human action recognition by encrypted neural network inference,"Advanced computer vision technology can provide near real-time home monitoring to support ""aging in place"" by detecting falls and symptoms related to seizures and stroke. In this paper, the authors propose a strategy that uses homomorphic encryption, which guarantees information confidentiality while retaining action detection. Advanced computer vision technology can provide near real-time home monitoring to support ""aging in place"" by detecting falls and symptoms related to seizures and stroke. Affordable webcams, together with cloud computing services (to run machine learning algorithms), can potentially bring significant social benefits. However, it has not been deployed in practice because of privacy concerns. In this paper, we propose a strategy that uses homomorphic encryption to resolve this dilemma, which guarantees information confidentiality while retaining action detection. Our protocol for secure inference can distinguish falls from activities of daily living with 86.21% sensitivity and 99.14% specificity, with an average inference latency of 1.2 seconds and 2.4 seconds on real-world test datasets using small and large neural nets, respectively. We show that our method enables a 613x speedup over the latency-optimized LoLa and achieves an average of 3.1x throughput increase in secure inference compared to the throughput-optimized nGraph-HE2.",Not About Sufficiency
Access to care through telehealth among US Medicare beneficiaries in the wake of the COVID-19 pandemic,"BackgroundThe coronavirus disease 2019 (COVID-19) public health emergency has amplified the potential value of deploying telehealth solutions. Less is known about how trends in access to care through telehealth changed over time. ObjectivesTo investigate trends in forgone care and telehealth coverage among Medicare beneficiaries during the COVID-19 pandemic. MethodsA cross-sectional study design was used to analyze the outcomes of 31,907 Medicare beneficiaries using data from three waves of survey data from the Medicare Current Beneficiary Survey COVID-19 Supplement (Summer 2020, Fall 2020, and Winter 2021). We identified informative variables through a multivariate classification analysis utilizing Random Forest machine learning techniques. FindingsThe rate of reported forgone medical care because of COVID-19 decreased largely (22.89-3.31%) with a small increase in telehealth coverage (56.24-61.84%) from the week of June 7, 2020, to the week of April 4 to 25, 2021. Overall, there were 21.97% of respondents did not know whether their primary care providers offered telehealth services; the rates of forgone care and telehealth coverage were 11.68 and 59.52% (11.73 and 81.18% from yes and no responses). Our machine learning model predicted the outcomes accurately utilizing 43 variables. Informative factors included Medicare beneficiaries' age, Medicare-Medicaid dual eligibility, ability to access basic needs, certain mental and physical health conditions, and interview date. ConclusionsThis cross-sectional survey study found proliferation and utilization of telehealth services in certain subgroups during the COVID-19 pandemic, providing important access to care. There is a need to confront traditional barriers to the proliferation of telehealth. Policymakers must continue to identify effective means of maintaining continuity of care and growth of telehealth services.",Not About Sufficiency
Ventilation Effectiveness of Building Cluster,"While every one of us is striving for a better standard of living, one should reckon that both indoor and outdoor air quality are the key parameters attributed to our living standard. There is a growing concern that outdoor air pollutants could result in the deterioration of indoor air quality. The use of natural ventilation in buildings has received much attention in the last decade, which is a part of the building energy efficiency strategy. A key consideration in adopting natural ventilation is the pollutant concentration of outdoor air near the building. From the standpoint of energy conservation and indoor air quality, it is imperative to have not only fresh incoming air, but also minimal pollutants from the buildings' proximity. Ventilation study is not only important for improving indoor air quality, it also ensures better outdoor living environment in city development. Urbanization is undergoing in many countries as their population grows. In the past few decades, high-rise buildings were erected in cities as a landmark of their modernizations. This has changed not only the outlook of a city, but also the local environment that surrounds the buildings as well as the microclimatic environment of the city. While the constructions of new high-rises are inevitable and imminent, examination of the airflow pattern and ventilation efficiencies of a single and group of buildings is crucial in understanding the whole city ventilation. Besides, thermal convection boundary layer airflow, due to the building wall surface temperature or heat flux, could play an important role in the ventilation of a building cluster. This study contributes to the understanding of the microclimatic environment of a building cluster. This will facilitate the design of urban ventilation strategies, and help landscape architecture and urban planning to achieve a better urban environment.",Not About Sufficiency
Machine learning insights on activities of daily living disorders in Chinese older adults,"Objective: This study on the aged population in China first used a large-scale longitudinal survey database to explore how different life factors affect their ability to engage in daily activities. We select and integrate multiple machine models to obtain an excellent model for analyzing relationships. Based on the identified factors, our goal is to help them maintain a good daily life and quality of life. Method: We analyzed data from 13,220 older individuals participating in the China Longitudinal Health Longevity Survey (CLHLS) from 2002 to 2018. ADL was measured based on participants' self-reported results. Nine machine learning algorithms, including neural networks and an ensemble model, were employed with a 2/3 training and 1/3 testing split. Model performance was evaluated using the area under the curve (AUC), sensitivity, and specificity, while logistic regression assessed the relationship between lifestyle changes and ADL disorders. Result: The K-nearest neighbors (KNN) and decision tree algorithms showed the best performance, with AUCs of 0.8598 and 0.8322, respectively. Combining results from all models improved the AUC to 0.8619. Activities, such as playing mahjong, engaging in outdoor work, and reducing TV time, were linked to lower ADL decline, with greater participation in social activities and pet care also being beneficial. Conclusion: Machine learning algorithms, especially ensemble models, can effectively identify older adults at risk for ADL disorders. Increased outdoor activity, social engagement, and dietary adjustments are associated with a decreased risk of ADL deterioration. Translational significance:",Not About Sufficiency
Planning for urban life: A new approach of sustainable land use plan based on transit-oriented development,"Transit-oriented development (TOD) is an integration of transportation systems with land use and has been given priority in sustainability strategies. However, most of the existing studies on TOD emphasize the economic and environmental perspectives of sustainability, paying little attention to social equity. Moreover, despite governments worldwide are gradually trying to address unsustainable issues associated with dramatic urbanization through a framework of TOD development, the improvement of land use planning necessary to achieve variegated sustainability within a safe trajectory is not being targeted or achieved. This paper aims to establish a framework of TOD planning in China's context that could be applied beyond the concept to planning experts and policymakers on how to integrate land use planning with TOD to achieve sustainability. We further applied an empirical study of Jiaomei, China to demonstrate the application of the designed framework. The study provided a new framework for understanding sustainable transportation development with land use management as applied to the urban planning process and for exploring new paths in practice toward sustainability.",Not About Sufficiency
iDVEIP: A computer-aided approach for the prediction of viral entry inhibitory peptides,"With the notable surge in therapeutic peptide development, various peptides have emerged as potential agents against virus-induced diseases. Viral entry inhibitory peptides (VEIPs), a subset of antiviral peptides (AVPs), offer a promising avenue as entry inhibitors (EIs) with distinct advantages over chemical counterparts. Despite this, a comprehensive analytical platform for characterizing these peptides and their effectiveness in blocking viral entry remains lacking. In this study, we introduce a groundbreaking in silico approach that leverages bioinformatics analysis and machine learning to characterize and identify novel VEIPs. Cross-validation results demonstrate the efficacy of a model combining sequence-based features in predicting VEIPs with high accuracy, validated through independent testing. Additionally, an EI type model has been developed to distinguish peptides specifically acting as Eis from AVPs with alternative activities. Notably, we present iDVEIP, a web-based tool accessible at , designed for automatic analysis and prediction of VEIPs. Emphasizing its capabilities, the tool facilitates comprehensive analyses of peptide characteristics, providing detailed amino acid composition data for each prediction. Furthermore, we showcase the tool's utility in identifying EIs against severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2).",Not About Sufficiency
Enabling Global Clinical Collaborations on Identifiable Patient Data: The Minerva Initiative,"The clinical utility of computational phenotyping for both genetic and rare diseases is increasingly appreciated; however, its true potential is yet to be fully realized. Alongside the growing clinical and research availability of sequencing technologies, precise deep and scalable phenotyping is required to serve unmet need in genetic and rare diseases. To improve the lives of individuals affected with rare diseases through deep phenotyping, global big data interrogation is necessary to aid our understanding of disease biology, assist diagnosis, and develop targeted treatment strategies. This includes the application of cutting-edge machine learning methods to image data. As with most digital tools employed in health care, there are ethical and data governance challenges associated with using identifiable personal image data. There are also risks with failing to deliver on the patient benefits of these new technologies, the biggest of which is posed by data siloing. The Minerva Initiative has been designed to enable the public good of deep phenotyping while mitigating these ethical risks. Its open structure, enabling collaboration and data sharing between individuals, clinicians, researchers and private enterprise, is key for delivering precision public health.",Not About Sufficiency
OSG and GPUs: A tale of two use cases,"With the increase of power and reduction of cost of GPU accelerated processors a corresponding interest in their uses in the scientific domain has spurred. OSG users are no different and they have shown an interest in accessing GPU resources via their usual workload infrastructures. Grid sites that have these kinds of resources also want to make them grid available. In this talk, we discuss the software and infrastructure challenges and limitations of the OSG implementations to make GPU's widely accessible over the grid. Two use cases are considered for this. First: IceCube, a big VO with a well-curated software stack taking advantage of GPUs with OpenCL. Second, a more general approach to supporting the grid use of industry and academia maintained machine learning libraries like Tensorflow, and Keras on the grid using Singularity.",Not About Sufficiency
General Protocol for the Accurate Prediction of Molecular <SUP>13</SUP>C/<SUP>1</SUP>H NMR Chemical Shifts via Machine Learning Augmented DFT,"An accurate prediction of NMR chemical shifts at affordable computational cost is very important for different types of structural assignments in experimental studies. Density functional theory (DFT) and gauge-including atomic orbital (GIAO) are two of the most popular computational methods for NMR calculation, yet they often fail to resolve ambiguities in structural assignments. Here, we present a new method that uses machine learning (ML) techniques (DFT + ML) that significantly increases the accuracy of C-13/H-1 NMR chemical shift prediction for a variety of organic molecules. The input of the generalizable DFT + ML model contains two critical parts: one is a vector providing insights into chemical environments, which can be evaluated without knowing the exact geometry of the molecule; the other one is the DFT-calculated isotropic shielding constant. The DFT + ML model was trained with a data set containing 476 C-13 and 270 H-1 experimental chemical shifts. For the DFT methods used here, the root mean square deviations (RMSDs) for the errors between predicted and experimental C-13/H-1 chemical shifts can be as small as 2.10/0.18 ppm, which is much lower than those from simple DFT (5.54/0.25 ppm), or DFT + linear regression (LR) (4.77/0.23 ppm) approaches. It also has a smaller maximum absolute error than two previously proposed NMR-predicting ML models. The robustness of the DFT + ML model is tested on two classes of organic molecules (TIC10 and hyacinthacines), where the correct isomers were unambiguously assigned to the experimental ones. Overall, the DFT + ML model shows promise for structural assignments in a variety of systems, including stereoisomers, that are often challenging to determine experimentally.",Not About Sufficiency
Expanding infrastructure and growing anthropogenic impacts along Arctic coasts,"The accelerating climatic changes and new infrastructure development across the Arctic require more robust risk and environmental assessment, but thus far there is no consistent record of human impact. We provide a first panarctic satellite-based record of expanding infrastructure and anthropogenic impacts along all permafrost affected coasts (100 km buffer, approximate to 6.2 Mio km(2)), named the Sentinel-1/2 derived Arctic Coastal Human Impact (SACHI) dataset. The completeness and thematic content goes beyond traditional satellite based approaches as well as other publicly accessible data sources. Three classes are considered: linear transport infrastructure (roads and railways), buildings, and other impacted area. C-band synthetic aperture radar and multi-spectral information (2016-2020) is exploited within a machine learning framework (gradient boosting machines and deep learning) and combined for retrieval with 10 m nominal resolution. In total, an area of 1243 km(2) constitutes human-built infrastructure as of 2016-2020. Depending on region, SACHI contains 8%-48% more information (human presence) than in OpenStreetMap. 221 (78%) more settlements are identified than in a recently published dataset for this region. 47% is not covered in a global night-time light dataset from 2016. At least 15% (180 km(2)) correspond to new or increased detectable human impact since 2000 according to a Landsat-based normalized difference vegetation index trend comparison within the analysis extent. Most of the expanded presence occurred in Russia, but also some in Canada and US. 31% and 5% of impacted area associated predominantly with oil/gas and mining industry respectively has appeared after 2000. 55% of the identified human impacted area will be shifting to above 0 C-circle ground temperature at two meter depth by 2050 if current permafrost warming trends continue at the pace of the last two decades, highlighting the critical importance to better understand how much and where Arctic infrastructure may become threatened by permafrost thaw.",Not About Sufficiency
'Scaling up' our understanding of environmental effects of marine renewable energy development from single devices to large-scale commercial arrays,"Global expansion of marine renewable energy (MRE) technologies is needed to help address the impacts of climate change, to ensure a sustainable transition from carbon-based energy sources, and to meet national energy security needs using locally-generated electricity. However, the MRE sector has yet to realize its full potential due to the limited scale of device deployments (i.e., single devices or small demonstration-scale arrays), and is hampered by various factors including uncertainty about environmental effects and how the magnitude of these effects scale with an increasing number of devices. This paper seeks to expand our understanding of the environmental effects of MRE arrays using existing frameworks and through the adaptation and application of cumulative environmental effects terminology to key stressor-receptor interactions. This approach facilitates the development of generalized concepts for the scaling of environmental effects for key stressor-receptor in-teractions, identifying high priority risks and revealing knowledge gaps that require investigation to aid expansion of the MRE sector. Results suggest that effects of collision risk for an array may be additive, antag-onistic, or synergistic, but are likely dependent on array location and configuration. Effects of underwater noise are likely additive as additional devices are deployed in an array, while the effects of electromagnetic fields may be dominant, additive, or antagonistic. Changes to benthic habitats are likely additive, but may be dependent on array configuration and could be antagonistic or synergistic at the ecosystem scale. Effects of displacement, entanglement, and changes to oceanographic systems for arrays are less certain because little information is available about effects at the current scale of MRE development.",Not About Sufficiency
Metropolitan fringes as strategic areas for urban resilience and sustainable transitions: Insights from Barcelona Metropolitan Area,"In an urbanized world, the challenges posed by climate change need to be met through innovative planning for cities and regions. Urban resilience demands that cities adopt new models centered on carbon neutrality, ecosystem services and biodiversity enhancement, circularity, and social inclusion. In light of the multiple interdependencies of metropolitan systems and the natural and territorial potentialities present on their fringes, this paper discusses the metropolitan fringes as strategic for urban resilience and sustainable transitions. By combining a mixed-method analysis of adaptation and urban plans with semi-structured interviews performed with key technicians, the analysis is focused on an inter-municipal fringe zone within the Barcelona Metropolitan Area - the Bes`os territory. Aiming to contribute to the discussions related to Territorial innovation for cities and regions, the paper offers a multi-scale perspective and illustrates innovative urban transformation strategies across scales. The results highlight that governance and multiple planning and participation instances together with technical and financial support are essential for consensus building in an incremental process. Furthermore, the planning strategies of the case study sought to integrate and enhance the green and blue infrastructures and transform consolidated urban areas with a focus on energy transition, sustainable mobility, circularity, and social inclusion. In conclusion, it is argued that resilience has a multi-scalar perspective within the metropolitan context and should be integrated with planning policies at different scales from a coordinated vision. Although adaptation and urban projects have a critical local element, a broader and more strategic vision is necessary, especially in metropolitan fringe areas.",Not About Sufficiency
CryptoSPN: Privacy-Preserving Sum-Product Network Inference,"AI algorithms, and machine learning (ML) techniques in particular, are increasingly important to individuals' lives, but have caused a range of privacy concerns addressed by, e.g., the European GDPR. Using cryptographic techniques, it is possible to perform inference tasks remotely on sensitive client data in a privacy-preserving way: the server learns nothing about the input data and the model predictions, while the client learns nothing about the ML model (which is often considered intellectual property and might contain traces of sensitive data). While such privacy-preserving solutions are relatively efficient, they are mostly targeted at neural networks, can degrade the predictive accuracy, and usually reveal the network's topology. Furthermore, existing solutions are not readily accessible to ML experts, as prototype implementations are not well-integrated into ML frameworks and require extensive cryptographic knowledge. In this paper, we present CryptoSPN, a framework for privacy-preserving inference of sum-product networks (SPNs). SPNs are a tractable probabilistic graphical model that allows a range of exact inference queries in linear time. Specifically, we show how to efficiently perform SPN inference via secure multi-party computation (SMPC) without accuracy degradation while hiding sensitive client and training information with provable security guarantees. Next to foundations, CryptoSPN encompasses tools to easily transform existing SPNs into privacy-preserving executables. Our empirical results demonstrate that CryptoSPN achieves highly efficient and accurate inference in the order of seconds for medium-sized SPNs.",Not About Sufficiency
DACOS-A Manually Annotated Dataset of Code Smells,"Researchers apply machine-learning techniques for code smell detection to counter the subjectivity of many code smells. Such approaches need a large, manually annotated dataset for training and benchmarking. Existing literature offers a few datasets; however, they are small in size and, more importantly, do not focus on the subjective code snippets. In this paper, we present DACOS, a manually annotated dataset containing 10, 267 annotations for 5, 192 code snippets. The dataset targets three kinds of code smells at different granularity-multifaceted abstraction, complex method, and long parameter list. The dataset is created in two phases. The first phase helps us identify the code snippets that are potentially subjective by determining the thresholds of metrics used to detect a smell. The second phase collects annotations for potentially subjective snippets. We also offer an extended dataset DACOSX that includes definitely benign and definitely smelly snippets by using the thresholds identified in the first phase. We have developed TAGMAN, a web application to help annotators view and mark the snippets one-by-one and record the provided annotations. We make the datasets and the web application accessible publicly. This dataset will help researchers working on smell detection techniques to build relevant and context-aware machine-learning models.",Not About Sufficiency
"Transport indicator analysis and comparison of 151 urban areas, based on open source data","IntroductionThe accurate analysis and comparison of transport indicators from a large variety of urban areas can help to evaluate the performance of different adopted transport policies. This paper attempts to determine important transport and socio-economic indicators from 151 urban areas and 51 countries, based on comparable, directly observable open-source data such as OpenstreetMap (OSM) and the TomTom database.AnalysisThis is the first, systematic indicator-analysis using recent, open source data from different urban areas around the world. The indicator road kilometers per person, sometimes cited as infrastructure accessibility is calculated by processing OSM data. Information on congestion levels have been taken from the TomTom database and socio-economic data from various, publicly accessible databases. Relations between indicators are identified through correlations and regression models are calibrated, quantifying the relation between transport infrastructure and performance indicators. Three sub-categories of cities with different population sizes (small cities, large cities and metropolises) are defined and studied individually. In addition, a qualitative analysis is performed, putting five different indicators into relation.Results & ConclusionsThe main results reconfirm previous findings but with a larger sample size and more comparable data. Good correlation values between infrastructure accessibility, socio-economic indicators, and congestion levels are demonstrated. It is shown that cities with higher GDP have generally built more infrastructure which in turn reduces their congestion levels. In particular, for cities with low population density (above approximately 1500 inh. Per sq.km), more roads per inhabitant lead to lower congestion levels; cities with high population density have in general lower congestion levels if the rail infrastructure per person ratio is high. Furthermore, these cities increasing railways per person is more effective in reducing congestions than increasing road length per person.",Not About Sufficiency
Improving reinforcement learning with human assistance: an argument for human subject studies with HIPPO Gym,"Reinforcement learning (RL) is a popular machine learning paradigm for game playing, robotics control, and other sequential decision tasks. However, RL agents often have long learning times with high data requirements because they begin by acting randomly. In order to better learn in complex tasks, we argue that an external teacher can often significantly help the RL agent learn. OpenAI Gym is a common framework for RL research, including a large number of standard environments and agents, making RL research significantly more accessible. This article introduces our new open-source RL framework, the Human Input Parsing Platform for Openai Gym (HIPPO Gym), and the design decisions that went into its creation. The goal of this platform is to facilitate human-RL research, making human-in-the-loop RL more accessible, including learning from demonstrations, learning from feedback, or curriculum learning. In addition, all experiments can be conducted over the internet without any additional software needed on the client's computer, making experiments at scale significantly easier.",Not About Sufficiency
Associations of multiple exposures to persistent toxic substances with the risk of hyperuricemia and subclinical uric acid levels in BIOAMBIENT.ES study,"Hyperuricemia is becoming a serious public health issue, which is highly influenced by environmental factors, although there is still controversial information on the potential influence of the exposure to Persistent Toxic Substances (PTSs) in the general population. In this study we aimed to assess the association. PTS exposure with uric acid homeostasis in a sample of the Spanish population. Participants were recruited during 2009-2010 in all the main geographical areas of Spain. Exposure to 34 PTSs was estimated by chemical analyses of serum levels of 6 Polychlorinated Biphenyls (PCBs, n = 950), 13 Organochlorine Pesticides (OCPs, n = 453), 6 Perfluoroalkyl Substances (PFAs, n = 755), 7 Polybrominated Diphenyl Ethers (PBDEs, n = 365), urinary Cadmium (n = 926), and Lead in whole blood (n = 882). The two study outcomes were defined as the prevalence of hyperuricemia in the study population and uric acid levels, the latter only in individuals with no previous diagnosis of hyperuricemia. Statistical analyses were performed by means of binomial logistic regression and linear regression, and mixture effects were screened using Weighted Quantile Sum Regression (WQS). Serum concentrations of gamma-HCH, o,p'-DDE, PCB-138, PCB-153, PFOA, and urinary Cadmium were associated with an increased risk of hyperuricemia, while PBDE-153 showed an inverse association with the effect. Furthermore, exposure to Cadmium, PCB-138, and to PCB-153 was positively associated with uric acid levels. Results were consistent after lipid adjustment or standardization. WQS analyses revealed a major contribution of PCB-153 within the PCB mixture on both the risk of hyperuricemia and uric acid levels. Sensitivity analyses were performed by adjusting for dietary habits, fasting glucose and estimated glomerular filtration rate. Overall, we found novel associations between human exposure to mixtures of PTSs and disturbances in uric acid homeostasis. However, we cannot completely rule out potential residual confounding effect or reversed-causality related to the cross-sectional design.",Not About Sufficiency
Introduction to (teaching/learning about) digital libraries,"This tutorial provides a thorough and deep introduction to the DL field, building upon a firm theoretical foundation (starting with ""5S"": Streams, Structures, Spaces, Scenarios, Societies [1]), giving careful definitions and explanations of all the key parts of a ""minimal digital library"", and expanding from that basis to cover key DL issues, illustrated with a well-chosen set of case studies. Goals are to: aid those with CS, library, or information science backgrounds to enter the DL field clarify key terms and concepts to provide a basis to understand JCDL discussions explain how DL services fit into a simple taxonomic framework, supporting composition enhance concern for quality in DLs by providing a contextual setting in the Information Life Cycle, and precisely specifying popular indicators show those teaching a DL course how to use a forthcoming book, by the presenters personalize the tutorial based on a list of top priority goals from each attendee, making use of having 2 presenters who can switch off or handle different groups Duration: fall-day Target audience, including level of experience: Audience 1: Those attending JCDL for the first time, to become oriented. Audience 2: Those interested in DL theory in general, or 5S in particular. Audience 3: nose teaching DL courses, so as to be prepared to use the new book. Level of experience required: introductory. Those at intermediate or advanced levels could benefit as well, since the 5S framework has broad applicability for planners, designers, implementers, and evaluators. Outline: Introduction: Definitions, Motivation, Goals, History, Related Areas The 'Ss' Streams Issues on Processing and Analysis of Text, Image Video, and Audio, Integrating Streams (e.g, synchronization) Structures Digital Objects and Metadata, Knowledge Structures and Representations (Databases, Ontologies, Thesauri, Dictionary/ Lexicon/ Authority Files, Indexes, Clusters/ Classification Schemes) Spaces Retrieval Models, User Interfaces, and Visualization Scenarios Information Needs/Access, Scenario-Based Design, Usability, Logging Societies User Communities, Social-Economical Issues Higher Level Concepts Collections Packages, Granularity, Collection Development Policies, Large and Distributed Collections Catalogs Cataloguing (Costs, Sharing, AACR2), Manual vs. (Semi)Automatic, Distributed vs. Decentralized, OPACs, Coverage, Breadth, Specificity, Depth, Management Repositories/Archives Naming, Identifiers, Types (Institutional, personal, genrespecific, aggregate), Architectures, Interoperability, Preservation, Archive, Scalability Services Creational, Preservational, Value-Added, Information Satisfaction Services Systems Architectures, Descriptions, and Comparisons. DL Case Studies Advanced Topics: Quality, Integration, Research Challenges.",Not About Sufficiency
An optimisation method for planning and operating nearshore island power and natural gas energy systems,"As the development and use of offshore islands increase, so do the diversity of their energy needs, the cost of utilising the mainland for resource replenishment, and the instability of island energy systems. To address this challenge, this paper introduces an innovative system integrating fossil and renewable energy tailored for these islands, considering seasonal variations and external energy fluctuations. Leveraging a P-graph, an optimisation framework is designed to assess different optimised solutions. A comprehensive analysis is also performed to compare the solutions, while sensitivity assessments evaluate factors including electricity and gas price fluctuations, renewable energy availability, and natural gas demand. Additionally, the carbon emissions resulting from different power generation scenarios are visually represented and thoroughly analysed. The results support two key findings: 1) the cost of combined energy supply is lowest when wind energy accounts for 59 % of the total energy, mainland grid connection accounts for 3 %, fossil fuel energy accounts for 28 %, and natural gas accounts for 10 % of the energy mix; and 2) the price of electricity and natural gas supplied to the islands and the variability in the energy sources of wind and natural gas can significantly affect the cost of the energy system.",Not About Sufficiency
Predicting Urban Land Use and Mitigating Land Surface Temperature: Exploring the Role of Urban Configuration with Convolutional Neural Networks,"The objective of this research was to examine the influence of urban configuration on the mitigation of land surface temperature (LST) and the prediction of land use and land cover change through the utilization of convolutional neural network modeling. The results indicate that the formation of different urban heat island patterns is significantly influenced by both urban geometry and land use land cover (LULC) types. However, there is no significant correlation between these factors and LST across all configuration metrics. The associations between landscape configuration and land cover types exhibit variability contingent upon the particular forest cover categories under examination. Furthermore, the application of predictive LULC mapping reveals a divergent pattern, characterized by a rise in the overall extent of vegetation but a decline in the inner context of the Shiraz metropolitan area. The projected trajectory of built-up areas indicates a continued trend of urban expansion. The unique landscape patterns are a result of the distinct characteristics of each LULC. According to recommendations, to address the issue of mean LST, it is advisable for urban landscape planning to give priority to cohesion, density, and continuity while simultaneously minimizing fragmentation, variability, and complexity. This research provides valuable insights into the following aspects for urban planners, policymakers, and practitioners to address the following. (1) Selecting appropriate landscape metrics: this study identifies suitable landscape metrics to represent and interpret different landscape structures and land use land cover changes (LULCCs) over time. (2) Understanding regional variations: this research highlights that different landscape metrics have distinct effectiveness in different regions. This knowledge helps urban planners and policymakers to tailor their strategies and interventions based on specific regional characteristics, ensuring more effective mitigation of the urban heat island effect. (3) Different correlations with configuration metrics: this study reveals that the correlations between landscape configuration and LST differ in land cover types. (4) Anticipating future changes: this research utilizes machine learning models to predict future LULCC and landscape metrics. This information is valuable for urban planners and policymakers in anticipating and preparing for future urban expansion and changes in vegetation areas. It enables them to proactively design and implement strategies to manage the surface urban heat island effect. Urban planners and policymakers can utilize these insights to develop comprehensive strategies that integrate land-use design, landscape configuration, and urban form to mitigate the negative impacts of urbanization.",Not About Sufficiency
Real-Time Resilience Optimization Combining an AI Agent With Online Hard Optimization,"In the highly interdependent environment of a large city, failures in the Electrical Distribution System (EDS) can cause direct or indirect consequences to other critical infrastructures and the well-being of the citizens. To increase the resilience of the supply of electricity to the city, this work combines the pre-training of an AI agent and very fast calculation of the optimum recovery path after the number and location of the electrical faults are known. In the introduced Soft-Hard Optimal Convergence (SHOC) method, machine learning techniques are used to train an AI agent with thousands of off-line scenarios for optimum system restoration. In real-time, after the actual fault information is known, the agent will provide a subset of solutions (soft solution) to be considered for hard optimization algorithms. The Infrastructure Interdependencies Simulator (i2SIM) is used to assist the prioritization of the sequence of fault recovery and topological reconfiguration to minimize the black-out time of the most critical loads. A 70-node distribution system case is used to demonstrate the proposed methodology, with solution times in the order of seconds to find the optimum repair sequence and switches topological reconfiguration to optimize the city's resilience index.",Not About Sufficiency
"The role of transport facilities, land use, and population density in reducing car dependency: A structural equation modelling approach","Introduction: Increasing car use in urban cities has caused negative concerns towards the sustainable transport agenda. In this study, an integrated model has been developed to measure the effect of existing transport facilities, land use and population density on car usage in an urban city. Methods: Existing transport facilities for walking, cycling and public transport were examined through on-site assessment. Land use and population density were identified based on authority reports; meanwhile, on-site traffic count surveys were done during morning and evening rush hours. Next, the statistical analyses were performed using partial least squares structural equation modelling (PLS-SEM) to investigate the relationship between all variables above towards car dependency. Results: The findings indicate that the zones with high population density, which provided appropriate walking infrastructure, significantly have lesser private vehicle dependency on peak hours. Appropriate walking infrastructures such as sidewalks, signage, and pedestrian crosswalks are essential for encouraging walking to access public transport. Land use, cycling, and public transport facilities were not significant factors in car dependency for both peak hours. Conclusions: Based on the findings, transport authorities and policymakers can develop effective strategies to provide appropriate transport facilities to reduce car dependency, especially in highdensity populations. These recommendations have practical implications, providing a roadmap for developing sustainable, inclusive transport systems in urban cities.",Not About Sufficiency
Drivers of land cover and land use changes in St. Louis metropolitan area over the past 40 years characterized by remote sensing and census population data,"In this study, we explored the spatial and temporal patterns of land cover and land use (LCLU) and population change dynamics in the St. Louis Metropolitan Statistical Area. The goal of this paper was to quantify the drivers of LCLU using long-term Landsat data from 1972 to 2010. First, we produced LCLU maps by using Landsat images from 1972, 1982, 1990, 2000, and 2010. Next, tract level population data of 1970, 1980, 1990, 2000, and 2010 were converted to 1-km square grid cells. Then, the LCLU maps were integrated with basic grid cell data to represent the proportion of each land cover category within a grid cell area. Finally, the proportional land cover maps and population census data were combined to investigate the relationship between land cover and population change based on grid cells using Pearson's correlation coefficient, ordinary least square (OLS), and local level geographically weighted regression (GWR). Land cover changes in terms of the percentage of area affected and rates of change were compared with population census data with a focus on the analysis of the spatial-temporal dynamics of urban growth patterns. The correlation coefficients of land cover categories and population changes were calculated for two decadal intervals between 1970 and 2010. Our results showed a causal relationship between LCLU changes and population dynamics over the last 40 years. Urban sprawl was positively correlated with population change. However, the relationship was not linear over space and time. Spatial heterogeneity and variations in the relationship demonstrate that urban sprawl was positively correlated with population changes in suburban area and negatively correlated in urban core and inner suburban area of the St. Louis Metropolitan Statistical Area. These results suggest that the imagery reflects processes of urban growth, inner-city decline, population migration, and social spatial inequality. The implications provide guidance for sustainable urban planning and development. We also demonstrate that grid cells allow robust synthesis of remote sensing and socioeconomic data to advance our knowledge of urban growth dynamics from both spatial and temporal scales and its association with population change. (C) 2014 Elsevier B.V. All rights reserved.",Not About Sufficiency
Using Arabic Tweets to Understand Drug Selling Behaviors,"Twitter is a popular platform for e-commerce in the Arab region-including the sale of illegal goods and services. Social media platforms present multiple opportunities to mine information about behaviors pertaining to both illicit and pharmaceutical drugs and likewise to legal prescription drugs sold without a prescription, i.e., illegally. Recognized as a public health risk, the sale and use of illegal drugs, counterfeit versions of legal drugs, and legal drugs sold without a prescription constitute a widespread problem that is reflected in and facilitated by social media. Twitter provides a crucial resource for monitoring legal and illegal drug sales in order to support the larger goal of finding ways to protect patient safety. We collected our dataset using Arabic keywords. We then categorized the data using four machine learning classifiers. Based on a comparison of the respective results, we assessed the accuracy of each classifier in predicting two important considerations in analysing the extent to which drugs are available on social media: references to drugs for sale and the legality/illegality of the drugs thus advertised. For predicting tweets selling drugs, Support Vector Machine, yielded the highest accuracy rate (96%), whereas for predicting the legality of the advertised drugs, the Naive Bayes, classifier yielded the highest accuracy rate (85%). (C) 2019 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/) Peer-review under responsibility of the scientific committee of the CENTERIS -International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies.",Not About Sufficiency
Detection and Isolation of Emetic Bacillus cereus Toxin Cereulide by Reversed Phase Chromatography,"The emetic toxin cereulide is a 1.2 kDa dodecadepsipeptide produced by the food pathogen Bacillus cereus. As cereulide poses a serious health risk to humans, sensitive and specific detection, as well as toxin purification and quantification, methods are of utmost importance. Recently, a stable isotope dilution assay tandem mass spectrometry (SIDA-MS/MS)-based method has been described, and an method for the quantitation of cereulide in foods was established by the International Organization for Standardization (ISO). However, although this SIDA-MS/MS method is highly accurate, the sophisticated high-end MS equipment required for such measurements limits the method's suitability for microbiological and molecular research. Thus, we aimed to develop a method for cereulide toxin detection and isolation using equipment commonly available in microbiological and biochemical research laboratories. Reproducible detection and relative quantification of cereulide was achieved, employing reversed phase chromatography (RPC). Chromatographic signals were cross validated by ultraperformance liquid chromatography-mass spectrometry (UPLC-MS/MS). The specificity of the RPC method was tested using a test panel of strains that included non-emetic representatives of the B. cereus group, emetic B. cereus strains, and cereulide-deficient isogenic mutants. In summary, the new method represents a robust, economical, and easily accessible research tool that complements existing diagnostics for the detection and quantification of cereulide.",Not About Sufficiency
Data model harmonization for the All Of Us Research Program: Transforming i2b2 data into the OMOP common data model,"Background The All Of Us Research Program (AOU) is building a nationwide cohort of one million patients' EHR and genomic data. Data interoperability is paramount to the program's success. AOU is standardizing its EHR data around the Observational Medical Outcomes Partnership (OMOP) data model. OMOP is one of several standard data models presently used in national-scale initiatives. Each model is unique enough to make interoperability difficult. The i2b2 data warehousing and analytics platform is used at over 200 sites worldwide, which uses a flexible ontology-driven approach for data storage. We previously demonstrated this ontology system can drive data reconfiguration, to transform data into new formats without site-specific programming. We previously implemented this on our 12-site Accessible Research Commons for Health (ARCH) network to transform i2b2 into the Patient Centered Outcomes Research Network model. Methods and results Here, we leverage our investment in i2b2 high-performance transformations to support the AOU OMOP data pipeline. Because the ARCH ontology has gained widespread national interest (through the Accrual to Clinical Trials network, other PCORnet networks, and the Nebraska Lexicon), we leveraged sites' existing investments into this standard ontology. We developed an i2b2-to-OMOP transformation, driven by the ARCH-OMOP ontology and the OMOP concept mapping dictionary. We demonstrated and validated our approach in the AOU New England HPO (NEHPO). First, we transformed into OMOP a fake patient dataset in i2b2 and verified through AOU tools that the data was structurally compliant with OMOP. We then transformed a subset of data in the Partners Healthcare data warehouse into OMOP. We developed a checklist of assessments to ensure the transformed data had self-integrity (e.g., the distributions have an expected shape and required fields are populated), using OMOP's visual Achilles data quality tool. This i2b2-to-OMOP transformation is being used to send NEHPO production data to AOU. It is open-source and ready for use by other research projects.",Not About Sufficiency
The Emotographic Iceberg: Modelling Deep Emotional Affects Utilizing Intelligent Assistants and the IoT,"Ninety percent of an iceberg is said to reside below the surface, in the hidden depths of the water, leaving only ten percent to be easily observed. In this paper the authors posit that many human emotion indicators emulate this trait, residing within the inferential data from interactions with popular IoT devices and applications. The visible 'tip of the iceberg' encapsulates the most widely studied ""tells"" of emotion in the form of facial analysis, natural language processing and voice analysis. These provide a discrete frozen snapshot of a person's emotional disposition. This paper presents the hypothesis that below the surface lies a largely untapped, vast resource of submerged data that may be used to infer the emotional state of an individual. The phenomenon of the Internet of Things has cultivated a societal shift where sensors and applications gather data relating to every facet of daily life. This data is centralized by hub devices such as Voice Command Devices and accessible via Intelligent Assistants such as the Amazon Echo and Alexa. Emotographic Modelling is a new concept rendering how human emotional state may be gleaned from the raft of digital indicators available from these hubs. The `Emotographic' classifications generated are constituted by study of the statistical data relating to digital emotion indicators. By utilizing the IoT, the Cloud and Machine Learning, the inferential depths of the iceberg may be explored to provide insight into sleep, diet, exercise and other routines and habits. The complex ""hidden"" portion of the Emotographic Iceberg may reveal patterns that indicate emotion over a continuous timescale. Changes in these patterns may allow for a more sagacious comprehension of an individual's state of mind for healthcare clinicians and marketers. Preliminary testing is outlined in which the authors demonstrate how the emotion of sadness may be inferred from a range of questions asked to an IoT connected Amazon Echo Voice Command Device.",Not About Sufficiency
An Empirical Analysis of Mode Choice Decision for Utilitarian and Hedonic Trips: Evidence from Iran,"A sizeable body of literature reveals a strong relationship between mode choice and health status. Therefore, society would benefit from travel if transportation and urban planners motivated more individuals to satisfy their desire for travel by active transportation rather than motorized transportation. Despite rich existing literature about the relations between the built environment and travel, we still need to address some research gaps in explaining travel mode choice. As a shortcoming, identifying and measuring the primary motivations for trips, and then incorporating such motivations into travel mode choice modelling, has received less attention in previous research. In this regard the current paper follows two main goals. It aims to differentiate between trips by determining the main utility of travelers and then analyzes the impact of the built environment measurements and subjective attributes on mode choice decision. Using data from a survey of 515 participants who reside in Isfahan, Iran, we conducted a series of binary logistic models to explore how the built environment influences mode choice decisions for different trips, controlling for socio-economics and subjective attributes. The results show that the number of hedonic trips were sizably more than utilitarian trips. It was found that travel mode choice for utilitarian and hedonic trips is influenced by travel habits and subjective attitudes, but the built environment also matters. Specifically, two built environment characteristics, including density and diversity, can substitute walking/cycling for driving for utilitarian trips. In addition, car use for hedonic trips is not influenced by built environment measurements. It seems that the utility and desire of hedonic driving depends on mode of travel. It is concluded that driving and walking/biking for hedonic and utilitarian trips are not single behaviors and differentiating between trips according to their main utility and considering both objective and subjective attributes helps urban and transportation planners prescribe appropriate spatial and nonspatial strategies to encourage walking/biking.",Not About Sufficiency
The Politics and Policies of Regulating Generics in Latin America: A Survey of Seventeen States,"When patents expire, are equivalent generic alternatives available to citizens? This article contributes to current discussion on access to medicine in the aftermath of the World Trade Organization's Agreement on Trade-Related Aspects of Intellectual Property Rights (TRIPS). The focus is on off-patent or ""generic"" medicines: their product definitions, quality standards and prescription procedures. Drawing from a survey conducted of seventeen countries across the Latin American region, this article examines the differences in definition of off-patent products and the paradox of their relatively lower consumption across multiple developing states. The findings point to pathways for improving standards, consumer information, and access in off patent pharmaceutical markets.",Not About Sufficiency
A shift toward personalized healthcare: does the Affordable Care Act provide enough incentive for change?,"Personalized healthcare, which uses individual characteristics to better predict and prevent disease and customize therapies, is a potential solution to our current healthcare crisis. Personalized care aims to improve quality of care and reduce overall healthcare costs. Despite its potential, adoption of personalized healthcare has been slow for several reasons, one of which is related to financial incentives toward change. This perspective piece discusses how the Affordable Care Act (ACA), through support for preventive care and comparative effectiveness research, begins to align the right incentives toward innovation around personalized risk prediction, innovation that is much needed as we aim to improve the health of individuals and communities.",Not About Sufficiency
MIGRANT WORKERS AND THE MANY STATES OF PROTEST IN HONG KONG,"Migrant domestic workers rarely take part in let alone organize public protests in the countries where they work. Public protests are virtually unheard of among migrant domestic workers in Singapore, Taiwan, and Malaysia, and especially in the Middle East and the Gulf States. Over the past decade and a half, however, migrant domestic workers in Hong Kong mostly Filipinas and Indonesian women have become highly active, organizing and participating in political protests. Hong Kong's migrant domestic workers protest in a place where they are guest workers and temporary migrants, denied the opportunity of becoming legal citizens or permanent residents. Increasingly, these workers, their grassroots activist organizations, and the nongovernmental organizations with which they are affiliated frame their concerns in terms of global, transnational, and human rights, not merely local migrant worker rights. This article takes the Consulate Hopping Protest and Hall of Shame Awards event part of the anti-World Trade Organization protests in Hong Kong in 2005 as an ethnographic example of domestic worker protest and as an entree through which to ask what it is about Hong Kong and about the position of women migrant workers whose mobility and voice is both a product and a symptom of globalization that literally permits public protests and shapes their form and content. The article illustrates how migrant workers' protests and activism have been shaped by domestic worker subjectivities, by the dynamics of inter-ethnic worker affiliations, and by the sociohistorical context of Hong Kong as a post-colonial global city and a neoliberal space of exception.",Not About Sufficiency
"A Review of Cybersecurity in Grid-Connected Power Electronics Converters: Vulnerabilities, Countermeasures, and Testbeds","With the increasing installations of solar energy, electric vehicles, and other distributed energy resources and the deeper developments of digitalization and standardization, cybersecurity became more and more essential and critical in modern power systems. Unfortunately, most prior research work focuses on the cybersecurity of power transmission and distribution networks other than distributed energy devices and their grid-connected power converters. Focusing on the Grid-Connected Power Electronics Converters (GCPECs), this article does a comprehensive review of existing outcomes from selected references, in the aspects of vulnerabilities, countermeasures, and testbeds. By analyzing the GCPEC's layout and countermeasure candidates, it is found that the vulnerabilities of GCPECs include both cyber and physical layers that are easily accessible to malicious hackers. These vulnerabilities in the two layers must be considered simultaneously and coordinate well with each other. Especially, hardware hardening is an essential approach to enhance cybersecurity within GCPECs. It is also noticed that the detection and mitigation approaches should consider the complexity of algorithms to be applied and assess the limits of computing and data processing capabilities in GCPECs while evaluating the feasibility of countermeasure candidates to cyberattacks in testbeds. In addition, the countermeasures should meet relevant standards, such as IEEE-1547.1, IEEE-2030.5, IEC-61850, and IEC-62351, to ensure the interoperability and cybersecurity of GCPEC devices in smart grids. Finally, based on the review and analysis, four recommendations are raised for future research on GCPEC's cybersecurity and their applications in smart grids.",Not About Sufficiency
Efficient Pause Location Prediction Using Quantum Annealing Simulations and Machine Learning,"Despite increases in qubit count and connectivity in quantum annealers, a quantum speedup has yet to be observed for problems of practical significance. In order to further improve annealer performance, some researchers focus on tuning annealer parameters, such as the annealing schedule. In this work, we focus on pausing, an annealing schedule modification that has been shown to improve the probability of solving an optimization problem by orders of magnitude. However, a challenge associated with pausing is selecting an appropriate pause location, as pausing is only effective in problem-dependent regions and ineffective elsewhere. Moreover, there is little advice on how to determine where pausing is effective. Thus, pausing effectively is difficult and often inaccessible to the majority of users. To address these issues, we propose a data-driven method that leverages machine learning to predict optimal pause locations. First, we construct a dataset consisting of optimization problems and their corresponding optimal pause locations. The optimal pause locations are determined using spin-vector Monte Carlo, a method known to yield results similar to quantum annealing. Next, we train a convolutional neural network on this dataset, demonstrating its ability to distinguish between problem types and accurately predict the optimal pause location. Finally, we evaluate the model on multiple types of optimization problems. Our results show that the pause locations predicted by our method improve solution quality for all selected problem types. Additionally, our model can be pre-trained and easily distributed, making the power of pausing accessible to users unfamiliar with annealing schedule modifications.",Not About Sufficiency
LEARNING OBJECTS AND THEIR ROLE IN ENHANCING THE QUALITY OF WEB-BASED TEACHER TRAINING COURSES,"Web based teacher training courses are gaining more and more popularity as they provide rich learning experience in an easily accessible format. The design and implementation of such courses poses certain challenges to trainers as they have to choose methodologies and didactic strategies that allow for the creation of digital content with high educational value, efficiency and interoperability. The applicability of learning objects for solving the problem of developing challenging, interactive, reusable and portable e-learning materials for the training of primary school English language teachers is a subject of discussion in the current paper.",Not About Sufficiency
Social capital and its transition that enabled long-distance collective relocation after disasters -the case of 1889 flood reconstruction and domestic migration in Japan-,"The 1889 flood disaster in Totsukawa Village, Nara Prefecture, damaged a large part of the vil- lage due to the formation and breaking of a dammed lake, and many of the residents decided to relocate to Hokkaido, more than 1500 km away. The purpose of this study is to examine long- distance collective relocation, which is currently often avoided, from a long-term perspective, the implementation of collective relocation and the gradual change in the cohesion of the residents after relocation, and to discuss the possibility of presenting long-distance collective relocation as an option in the event of future disasters. Based on a literature review and interviews, the study clarified the factors that led to the long-distance group relocation, which is shunned today, and the policy background of the time. It also clarified the administrative considerations that were implemented to ensure the continuation of life at the long-distance group relocation site and the cohesiveness of the residents, which changed over time. As a result, it was found that the reloca- tion was carried out in a way that combined the Hokkaido development policy, which was recom- mended to secure farmland and residential land and to defend the northern areas, with the secur- ing of residential areas for disaster recovery purposes and livelihood support, and that relatively good conditions were selected, such as the development of railroads close to the relocation site. At the same time, the government tried to distribute housing in the same way as it does today, so that as many of the original settlements as possible would be the same at the new location. The way in which the residents are united has also changed, and the unity of the hometowns has also shown flexibility in tolerating cultural and lifestyle diversity as they eventually mix with other communities through rice cultivation. Despite the distance of 1500 km between the source and destination districts, administrative staff and residents continue to interact with each other, and the social capital that can be identified between the source and destination districts continues to change its form as they repeatedly adapt to their society and environment over time and distance.",Not About Sufficiency
The Correlation between Urban Form and Carbon Emissions: A Bibliometric and Literature Review,"Urban carbon emissions contribute significantly to global warming, but various factors impact these emissions. This study focuses on the correlation between urban form and carbon emissions. Urban form is an entity that can be directly manipulated and optimized by disciplines such as architecture, urban design, and urban planning. The improvement of urban form, particularly at the meso-micro scale, is relatively rapid and affordable compared to other carbon-related factors, such as macro-industry or energy structure. Therefore, conducting a study on the correlation between urban form and carbon emissions is crucial, and the findings will provide direct scientific support for low-carbon city planning. The paper combines bibliometric analysis with a literature review. First, we explore research hotspots and trends using bibliometric analysis. Second, we organize the literature review based on the main research components, methods, and findings in this field. Finally, we propose a framework and direction for future research. It was found that (1) numerous study methodologies are currently being used to investigate the direct and indirect impacts of urban form on carbon emissions, with Chinese scholars' research progressing rapidly; (2) the primary focus of the study is on the carbon emissions related to residents' consumption, and there are still issues with inconsistent measurement approaches; (3) there is more research conducted on the macro-scale of cities but not enough on the meso-micro scale. Future research must focus more on meso-microscale analysis, quantifying the key influences and pathways of urban form on carbon emissions. Additionally, it is crucial to establish a comprehensive research framework that can serve as a guide for more effective urban development aimed at reducing carbon emissions.",Not About Sufficiency
Automated identification of insomnia using optimal bi-orthogonal wavelet transform technique with single-channel EEG signals,"Nowadays, sleep studies have gained a lot of attention from researchers due to the immense importance of quality sleep. Human beings spend nearly one-third of their lives in sleep. Therefore, adequate quality sleep is indispensable for a healthy life. The sleep pattern may not be the same for every individual as one may either suffer from various sleep ailments such as insomnia, apnea, bruxism, epilepsy, narcolepsy, or maybe healthy with no sleep disorder. Insomnia is a prevalent sleep disorder that can lead to many health-related issues in human beings. Usually, polysomnogram (PSG) signals are used to detect the sleep stages and sleep disorders. The PSG signals are difficult to handle, time-consuming, and not convenient for patients. Hence, in this work, we have used single-channel electroencephalogram (EEG) signals to detect insomnia automatically. To the best of our knowledge, this is the first study on automated insomnia identification using the CAP database and EEG alone. We have used the single-channel EEG channel and created eight different subsets based on sleep-stages annotations according to American Academy of Sleep Medicine (AASM) guidelines for sleep stage scoring. The classification task is performed on each subset for the automated identification of insomnia. A new class of an optimal bi-orthogonal filter bank is used for wavelet decomposition. The wavelet-based norm features are extracted using the optimal filter bank. Then these features are fed to various machine learning algorithms. The proposed model has attained the highest classification performance with the area under receivers' operating characteristic curve (AROC) of 0.97, F1-score of 0.9645, the accuracy of 95.60%, and Cohen's Kappa value of 0.9067 using an ensemble bagged decision trees (EBDT) classifier. Our developed model can be used to detect insomnia using sleep EEG signals accurately and provide early treatment. The method is simple and computationally fast. The proposed system can be used at home as well as at sleep labs to monitor insomnia. (C) 2021 Elsevier B.V. All rights reserved.",Not About Sufficiency
"Planning and affordable housing in Australia, New Zealand and England: common culture; different mechanisms","This paper compares approaches to planning and delivery of affordable housing across England, Australia and New Zealand. While all three nations began with a common starting point-early British town planning legislation-underlying differences in urban regulation, property rights and housing provision soon emerged. However, signs of convergence have lately re-appeared, as all three countries have responded to affordable housing shortages by exploring new strategies to boost supply through the planning system. In the tradition of comparative housing research, this paper examines these strategies in the context of each country's particular historical, socio-cultural, governance and urban planning frameworks. Our analysis shows how differences in planning systems and approaches to housing assistance can delimit opportunities to secure new affordable homes, particularly in the context of increasing land values. Effective delivery of affordable housing through the planning system depends on consistent and enforceable policy articulation, government commitment, a mature affordable housing sector, and particular market conditions.",Not About Sufficiency
Visualization and analytics tools for infectious disease epidemiology: A systematic review,"Background: A myriad of new tools and algorithms have been developed to help public health professionals analyze and visualize the complex data used in infectious disease control. To better understand approaches to meet these users' information needs, we conducted a systematic literature review focused on the landscape of infectious disease visualization tools for public health professionals, with a special emphasis on geographic information systems (GIS), molecular epidemiology, and social network analysis. The objectives of this review are to: (1) identify public health user needs and preferences for infectious disease information visualization tools; (2) identify existing infectious disease information visualization tools and characterize their architecture and features; (3) identify commonalities among approaches applied to different data types; and (4) describe tool usability evaluation efforts and barriers to the adoption of such tools. Methods: We identified articles published in English from January 1, 1980 to June 30, 2013 from five bibliographic databases. Articles with a primary focus on infectious disease visualization tools, needs of public health users, or usability of information visualizations were included in the review. Results: A total of 88 articles met our inclusion criteria. Users were found to have diverse needs, preferences and uses for infectious disease visualization tools, and the existing tools are correspondingly diverse. The architecture of the tools was inconsistently described, and few tools in the review discussed the incorporation of usability studies or plans for dissemination. Many studies identified concerns regarding data sharing, confidentiality and quality. Existing tools offer a range of features and functions that allow users to explore, analyze, and visualize their data, but the tools are often for siloed applications. Commonly cited barriers to widespread adoption included lack of organizational support, access issues, and misconceptions about tool use. Discussion and conclusion: As the volume and complexity of infectious disease data increases, public health professionals must synthesize highly disparate data to facilitate communication with the public and inform decisions regarding measures to protect the public's health. Our review identified several themes: consideration of users' needs, preferences, and computer literacy; integration of tools into routine workflow; complications associated with understanding and use of visualizations; and the role of user trust and organizational support in the adoption of these tools. Interoperability also emerged as a prominent theme, highlighting challenges associated with the increasingly collaborative and interdisciplinary nature of infectious disease control and prevention. Future work should address methods for representing uncertainty and missing data to avoid misleading users as well as strategies to minimize cognitive overload. (C) 2014 The Authors. Published by Elsevier Inc.",Not About Sufficiency
Plant disease detection and classification techniques: a comparative study of the performances,"One of the essential components of human civilization is agriculture. It helps the economy in addition to supplying food. Plant leaves or crops are vulnerable to different diseases during agricultural cultivation. The diseases halt the growth of their respective species. Early and precise detection and classification of the diseases may reduce the chance of additional damage to the plants. The detection and classification of these diseases have become serious problems. Farmers' typical way of predicting and classifying plant leaf diseases can be boring and erroneous. Problems may arise when attempting to predict the types of diseases manually. The inability to detect and classify plant diseases quickly may result in the destruction of crop plants, resulting in a significant decrease in products. Farmers that use computerized image processing methods in their fields can reduce losses and increase productivity. Numerous techniques have been adopted and applied in the detection and classification of plant diseases based on images of infected leaves or crops. Researchers have made significant progress in the detection and classification of diseases in the past by exploring various techniques. However, improvements are required as a result of reviews, new advancements, and discussions. The use of technology can significantly increase crop production all around the world. Previous research has determined the robustness of deep learning (DL) and machine learning (ML) techniques such as k-means clustering (KMC), naive Bayes (NB), feed-forward neural network (FFNN), support vector machine (SVM), k-nearest neighbor (KNN) classifier, fuzzy logic (FL), genetic algorithm (GA), artificial neural network (ANN), convolutional neural network (CNN), and so on. Here, from the DL and ML techniques that have been included in this particular study, CNNs are often the favored choice for image detection and classification due to their inherent capacity to autonomously acquire pertinent image features and grasp spatial hierarchies. Nevertheless, the selection between conventional ML and DL hinges upon the particular problem, the accessibility of data, and the computational capabilities accessible. Accordingly, in numerous advanced image detection and classification tasks, DL, mainly through CNNs, is preferred when ample data and computational resources are available and show good detection and classification effects on their datasets, but not on other datasets. Finally, in this paper, the author aims to keep future researchers up-to-date with the performances, evaluation metrics, and results of previously used techniques to detect and classify different forms of plant leaf or crop diseases using various image-processing techniques in the artificial intelligence (AI) field.",Not About Sufficiency
"Analysis of willingness for relocation of the local communities living in the Critical Tiger Habitat of the Sariska Tiger Reserve, India","Many Indian Protected Areas (PAs) act as a support system for the communities living in and around them. Large-scale human interventions in these PAs have resulted in biodiversity loss, threat to wildlife and habitat fragmentation. The Sariska Tiger Reserve (STR) is no exception. In this reserve, tiger (Panthera tigris) became extinct in 2004. To create inviolate space for the reintroduced tigers, government has planned voluntary relocation of villages located inside Critical Tiger Habitat. The voluntary relocation plan will be more challenging if people are not willing to get relocated from PAs. Therefore, we have empirically analysed the identified factors influencing local communities' willingness in getting relocated outside the STR using logit model. Results revealed that ""restriction of access"" and ""market access"" are the most influential factors and positively associated while forest dependency is negatively associated with local communities' willingness. Based on these results, it was recommended that policy should be directed towards restriction on accessing forest resources along with reduction in forest dependency by nurturing and strengthening villagers' livelihood to ensure successful relocation. Displacement of existed small markets around the reserve will also persuade them to relocate in areas more connected to market and other facilities.",Not About Sufficiency
Artificial intelligence and remote sensing for spatial prediction of daily air temperature: case study of Souss watershed of Morocco,"Air temperature (Tair) is a fundamental variable in climate research and climate impact management. Conventional field observations do not accurately capture its spatial distribution due to the sparse and uneven distribution of weather stations, especially in remote areas where the local variability is high. To circumvent this problem, in this study, remote sensing and weather station data were used to estimate Tair in the Souss watershed in Morocco. Two statistical methods, including linear regression and partial least squares (PLS), and four machine learning algorithms, namely k-nearest neighbors, random forest (RF), extreme gradient boost, and Cubist, were used for modeling and predicting Tair and its performance were evaluated using random subsets and cross-validation. Moderate resolution imaging spectroradiometer predictors, including Terra band 32 emissivity, Terra nighttime land surface temperature, Terra local time of night observation, Aqua band 31 emissivity, Aqua daytime land surface temperature, and Aqua nighttime land surface temperature (ALSTN), and auxiliary inputs, including sky-view, elevation, slope, and hillshade, were used as inputs for modeling. The results showed that the Cubist and RF were the most accurate models (RMSE = 2.09 degrees C and 2.13 degrees C, R-2 = 0.91 and 0.90, respectively), while PLS had the lowest predictive power (RMSE = 2.71 degrees C; R-2 = 0.83). The overall performance of the models for estimating Tair in the study area was generally satisfactory, with RMSE limited to less than 3 degrees C for all models. Nevertheless, the station data reliability was still an issue, with only four of the seven stations marked by complete meteorological data.",Not About Sufficiency
Digital Libraries: opportunities and challenges for the English-speaking Caribbean,"The purpose of this paper is to explore some of the major issues surrounding the development of digital libraries in the developing countries of the English-speaking Caribbean known also as the Caricom region. The impetus for digital libraries comes from the desire of these countries to become integrated in the new, global information economy. As part of this global information trend, digital libraries technology holds the key to improved information access. Digital libraries, which have become affordable and manageable provide increased access to global information and at the same time, increase the visibility of indigenous Caribbean information resources. The paper looks at the major issues that must be addressed in the process of digital libraries development. These include the current state of ICT infrastructure, economic and data sustainability, issues of interoperability and the generation of local content. Examples of current digital library projects undertaken in academic, public and special libraries are identified. The paper ends with a set of recommendations that can be used by information professionals and other stakeholders in developing countries to organize the way forward for the application, growth and development of digital libraries.",Not About Sufficiency
Dynamic Pricing and Inventory Management Under Inventory-Dependent Demand,"We analyze a finite horizon periodic review joint pricing and inventory management model for a firm that replenishes and sells a product under the scarcity effect of inventory. The demand distribution in each period depends negatively on the sales price and customer-accessible inventory level at the beginning of the period. The firm can withhold or dispose of its on-hand inventory to deal with the scarcity effect. We show that a customer-accessible-inventory-dependent order-upto/dispose-down-to/display-up-to list-price policy is optimal. Moreover, the optimal order-up-to/display-up-to and list-price levels are decreasing in the customer-accessible inventory level. When the scarcity effect of inventory is sufficiently strong, the firm should display no positive inventory and deliberately make every customer wait. The analysis of two important special cases wherein the firm cannot withhold (or dispose of) inventory delivers sharper insights showing that the inventory-dependent demand drives both optimal prices and order-up-to levels down. In addition, we demonstrate that an increase in the operational flexibility (e.g., a higher salvage value or the inventory withholding opportunity) mitigates the demand loss caused by high excess inventory and increases the optimal order-up-to levels and sales prices. We also generalize our model by incorporating responsive inventory reallocation after demand realizes. Finally, we perform extensive numerical studies to demonstrate that both the profit loss of ignoring the scarcity effect and the value of dynamic pricing under the scarcity effect are significant.",Not About Sufficiency
Breaking the polar-nonpolar division in solvation free energy prediction,"Implicit solvent models divide solvation free energies into polar and nonpolar additive contributions, whereas polar and nonpolar interactions are inseparable and nonadditive. We present a feature functional theory (FFT) framework to break this ad hoc division. The essential ideas of FFT are as follows: (i) representability assumption: there exists a microscopic feature vector that can uniquely characterize and distinguish one molecule from another; (ii) feature-function relationship assumption: the macroscopic features, including solvation free energy, of a molecule is a functional of microscopic feature vectors; and (iii) similarity assumption: molecules with similar microscopic features have similar macroscopic properties, such as solvation free energies. Based on these assumptions, solvation free energy prediction is carried out in the following protocol. First, we construct a molecular microscopic feature vector that is efficient in characterizing the solvation process using quantum mechanics and Poisson-Boltzmann theory. Microscopic feature vectors are combined with macroscopic features, that is, physical observable, to form extended feature vectors. Additionally, we partition a solvation dataset into queries according to molecular compositions. Moreover, for each target molecule, we adopt a machine learning algorithm for its nearest neighbor search, based on the selected microscopic feature vectors. Finally, from the extended feature vectors of obtained nearest neighbors, we construct a functional of solvation free energy, which is employed to predict the solvation free energy of the target molecule. The proposed FFT model has been extensively validated via a large dataset of 668 molecules. The leave-one-out test gives an optimal root-mean-square error (RMSE) of 1.05 kcal/mol. FFT predictions of SAMPL0, SAMPL1, SAMPL2, SAMPL3, and SAMPL4 challenge sets deliver the RMSEs of 0.61, 1.86, 1.64, 0.86, and 1.14 kcal/mol, respectively. Using a test set of 94 molecules and its associated training set, the present approach was carefully compared with a classic solvation model based on weighted solvent accessible surface area. (c) 2017 Wiley Periodicals, Inc.",Not About Sufficiency
Photonic platform coupled with machine learning algorithms to detect pyrolysis products of crack cocaine in saliva: A proof-of-concept animal study,"The non-invasive detection of crack/cocaine and other bioactive compounds from its pyrolysis in saliva can provide an alternative for drug analysis in forensic toxicology. Therefore, a highly sensitive, fast, reagent-free, and sustainable approach with a non-invasive specimen is relevant in public health. In this animal model study, we evaluated the effects of exposure to smoke crack cocaine on salivary flow, salivary gland weight, and salivary composition using Attenuated total reflection-Fourier transform infrared (ATR-FTIR) spectroscopy. The exposure to crack cocaine was performed in an acrylic box apparatus with a burned activation of crack/cocaine 400 mg for 10 min for 14 consecutive days. Crack/cocaine exposure increased the salivary secretion without changes in parotid and submandibular weights. Hierarchical Clustering Analysis (HCA) was applied to depict subgrouping patterns in infrared spectra, and Principal components analysis (PCA) explained 83.2 % of the cumulative variance using 3 PCs. ATR-FTIR platforms were coupled to AdaBoost, Artificial Neural Networks, Na & iuml;ve Bayes, Random Forest, and Support Vector Machine (SVM) algorithms tool to identify changes in the infrared salivary spectra of rats exposed to crack cocaine. The best classification of crack cocaine exposure using the salivary spectra was performed by Na & iuml;ve Bayes, presenting a sensitivity of 100 %, specificity of 80 %, and accuracy of 90 % between crack cocaine and control rats. The SHAP features of salivary infrared spectra mostly indicate the vibrational modes at 1331 cm- 1 and 2806 cm-1 , representing CH2 wagging commonly linked in lipids and C-H stretch often attributed to the CH2 or CH3 groups in lipid molecules, respectively, as the main responsible vibrational modes for crack cocaine exposure discrimination. In summary, the present pre-clinical findings indicate the potential of the ATR-FTIR platform coupled with machine learning to effectively detect changes in salivary infrared spectra promoted by exposure to crack cocaine.",Not About Sufficiency
A Machine Learning Based Methodology for Automatic Annotation and Anonymisation of Privacy-Related Items in Textual Documents for Justice Domain,"Textual resources annotation is currently performed both manually by human experts selecting hand-crafted features and automatically by any trained systems. Human experts annotations are very accurate but require heavy effort in compilation and most often are not publicly accessible. Automatic approaches save efforts but don't perform yet with the required accuracy, mostly because of the great difficulty and labor required to represent domain experts' knowledge in a machine readable format. This work tackles the issue of automatically annotate plain text resources; it was motivated by the need of supporting Italian justice officers in detecting sensible information included in large amounts of judgements documents, for privacy preservation aims. We suggest a novel methodology, based on unsupervised machine learning techniques, to facilitate human experts in detecting sensible information. We performed experiments over about 20.000 plain text documents and we obtained an accuracy rate of about 75%, in the preliminary validation stage.",Not About Sufficiency
Three-dimensional augmentation for hyperspectral image data of water quality: An Integrated approach using machine learning and numerical models,"This research introduces a comprehensive methodology to enhance hyperspectral image data (HSD) utility, specifically focusing on the three-dimensional (3-D) augmentation of Chlorophyll-a (Chl-a). This study comprises three significant steps: (1) the augmentation of limited field water quality data in terms of time interval and number of variables using neural network models, (2) the generation of 3-D data using numerical models, and (3) the extension of the hyperspectral image data into 3-D data using machine learning models. In the first phase, Multilayer Perceptron (MLP) models were developed to train water quality interactions and successfully generated high-frequency water quality data by adjusting biased measurements and predicting detailed water quality variables. In the second phase, high-frequency data generated by MLP models were applied to develop two numerical models. These numerical models successfully generated 3-D data, thereby demonstrating the effectiveness of integrating numerical modeling with neural networks. In the final phase, ten machine learning models were trained to generate 3-D Chl-a data from HSD. Notably, the Gaussian Process Regression model exhibited superior performance, effectively estimating 3-D Chl-a data with robust accuracy, as evidenced by an R-square value of 0.99. The findings align with theories of algal bloom dynamics, further validating the effectiveness of the approach. This study demonstrated the successful integrated development for HSD extension using machine learning models, numerical models, and original HSD, highlighting the potential of such integrated methodologies in advancing water quality monitoring and estimation. Notably, the approach leverages readily accessible data, allowing for the swift generation of results and bypassing time-consuming data collection processes. This research marks a significant step towards more robust, comprehensive water quality monitoring and prediction, thereby facilitating better management of aquatic ecosystems.",Not About Sufficiency
"The health impacts of traffic-related exposures in urban areas: Understanding real effects, underlying driving forces and co-producing future directions","The world is currently witnessing its largest surge of urban growth in human history; a trend that draws attention to the need to understand and address health impacts of urban living. Whilst transport is instrumental in this urbanisation wave, it also has significant positive and negative impacts on population health, which are disproportionately distributed. In this paper, we bring together expertise in transport engineering, transport and urban planning, research and strategic management, epidemiology and health impact assessment in an exercise to scope and discuss the health impacts of transport in urban areas. Adopting a cross-disciplinary, co-production approach, we explore the key driving forces behind the current state of urban mobility and outline recommendations for practices that could facilitate positioning health at the core of transport design, planning and policy. Current knowledge on the health-related impacts of urban transport shows that motor vehicle traffic is causing significant premature mortality and morbidity through motor vehicle crashes, physical inactivity and traffic -related environmental exposures including increases in air pollution, noise and temperature levels, as well as reductions in green space. Trends of rapid and car-centred urbanisation, mass motorisation and a tendency of policy to favour car mobility and undervalue health in the transport and development agenda has both led to, and exacerbated the negative health impacts of the transport systems. Simultaneously, we also argue that the benefits of new transport schemes on the economy are emphasised whilst the range and severity of identified health impacts associated with transport are often downplayed. We conclude the paper by outlining stakeholders' recommendations for the adoption of a cross-disciplinary co-production approach that takes a health-aware perspective and has the potential to promote a paradigm shift in transport practices. (C) 2016 Elsevier Ltd. All rights reserved.",Not About Sufficiency
An Autoencoder-Based Deep Learning Classifier for Efficient Diagnosis of Autism,"Autism spectrum disorder (ASD) is a neurodevelopmental disorder characterized by a lack of social communication and social interaction. Autism is a mental disorder investigated by social and computational intelligence scientists utilizing advanced technologies such as machine learning models to enhance clinicians' ability to provide robust diagnosis and prognosis of autism. However, with dynamic changes in autism behaviour patterns, these models' quality and accuracy have become a great challenge for clinical practitioners. We applied a deep neural network learning on a large brain image dataset obtained from ABIDE (autism brain imaging data exchange) to provide an efficient diagnosis of ASD, especially for children. Our deep learning model combines unsupervised neural network learning, an autoencoder, and supervised deep learning using convolutional neural networks. Our proposed algorithm outperforms individual-based classifiers measured by various validations and assessment measures. Experimental results indicate that the autoencoder combined with the convolution neural networks provides the best performance by achieving 84.05% accuracy and Area under the Curve (AUC) value of 0.78.",Not About Sufficiency
Benchmark of 3D conformer generation and molecular property calculation for medium-sized molecules,"Medium-sized molecules have attracted significant attention as new chemical modalities. In this study, we compared the performances of three methods of 3D structure generation for medium-sized molecules using free and commercial software. The benchmark dataset consisted of 2131 protein-binding ligands with molecular weights greater than 600, which were selected from the Protein Data Bank (PDB). When selecting the smallest root mean square deviation between the generated 3D conformers and the PDB ligand structures, 43% of the conformations determined with the software CORINA were within 1 angstrom, followed by 10% from OMEGA and 5% from RDKit. According to our results, comparing the polar solvent-accessible surface area (PSA) and normalized principal moment of inertia ratio (NPR) among the three methods, 83% of the conformers generated with CORINA were within 20 in PSA, and 53% of the conformers from CORINA were within 0.05 in the NPR1 and NPR2 spaces. Thus, we concluded that CORINA has the highest performance in terms of efficient conformer generation. We also examined 3D descriptor calculation using Mordred, which is a free descriptor computation tool. The results showed that OMEGA-generated conformers exhibited the highest success rate, indicating that OMEGA is a suitable conformer generation tool for various 3D descriptors. Our results could contribute to the selection of conformation generators for the rapid construction of various predictive models for medium-sized molecules and can be shared with the research community for further validation.",Not About Sufficiency
Distributed and Collaborative Software Analysis,"Throughout the years software engineers have come up with a myriad of specialized tools and techniques that focus on a certain type of software analysis such as source code analysis, co-change analysis or bug prediction. However, easy and straight forward synergies between these analyses and tools rarely exist because of their stand-alone nature, their platform dependence, their different input and output formats and the variety of data to analyze. As a consequence, distributed and collaborative software analysis scenarios and in particular interoperability are severely limited. We describe a distributed and collaborative software analysis platform that allows for a seamless interoperability of software analysis tools across platform, geographical and organizational boundaries. We realize software analysis tools as services that can be accessed and composed over the Internet. These distributed analysis services shall be widely accessible in our incrementally augmented Software Analysis Broker where organizations and tool providers can register and share their tools. To allow (semi-) automatic use and composition of these tools, they are classified and mapped into a software analysis taxonomy and adhere to specific meta-models and ontologies for their category of analysis.",Not About Sufficiency
A Machine-Learning-Driven Pathophysiology-Based New Approach Method for the Dose-Dependent Assessment of Hazardous Chemical Mixtures and Experimental Validations,"Environmental chemicals, such as PFAS, exist as mixtures and are frequently encountered at varying concentrations, which can lead to serious health effects, such as cancer. Therefore, understanding the dose-dependent toxicity of chemical mixtures is essential for health risk assessment. However, comprehensive methods to assess toxicity and identify the mechanisms of these harmful mixtures are currently absent. In this study, the dose-dependent toxicity assessments of chemical mixtures are performed in three methodologically distinct phases. In the first phase, we evaluated our machine-learning method (AI-HNN) and pathophysiology method (CPTM) for predicting toxicity. In the second phase, we integrated AI-HNN and CPTM to establish a comprehensive new approach method (NAM) framework called AI-CPTM that is targeted at refining prediction accuracy and providing a comprehensive understanding of toxicity mechanisms. The third phase involved experimental validations of the AI-CPTM predictions. Initially, we developed binary, multiclass classification, and regression models to predict binary, categorical toxicity, and toxic potencies using nearly a thousand experimental mixtures. This empirical dataset was expanded with assumption-based virtual mixtures, compensating for the lack of experimental data and broadening the scope of the dataset. For comparison, we also developed machine-learning models based on RF, Bagging, AdaBoost, SVR, GB, KR, DT, KN, and Consensus methods. The AI-HNN achieved overall accuracies of over 80%, with the AUC exceeding 90%. In the final phase, we demonstrated the superior performance and predictive capability of AI-CPTM, including for PFAS mixtures and their interaction effects, through rigorous literature and statistical validations, along with experimental dose-response zebrafish-embryo toxicity assays. Overall, the AI-CPTM approach significantly improves upon the limitations of standalone AI models, showing extensive enhancements in identifying toxic chemicals and mixtures and their mechanisms. This study is the first to develop a hybrid NAM that integrates AI with a pathophysiology method to comprehensively predict chemical-mixture toxicity, carcinogenicity, and mechanisms.",Not About Sufficiency
Health Information Systems in the COVID-19 Pandemic: A Short Survey of Experiences and Lessons Learned From the European Region,"Introduction: The COVID-19 crisis provides an opportunity to reflect on what worked during the pandemic, what could have been done differently, and what innovations should become part of an enhanced health information system in the future. Methods: An online qualitative survey was designed and administered online in November 2020 to all the 37 Member States that are part of the WHO European Health Information Initiative and the WHO Central Asian Republics Information Network. Results: Nineteen countries responded to the survey (Austria, Belgium, Croatia, Czech Republic, Finland, Greece, Iceland, Ireland, Israel, Italy, Kazakhstan, Latvia, Lithuania, Romania, Russian Federation, Sweden, Turkey, United Kingdom, and Uzbekistan). The COVID-19 pandemic required health information systems (HIS) to rapidly adapt to identify, collect, store, manage, and transmit accurate and timely COVID-19 related data. HIS stakeholders have been put to the test, and valuable experience has been gained. Despite critical gaps such as under-resourced public health services, obsolete health information technologies, and lack of interoperability, most countries believed that their information systems had worked reasonably well in addressing the needs arising during the COVID-19 pandemic. Conclusion: Strong enabling environments and advanced and digitized health information systems are vital to controlling epidemics. Sustainable finance and government support are required for the continued implementation and enhancement of HIS. It is important to promote digital solutions beyond the COVID-19 pandemic. Now is the time to discuss potential solutions to obtain timely, accurate, and reliable health information and steer policy-making while protecting privacy rights and meeting the highest ethical standards.",Not About Sufficiency
Environmental impacts of restructuring the EU's natural gas supply and consumption: Learnings from the 2022 energy crisis,"In 2022, the European Union put forward the REPowerEU plan in response to Russia's invasion of Ukraine, aiming at enhancing short-term energy security by diversifying imports and reducing natural gas demand while accelerating the deployment of renewable alternatives in the long term. Here, we quantify the life cycle environmental impacts of both REPowerEU's short-term measures, including the controversial extended coal-fired power plant operations, and how the first year of the crisis was managed in practice. We find that the policy measures' impact on greenhouse gas (GHG) emissions would be negligible, although they could have detrimental effects on other environmental categories. In practice, GHG emissions dropped by 8.6% driven by energy savings, yet other environmental burdens worsened, primarily due to coal and oil use. Our results could support the development and analysis of long-term policies to enhance energy security via natural gas demand reduction while considering multiple environmental sustainability indicators to avoid collateral damage.",Not About Sufficiency
TRANSITION TO A SUSTAINABLE ENERGY PRODUCTION AND CONSUMPTION MODEL - MAPPING THE PATTERNS OF SUCCESS,"This research paper fills a literature gap by tapping into Europe???s Just Transition to green energy production and consumption models and introduces empirically derived trends. The mix of analysed variables was targeted at sustainable energy transformation models according to complementary economic research areas. Based on the clustered heat maps method, double dendrograms were constructed to identify socioeconomic and environmental patterns that characterize highly economic-efficient and sustainable transition patterns from the standard energy production and consumption model to the environmental-friendly and cost-efficient model in the European area. Results show that Austria, Denmark, Norway, Ireland, and Luxembourg followed the patterns of highly economically performant European countries that turned energy and resource productivity in their favour based on multiple factors: recycling, R&D, innovation, digitization. The opposite was observed in the case of many South-Eastern European countries. Robustness checks were performed based on the linear discriminant analysis methodology and results confirmed that Northern European countries lead the change to a more sustainable future. In this context, this research brings empirical results for decision-makers and aims to facilitate a better understanding for adapting policies according to national needs, as well as to the patterns of success identified in the case of European leaders.",Not About Sufficiency
Estimating Acceleration from a Single Uniform Linear Motion-Blurred Image using Homomorphic Mapping and Machine Learning,"Context: Vision -based measurement (VBM) systems are becoming popular as an affordable and suitable alternative for scientific and engineering applications. When cameras are used as instruments, motion blur usually emerges as a recurrent and undesirable image degradation, which in fact contains kinematic information that is usually dismissed. Method: This paper introduces an alternative approach to measure relative acceleration from a real invariant uniformly accelerated linear motion -blurred image. This is done by using homomorphic mapping to extract the characteristic Point Spread Function (PSF) of the blurred image, as well as machine learning regression. A total of 125 uniformly accelerated motion -blurred pictures were taken in a light- and distance -controlled environment, at five different accelerations ranging between 0,64 and 2,4 m/s(2). This study evaluated 19 variants such as tree ensembles, Gaussian processes (GPR), and linear, support vector machine (SVM), and tree regression. Results: The best RMSE result corresponds to GPR (Matern 5/2), with 0,2547 m/s(2) and a prediction speed of 530 observations per second (obs/s). Additionally, some novel deep learning methods were used to obtain the best RMSE value (0,4639 m/s(2) for Inception ResNet v2, with a prediction speed of 11 obs/s. Conclusions: The proposed method (homomorphic mapping and machine learning) is a valid alternative for calculating acceleration from invariant motion blur in real-time applications when additive noise is not dominant, even surpassing the deep learning techniques evaluated.",Not About Sufficiency
The link between agriculture and rural food security in the ecoregions of Mexico: path diagrams and underlying dataset,"In this research, we build two food systems datasets in Mexico; The first one describes the structure of agricultural production units and the second one describes food security aspects of the rural population in these agricultural production units. We also build a third dataset, consisting of path diagrams and path coefficients (derived from Structural Equation Modeling) that relate the first dataset to the second dataset in the four most populated ecoregions of Mexico. The description of the path models and the insights they bring to the current state of food security in Mexican rural households are detailed in an associated article entitled ""Is food security primarily associated with smallholder agriculture or with commercial agriculture?: An approach to the case of Mexico using structural equation modeling"" (https://doi.org/10.1016/j.agsy.2021.103091). The agricultural variables (in the first dataset) include farm size, destination of the farmer's production, cultivation practice / water management, predominant source of income of the household, land tenure type, crop diversity, agricultural surface expansion, and the presence of forest cover. They are based on the primary data of the full, latest available agricultural census in Mexico and corresponding official land use / land cover data. The second dataset consists of four food security indicators designed and built for the first food security model in Mexico that incorporates food availability, food accessibility and food utilization aspects. They include the Food Self-sufficiency Index (the balance between food production and food consumption), the Food Access Index (inversely related to marginalization), the Entitlement to Public Health Care index, and the Undernutrition Infrequency index (related to hospital sickness records). We provide the path tables and diagrams that describe the links between the agricultural structure and food security. These diagrams provide the first nationwide statistical evidence for the prominent role of smallholder agriculture in rural food security at the national level and at ecoregion scale for a country of the global South. In order to further investigate the structure of the agricultural production units and their relationships with socioeconomic, territorial and landscape data, artificial intelligence (i.e. data mining and machine learning) techniques could be performed on this compendium of datasets. The food security data may stir the development of more food security models in Mexico in relation to other drivers such as consumption habits and non-agricultural activities of rural households. (c) 2022 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/)",Not About Sufficiency
Developing policy thresholds for objectively measured environmental features to support active travel,"A novel evidence-based methodology is presented for determining place-based thresholds of objectively measured built environment features' relationships with active travel. Using an innovative machine-learning based Generalized Additive Modeling framework, systematic heterogeneity fundamental to the development of well-justified and objective environmental thresholds is accounted for. The methodology is employed to model an individual's likelihood of transport walking as a function of environmental factors using California Household Travel Survey linked with comprehensive built environment data. The results reveal strong and complex non-linear dependencies of likelihood of transport walking on environmental features that cannot be quantified using standard threshold detection methods. Thresholds for key environmental features to enhance active travel vary significantly across different socioeconomic groups. Accounting for strong income-based differences in development of environmental benchmarks is emphasized. The thresholds can serve as a useful guiding tool for policymakers, planners, engineers, and public health officials to track existing environmental conditions and healthy behaviors.",Not About Sufficiency
Enhancing Arabic text-to-speech synthesis for emotional expression in visually impaired individuals using the artificial hummingbird and hybrid deep learning model,"Depression is one of the most dangerous mental health conditions, often leading to suicide, which is the fourth leading cause of death in the Middle East. Particularly, Egypt has the highest suicide rate in the region, making it crucial to recognize depression and suicidal thoughts early. In Arab culture, awareness of mental health issues is limited, but in recent years, people have increasingly expressed their feelings on social media platforms. This shift presents an opportunity for mental health intervention through digital means. Furthermore, while facial expressions are not accessible to the blind and visually impaired, voice signals can convey emotional nuances, offering an alternative method for detecting mental health states. Natural Language Processing (NLP) and machine learning (ML) techniques provide powerful tools for analysing social media text data, helping detect emotional distress and providing timely support. By applying these technologies, AI-driven solutions can assist in understanding and addressing mental health concerns more inclusively. This study designs an Arabic Mood Changing and Depression Detection using the Artificial Hummingbird Optimization Algorithm with Deep Learning (AMCDD-AHODL) technique for visually impaired individuals. The AMCDD-AHODL technique detects different kinds of emotions and depression using Arabic tweets. After pre-processing, the word embedding process is carried out using the AraBERT model. Furthermore, the AMCDD-AHODL technique utilizes a hybrid LSTM+BiGRU model for the recognition and classification model. To improve the performance of the hybrid LSTM+BiGRU methodology, the AMCDD-AHODL technique comprises an AHO-based hyperparameter tuning process. Finally, the WaveNet model enhances the naturalness and clarity of text-to-speech synthesis, delivering high-quality, human-like audio output. The AMCDD-AHODL approach is examined using the Modern Standard Arabic dataset containing 1229 records. The performance validation of the AMCDD-AHODL approach portrayed a superior accuracy value of 95.80 % compared to the existing ML and DL models. Therefore, the AMCDDAHODL technique is applied for the early identification of various kinds of depression that can decrease the distress from the illness and the stigma related to mental health problems.",Not About Sufficiency
Systems of social practice and automation in an energy efficient home,"The development of residential automation and energy management systems have aimed to achieve energy reduction within the home. These systems have been developed to compliment human behaviour and potentially impart change in the way people consume energy. The development of energy efficient buildings has been considered a sustainable approach to better manage or optimise energy consumption and generation within a building however, multiple studies have demonstrated some limitations to these buildings and systems. The advancement of the home system of practice using the basis of Social Practice Theory has demonstrated a new focus for the design methodology of these systems. The home system of practice outlines the routines and re-petitive nature of individual lifestyles, and these routines result in the temporal characteristic of energy con-sumption. This study monitored an Australian home that consisted of renewable energy systems, smart technology and automated systems designed to reduce the use of grid electricity. The aim was to understand the performance of the automation systems and interaction with the occupants and their routines. The paper dis-cusses the effectiveness of incorporating social theories and the home system of practice into the design of energy efficient buildings and energy management systems. The methodology utilised machine learning techniques to identify patterns in energy consumption data and related these patterns to the routines and lifestyles of the home occupants. These patterns aimed to describe why the automation system incorporated into the study home did not operate as intended and how design changes can increase effectiveness and performance.",Not About Sufficiency
Using the LSTM network to forecast the demand for hard coal,"Securing the certainty of supplies of the necessary minimum energy in each country is a basic condition for the energy security of the state and its citizens. The concept of energy security combines several aspects at the same time, as it can be considered in terms of the availability of own energy resources, it concerns technical aspects related to technical infrastructure, as well as political aspects related to the management and diversification of energy supplies. Another aspect of the issue of energy security is the environmental perspective, which is now becoming a priority in the light of the adopted objectives of the European Union's energy policy. The restrictive requirements for reducing greenhouse gas emissions and increasing the required level of renewable energy sources in the energy balance of the Member States is becoming a challenge for economies that use fossil fuels to a large extent in the raw material structure, including Poland. Poland is the largest producer of hard coal in the European Union and hard coal is a strategic raw material as it satisfies about 50% of the country's energy demand. In this context, the main goal of the article was to determine the future sale of hard coal by 2030 in relation to environmental regulations introduced in the energy sector. For this purpose, a mathematical model with a 95% confidence interval was developed using artificial LSTM neural networks, which belong to deep learning machine learning techniques, which reflects the key relationships between hard coal mining and the assumptions adopted in the National Energy and Climate Plan for the years 2021-2030 (NECP).",Not About Sufficiency
Probing uncertainty levels of electrification in informal urban settlements: A case from South Africa,"This paper assesses the different levels of uncertainty that affect the analysis of informal urban settlements and the implementation of upgrading policies, with a specific focus on electrification. The rapid growth of informal settlements in the cities of the Global South poses serious challenges to the management of energy systems, particularly when it comes to the electricity grid. Informal urban settlements are characterized by the lack of urban planning and low or absent provision of public services. Exponential population growth increases the complexity of urban planning. An inadequate understanding of uncertainty can undermine the effectiveness of informal settlement upgrading and deepen social inequalities. Based on the case study of the Enkanini settlement in Stellenbosch, South Africa, this paper probes three levels of uncertainty: (i) methodological uncertainty associated with the challenge of estimating energy demand and demographic changes, (ii) technical uncertainty associated with the expansion of the electric grid and securing revenues, and (iii) epistemological uncertainty associated with the definition of the relevant problems and pertinent solutions for informal settlements. The paper highlights how the focus of technical uncertainty displaces the debate on the socio-political challenges of informal settlement upgrading. (C) 2016 Elsevier Ltd. All rights reserved.",Not About Sufficiency
"An actor-centered, scalable land system typology for addressing biodiversity loss in the world's tropical dry woodlands","Land use is a key driver of the ongoing biodiversity crisis and therefore also a major opportunity for its mitigation. However, appropriately considering the diversity of land -use actors and activities in conservation assessments and planning is challenging. As a result, top -down conservation policy and planning are often criticized for a lack of contextual nuance widely acknowledged to be required for effective and just conservation action. To address these challenges, we have developed a conceptually consistent, scalable land system typology and demonstrated its usefulness for the world 's tropical dry woodlands. Our typology identifies key land -use actors and activities that represent typical threats to biodiversity and opportunities for conservation action. We identified land systems in a hierarchical way, with a global level allowing for broad -scale planning and comparative work. Nested within it, a regionalized level provides social -ecological specificity and context. We showcase this regionalization for five hotspots of land -use change and biodiversity loss in dry woodlands in Argentina, Bolivia, Mozambique, India, and Cambodia. Unlike other approaches to present land use, our typology accounts for the complexity of overlapping land uses. This allows, for example, assessment of how conservation measures conflict with other land uses, understanding of the social -ecological co -benefits and tradeoffs of area -based conservation, mapping of threats, or targeting area -based and actor -based conservation measures. Moreover, our framework enables cross -regional learning by revealing both commonalities and socialecological differences, as we demonstrate here for the world 's tropical dry woodlands. By bridging the gap between global, top -down, and regional, bottom -up initiatives, our framework enables more contextually appropriate sustainability planning across scales and more targeted and social -ecologically nuanced interventions.",Not About Sufficiency
Understanding hate speech: the HateInsights dataset and model interpretability,"The persistence of hate speech continues to pose an obstacle in the realm of online social media. Despite the continuous evolution of advanced models for identifying hate speech, the critical dimensions of interpretability and explainability have not received proportional scholarly attention. In this article, we introduce the HateInsights dataset, a groundbreaking benchmark in the fi eld of hate speech datasets, encompassing diverse aspects of this widespread issue. Within our dataset, each individual post undergoes thorough annotation from dual perspectives: fi rstly, conforming to the established 3-class classification fi cation paradigm that includes hate speech, offensive language, and normal discourse; secondly, incorporating rationales that outline specific fi c segments of a post supporting the assigned label (categorized as hate speech, offensive language, or normal discourse). Our exploration yields a significant fi cant fi nding by harnessing cutting-edge state-of-the-art models: even models demonstrating exceptional proficiency fi ciency in classification fi cation tasks yield suboptimal outcomes in crucial explainability metrics, such as model plausibility and faithfulness. Furthermore, our analysis underscores a promising revelation concerning models trained using human-annotated rationales. To facilitate scholarly progress in this realm, we have made both our dataset and codebase accessible to fellow researchers. This initiative aims to encourage collaborative involvement and inspire the advancement of the hate speech detection approach characterized by increased transparency, clarity, and fairness.",Not About Sufficiency
Novel Comparative Study for the Detection of COVID-19 Using CT Scan and Chest X-ray Images,"The number of coronavirus disease (COVID-19) cases is constantly rising as the pandemic continues, with new variants constantly emerging. Therefore, to prevent the virus from spreading, coronavirus cases must be diagnosed as soon as possible. The COVID-19 pandemic has had a devastating impact on people's health and the economy worldwide. For COVID-19 detection, reverse transcription-polymerase chain reaction testing is the benchmark. However, this test takes a long time and necessitates a lot of laboratory resources. A new trend is emerging to address these limitations regarding the use of machine learning and deep learning techniques for automatic analysis, as these can attain high diagnosis results, especially by using medical imaging techniques. However, a key question arises whether a chest computed tomography scan or chest X-ray can be used for COVID-19 detection. A total of 17,599 images were examined in this work to develop the models used to classify the occurrence of COVID-19 infection, while four different classifiers were studied. These are the convolutional neural network (proposed architecture (named, SCovNet) and Resnet18), support vector machine, and logistic regression. Out of all four models, the proposed SCoVNet architecture reached the best performance with an accuracy of almost 99% and 98% on chest computed tomography scan images and chest X-ray images, respectively.",Not About Sufficiency
"Early prediction of SARS-CoV-2 reproductive number from environmental, atmospheric and mobility data: A supervised machine learning approach","Introduction: SARS-CoV-2 was declared a pandemic by the WHO on March 11th, 2020. Public protective measures were enforced in every country to limit the diffusion of SARS-CoV-2. Its transmission, mainly by droplets, has been measured by the effective reproduction number (Rt) that counts the number of secondary cases caused in a population by an average infectious individual at time t. Current strategies to calculate Rt reflect the number of secondary cases after several days, due to a delay from symptoms onset to reporting. We propose a comple-mentary Rt estimation using supervised machine learning techniques to predict short term variations with more timely results.Material and methods: Our primary goal was to predict Rt of the current day in the twelve provinces of Lombardy with the highest possible accuracy, and with no influence of the local testing strategies. We gathered data about mobility, weather, and pollution from different public sources as a proxy of human behavior and public health measures. We built four supervised machine learning algorithms with different strategies: the outcome variable was the daily median Rt values per province obtained from officially adopted algorithms.Results: Data from 243 days for every province were presented to our four models (from February 15th, 2020, to October 14th, 2020). Two models using differential calculation of Rt instead of the raw values showed the highest mean coefficient of determination (0.93 for both) and residuals reported the lowest mean error (-0.03 and 0.01) and standard deviation (0.13 for both) as well. The one with access to the value of Rt of the day before heavily relied on that feature for prediction, while the other one had more distributed weights. Discussion: The model that had not access to the Rt value of the previous day and used Rt differential value as outcome (FDRt) was considered the most robust according to the metrics. Its forecasts were able to predict the trend that Rt values would have developed over different weeks, but it was not particularly accurate in predicting the precise value of Rt. A correlation among mobility, atmospheric, features, pollution and Rt values is plausible, but further testing should be performed.",Not About Sufficiency
When tribal sovereignty challenges democracy: American Indian education and the democratic ideal,"The lessons of American Indian education-a grand experiment in standardization-can lead to a more equitable educational system for all U.S. citizens. While masquerading as a tool for equal opportunity, standardization has marginalized Native peoples. We argue for diversity-not standardization-as a foundational value for a just multicultural democracy, but diversity is feared by some as a threat to the nation's integrity. Critical historical analysis of the apparently contradictory policies and practices within American Indian education reveals a patterned response to cultural and linguistic diversify, as the federal government has attempted to distinguish ""safe"" from ""dangerous"" Native practices. Examples of the contest between Indigenous self-determination (rooted in internal sovereignty) and federal control illustrate the profound national ambivalence toward diversity but also the potential to nourish ""places of difference"" within a healthy democracy.",Not About Sufficiency
Explainable machine learning for phishing feature detection,"Phishing is a very dangerous security threat that affects individuals as well as companies and organizations. To fight the risks associated with this threat, it is important to detect phishing websites in a timely manner. Machine learning models work well for this purpose as they can predict phishing cases, using information on the underlying websites. In this paper, we contribute to the research on the detection of phishing websites by proposing an explainable machine learning model that can provide not only accurate predictions of phishing, but also explanations of which features are most likely associated with phishing websites. To this aim, we propose a novel feature selection model based on Lorenz Zonoids, the multidimensional extension of Gini coefficient. We illustrate our proposal on a real dataset that contains features of both phishing and legitimate websites.",Not About Sufficiency
XCloud-pFISTA: A Medical Intelligence Cloud for Accelerated MRI,"Machine learning and artificial intelligence have shown remarkable performance in accelerated magnetic resonance imaging (MRI). Cloud computing technologies have great advantages in building an easily accessible platform to deploy advanced algorithms. In this work, we develop an open-access, easy-to-use and high-performance medical intelligence cloud computing platform (XCloud-pFISTA) to reconstruct MRI images from undersampled k-space data. Two state-of-the-art approaches of the Projected Fast Iterative Soft-Thresholding Algorithm (pFISTA) family have been successfully implemented on the cloud. This work can be considered as a good example of cloud-based medical image reconstruction and may benefit the future development of integrated reconstruction and online diagnosis system.",Not About Sufficiency
Making waves: Integrating wastewater surveillance with dynamic modeling to track and predict viral outbreaks,"Wastewater surveillance has proved to be a valuable tool to track the COVID-19 pandemic. However, most studies using wastewater surveillance data revolve around establishing correlations and lead time relative to reported case data. In this perspective, we advocate for the integration of wastewater surveillance data with dynamic within-host and between-host models to better understand, monitor, and predict viral disease outbreaks. Dynamic models overcome emblematic difficulties of using wastewater surveillance data such as establishing the temporal viral shedding profile. Complementarily, wastewater surveillance data bypasses the issues of time lag and underreporting in clinical case report data, thus enhancing the utility and applicability of dynamic models. The integration of wastewater surveillance data with dynamic models can enhance real-time tracking and prevalence estimation, forecast viral transmission and intervention effectiveness, and most importantly, provide a mechanistic understanding of infectious disease dynamics and the driving factors. Dynamic modeling of wastewater surveillance data will advance the development of a predictive and responsive monitoring system to improve pandemic preparedness and population health.",Not About Sufficiency
Optimization of the automated Sunnybrook Facial Grading System - Improving the reliability of a deep learning network with facial landmarks,"Objective. - The Sunnybrook Facial Grading System (SFGS) is a well-established grading system to assess the severity and progression of a unilateral facial palsy. The automation of the SFGS makes the SFGS more accessible for researchers, students, clinicians in training, or other untrained co-workers and could be implemented in an eHealth environment. This study investigated the impact on the reliability of the automated SFGS by adding a facial landmark layer in a previously developed convolutional neural network (CNN). Methods. - An existing dataset of 116 patients with a unilateral peripheral facial palsy and 9 healthy subjects performing the SFGS poses was used to train a CNN with a newly added facial landmark layer. A separate model was trained for each of the 13 elements of the SFGS and then used to calculate the SFGS subscores and composite score. The intra-class coefficient of the automated grading system was calculated based on three clinicians experienced in the grading of facial palsy. Results. - The inter-rater reliability of the CNN with the additional facial landmarks increased in performance for all composite scores compared to the previous model. The intra-class coefficient for the composite SFGS score increased from 0.87 to 0.91, the resting symmetry subscore increased from 0.45 to 0.62, the symmetry of voluntary movement subscore increased from 0.89 to 0.92, and the synkinesis subscore increased from 0.75 to 0.78. Conclusion. - The integration of a facial landmark layer into the CNN significantly improved the reliability of the automated SFGS, reaching a performance level comparable to human observers. These results were attained without increasing the dataset underscoring the impact of incorporating facial landmarks into a CNN. These findings indicate that the automated SFGS with facial landmarks is a reliable tool for assessing patients with a unilateral peripheral facial palsy and is applicable in an eHealth environment. (c) 2024 Les Auteurs. Publie<acute accent> par Elsevier Masson SAS. Cet article est publie<acute accent> en Open Access sous licence CC BY (http://creativecommons.org/licenses/by/4.0/).",Not About Sufficiency
"Can informal employment be compared in South America? Analysis of its definition, measurement and classification","Objective: To characterize and analyze the situation of informal employment with regard to its definition, measurement and classification in South American countries. Methods: A literature review was conducted from four databases and grey literature through a scoping review, which included reports from international organizations and from the 12 countries in South America. The data were analyzed by evaluating content and establishing similarities among countries. Results: The data reviewed showed a disparity in the definitions used, although many countries define informal employment as workers with no contract. Most countries measured informal employment through household surveys, but due to the differences in classifications, the information found was heterogeneous, with little standardization among registries. Therefore, the data could not be compared at a regional level. The definition of the International Labour Organization was not useful to study informal employment in the countries studied. The definition should include protected and unprotected workers. Conclusions: An appropriate and specific definition of informal employment would allow nuances to be studied within the concept, revealing the loopholes faced by most of the population working informally. The key to meaningful comparisons within the study region is to incorporate common indicators among local registration systems (measurement) in order to determine the public health impact in the informally employed population. (C) 2014 SESPAS. Published by Elsevier Espana, S.L.U. All rights reserved.",Not About Sufficiency
Measuring Community Resilience and Its Determinants: Relocated Vulnerable Community in Western China,"With the full implementation of poverty alleviation resettlement (PAR), the restoration and improvement of the comprehensive living standards of relocated households have received increasing attention from policy researchers. The measurement of resilience and its determinants provides new ideas for PAR at the community level. This article proposes a method for examining community resilience in the context of PAR through a survey of 459 relocated households in western China and uses regression analysis to identify the determinants of community resilience. The results showed that the four dimensions of community resilience, in descending order, included: environmental resilience, economic resilience, management resilience, and social resilience. Income level and livelihood diversification were positively correlated with the community resilience index. Relocation time, relocation type, and resettlement mode were all essential determinants of the community resilience of relocated households. Finally, some suggestions were put forward, such as the need to build an interpersonal relationship network, guide pure farmers and non-farmers to transform into diversified livelihood households, and formulate a unified community action plan and interest protection mechanism so as to provide a reference for decision-making among managers to make decisions.",Not About Sufficiency
"Heart Healthy Cities - Genes Load the Gun, the Environment Pulls the Trigger","The World Health Organization states that urban planning is now recognized as a critical part of a comprehensive solution to address harmful health effects of the environment. The present review deals with non-communicable diseases with an emphasis on cardiovascular diseases and the urbanization process in relation to environmental risks such as noise, air pollution, temperature and outdoor light. It discusses why heat islands can develop in urban areas and how green spaces in cities can improve public health and address climate issues, sustainability and viability. In addition, we discuss urban planning, traffic interventions and novel technologies for assessing external environmental exposures, e.g. use of digital technologies to promote heart-healthy cities in the future. Lastly, we highlight new paradigms of integrative thinking, such as the exposome and planetary health, by critically evaluating the relationship between exposure and health outcomes and by broadening our understanding of the totality of exposures to the human environment.",Not About Sufficiency
Natural language model for automatic identification of Intimate Partner Violence reports from Twitter,"Intimate partner violence (IPV) is a preventable public health problem that affects millions of people worldwide. Approximately one in four women are estimated to be or have been victims of severe violence at some point in their lives, irrespective of age, ethnicity, and economic status. Victims often report IPV experiences on social media, and automatic detection of such reports via machine learning may enable improved surveillance and targeted distribution of support and/or interventions for those in need. However, no artificial intelligence systems for automatic detection currently exists, and we attempted to address this research gap. We collected posts from Twitter using a list of IPV-related keywords, manually reviewed subsets of retrieved posts, and prepared annotation guidelines to categorize tweets into IPV-report or non-IPV-report. We annotated 6,348 tweets in total, with the inter-annotator agreement (IAA) of 0.86 (Cohen's kappa) among 1,834 double-annotated tweets. The class distribution in the annotated dataset was highly imbalanced, with only 668 posts (similar to 11%) labeled as IPV-report. We then developed an effective natural language processing model to identify IPV-reporting tweets automatically. The developed model achieved classification F-1-scores of 0.76 for the IPV-report class and 0.97 for the non-IPV-report class. We conducted post-classification analyses to determine the causes of system errors and to ensure that the system did not exhibit biases in its decision making, particularly with respect to race and gender. Our automatic model can be an essential component for a proactive social media-based intervention and support framework, while also aiding population-level surveillance and large-scale cohort studies.",Not About Sufficiency
Frontline clinical diagnosis-FTIR on pancreatic cancer,"Objective: Accurate, easily accessible and economically viable cancer diagnostic tools are pivotal in improving the abysmal 5% survival rate of pancreatic cancer.Methods: A novel, affordable, non-invasive diagnostic method has been developed by combining measurement precision of infrared spectroscopy with classification using machine learning tools.Results: Diagnosis accuracy as high as 90% has been achieved. The study investigated urine and blood from pancreas cancer patients and healthy volunteers, and significantly improved accuracy by focusing on sweet-spots within blood plasma fractions containing molecules within a narrow range of molecular weights.",Not About Sufficiency
Deep Graph Machine Learning Models for Epidemic Spread Prediction and Prevention,"Epidemic spread prediction and prevention have been of paramount significance for safeguarding the public health and quality of life. However, the adoption of the appropriate safety measures and actions should also take into account other societal challenges, such as the impact on the local economy or the psychological strain on its inhabitants. Recent approaches for preventing an epidemic spread have led to the adoption of rather aggressive strategies with significantly negative side-effects. In this work we address the aforementioned issue by developing a dynamic and data-driven prevention strategy, using modern graph machine learning predictive models. This strategy imposes more realistic assumptions about the pandemic spread and the underlying network structure, while also minimizing the negative side-effects. Finally, the experimental evaluation of our novel architecture for the predictive model demonstrates that we significantly outperform existing methods.",Not About Sufficiency
Implementing a research agenda for complementary and alternative medicine,"Complementary and alternative medicine (CAM) consists of diverse clinical interventions that are practiced because of their popularity rather than the prior demonstration of safety and efficacy required for conventional agents. CAM therapies can be grouped into five categories: biologically based therapies, manipulative and body-based interventions, mind-body interventions, ""energy"" therapies, and alternative medical systems. The present evidence that individual CAM interventions are efficacious is largely anecdotal, but hundreds of small trials have yielded positive results. For a few modalities, existing data are either very encouraging or else sufficient to conclude that they are ineffective. CAM interventions are presumed to be safe, yet they may not be, particularly in the case of botanical agents with inherent toxicities, significant drug interactions, or potent adulterants. The public health questions regarding CAM can only be addressed through a research agenda that defines which interventions have favorable therapeutic indices. Implementation of this agenda involves adequate characterization and standardization of the product or practice, with rigorous investigation to demonstrate its safety, mechanism of action, and efficacy.",Not About Sufficiency
Compact urbanism and the synergic potential of its integration with data-driven smart urbanism : An extensive interdisciplinary literature review,"Sustainable cities have, since the early 1990s, been the leading global paradigm of urbanism thanks to the different models of sustainable urban form proposed as new frameworks for the redesigning and restructuring of urban places to make urban living more sustainable. The compact city is the most preferred model of sustainable urbanism for responding to the challenges of sustainable development. However, despite the benefits claimed by the advocates of this model, its critics highlight a number of conflicts and contentions. This is coupled with several problems, issues, and challenges considering the very fragmented picture that arises of change on the ground in the face of urbanization. In this context, it has been suggested that the compact city needs to embrace and leverage what advanced ICT has to offer so as to improve, advance, and maintain its contribution to sustainability. With the above in regard, this paper provides a comprehensive state of the art review of compact urbanism as a set of planning and development practices and strategies, focusing on the three dimensions of sustainability and the significant, yet untapped, potential of big data technology for enhancing such practices and strategies under what is labelled 'data-driven smart sustainable urbanism.' This paper identifies compactness, density, diversity, mixed land use, sustainable transportation, and green space as the prevalent design strategies of the compact city. At the heart of this model is the clear synergy between the underlying strategies in terms of their cooperation to produce combined effects greater than the sum of their separate effects as regards the tripartite value of sustainability. Indeed, this paper corroborates that the compact city is justified by its ability to contribute to the environmental, economic, and social goals of sustainability. Nevertheless, the economic goals seem to dominate over the environmental and social goals, notwithstanding the general claim about the three sustainability dimensions being equally important and mutually dependent. Further, this paper reveals that big data technology holds great potential for enhancing compact urbanism with respect to sustainability. This thorough review of and critique on the existing work on the compact city provides a reference for researchers and practitioners in related communities and the necessary material to inform these communities of the latest developments in the field of compact urbanism and its relation to data driven smart urbanism. This work serves to inform various urban stakeholders about the benefits of data-driven smart solutions for advancing sustainability.",Not About Sufficiency
Molecular quantum chemical data sets and databases for machine learning potentials,"The field of computational chemistry is increasingly leveraging machine learning (ML) potentials to predict molecular properties with high accuracy and efficiency, providing a viable alternative to traditional quantum mechanical (QM) methods, which are often computationally intensive. Central to the success of ML models is the quality and comprehensiveness of the data sets on which they are trained. Quantum chemistry data sets and databases, comprising extensive information on molecular structures, energies, forces, and other properties derived from QM calculations, are crucial for developing robust and generalizable ML potentials. In this review, we provide an overview of the current landscape of quantum chemical data sets and databases. We examine key characteristics and functionalities of prominent resources, including the types of information they store, the level of electronic structure theory employed, the diversity of chemical space covered, and the methodologies used for data creation. Additionally, an updatable resource is provided to track new data sets and databases at https://github.com/Arif-PhyChem/datasets_and_databases_4_MLPs. This resource also has the overview in a machine-readable database format with the Jupyter notebook example for analysis. Looking forward, we discuss the challenges associated with the rapid growth of quantum chemical data sets and databases, emphasizing the need for updatable and accessible resources to ensure the long-term utility of them. We also address the importance of data format standardization and the ongoing efforts to align with the FAIR principles to enhance data interoperability and reusability. Drawing inspiration from established materials databases, we advocate for the development of user-friendly and sustainable platforms for these data sets and databases.",Not About Sufficiency
Enriching geospatial data with computer vision to identify urban environment determinants of social interactions,"Characteristics of urban space (co-)determine human behaviour, including their social interaction patterns. However, despite numerous studies that have examined how the urban space impacts social interactions, their relationships are still poorly understood. Recent developments in computer vision and machine learning fields offer promising new ways to analyse and collect data on social interactions. This study proposes a new computer vision-based approach to study how the urban space impacts social interactions. The proposed method uses pre-trained object detection models to detect social interactions (including their geo-locations) from street-view imagery. After that, it regresses urban space characteristics -which are also detected using object detection models - on social interactions. For this study, 294,852 street-level images from three Dutch cities are analysed. Results from linear regression analysis show that for these three Dutch cities people tend to meet in places with a strong presence of recreational attractions and bicycles. Also, the results of data collection and image processing can be used to identify the areas most likely to produce social interactions in urban space to conduct urban studies.",Not About Sufficiency
Development and Validation of the COVID-19 Hospitalized Patient Deterioration Index,"OBJECTIVES: To develop a COVID-19-specific deterioration index for hospitalized patients: the COVID Hospitalized Patient Deterioration Index (COVID-HDI). This index builds on the proprietary Epic Deterioration Index, which was not developed for predicting respiratory deterioration events among patients with COVID-19. STUDY DESIGN: A retrospective observational cohort was used to develop and validate the COVID-HDI model to predict respiratory deterioration or death among hospitalized patients with COVID-19. Deterioration events were defined as death or requiring high -flow oxygen, bilevel positive airway pressure, mechanical ventilation, or intensive -level care within 72 hours of run time. The sample included hospitalized patients with COVID-19 diagnoses or positive tests at Kaiser Permanente Southern California between May 3, 2020, and October 17, 2020. METHODS: Machine learning models and 118 candidate predictors were used to generate benchmark performance. Logit regression with least absolute shrinkage and selection operator and physician input were used to finalize the model. Split -sample cross -validation was used to train and test the model. RESULTS: The area under the receiver operating curve was 0.83. COVID-HDI identifies patients at low risk (negative predictive value [NPV] > 98.5%) and borderline low risk (NPV > 95%) of an event. Of all patients, 74% were identified as being at low or borderline low risk at some point during their hospitalization and could be considered for discharge with or without home monitoring. A high -risk group with a positive predictive value of 51% included 12% of patients. Model performance remained high in a recent cohort of patients. CONCLUSIONS: COVID-HDI is a parsimonious, wellcalibrated, and accurate model that may support clinical decision -making around discharge and escalation of care.",Not About Sufficiency
Confounding amplifies the effect of environmental factors on COVID-19,"The global COVID-19 pandemic has severely impacted human health and socioeconomic development, posing an enormous public health challenge. Extensive research has been conducted into the relationship between environmental factors and the transmission of COVID-19. However, numerous factors in fluence the development of pandemic outbreaks, and the presence of confounding effects on the mechanism of action complicates the assessment of the role of environmental factors in the spread of COVID-19. Direct estimation of the role of environmental factors without removing the confounding effects will be biased. To overcome this critical problem, we developed a Double Machine Learning (DML) causal model to estimate the debiased causal effects of the in fluencing factors in the COVID-19 outbreaks in Chinese cities. Comparative experiments revealed that the traditional multiple linear regression model overestimated the impact of environmental factors. Environmental factors are not the dominant cause of widespread outbreaks in China in 2022. In addition, by further analyzing the causal effects of environmental factors, it was veri fied that there is signi ficant heterogeneity in the role of environmental factors. The causal effect of environmental factors on COVID-19 changes with the regional environment. It is therefore recommended that when exploring the mechanisms by which environmental factors in fluence the spread of epidemics, confounding factors must be handled carefully in order to obtain clean quantitative results. This study offers a more precise representation of the impact of environmental factors on the spread of the COVID-19 pandemic, as well as a framework for more accurately quantifying the factors in flu- encing the outbreak. (c) 2024 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",Not About Sufficiency
Post-disaster recovery planning for infrastructure systems based on residents' needs: A hypernetwork approach,"The complexity of post-disaster recovery increasingly challenges urban resilience and the effective fulfillment of residents' needs. Traditional recovery models often prioritize infrastructure restoration, overlooking how residents' needs interact with social agencies and facilities. This study introduces a hypernetwork-based framework to guide the recovery planning of infrastructure systems, analyzing the generation and dissolution of hyperedges over time to quantify recovery dynamics. To demonstrate the framework's applicability, we analyzed 336,928 residentgenerated appeals and their corresponding resolution records collected from the government in Beijing. Results show that (1) hyperedges associated with essential needs for emergency communication, road access, and water supply are the most critical in disaster response and recovery, as it exhibits the highest hyperedge degree and frequency, with over 40 % nodes and 50 % hyperedges underperform in their respective metrics. (2) Resource misallocations across these hyperedges degrade recovery performance, with specific needs requiring pre-disaster preparedness in communication systems, labor-intensive actions in transportation systems, and crossdepartmental coordination in water supply systems. (3) High-frequency hyperedges show low similarity (average below 0.33), suggesting insufficient preparedness of distributed resources for critical needs. (4) Resource-based hyperedge interactions dominate during the early recovery stages through infrastructure restoration, while administrative interventions become more significant in later stages. Theoretically, this study advances urban recovery modeling by bridging interactions among residents' needs, social agencies and facility restoration within a hypernetwork. Practically, it provides tools for policymakers to implement prioritized planning and identify critical actors, thereby supporting a recovery process aligned with residents' needs.",Not About Sufficiency
Queering Housing Policy: Questioning Urban Planning Assumptions in Namibian Cities,"Heteronormative models of the home have permeated housing policies for decades, only adding to economic and spatial inequalities in a landscape of housing injustices. Half of the urban population in Namibia lives in precarious housing conditions. Cities like Windhoek and Walvis Bay are among the most unequal in the world. Such inequalities translate into significant gaps in housing quality, security, and service provision. These inequalities are acutely felt by LGBTIQ+ populations that already face other forms of exclusion from economic and social life and fundamental human rights. A new National Housing Policy-emphasizing the right to housing-is about to be adopted in Namibia, but would it address the concerns of queer populations? This article asks what it means to engage with Namibia's new National Housing Policy through the lens of queer decolonial thought. It presents an exploratory study of the questions emerging at the margins of the discussion on the National Housing Policy. The objective was to develop an exploratory research agenda for a queer decolonial perspective on housing in Namibia. In the context of enormous housing shortages, a queer decolonial perspective emphasizes radical inclusion as a principle for housing provision. The exploration of shared queer experiences in accessing housing suggests that the themes of belonging, identity, and safety may support the development of such an agenda. Queer decolonial thought has thus three implications for an agenda of research on housing in Namibia. First, it calls for understanding what community and belonging mean for LGBTIQ+ people. Second, queer decolonial thought poses questions about citizenship, particularly given the shift to a view of the state as creating housing opportunities (through land rights and basic services) and support mechanisms for incremental housing. Queer decolonial thought calls for identifying the multiple ways the state misrecognizes individuals who do not conform to prescribed identities and sexual orientations. Third, queer decolonial thought invites reflection on the constitution of safe spaces in aggressive urban environments and the multiple layers of perceived safety constructed through diverse institutions and public spaces.",Not About Sufficiency
Balanced spatial distribution of green areas creates healthier urban landscapes,"The benefits of green infrastructure on human well-being in urban areas are already well-established, with strong evidence of the positive effects of the amount and proximity to green areas. However, the understanding of how the spatial distribution and type of green areas affect health is still an open question. Here, through a land sharing and sparing framework, we explore how different spatial configurations of green and built-up areas and how different types of green areas can affect cardiovascular and respiratory hospitalizations in Sao Paulo city, Brazil. Sharing/sparing indicators were selected as the main explanatory factors in the control of all groups of diseases. Land sharing appeared as a favourable spatial condition to prevent cardiovascular hospitalization, while land sparing and arboreal vegetation were relevant to reduce hospitalization by lower respiratory diseases. For upper respiratory diseases, forests seem to provide a disservice, once they were associated with increased rates of hospitalization by respiratory allergies causes. Considering that hospitalization rates and severity of cardiovascular diseases are substantially higher than those of upper respiratory ones, dense vegetation tends to provide more services than disservices. The land sharing configuration, which is characterized by green areas spread throughout the urban network (in streets, gardens, small squares or parks), should lead to higher exposure and use of the benefits of green areas, which may then explain the greater prevention of cardiovascular diseases. These novel results indicate that a more balanced distribution of green areas across built-up areas creates healthier urban spaces, and thus can be used as an urban planning strategy to leverage the health benefits provided by green infrastructure. Policy implications. Aiming to reduce hospitalizations by cardiovascular and pulmonary causes, urban planning should promote the spreading of green areas across the cities, in order to increase daily contact with natural attributes, giving preference to distribution over total quantity of green in urban landscape.",Not About Sufficiency
"Precision public health, the key for future outbreak management: A scoping review","Background Precision Public Health (PPH) is a newly emerging field in public health medicine. The application of various types of data allows PPH to deliver more tailored interventions to a specific population within a specific timeframe. However, the application of PPH possesses several challenges and limitations that need to be addressed.Objective We aim to provide evidence of the various use of PPH in outbreak management, the types of data that could be used in PPH application, and the limitations and barriers in the application of the PPH approach.Methods and analysis Articles were searched in PubMed, Web of Science, and Science Direct. Our selection of articles was based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) for Scoping Review guidelines. The outcome of the evidence assessment was presented in narrative format instead of quantitative.Results A total of 27 articles were included in the scoping review. Most of the articles (74.1%) focused on PPH applications in performing disease surveillance and signal detection. Furthermore, the data type mostly used in the studies was surveillance (51.9%), environment (44.4), and Internet query data. Most of the articles emphasized data quality and availability (81.5%) as the main barriers in PPH applications followed by data integration and interoperability (29.6%).Conclusions PPH applications in outbreak management utilize a wide range of data sources and analytical techniques to enhance disease surveillance, investigation, modeling, and prediction. By leveraging these tools and approaches, PPH contributes to more effective and efficient outbreak management, ultimately reducing the burden of infectious diseases on populations. The limitation and challenges in the application of PPH approaches in outbreak management emphasize the need to strengthen the surveillance systems, promote data sharing and collaboration among relevant stakeholders, and standardize data collection methods while upholding privacy and ethical principles.",Not About Sufficiency
Artificial Intelligence in the Fight Against COVID-19: Scoping Review,"Background: In December 2019, COVID-19 broke out in Wuhan, China, leading to national and international disruptions in health care, business, education, transportation, and nearly every aspect of our daily lives. Artificial intelligence (AI) has been leveraged amid the COVID-19 pandemic; however, little is known about its use for supporting public health efforts. Objective: This scoping review aims to explore how AI technology is being used during the COVID-19 pandemic, as reported in the literature. Thus, it is the first review that describes and summarizes features of the identified AI techniques and data sets used for their development and validation. Methods: A scoping review was conducted following the guidelines of PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews). We searched the most commonly used electronic databases (eg, MEDLINE, EMBASE, and Psyclnfo) between April 10 and 12, 2020. These terms were selected based on the target intervention (ie, AI) and the target disease (ie, COVID-19). Two reviewers independently conducted study selection and data extraction. A narrative approach was used to synthesize the extracted data. Results: We considered 82 studies out of the 435 retrieved studies. The most common use of AI was diagnosing COVID-19 cases based on various indicators. AI was also employed in drug and vaccine discovery or repurposing and for assessing their safety. Further, the included studies used AI for forecasting the epidemic development of COVID-19 and predicting its potential hosts and reservoirs. Researchers used AI for patient outcome-related tasks such as assessing the severity of COVID-19, predicting mortality risk, its associated factors, and the length of hospital stay. AI was used for infodemiology to raise awareness to use water, sanitation, and hygiene. The most prominent AI technique used was convolutional neural network, followed by support vector machine. Conclusions: The included studies showed that AI has the potential to fight against COVID-19. However, many of the proposed methods are not yet clinically accepted. Thus, the most rewarding research will be on methods promising value beyond COVID-19. More efforts are needed for developing standardized reporting protocols or guidelines for studies on AI.",Not About Sufficiency
The health benefits of urban green spaces: a review of the evidence,"Background Urban development projects can be costly and have health impacts. An evidence-based approach to urban planning is therefore essential. However, the evidence for physical and non-physical health benefits of urban green space is unclear. Methods A literature search of academic and grey literature was conducted for studies and reviews of the health effects of green space. Articles found were appraised for their relevance, critically reviewed and graded accordingly. Their findings were then thematically categorized. Results There is weak evidence for the links between physical, mental health and well-being, and urban green space. Environmental factors such as the quality and accessibility of green space affects its use for physical activity. User determinants, such as age, gender, ethnicity and the perception of safety, are also important. However, many studies were limited by poor study design, failure to exclude confounding, bias or reverse causality and weak statistical associations. Conclusion Most studies reported findings that generally supported the view that green space have a beneficial health effect. Establishing a causal relationship is difficult, as the relationship is complex. Simplistic urban interventions may therefore fail to address the underlying determinants of urban health that are not remediable by landscape redesign.",Not About Sufficiency
WHAT CAN INFORMATION-SYSTEMS DO FOR PRIMARY HEALTH-CARE - AN INTERNATIONAL PERSPECTIVE,"The reform of health information systems has been made a priority by health managers, public health specialists and technocrats. While each of these groups has promised major benefits from improvements in information systems, insufficient attention has been paid to the limitations placed upon the theoretical possibilities of information technology by the characteristics of the health system of which the information system is but a part. Managers anticipate improved efficiency and rational allocation of resources, but rational decision making does not automatically follow from improvements in information. Epidemiologists and public health specialists seek more effective and equitable health systems but methodological problems and the expense of many conventional epidemiological approaches continue to limit the usefulness of disease surveillence, programme monitoring and evaluation. Both managers and epidemiologists are confronted with the conflicts which arise in seeking to create locally sensitive information systems within centralised health systems. Technocrats see microcomputers as essential for information systems to be truly effective and as a means of liberating health workers from the drudgery of form filling. However, the rate of organisational evolution in the health system has not kept pace with the rapid development of information technology. There are good prospects for considerable health pin to be wrought from reforms in health information systems but to realise these it is necessary that this process be 'action-led' rather than, as is conventional, 'data-led'. The latter approach sees data as the end in itself; the 'action-led' approach, in contrast, regards information as needs to interventions with a focus on how information will influence decisions. For improvements in information to result in improved health, strategies must be adopted which will ensure that information routinely informs decisions and is seen as a means to the end of improving health.",Not About Sufficiency
Exploration of the oxidation and ablation resistance of ultra-high-temperature ceramic coatings using machine learning,"Carbon fiber-reinforced carbon matrix composites (C/C) will be easily oxidized in high temperatures, which will have a great negative effect on their performance. Preparing ultra-high-temperature ceramic (UHTC) coatings is a well-established method to improve the oxidation and ablation resistance of C/C. However, it is time-consuming and costly to obtain these coatings through the traditional experimental method. Motivated by the outstanding performance of machine learning (ML) algorithms in many fields, this study adopts ML algorithms based on historical experimental datasets to build a model. This model will predict the oxidation and ablation resistance, represented by mass ablation rate. For this purpose, variables that affect the mass ablation rate and are easily accessible were used as input features. That includes the chemical composition and essential physics/ chemistry properties of coatings and experimental parameters. Seven different ML algorithms were used to establish the model; namely, ridge regression (Ridge), lasso regression (Lasso), kernel ridge regression (KRR), support vector regression (SVR), random forest regression (RFR), AdaBoost regression (ABR), and bagging regression (Bagging). The results show that RFR has the optimal generalization performance with a mean ab-solute error (MAE) of 0.55, mean-squared error (MSE) of 0.71 and coefficient of determination (R2) of 0.87 on the testing set. SHapley Additive exPlanations (SHAP) analysis of the RFR model explained how these input features affect the mass ablation rate and further provided the critical features for performance prediction. The model established in this study can predict coating performance accurately and accelerate the development of UHTC-coated C/C composites from a data-driven perspective.",Not About Sufficiency
Objective and perceived service accessibility and mental health in older adults,"ObjectivesService accessibility plays a pivotal role in older adults' mental health. However, accessibility measures used in previous studies are either objective or perceived. This study aimed to integrate both objective and perceived measures of service accessibility to explore the relationship between environmental cognition on service accessibility and mental health in older adults and the pathways.MethodsWe used both questionnaire data collected from 2,317 older adults in Hong Kong and geographical data to explore the direct and indirect effect of environmental cognition (i.e. positive, negative, and matching evaluation) relating to service accessibility on mental health and two pathways (i.e. physical activity and sense of belonging) based on a structural equation model.ResultsPhysical activity mediated the positive relationship between non-negative perceptions toward access to convenience stores, leisure facilities, clinics, community centers, places of worship and mental health. Sense of community can significantly mediate the positive relationships between non-negative perceptions toward all 10 types of services and mental health.ConclusionThis study provides an empirical contribution to environmental cognition theory and person-environment fit theory; its findings have implications for urban planning policy. The findings from this study provide significant evidence that environmental cognition distortion, especially negative perception, can be significantly associated with lower mental health through physical activity and a sense of community. This suggests that policies focused on changing environmental cognitions could be a promising public health strategy. Environmental cognition theory suggests that improving awareness of setting could help improve the precision of cognitive mapping of environmental reality. This can be very important where it is difficult to change the objective environment due to the deep-rooted and long-standing urban structure.",Not About Sufficiency
Protection against failure of machine-learning-based QoT prediction,"Machine learning (ML)-based methods are being widely explored to predict the quality of transmission (QoT) of a lightpath. They are expected to reduce the signal-to-noise ratio margin reserved for the lightpath, thus improving the spectrum efficiency of an optical network. However, many studies on this prediction are often based on synthetic datasets or datasets obtained from laboratories. As such, these datasets may not accurately represent the entire state space of a practical optical network, which is exposed in harsh environments. There are risks of failure when using these ML-based QoT prediction models. Thus, it is necessary to develop a mechanism that can guarantee the reliability of lightpath service even if the prediction model fails. For this scenario, we propose to employ the conventional network protection techniques that are usually implemented to protect against network node/link failures to also protect against the failure of QoT prediction. Based on the two representative protection techniques, i.e., 1 + 1 dedicated path protection and shared backup path protection (SBPP), the performance of the proposed protection mechanism is evaluated by reserving different margins for the working and protection lightpaths. For 1 + 1 path protection, we find that the proposed mechanism can achieve a zero design margin (D-margin) for a working lightpath, thereby significantly improving network spectrum efficiency, while not sacrificing the availability of lightpath services. For SBPP, we find that an optimal D-margin should be identified to balance the spectrum efficiency and service availability; the proposed mechanism can save up to 0.5-dB D-margin for a working lightpath, while guaranteeing service availability. (C) 2022 Optica Publishing Group",Not About Sufficiency
Estimating city-wide hourly bicycle flow using a hybrid LSTM MDN,"Cycling can reduce greenhouse gas emissions and air pollution and increase public health. Hence, policymakers in cities worldwide seek to improve bicycle mode shares. Efforts to increase the bicycle's mode share involve many measures, one of them being the improvement of cycling safety often requiring an analysis of the factors surrounding accidents. However, meaningful analysis of cycling safety requires accurate bicycle flow data that are generally sparse or only available at the aggregate level. Therefore, safety engineers often rely on aggregated variables or calibration factors that fail to account for variations in the cycling traffic relevant to policymaking.This paper illustrates how machine learning can support policy analysis by delivering detailed bicycle flow predictions. The illustration applies a Deep Learning approach, the Long Short-Term Memory Mixture Density Network (LSTMMDN), to estimate hourly bicycle flow in Copenhagen, conditional on weather, temporal and road conditions at the segment level. The method addresses some shortcomings in the calibration factor method resulting in 66-77% more accurate bicycle traffic estimates.To quantify the impact of more accurate bicycle traffic estimates in cycling safety analysis, we test the effect of different flow estimates in a bicycle crash risk model, i.e. the models are identical except for the exposure variables. One model is estimated using the LSTMMDN estimates, one using the calibration-based estimates, and one using yearly mean traffic estimates. The results show that investing in more advanced methods for obtaining bicycle volume estimates can improve the quality of safety analyses and other performance measures.",Not About Sufficiency
Application of an Integrated Model for Analyzing Street Greenery through Image Semantic Segmentation and Accessibility: A Case Study of Nanjing City,"Urban street greening, a key component of urban green spaces, significantly impacts residents' physical and mental well-being, contributing substantially to the overall quality and welfare of urban environments. This paper presents a novel framework that integrates street greenery with accessibility, enabling a detailed evaluation of the daily street-level greenery visible to residents. This pioneering approach introduces a new measurement methodology to quantify the quality of urban street greening, providing robust empirical evidence to support its enhancement. This study delves into Nanjing's five districts, employing advanced image semantic segmentation based on machine learning techniques to segment and extract green vegetation from Baidu Street View (BSV) images. Leveraging spatial syntax, it analyzes street network data sourced from OpenStreetMap (OSM) to quantify the accessibility values of individual streets. Subsequent overlay analyses uncover areas characterized by high accessibility but inadequate street greening, underscoring the pressing need for street greening enhancements in highly accessible zones, thereby providing valuable decision-making support for urban planners. Key findings revealed that (1) the green view index (GVI) of sampled points within the study area ranged from 15.79% to 38.17%, with notably better street greening conditions observed in the Xuanwu District; (2) the Yuhua District exhibited comparatively lower pedestrian and commuting accessibility than the Xuanwu District; and (3) approximately 139.62 km of roads in the study area demonstrated good accessibility but lacked sufficient greenery visibility, necessitating immediate improvements in their green landscapes. This research utilizes the potential of novel data and methodologies, along with their practical applications in planning and design practices. Notably, this study integrates street greenery visibility with accessibility to explore, from a human-centered perspective, the tangible benefits of green landscapes. These insights highlight the opportunity for local governments to advance urban planning and design by implementing more human-centered green space policies, ultimately promoting societal equity.",Not About Sufficiency
Development of an engineering-friendly evaluation model for solar spectral irradiance using readily accessible subaerial meteorology,"Solar spectral irradiance has a crucial impact on building energy conservation, especially on photovoltaic (PV) generation. However, it takes a high cost to measure and predict the dynamic solar spectral irradiance for various atmosphere conditions and sun positions. Combining with machine learning, this paper developed a novel solar spectral irradiance estimation model to evaluate the annual solar spectral property in a region. This paper employs the readily accessible subaerial meteorology as model input. The average photon energy (APE) serves as a connection between the normalized solar spectral irradiance and the meteorology parameters. Verification showed the model this paper proposed estimated the normalized solar spectral irradiance well. Further, annual simulation of solar spectral irradiance was conducted by inputting typical meteorology year (TMY) dataset. The annual difference of the normalized spectral irradiance reached to 10.57 %, which reflects the great importance to determine the practical solar spectral irradiance. A typical day of spectra was proposed for each month to reveal the monthly variation in solar spectral irradiance. This study provides a convenient technical method to evaluate the solar spectral property for engineering applications. The results may guide industries in selecting suitable solar cells for the region, thereby prompting the development of solar applications.",Not About Sufficiency
Combining economic and social goals in the design of production systems by using ergonomics standards,"In designing of production systems, economic and social goals can be combined, if ergonomics is integrated into the design process. More than 50 years of ergonomics research and practice have resulted in a large number of ergonomics standards for designing physical and organizational work environments. This paper gives an overview of the 174 international ergonomics standards from the International Organization for Standardization (ISO) and European ergonomics standards from the Comite Europeen de Normalisation (CEN) standards in this field, and discusses their applicability in design processes. The available standards include general recommendations for integrating ergonomics into the design process, as well as specific requirements for manual handling, mental load, task design, human-computer-interaction, noise, heat, body measurements, and other topics. The standards can be used in different phases of the design process: allocation of system functions between humans and machines, design of the work organization, work tasks and jobs, design of work environment, design of work equipment, hardware and software, and design of workspace and workstation. The paper is meant to inform engineers and managers involved in the design of production systems about the existence of a large number of ISO and CEN standards on ergonomics, which can be used to optimize human well-being and overall system performance. (C) 2004 Elsevier Ltd. All rights reserved.",Not About Sufficiency
THE STALEMATE IN CHINA'S GPA ACCESSION NEGOTIATIONS-IS THERE A WAY OUT?,"Why have China's negotiations to accede to the Government Procurement Agreement (hereinafter ""GPA"") dragged on for almost two decades since China's commitment to join as part of its World Trade Organization (WTO) accession? This is puzzling given the GPA 's estimated market coverage of $1.7 trillion annually, and China's government procurement market estimated at over $280 billion annually. On the domestic front, China has improved its government procurement regime significantly in recent years. On the international front, China tabled a seventh offer in October 2019, after a period of inaction since its last revised offer at the end of 2014. In this article, we analyze the factors that led to the stalemate in the negotiations, and the consequent five-year gap between China 's sixth and the seventh offers. We discuss a number of problematic issues for various parties, some relevant to any state in the process of acceding to the GPA, others, specific to China. We argue that rationalist calculations on the part of China and its trading partners have led to the prolonged negotiations. In a wider context, we suggest that inconclusive negotiations would have troublesome implications for the international economic legal order.",Not About Sufficiency
Measuring Toxic Stress in Childhood and Youth: A Systematic Review,"Introduction: Toxic stress among children/youth is a significant public health concern that has been linked to serious morbidities and premature mortality in adulthood. Uniformity in measurement of a toxic stress variable for use in scholarly research can aid in a more comprehensive understanding of its indicators and implications. Method: The purpose of this systematic review was to aggregate, organize, and summarize the literature around current practices for operationalizing toxic stress among children/youth. A strategic methodology was conducted according to PRISMA guidelines, and two databases were consulted for relevant studies. Results: Over 30 different measurement tools were identified across 13 included studies, ranging from biomarkers and physiological indicators to scales, diagnoses, and other assessment instruments. Discussion: By synthesizing current measurement methods, this review informs research and clinical communities of different approaches to toxic stress measurement, advances standardization efforts, and justifies the necessity of a singular, scientifically validated tool. J Pediatr Health Care. (2024) 38 , 836-849",Not About Sufficiency
Accessibility and interoperability in e-government systems: outlining an inclusive development process,"The multidisciplinary nature of e-government demands a research agenda that includes issues related to social inclusion, universal accessibility, interoperability, privacy, security, and citizen participation, to name a few. Understanding the underlying cultural context, the involvement of citizens in the proposal and evaluation of services, and the promotion of quality in use are aspects that need special consideration in the development of systems to support government. This paper provides an outline for a process model for promoting the identification and specification of accessible e-government services with the participation of the interested parties. A socially shared perspective is adopted toward the comprehension of the involved problems and the elaboration of potential solutions. The proposed model is a result of practice in the domain, using organizational semiotics artifacts to stimulate participation and discussion.",Not About Sufficiency
Residents' self-perceived health and its relationships with urban neighborhood green infrastructure,"Research has established that being in green elements in the landscape affect health and well-being. This paper presents the findings whether neighborhood green infrastructure (GI) in a community's living environment is an underlying mechanism for urban residents' self-perceived health. This study seeks residents' participations through their responses. We investigated (a) residents' inputs in the ways they use and perceive GI near their homes, and (b) the residents' self-perceived health based on their reported status of health. When an association between the GI and self-perceived health, we analyzed whether this could explain the relationship between the two parameters. The results are important in addressing the relationship between GI with respondents' health status. 650 residents living in Bandar Tun Razak town responded to the survey questionnaires. Bandar Tun Razak is one of the established towns within the periphery of Kuala Lumpur. The analyses suggest that residents frequently spent time in Taman Tasik Permaisuri (a recreational park), their home gardens and open spaces in their neighborhoods. The research also found that there is an association between GI with their self-perceived health. This finding can be translated into policy on health promotion in Malaysia through landscape design and planning of urban green spaces. The study is also relevant to multidisciplinary fields of study such as urban planning and public health promotion. (C) The Authors. Published by Elsevier B.V.",Not About Sufficiency
Distributed Learning Over Networks With Graph-Attention-Based Personalization,"In conventional distributed learning over a network, multiple agents collaboratively build a common machine learning model. However, due to the underlying non-i.i.d. data distribution among agents, the unified learning model becomes inefficient for each agent to process its locally accessible data. To address this problem, we propose a graph-attention-based personalized training algorithm (GATTA) for distributed deep learning. The GATTA enables each agent to train its local personalized model while exploiting its correlation with neighboring nodes and utilizing their useful information for aggregation. In particular, the personalized model in each agent is composed of a global part and a node-specific part. By treating each agent as one node in a graph and the node-specific parameters as its features, the benefits of the graph attention mechanism can be inherited. Namely, instead of aggregation based on averaging, it learns the specific weights for different neighboring nodes without requiring prior knowledge about the graph structure or the neighboring nodes' data distribution. Furthermore, relying on the weight-learning procedure, we develop a communication-efficient GATTA by skipping the transmission of information with small aggregation weights. Additionally, we theoretically analyze the convergence properties of GATTA for non-convex loss functions. Numerical results validate the excellent performances of the proposed algorithms in terms of convergence and communication cost.",Not About Sufficiency
STUDY ON THE RELATIONSHIP OF SPATIO-TEMPORAL MATCHING BETWEEN WATER RESOURCES AND ECONOMIC DEVELOPMENT FACTORS IN THE YELLOW RIVER BASIN,"According to the statistical data of water resources of the Yellow River Basin in 2009-2013, by calculating the Gini coefficient of economic development factors of population, GDP and crops sown area with water resources, the time evolution law of matching relationship between water resources distribution, allocation and economic development factors in the Yellow River Basin was studied.The results showed that the matching relationship between water resources distribution and economic development factors in the Yellow River basin was extreme dismatched, but the degree of dismatching had slowed down. The relationship between water resources allocation and population, GDP was relative matching, and the matching degree with the area of crop planting was highly matching,moreover,the matching degree was trended to increase. On the other hand,by drawing the Lorenz curve of water resources and economic development factors of the 68 municipal administrative regions in Yellow River Basin,the paper analyzed the differences of spatial distribution of water resources in the the Yellow River Basin.By analysing the difference, this paper puts forward some principles of rational allocation of water resources in the Yellow River Basin and some suggestions to alleviate the shortage of urban water resources.",Not About Sufficiency
Pyramid: Enhancing Selectivity in Big Data Protection with Count Featurization,"Protecting vast quantities of data poses a daunting challenge for the growing number of organizations that collect, stockpile, and monetize it. The ability to distinguish data that is actually needed from data collected ""just in case"" would help these organizations to limit the latter's exposure to attack. A natural approach might be to monitor data use and retain only the working-set of in-use data in accessible storage; unused data can be evicted to a highly protected store. However, many of today's big data applications rely on machine learning (ML) workloads that are periodically retrained by accessing, and thus exposing to attack, the entire data store. Training set minimization methods, such as count featurization, are often used to limit the data needed to train ML workloads to improve performance or scalability. We present Pyramid, a limited-exposure data management system that builds upon count featurization to enhance data protection. As such, Pyramid uniquely introduces both the idea and proof-of-concept for leveraging training set minimization methods to instill rigor and selectivity into big data management. We integrated Pyramid into Spark Velox, a framework for ML-based targeting and personalization. We evaluate it on three applications and show that Pyramid approaches state-of-the-art models while training on less than 1% of the raw data.",Not About Sufficiency
Truck traffic related air pollution associated with asthma symptoms in young boys: a cross-sectional study,"Objectives: The aim of this study was to evaluate the influence of intensity of truck traffic on asthma symptomatology, and its relationship with age and gender. Study design: A cross-sectional study was conducted on children and adolescents from Galicia (North-West Spain). Methods: Following the methodology of the International Study of Asthma and Allergies in Childhood (ISAAC): children from schools randomly selected, answered a self-administered questionnaire included questions on asthma symptoms and some risk factors. The association between self-reported truck traffic on the street of residence and symptoms of asthma were investigated by logistic regression adjusted for body mass index, maternal education and parental smoking. Results: Almost 40% of children in Galicia are exposed to the frequent and constant passing of heavy goods vehicles. The odds of 6-7 year-old boys having severe or exercise induced asthma is tripled when they live in streets with the constant passing of heavy goods vehicles, compared with those living in streets where these vehicles never pass. In adolescents and 6-7 year-old girls, no relationship was observed between truck traffic and asthma symptoms. Conclusions: The results of this study appear to support a distinct effect of truck traffic on asthma symptoms depending on the age and sex of the exposed population, being more harmful for young males. (C) 2013 The Royal Society for Public Health. Published by Elsevier Ltd. All rights reserved.",Not About Sufficiency
Territorial Cohesion as a Policy Narrative: From Economic Competitiveness to 'Smart' Growth and Beyond,"During the last two decades, a lot of ink has been spent in favour of narrative analysis of policy. According to such approaches, policy processes are influenced by narratives that are spread around specific 'issues' and lead to their solutions. Following a similar vein, this article examines territorial cohesion as a policy narrative and how it can be perceived as a narrative constituted by a diverse narrative structure. Territorial cohesion is a dynamic narrative that changes through time. As time goes by and different politico-economic philosophies get more influential, technological changes also bring along different priorities, broader EU narratives change, and territorial cohesion adapts to such changes. Accordingly, within the post-2014 framework (2014-2020), territorial cohesion's (spatialised) social inclusion perspective was subdued to the economic competitiveness sub-narrative in a globalised world. For the new programming period (2021-2027), the European Cohesion Policy will continue to be increasingly linked to the place-based narrative and most of its funding will be directed towards a 'smarter' and 'greener' Europe within a global space of flows and fast technological changes. The aim of a 'smarter' Europe based on digital transformation and smart growth is a new version of the economic competitiveness sub-narrative, while a 'greener' Europe is the new policy meta-imperative (""European Green Deal""). However, it must be considered how the Coronavirus crisis and the measures to fight its economic effects play out on these policy narratives.",Not About Sufficiency
Proteome analysis of the prefrontal cortex and the application of machine learning models for the identification of potential biomarkers related to suicide,"Introduction Suicide is a significant public health problem, with increased rates in low- and middle-income countries such as Mexico; therefore, suicide prevention is important. Suicide is a complex and multifactorial phenomenon in which biological and social factors are involved. Several studies on the biological mechanisms of suicide have analyzed the proteome of the dorsolateral prefrontal cortex (DLPFC) in people who have died by suicide. The aim of this work was to analyze the protein expression profile in the DLPFC of individuals who died by suicide in comparison to age-matched controls in order to gain information on the molecular basis in the brain of these individuals and the selection of potential biomarkers for the identification of individuals at risk of suicide. In addition, this information was analyzed using machine learning (ML) algorithms to propose a model for predicting suicide.Methods Brain tissue (Brodmann area 9) was sampled from male cases (n=9) and age-matched controls (n=7). We analyzed the proteomic differences between the groups using two-dimensional polyacrylamide gel electrophoresis and mass spectrometry. Bioinformatics tools were used to clarify the biological relevance of the differentially expressed proteins. In addition, this information was analyzed using machine learning (ML) algorithms to propose a model for predicting suicide.Results Twelve differentially expressed proteins were also identified (t 14 <= 0.5). Using Western blotting, we validated the decrease in expression of peroxiredoxin 2 and alpha-internexin in the suicide cases. ML models were trained using densitometry data from the 2D gel images of each selected protein and the models could differentiate between both groups (control and suicide cases).Discussion Our exploratory pathway analysis highlighted oxidative stress responses and neurodevelopmental pathways as key processes perturbed in the DLPFC of suicides. Regarding ML models, KNeighborsClassifier was the best predicting conditions. Here we show that these proteins of the DLPFC may help to identify brain processes associated with suicide and they could be validated as potential biomarkers of this outcome.",Not About Sufficiency
Understanding the accountability issues of the immunization workforce for the Expanded Program on Immunization (EPI) in Balochistan: An exploratory study,"Background Among all provinces of Pakistan, immunization coverage is poorest in Balochistan. There is no provincial immunization policy for Balochistan including a lack of human resource management policy. Maladministration and lack of accountability leading to health workforce demotivation and poor performance can be a crucial reason behind an inefficient and ineffective immunization program in Balochistan. The objective of this study was to better understand the accountability issues of EPI workforce at provincial and district level leading to poor program performance and to identify governance strategies for management of inefficiency, demotivation and absenteeism. Methods An exploratory qualitative study was carried out to explore issues related to human resource (HR) accountability within immunization program of Balochistan for developing strategies to improve performance of the program. Five districts were selected using purposive sampling based on the comparative poor and good routine immunization coverages and Human Development Index (HDI). Interviews were conducted with EPI Staff and District Health Officers (DHOs) in each district including provincial EPI Staff. A semi-structured and open-ended questionnaire was used. Thematic analysis was used to analyze the qualitative data. Results Major barriers to HR accountability included lack of a written HR policy, proper service structure including promotions and benefits and understanding of accurate job description coupled with inadequate HR development budget and activities. Most important demotivating factors were inadequate number of vaccinators, deficient budget with delayed wage and salary disbursements resulting in poor immunization coverage and a lack of appreciation/feedback from senior management for the frontline workers. Key challenge for vaccinators was poor community orientation and mobilization. Although, the participants proposed some solutions based on their perspective, none were elaborate or precise. Conclusions Adaptation of National Immunization Policy tailored to the provincial context and proper implementation is much needed. Review of current allocations of vaccinators and need based relocation along with recruitment of new vaccinators with clear job description and terms of reference is desirable. Review of current incentive structure is required. Finally, trust building between community and the vaccination program and social mobilization about the benefits of vaccinations through community influential is vital.",Not About Sufficiency
EU Development Strategy and Legal Control of Automated Vehicles,"Automated vehicles (AVs) have been an emerging industry in line with the development of artificial intelligence (AI), digital technology, and connected mobility since 2010. These technologies arrived suddenly on the market, and their future development is expected to accelerate. AVs promise many opportunities and benefits, but also pose many challenges regarding their social impact and the transport system. Regulation plays a key role in the development of AVs in the EU, ensuring AV safety, delineating legal responsibilities, privacy and consumer protection, risk control, fair competition, market access, public confidence, etc. The EU has paid a great deal of attention to AV because the automobile industry is crucially important to the European economy. This paper examines the development strategy and legal framework for AV in the EU, particularly focusing on the following core issues: the concept of AV, development, strategy, safety requirements, opportunities and challenges of the AV, the formation of Intelligent Transport System, the specific features of the EU's AV regulation and policies, the significance and impact of the same, and any implications for Taiwan.",Not About Sufficiency
Artificial intelligence and machine learning in mobile apps for mental health: A scoping review,"Mental health conditions can have significant negative impacts on wellbeing and healthcare systems. Despite their high prevalence worldwide, there is still insufficient recognition and accessible treatments. Many mobile apps are available to the general population that aim to support mental health needs; however, there is limited evidence of their effectiveness. Mobile apps for mental health are beginning to incorporate artificial intelligence and there is a need for an overview of the state of the literature on these apps. The purpose of this scoping review is to provide an overview of the current research landscape and knowledge gaps regarding the use of artificial intelligence in mobile health apps for mental health. The Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews (PRISMA-ScR) and Population, Intervention, Comparator, Outcome, and Study types (PICOS) frameworks were used to structure the review and the search. PubMed was systematically searched for randomised controlled trials and cohort studies published in English since 2014 that evaluate artificial intelligence- or machine learning-enabled mobile apps for mental health support. Two reviewers collaboratively screened references (MMI and EM), selected studies for inclusion based on the eligibility criteria and extracted the data (MMI and CL), which were synthesised in a descriptive analysis. 1,022 studies were identified in the initial search and 4 were included in the final review. The mobile apps investigated incorporated different artificial intelligence and machine learning techniques for a variety of purposes (risk prediction, classification, and personalisation) and aimed to address a wide range of mental health needs (depression, stress, and suicide risk). The studies' characteristics also varied in terms of methods, sample size, and study duration. Overall, the studies demonstrated the feasibility of using artificial intelligence to support mental health apps, but the early stages of the research and weaknesses in the study designs highlight the need for more research into artificial intelligence- and machine learning-enabled mental health apps and stronger evidence of their effectiveness. This research is essential and urgent, consid- ering the easy availability of these apps to a large population.",Not About Sufficiency
Using national environmental objectives in green public procurement: Method development and application on transport procurement in Sweden,"Green public procurement can play an important role in reducing the environmental impact of societies. While its uptake is continuously growing, barriers to its use still remain. One barrier previously identified in literature is related to the lack of accessible and easy to use tools that help standardize the development of criteria in green tenders. In this paper, to help overcome this barrier, a method is presented that can be used to develop green public procurement tools that follow previous studies recommendations about including life-cycle assessment-based data and basing procurement criteria on national environmental objectives. The method was then applied to develop a procurement tool for green procurement of public transport services in Sweden based on the Swedish environmental quality objectives. Results from the assessment of 18 pre-defined fuel systems are shown together with an illustrative example of how the tool can be used in the process leading up to procurements to set relevant criteria and during the procurement to adjust incoming tender prices. The results showed that waste-based biomethane and hydrogenated vegetable oil systems were well aligned with the Swedish environmental quality objectives due to being able to contribute positively to several objectives. Crop-based biofuels, on the other hand, performed worse due to negative effects from agricultural practices. The performance of the electric vehicle systems depended in large on how the electricity was generated, where renewable sources and low carbon sources performed better than non-renewable alternatives. (C) 2020 The Author(s). Published by Elsevier Ltd.",Not About Sufficiency
Distribution-invariant differential privacy,"Differential privacy is becoming one gold standard for protecting the privacy of publicly shared data. It has been widely used in social science, data science, public health, information technology, and the U.S. decennial census. Nevertheless, to guarantee differential privacy, existing methods may unavoidably alter the conclusion of original data analysis, as privatization often changes the sample distribution. This phenomenon is known as the trade-off between privacy protection and statistical accuracy. In this work, we mitigate this trade-off by developing a distribution-invariant privatization (DIP) method to reconcile both high statistical accuracy and strict differential privacy. As a result, any downstream statistical or machine learning task yields essentially the same conclusion as if one used the original data. Numerically, under the same strictness of privacy protection, DIP achieves superior statistical accuracy in in a wide range of simulation studies and real-world benchmarks.& COPY; 2022 Elsevier B.V. All rights reserved.",Not About Sufficiency
Preparing future physicians for complexity: a post-graduate elective in HIV psychiatry,"BackgroundPatients with complex care needs have multiple concurrent conditions (medical, psychiatric, social vulnerability or functional impairment), interfering with achieving desired health outcomes. Their care often requires coordination and integration of services across hospital and community settings. Physicians feel ill-equipped and unsupported to navigate uncertainty and ambiguity caused by multiple problems. A HIV Psychiatry resident elective was designed to support acquisition of integrated competencies to navigate uncertainty and disjointed systems of care - necessary for complex patient care.MethodsThrough qualitative thematic analysis of pre- and post-interviews with 12 participants - residents and clinic staff - from December 2019 to September 2022, we explored experiences of this elective.ResultsThis educational experience helped trainees expand their understanding of what makes patients complex. Teachers and trainees emphasize the importance of an approach to ""not knowing"" and utilizing integrative competencies for navigating uncertainty. Through perspective exchange and collaboration, trainees showed evidence of adaptive expertise: the ability to improvise while drawing on past knowledge.ConclusionsPostgraduate training experiences should be designed to facilitate skills for caring for complex patients. These skills help residents fill in practice gaps, improvise when standardization fails, and develop adaptive expertise. Going forward, findings will be used to inform this ongoing elective.",Not About Sufficiency
Predicting Protein-Protein Interactions via Random Ferns with Evolutionary Matrix Representation,"Protein-protein interactions (PPIs) play a crucial role in understanding disease pathogenesis, genetic mechanisms, guiding drug design, and other biochemical processes, thus, the identification of PPIs is of great importance. With the rapid development of high-throughput sequencing technology, a large amount of PPIs sequence data has been accumulated. Researchers have designed many experimental methods to detect PPIs by using these sequence data, hence, the prediction of PPIs has become a research hotspot in proteomics. However, since traditional experimental methods are both time-consuming and costly, it is difficult to analyze and predict the massive amount of PPI data quickly and accurately. To address these issues, many computational systems employing machine learning knowledge were widely applied to PPIs prediction, thereby improving the overall recognition rate. In this paper, a novel and efficient computational technology is presented to implement a protein interaction prediction system using only protein sequence information. First, the Position-Specific Iterated Basic Local Alignment Search Tool (PSI-BLAST) was employed to generate a position-specific scoring matrix (PSSM) containing protein evolutionary information from the initial protein sequence. Second, we used a novel data processing feature representation scheme, MatFLDA, to extract the essential information of PSSM for protein sequences and obtained five training and five testing datasets by adopting a five-fold cross-validation method. Finally, the random fern (RFs) classifier was employed to infer the interactions among proteins, and a model called MatFLDA_RFs was developed. The proposed MatFLDA_RFs model achieved good prediction performance with 95.03% average accuracy on Yeast dataset and 85.35% average accuracy on H. pylori dataset, which effectively outperformed other existing computational methods. The experimental results indicate that the proposed method is capable of yielding better prediction results of PPIs, which provides an effective tool for the detection of new PPIs and the in-depth study of proteomics. Finally, we also developed a web server for the proposed model to predict protein-protein interactions, which is freely accessible online at http://120.77.11.78:5001/webserver/MatFLDA_RFs.",Not About Sufficiency
"Human rights, development, and the WTO: The cases of intellectual property and competition policy","The World Trade Organization (WTO) has sometimes been portrayed as being at odds with the protection of human rights. This article takes issue with this perception, both generally and with specific reference to WTO agreements/activities in the areas of intellectual property (IP) and competition policy. The rules and procedures of the WTO are directly supportive of civil rights in the sense of freedom to participate in markets and freedom from arbitrary governmental procedures. In addition, the system contributes to development and to the realization of broader economic, social, and cultural rights, by stimulating economic growth and thereby helping to generate the resources that are needed for the fulfilment of such rights. The article examines various human rights and public interest rationales for the protection of intellectual property rights (IPRs). The recent amendment to the Agreement on Trade-Related Aspects of Intellectual Property Rights (TRIPS) to facilitate access to medicines in the event of public health emergencies is outlined. With respect to competition policy, such policy constitutes an important aspect of governance in successful market-based economies. There is a clear need for cooperative approaches to the implementation of national competition policies. The appropriate scope and venue for such cooperation are a matter for further deliberation.",Not About Sufficiency
Utilization of Machine Learning for the Detection of Self-admitted Vulnerabilities,"Motivation: Technical debt is a metaphor that describes not-quite-right code introduced for short-term needs. Developers are aware of it and admit it in source code comments, which is called Self-Admitted Technical Debt (SATD). Therefore, SATD indicates weak code that developers are aware of. Problem statement: Inspecting source code is time-consuming; automatically inspecting source code for its vulnerabilities is a crucial aspect of developing software. It helps practitioners reduce the time-consuming process and focus on vulnerable aspects of the source code. Proposal: Accurately identify and better understand the semantics of self-admitted technical debt (SATD) by leveraging NLP and NL-PL approaches to detect vulnerabilities and the related SATD. Finally, a CI/CD pipeline will be proposed to make the vulnerability discovery process easily accessible to practitioners.",Not About Sufficiency
Recent advances on artificial intelligence-based approaches for food adulteration and fraud detection in the food industry: Challenges and opportunities,"Food adulteration is the deceitful practice of misleading consumers about food to profit from it. The threat to public health and food quality or nutritional valuable make it a major issue. Food origin and adulteration should be considered to safeguard customers against fraud. It has been established that artificial intelligence is a cuttingedge technology in food science and engineering. In this study, it has been explained how AI detects food tampering. Applications of AI such as machine learning tools in food quality have been studied. This review covered several food quality detection web-based information sources. The methods used to detect food adulteration and food quality standards have been highlighted. Various comparisons between state-of-the-art techniques, datasets, and outcomes have been conducted. The outcomes of this investigation will assist researchers choose the best food quality method. It will help them identify of foods that have been explored by researchers and potential research avenues.",Not About Sufficiency
"Determinants of COVID-19 vaccine hesitancy among students and parents in Sentinel Schools Network of Catalonia, Spain","Vaccine hesitancy is defined as a delay in acceptance of vaccines despite its availability, caused by many determinants. Our study presents the key reasons, determinants and characteristics associated with COVID-19 vaccine acceptability among students over 16 years and parents of students under 16 years and describe the COVID-19 vaccination among students in the settings of sentinel schools of Catalonia, Spain. This is a cross-sectional study that includes 3,383 students and the parents between October 2021 and January 2022. We describe the student's vaccination status and proceed a univariate and multivariate analysis using a Deletion Substitution Addition (DSA) machine learning algorithm. Vaccination against COVID-19 reached 70.8% in students under 16 years and 95.8% in students over 16 years at the end of the study project. The acceptability among unvaccinated students was 40.9% and 20.8% in October and January, respectively, and among parents was proportionally higher among students aged 5-11 (70.2%) in October and aged 3-4 (47.8%) in January. The key reason to not vaccinate themselves, or their children, were concern about side effects, insufficient research about the effect of the vaccine in children, rapid development of vaccines, necessity for more information and previous infection by SARS-CoV-2. Several variables were associated with refusal end hesitancy. For students, the main ones were risk perception and use of alternative therapies. For parents, the age of students, sociodemographic variables, socioeconomic impact related to the pandemic, and use of alternative therapies were more evident. Monitoring vaccine acceptance and refusal among children and their parents has been important to understand the interaction between different multilevel determinants and we hope it will be useful to improve public health strategies for future interventions in this population.",Not About Sufficiency
House relocation: A redevelopment tool for rapidly changing urban environments,"House relocation is a circular reuse approach with potential to address multi-pronged community challenges in rapidly redeveloping cities. Historically used throughout the USA, Austin, Texas serves as the testing ground for this contemporary research on house relocation, exemplifying a urban context experiencing a range of challenges, including rampant demolition, housing shortages, exponential population increase, and unmet zero waste goals. Researchers took a mixed-methods approach, including literature review, interviews, and permit record analysis to assess the use of house relocation as a tool to address these urban challenges, while identifying associated community benefits, barriers, and applications. Written in parallel, the academic research also informed an educational guide for the development community and broader public audience released in 2023.",Not About Sufficiency
PRESERVATION OF THE ARCHAEOLOGICAL HERITAGE OF THE NORTH SEA USING WEBGIS,"The submerged coastal heritage forms an important aspect of our cultural and archaeological patrimony and offers huge possibilities for scientific and (inter-) cultural purposes. Since the knowledge of this heritage is limited, gathering more data is one of the main objectives of the multidisciplinary project SeArch (Archaeological heritage in the North Sea, www.sea-arch.be). Additionally, these data will be used as a basis for a sustainable management by government agencies. To share, integrate and visualize the gathered archaeological and environmental data and information in a user-friendly way, an interactive web-based Geographical Information System/Service is created. By implementing this webGIS, the benefits of standard map reading (such as providing insight) are combined with facilities such as easy accessible spatial analysis and feature querying in an interactive environment, which is accessible worldwide. To create an interactive webGIS-platform, a good structured spatial database is needed. This paper provides more information about the configuration of the spatial database and the application of the software package GeoServer. The development of a fully functional spatial data infrastructure (SDI) using the most novel, reliable and powerful technological components is described. The objective of this SDI is to increase the accessibility and interoperability of spatial data for a wide range of users.",Not About Sufficiency
Increasing fatigue performance in AHSS thick sheet by surface treatments,"Advanced High Strength Steels (AHSS) have been widely applied in the automotive industry as an affordable solution for car lightweighting, mainly in parts subjected to crash requirements. Heavy duty vehicle (HDV) can also benefit from the expertise learned in cars, but parts must be designed considering fatigue resistance, especially on trimmed areas, and stiffness. Mechanical surface treatments, as blasting or shot peening, help increasing fatigue life of AHSS in trimmed areas and will allow weight reduction in HDV through gauge downsizing. The expected decrease in stiffness through thickness reduction can be improved by design changes. However, scarce information about the effect of mechanical surface treatments on AHSS are available. Thus, the aim of this work is to evaluate the increment in fatigue life of two different steel grades (350 MPa, and 500MPa of yield strength) in thick sheet by means of mechanical surface treatment - sandblasting. High Cycle Fatigue [HCF] tests were conducted at alternating load [R=-1]. Residual stresses were measured by an X-ray tensometry prior fatigue tests. Also the surface roughness [Rz] and form is measured using an optical non-contact 3D microscope. On the other hand, the fracture surfaces of the test specimens were observed via scanning electron microscope (SEM) in order to determine the crack initiation points. The evaluation of fatigue life in terms of SN curves is also discussed, analysing how the sandblasting process modifies the surface roughness and introduce compressive residual stresses on the external layer of the material. Both phenomena enhance the fatigue strength of the evaluated steel grades.",Not About Sufficiency
"Flood Disaster Risk Perception and Sense of Place Among Households Along the Ocoy River in Negros Oriental, Philippines","This quantitative study using a survey method aims to understand the relationship between flood disaster risk perception and the sense of place of people living in communities along a river. The survey covered a non-probability sample of 120 respondents from households located along with the downstream, midstream, and upstream sections of the Ocoy River in Negros Oriental. Generally, the respondents have very high flood disaster risk perception and sense of place scores which do not significantly differ across communities. But the significant positive relationship between these two major variables contradicts the common understanding that disaster makes people devalue particular places and relocate to safer areas. The majority who conditionally agreed to relocate may not proceed if they perceived a more difficult life in the resettlement site. Adaptive resettlement programs and policies are recommended where the desired characteristics of a place of flood survivors are reconstructed. At the same time, risk reduction and mitigation mechanisms are designed for those who decided to remain in riverside communities.",Not About Sufficiency
The impact of new technologies on human population studies,"Human population studies involve clinical or epidemiological observations that associate environmental exposures with health endpoints and disease. Clearly, these are the most sought after data to support assessments of human health risk from environmental exposures. However, the foundations of many health risk assessments rest on experimental studies in rodents performed at high doses that elicit adverse outcomes, such as organ toxicity or tumors. Using the results of human studies and animal data, risk assessors define the levels of environmental exposures that may lead to disease in a portion of the population. These decisions on potential health risks are frequently based on the use of default assumptions that reflect limitations in our scientific knowledge. An important immediate goal of toxicogenomics, including proteomics and metabonomics, is to offer the possibility of making decisions affecting public health and public based on detailed toxicity, mechanistic, and exposure data in which many of the uncertainties have been eliminated. Ultimately, these global technologies will dramatically impact the practice of public health and risk assessment as applied to environmental health protection. The impact is already being felt in the practice of toxicology where animal experimentation using highly controlled dose-time parameters is possible. It is also being seen in human population studies where understanding human genetic variation and genomic reactions to specific environmental exposures is enhancing our ability to uncover the causes of variations in human response to environmental exposures. These new disciplines hold the promise of reducing the costs and time lines associated with animal and human studies designed to assess both the toxicity of environmental pollutants and efficacy of therapeutic drugs. However, as with any new science, experience must be gained before the promise can be fulfilled. Given the numbers and diversity of drugs, chemicals and environmental agents; the various species in which they are studied and the time and dose factors that are critical to the induction of beneficial and adverse effects, it is only through the development of a profound knowledge base that toxicology and environmental health can rapidly advance. The National Institute of Environmental Health Sciences (NIEHS), National Center for Toxicogenomics and its university-based Toxicogenomics Research Consortium (TRC), and resource contracts, are engaged in the development, application and standardization of the science upon which to the build such a knowledge base on Chemical Effects in Biological Systems (CEBS). In addition, the NIEHS Environmental Genome Project (EGP) is working to systematically identify and characterize common sequence polymorphisms in many genes with suspected roles in determining chemical sensitivity. The rationale of the EGP is that certain genes have a greater than average influence over human susceptibility to environmental agents. If we identify and characterize the polymorphism in those genes, we will increase our understanding of human disease susceptibility. This knowledge can be used to protect susceptible individuals from disease and to reduce adverse exposure and environmentally induced disease. (C) 2003 Published by Elsevier B.V.",Not About Sufficiency
Ability of Procalcitonin and C-Reactive Protein for Discriminating between Bacterial and Enteroviral Meningitis in Children Using Decision Tree,"Bacterial meningitis (BM) is a public health burden in developing countries, including Central Asia. This disease is characterized by a high mortality rate and serious neurological complications. Delay with the start of adequate therapy is associated with an increase in mortality for patients with acute bacterial meningitis. Cerebrospinal fluid culture, as a gold standard in bacterial meningitis diagnosis, is time-consuming with modest sensitivity, and this is unsuitable for timely decision-making. It has been shown that bacterial meningitis differentiation from viral meningitis could be done through different parameters such as clinical signs and symptoms, laboratory values, such as PCR, including blood and cerebrospinal fluid (CSF) analysis. In this study, we proposed the method for distinguishing the bacterial form of meningitis from enteroviral one. The method is based on the machine learning process deriving making decision rules. The proposed fast-and-frugal trees (FFTree) decision tree approach showed an ability to determine procalcitonin and C-reactive protein (CRP) with cut-off values for distinguishing between bacterial and enteroviral meningitis (EVM) in children. Such a method demonstrated 100% sensitivity, 96% specificity, and 98% accuracy in the differentiation of all cases of bacterial meningitis in this study. These findings and proposed method may be useful for clinicians to facilitate the decision-making process and optimize the diagnostics of meningitis.",Not About Sufficiency
Spectral Discrimination of Common Karoo Shrub and Grass Species Using Spectroscopic Data,"Rangelands represent about 25% of the Earth's land surface but are under severe pressure. Rangeland degradation is a gradually increasing global environmental problem, resulting in temporary or permanent loss of ecosystem functions. Ecological rangeland studies aim to determine the productivity of rangelands as well as the severity of their degradation. Rigorous in situ assessments comprising visual identification of plant species are required as such assessments are perceived to be the most accurate way of monitoring rangeland degradation. However, in situ assessments are expensive and time-consuming exercises, especially when carried out over large areas. In situ assessments are also limited to areas that are accessible. This study aimed to evaluate the effectiveness of multispectral (MS) and hyperspectral (HS) remotely sensed, unmanned aerial vehicle (UAV)-based data and machine learning (random forest) methods to differentiate between 15 dominant Nama Karoo plant species to aid ecological impact surveys. The results showed that MS imagery is unsuitable, as classification accuracies were generally low (37.5%). In contrast, much higher classification accuracies (>70%) were achieved when the HS imagery was used. The narrow bands between 398 and 430 nanometres (nm) were found to be vital for discriminating between shrub and grass species. Using in situ Analytical Spectral Device (ASD) spectroscopic data, additional important wavebands between 350 and 400 nm were identified, which are not covered by either the MS or HS remotely sensed data. Using feature selection methods, 12 key wavelengths were identified for discriminating among the plant species with accuracies exceeding 90%. Reducing the dimensionality of the ASD data set to the 12 key bands increased classification accuracies from 84.8% (all bands) to 91.7% (12 bands). The methodology developed in this study can potentially be used to carry out UAV-based ecological assessments over large and inaccessible areas typical of Karoo rangelands.",Not About Sufficiency
Enhancing IoT anomaly detection performance for federated learning,"Federated Learning (FL) with mobile computing and the Internet of Things (IoT) is an effective cooperative learning approach. However, several technical challenges still need to be addressed. For instance, dividing the training process among several devices may impact the performance of Machine Learning (ML) algorithms, often significantly degrading prediction accuracy compared to centralized learning. One of the primary reasons for such performance degradation is that each device can access only a small fraction of data (that it generates), which limits the efficacy of the local ML model constructed on that device. The performance degradation could be exacerbated when the participating devices produce different classes of events, which is known as the class balance problem. Moreover, if the participating devices are of different types, each device may never observe the same types of events, which leads to the device heterogeneity problem. In this study, we investigate how data augmentation can be applied to address these challenges and improving detection performance in an anomaly detection task using IoT datasets. Our extensive experimental results with three publicly accessible IoT datasets show the performance improvement of up to 22.9% with the approach of data augmentation, compared to the baseline (without relying on data augmentation). In particular, stratified random sampling and uniform random sampling show the best improvement in detection performance with only a modest increase in computation time, whereas the data augmentation scheme using Generative Adversarial Networks is the most time-consuming with limited performance benefits.",Not About Sufficiency
Endemicity and diversification of carbapenem-resistant Acinetobacter baumannii in an intensive care unit,"Background Carbapenem-resistant Acinetobacter baumannii (CRAB) is a major public health concern globally. Often studied in the context of hospital outbreaks, little is known about the persistence and evolutionary dynamics of endemic CRAB populations.Methods A three-month cross-sectional observational study was conducted in a 28-bed intensive care unit (ICU) in Hangzhou, China. A total of 5068 samples were collected from the hospital environment (n = 3985), patients (n = 964) and staff (n = 119). CRAB isolates were obtained from 10.5% of these samples (n = 532). All of these isolates, plus an additional 19 from clinical infections, were characterised through whole-genome sequencing.Findings The ICU CRAB population was dominated by OXA-23-producing global clone 2 isolates (99.3% of all isolates) that could be divided into 20 distinct clusters, defined through genome sequencing. CRAB was persistently present in the ICU, driven by regular introductions of distinct clusters. The hospital environment was heavily contaminated, with CRAB isolated from bed units on 183/335 (54.6%) sampling occasions but from patients on only 72/299 (24.1%) occasions. CRAB was spread to adjacent bed units and rooms, and following relocation of patients within the ICU. We also observed three horizontal gene transfer events between CRAB strains in the ICU, involving three different plasmids.Interpretation The epidemiology of CRAB in this setting contrasted with previously described clonal outbreaks in high-income countries, highlighting the importance of environmental CRAB reservoirs in ICU epidemiology and the unique challenges in containing the spread of CRAB in ICUs where this important multidrug-resistant pathogen is endemic.",Not About Sufficiency
Machine Learning-Based Batch Processing for Calibration of Model and Noise Parameters,"Non-Gaussian or non-whiteness of noise sources often occurs in many digital avionics systems. Incorrect modeling of the system degrades the performance of parametric model-based estimators and controllers. To calibrate the model and noise parameters, this paper proposes a machine learning-based batch processing approach. We first mathematically formulate a state augmentation system containing three types of noise: color noise, state-dependent noise, and correlation noise. Next, we define accessible process and measurement residuals to create the training data set. Finally, we propose offline batch processing that recursively utilizes a machine learning technique to calibrate the model and noise parameters. Simulation results under various conditions validate the calibration performance of the proposed approach.",Not About Sufficiency
Multiresolution feature fusion for smart diagnosis of schizophrenia in adolescents using EEG signals,"Numerous studies on early detection of schizophrenia (SZ) have utilized all available channels or employed set of a few time domain or frequency domain features, while a limited number of features may not be sufficient enough to perform diagnosis efficiently. To encounter these problems, an automated diagnosis model is proposed for the efficient diagnosis of schizophrenia symptomatic adolescent subjects from electroencephalogram (EEG) signals via machine intelligence. A publicly accessible EEG dataset featuring 16-channels EEG obtained from 84 adolescents (45 SZ symptomatic and 39 healthy control) is used to demonstrate the work. Initially, the signals are decomposed into sub-bands using two multi-resolution signal analysis methods: Empirical Wavelet Transform and Empirical mode decomposition. 75 unique features from each sub-bands are extracted and the few selective prominent features are applied to machine learning classifiers for optimal sub-band selection. Subsequently, a hybrid model is proposed, combining convolutional neural network (CNN) and ensemble bagged tree, incorporating both deep learning and handcrafted features to perform SZ diagnosis. This innovative model achieved superior classification performance compared to existing methods, offering a promising approach for SZ diagnosis. Furthermore, the study explores the impact of different brain regions and combined regional data in SZ diagnosis comprehensively. Hence, this computer-assisted decision-making model minimizes the limitations of prior studies by providing a more robust and efficient diagnostic system for schizophrenia.",Not About Sufficiency
"A ""pivot"" Model to set up Large Scale Rare Diseases Information Systems: Application to the Fibromuscular Dysplasia Registry","The SIR-FMD project is a partnership between the Department of Genetics and Reference Centre for Rare Vascular Diseases at the Georges Pompidou European Hospital in Paris and the Medical Informatics and Knowledge Engineering Laboratory of Inserm. Its aim is to use an ontological approach to implement an information system for the French Fibromuscular Dysplasia Registry. The existing data was dispersed in numerous databases, which had been created independently. These databases have different structures and contain data of diverse quality. The project aims to provide generic solutions for the management of the communication of medical data. The secondary objective is to demonstrate the applicability of these generic solutions in the field of rare diseases (RD) in an operational context. The construction of the French FMD registry was a multistep process. A secure platform has been available since the beginning of November 2013. The medical records of 471 patients from the initial dataset provided by the HEGP-Paris, France have been included, and are accessible from a secure user account. Users are organized into a collaborative group, and can access patient groups. Each electronic patient record contains more than 2,200 items. The problem of semantic interoperability has become one of the major challenges for the development of applications requiring the sharing and reuse of data. The information system component of the SIR-FMD project has a direct impact on the standardisation of coding of rare diseases and thereby contributes to the development of e-Health.",Not About Sufficiency
Timeliness of information disclosure during the low transmission period of COVID-19: resident-level observational study in China,"Background: Only when people feel they have received timely disclosure will they have sufficient incentive to implement community prevention and control measures. The timely and standardized information published by authorities as a response to the crisis can better inform the public and enable better preparations for the pandemic during the low transmission period of COVID-19; however, there is limited evidence of whether people consent that information is disclosed timely and influencing factors. Methods: A cross-sectional survey was conducted in China from 4 to 26 February 2021. Convenient sampling strategy was adopted to recruit participators. Participants were asked to filled out the questions that assessed questionnaire on the residents' attitudes to information disclosure timely. A binary logistic regression analysis was performed to identify the risk factors affecting the residents' attitudes. Results: A total of 2361 residents filled out the questionnaire. 1704 (72.17%) consented COVID-19 information has been disclosed timely. Furthermore, age (OR= 0.093, 95%CI =0.043 similar to 0.201), gender (OR= 1.396, 95%CI =1.085 similar to 1.797), place of residence (OR= 0.650, 95%CI = 0.525 similar to 0.804), employed status (OR= 2.757, 95%CI =1.598-4.756), highest educational level (OR= 0.394, 95%CI = 0.176 -0.880), region (OR= 0.561, 95%CI = 0.437-0.720) and impact on life by the COVID-19 (OR= 0.482, 95%CI = 0.270 -0.861) were mainly factors associated with residents' attitudes. Conclusions: The aims of this study were to evaluate the residents attitudes to information disclosure timely during the low transmission period in China and to provide a scientific basis for effective information communication in future public health crises. Timely and effective efforts to disclose information need to been made during the low transmission period. Continued improvements to local authority reporting will contribute to more effective public communication and efficient public health research responses. The development of protocols and the standardization of epidemic message templates-as well as the use of uniform operating procedures to provide regular information updates-should be prioritized to ensure a coordinated national response.",Not About Sufficiency
"Spatiotemporal epidemiology and forecasting of dengue in the state of Punjab, India: Study protocol","Dengue burden in India is a major public health problem. The present study has been designed to understand mechanisms by which routine data generate evidence. Secondary data analysis of routine datasets to understand spatiotemporal epidemiology and forecast dengue will be conducted. Data science approach will be adopted to generate a reproducible framework in the R environment. The lab-confirmed dengue reported by the state health authorities from 01 January 2015 to 31 December 2019 will be included. Multiple climatic variables from satellite imagery, climatic models, vegetation and built-up indices, and sociodemographic variables will be explored as risk factors. Exploratory data analysis followed by statistical analysis and machine learning will be performed. Data analysis will include geospatial information analysis, time series analysis, and spatiotemporal analysis. The study will provide value addition to the existing disease surveillance mechanisms by developing a framework for incorporating multiple routine data sources available in the country.",Not About Sufficiency
Human Digital Twin in the context of Industry 5.0,"Human-centricity, a core value of Industry 5.0, places humans in the center of production. It leads to the prioritization of human needs, spanning from health and safety to self-actualization and personal growth. The concept of the Human Digital Twin (HDT) is proposed as a critical method to realize human-centricity in smart manufacturing systems towards Industry 5.0. HDTs are digital representations of humans, aiming to change the practice of human-system integration by coupling humans' characteristics directly to the system design and its performance. In-depth analysis, critical insights, and application guidelines of HDT are essential to realize the concept of Industry 5.0 in practice and evolve the smart manufacturing paradigm in modern factories. However, the investigation on the development of HDT to evolve humans' roles and develop humans to their full potential is limited to date. Recent studies are rarely geared towards designing a standardized framework and architecture of HDT for diverse real-world applications. Thus, this work aims to close this research gap by carrying out a comprehensive survey on HDT in the context of Industry 5.0, summarizing the ongoing evolution, and proposing a proper connotation of HDT, before discussing the conceptual framework and system architecture of HDT and analyzing enabling technologies and industrial applications. This work provides guidance on possible avenues as well as challenges for the further development of HDT and its related concepts, allowing humans to reach their potential and accommodating their diverse needs in the futuristic smart manufacturing systems shaped by Industry 5.0.",Not About Sufficiency
Data-Driven and Machine-Learning Methods to Project Coronavirus Disease 2019 Pandemic Trend in Eastern Mediterranean,"Background: The coronavirus disease 2019 (COVID-19) pandemic has become a major public health crisis worldwide, and the Eastern Mediterranean is one of the most affected areas. Materials and Methods: We use a data-driven approach to assess the characteristics, situation, prevalence, and current intervention actions of the COVID-19 pandemic. We establish a spatial model of the spread of the COVID-19 pandemic to project the trend and time distribution of the total confirmed cases and growth rate of daily confirmed cases based on the current intervention actions. Results: The results show that the number of daily confirmed cases, number of active cases, or growth rate of daily confirmed cases of COVID-19 are exhibiting a significant downward trend in Qatar, Egypt, Pakistan, and Saudi Arabia under the current interventions, although the total number of confirmed cases and deaths is still increasing. However, it is predicted that the number of total confirmed cases and active cases in Iran and Iraq may continue to increase. Conclusion: The COVID-19 pandemic in Qatar, Egypt, Pakistan, and Saudi Arabia will be largely contained if interventions are maintained or tightened. The future is not optimistic, and the intervention response must be further strengthened in Iran and Iraq. The aim of this study is to contribute to the prevention and control of the COVID-19 pandemic.",Not About Sufficiency
Guessing where the goal posts are: managing health and well-being during the transition to university studies,"It is widely acknowledged that social conditions are directly associated with health and well-being. Significantly little is known about the impact of changing social conditions, including the transition to higher education, on young people's health and well-being. This qualitative research investigated perceptions and factors that influence health and well-being for first year university students. Governmental practices adopted in managing health and well-being during transition to a university context, were also investigated. Empirical data were collected, in 2010, via an online student questionnaire. Participants were completing their first year of study in a School of Health Sciences at an Australian university. They were asked to respond to a series of closed questions to collect demographic data and open-ended questions regarding their perceptions of health and well-being as well as factors that impact on them personally as they transition into university studies. Findings indicate that there are significant factors that impact on student well-being during this transition. These include: geographical relocation, engagement with university learning, sense of community as well as managing time and competing demands. Findings also indicate that whilst young people accept an individualised responsibility managing health and well-being, the social conditions of transition to university render this complex and problematic.",Not About Sufficiency
PROBABILISTIC POPULATION PROJECTION WITH JAMES II,"Predicting future populations and their structure is a central theme in demography. It is related to public health issues, political decision-making, or urban planning. Since these predictions are concerned with the evolution of a complex system, they exhibit a considerable uncertainty. Accounting for this inherent uncertainty is crucial for subsequent decision processes, as it reveals the range of possible outcomes and their likelihood. Consequently, probabilistic prediction approaches emerged over the past decades. This paper describes the probabilistic population projection model (PPPM), a recently developed method that allows detailed projections, but has a complex structure and requires much input data. We discuss the development of P3J, a tool that helps users in managing and executing projections and is built on top of the simulation system JAMES H. We outline how even specific tools like P3J profit from general-purpose simulation frameworks like JAMES II, and illustrate its usage by a simple example.",Not About Sufficiency
Mobilizing Local Authorities Around Public Health Priorities,"Large Analysis and Review of European Housing and Health Status (LARES) was conducted in Europe in 2002 to 2003 to study the relationship between citizens' health and built environments. One of its objectives was to put public health priorities on the agenda of local decision-makers to implement solutions for the community. We adapted the LARES protocol as a pilot project in a small French-Canadian town in Quebec Province in 2012. The distinguishing feature of this project was the collaborative approach taken with local actors, especially the municipality, which was committed a priori to using survey data from an urban planning perspective. The project produced interesting results that were used to motivate actions concerning people living in bad sanitary conditions; to draft the urban plan including the development of parks, green spaces, and bicycle paths; and to allow the municipality to meet eligibility criteria for access to renovation programs. If a partnership with the local actors and their commitment to promote and realize the project were obtained at the beginning, then the survey could be replicated in other communities.",Not About Sufficiency
Empathic computing,"Empathic computing is an emergent paradigm that enables a system to understand human states and feelings and to share this intimate information. The new paradigm is made possible by the convergence of affordable sensors, embedded processors and wireless ad-hoc networks. The power law for multi-resolution channels and mobile-stationary sensor webs is introduced to resolve the information avalanche problems. As empathic computing is sensor-rich computing, particular models such as semantic differential expressions and inverse physics are discussed. A case study of a wearable sensor network for detection of a falling event is presented. It is found that the location of the wearable sensor is sensitive to the results. From the machine learning algorithm, the accuracy reaches up to 90% from 21 simulated trials. Empathic computing is not limited to healthcare. It can also be applied to solve other everyday-life problems such as management of emails and stress.",Not About Sufficiency
Development of Classification Algorithms for the Detection of Postures Using Non-Marker-Based Motion Capture Systems,"The rapid development of algorithms for skeletal postural detection with relatively inexpensive contactless systems and cameras opens up the possibility of monitoring and assessing the health and wellbeing of humans. However, the evaluation and confirmation of posture classifications are still needed. The purpose of this study was therefore to develop a simple algorithm for the automatic classification of human posture detection. The most affordable solution for this project was through using a Kinect V2, enabling the identification of 25 joints, so as to record movements and postures for data analysis. A total of 10 subjects volunteered for this study. Three algorithms were developed for the classification of different postures in Matlab. These were based on a total error of vector lengths, a total error of angles, multiplication of these two parameters and the simultaneous analysis of the first and second parameters. A base of 13 exercises was then created to test the recognition of postures by the algorithm and analyze subject performance. The best results for posture classification were shown by the second algorithm, with an accuracy of 94.9%. The average degree of correctness of the exercises among the 10 participants was 94.2% (SD1.8%). It was shown that the proposed algorithms provide the same accuracy as that obtained from machine learning-based algorithms and algorithms with neural networks, but have less computational complexity and do not need resources for training. The algorithms developed and evaluated in this study have demonstrated a reasonable level of accuracy, and could potentially form the basis for developing a low-cost system for the remote monitoring of humans.",Not About Sufficiency
Discovery of Noninvasive Biomarkers for Radiation Exposure via LC-MS-Based Hair Metabolomics,"Ionizing radiation exposure from a potential nuclear energy plant leak or detonation of a nuclear weapon can cause massive casualties to both warfighters and civilians. RNA, proteins, and metabolite biomarkers in biological specimens like blood and tissue have shown potential to determine radiation dose levels. However, these biomarkers in blood and urine are short-lived, typically detectable within hours or a few days. To address the need for stable, long-term radiation exposure biomarkers, we developed two mass spectrometry-based methods using noninvasive hair samples to identify radiation-exposure biomarkers. Our results show that hippuric acid and 5-methoxy-3-indoleacetate significantly increase after higher (4 gray) doses of gamma irradiation compared to lower (1 and 2 Gy) doses or nonexposed hair samples. While 2-aminooctadec-4-ene-1,3-diol, oleoyl ethanolamide, palmitoylcarnitine, 25-hydroxy vitamin D3, vernolic acid, and azelaic acid significantly increased over time after exposure. Trimethylamine N-oxide (TMAO) was found in higher concentrations in female specimens across all time points. Further validation using a machine learning model suggested that these biomarkers can predict differences in the exposure dose and time point. Our findings highlight the potential of noninvasive hair sample analysis for assessing radiation exposure, offering a viable alternative to address critical public health concerns of unexpected radiation exposure.",Not About Sufficiency
Mathematical modeling for short term indoor room temperature forecasting using Box-Jenkins models An Indian evidence,"Purpose The rapid urbanization of Indian cities and the population surge in cities has steered a massive demand for energy, thereby increasing the carbon emissions in the environment. Information and technology advancements, aided by predictive tools, can optimize this energy demand and help reduce harmful carbon emissions. Out of the multiple factors governing the energy consumption and comfort of buildings, indoor room temperature is a critical one, as it envisages the need for regulating the temperature. This paper aims to propose a mathematical model for short-term forecasting of indoor room temperature in the Indian context to optimize energy consumption and reduce carbon emissions in the environment. Design/methodology/approach A study is conducted to forecast the indoor room temperature of an Indian corporate building structure, based upon various external environmental factors: temperature and rainfall and internal factors like cooling control, occupancy behavior and building characteristics. Expert insight and principal component analysis are applied for appropriate variables selection. The machine learning approach using Box-Jenkins time series models is used for the forecasting of indoor room temperature. Findings ARIMAX model, with lagged forecasted and explanatory variables, is found to be the best-fit model. A predictive short-term hourly temperature forecasting model is developed based upon ARIMAX model, which yields fairly accurate results for data set pertaining to the building conditions and climatic parameters in the Indian context. Results also investigate the relationships between the forecasted and individual explanatory variables, which are validated using theoretical proofs. Social implications This study has been conducted in India, which has seen a rapid surge in population growth and urbanization. Being a developing country, India needs to channelize its energy needs judiciously by minimizing the energy wastage and reducing carbon emissions. This study proposes certain pre-emptive measures that help in minimizing the consumption of available energy resources as well as reducing carbon emissions that have significant impact on the society and environment at large. Originality/value A large number of factors affecting the indoor room temperature present a research challenge for model building. The paper statistically identifies the parameters influencing the indoor room temperature forecasting and their relationship with the forecasted model. Considering Indian climatic, geographical and building structure conditions, the paper presents a systematic mathematical model to forecast hourly indoor room temperature for next 120 h with fair degree of accuracy.",Not About Sufficiency
Federated learning-driven IoT system for automated freshness monitoring in resource-constrained vending carts,"Street vendors in developing regions often lack access to portable and affordable cold storage, leading to accelerated food spoilage, financial losses, and health risks. Traditional refrigeration solutions are bulky and costly, while manual freshness assessment is error-prone. This study proposes a smart vending cart integrating IoT sensors and federated learning (FL) to address these challenges, offering real-time environmental monitoring, freshness classification, and privacy-preserving data handling. The smart vending cart incorporates IoT sensors to monitor temperature, humidity, and gas emissions. A Peltier cooling module and a humidifier maintain optimal conditions. Machine learning models classify food freshness, while federated learning ensures vendor privacy by training models locally on each cart. The study explores nine federated learning approaches to train machine learning models across multiple carts without sharing raw data, thus preserving vendor privacy. The Stacking Ensemble approach outperformed all other methods, achieving the highest accuracy, F1-Score, and Cohen's Kappa (0.99964), with the lowest log loss (0.0022). MetaLearning and Weighted Aggregation also demonstrated high performance but with marginally higher log loss values. Personalized models performed well in heterogeneous data environments but were less effective than ensemble methods. The developed smart vending cart system effectively reduces food spoilage and enhances vendor profitability through automated freshness classification and real-time environmental control. The integration of federated learning ensures privacy, while ensemble techniques improve robustness in resource-constrained settings, offering a scalable solution for street vendors.",Not About Sufficiency
Relationship between Long-Term Residential Green Exposure and Individuals' Mental Health: Moderated by Income Differences and Residential Location in Urban China,"Environmental health effects during urbanization have attracted much attention. However, knowledge is lacking on the relationship between long-term cumulative residential environment and health effects on individuals during rapid transformations in urban physical and social space. Taking Guangzhou, China, as a case example, this study analyzed the relationship between long-term exposure to green environments and residents' mental health under urban spatial restructuring. Based on a household survey in 2016, 820 residents who have lived in Guangzhou for more than 15 years were used as the sample. High-resolution remote sensing images were used to assess the long-term green exposure of residents. The results indicate that long-term green exposure in residential areas had a negative correlation with residents' mental health (p < 0.05), and the correlation was strongest for the cumulative green environment in the last five years. However, this significant effect was moderated by income and residential location. Green exposure had a positive relationship with mental health for low income groups, and a negative relationship for middle and high income groups. In addition, residents living farther away from the city center were likely to have fewer green environmental health benefits. Residential relocation in a rapidly urbanizing and transforming China has led to the continuous differentiation of residential green environments among different income groups, which has also caused different mental health effects from green exposure. It provides empirical evidence and theoretical support for policymakers to improve the urban environment and reduce environmental health disparities by considering social differences and residential location.",Not About Sufficiency
Nitroaromatic explosives' detection and quantification using an attention-based transformer on surface-enhanced Raman spectroscopy maps,"Rapidly and accurately detecting and quantifying the concentrations of nitroaromatic explosives is critical for public health and security. Among existing approaches, explosives' detection with Surface-Enhanced Raman Spectroscopy (SERS) has received considerable attention due to its high sensitivity. Typically, a preprocessed single spectrum that is the average of the entire or a selected subset of a SERS map is used to train various machine learning models for detection and quantification. Designing an appropriate averaging and preprocessing procedure for SERS maps across different concentrations is time-consuming and computationally costly, and the averaging of spectra may lead to the loss of crucial spectral information. We propose an attention-based vision transformer neural network for nitroaromatic explosives' detection and quantification that takes raw SERS maps as the input without any preprocessing. We produce two novel SERS datasets, 2,4-dinitrophenols (DNP) and picric acid (PA), and one benchmark SERS dataset, 4-nitrobenzenethiol (4-NBT), which have repeated measurements down to concentrations of 1 nM to illustrate the detection limit. We experimentally show that our approach outperforms or is on par with the existing methods in terms of detection and concentration prediction accuracy. With the produced attention maps, we can further identify the regions with a higher signal-to-noise ratio in the SERS maps. Based on our findings, the molecule of interest detection and concentration prediction using raw SERS maps is a promising alternative to existing approaches.",Not About Sufficiency
'A commons before the sea:' climate justice considerations for coastal zone management,"If climate change mitigation and adaptation are a human right, institutional change is needed that considers coastal ecosystem integrity as a common pool resource. Increasing risks in coastal zones necessitates adopting new and frequently controversial zoning, planning, and management practices, particularly as insurance programmes reform or require bailouts. In the U.S., current coastal policy frameworks employed by the Federal Emergency Management Agency (FEMA) and state-level authorities incentivize defensive strategies, especially in high-value tourism destinations, despite critiques of inequity and longer-term evidence demonstrating that hardening shorelines shifts erosion patterns. Other coastal regions and developing countries that cannot afford defensive strategies - particularly rural, minority, and impoverished communities located adjacent to estuarine areas - rely heavily on ecosystem services for protection and will likely disproportionately face buyouts, forced relocation, and retreat as seas rise.",Not About Sufficiency
Towards Life Cycle Sustainability Assessment of Alternative Passenger Vehicles,"Sustainable transportation and mobility are key components and central to sustainable development. This research aims to reveal the macro-level social, economic, and environmental impacts of alternative vehicle technologies in the U.S. The studied vehicle technologies are conventional gasoline, hybrid, plug-in hybrid with four different all-electric ranges, and full battery electric vehicles (BEV). In total, 19 macro level sustainability indicators are quantified for a scenario in which electric vehicles are charged through the existing U. S. power grid with no additional infrastructure, and an extreme scenario in which electric vehicles are fully charged with solar charging stations. The analysis covers all life cycle phases from the material extraction, processing, manufacturing, and operation phases to the end-of-life phases of vehicles and batteries. Results of this analysis revealed that the manufacturing phase is the most influential phase in terms of socio-economic impacts compared to other life cycle phases, whereas operation phase is the most dominant phase in the terms of environmental impacts and some of the socio-economic impacts such as human health and economic cost of emissions. Electric vehicles have less air pollution cost and human health impacts compared to conventional gasoline vehicles. The economic cost of emissions and human health impact reduction potential can be up to 45% and 35%, respectively, if electric vehicles are charged through solar charging stations. Electric vehicles have potential to generate income for low and medium skilled workers in the U.S. In addition to quantified sustainability indicators, some sustainability metrics were developed to compare relative sustainability performance alternative passenger vehicles. BEV has the lowest greenhouse gas emissions and ecological land footprint per $ of its contribution to the U.S. GDP, and has the lowest ecological footprint per unit of its energy consumption. The only sustainability metrics that does not favor the BEV is the water- energy ratio, where the conventional gasoline vehicle performed best.",Not About Sufficiency
Development and validation of a machine learning-based predictive model for assessing the 90-day prognostic outcome of patients with spontaneous intracerebral hemorrhage,"Background Spontaneous intracerebral hemorrhage (sICH) is associated with significant mortality and morbidity. Predicting the prognosis of patients with sICH remains an important issue, which significantly affects treatment decisions. Utilizing readily available clinical parameters to anticipate the unfavorable prognosis of sICH patients holds notable clinical significance. This study employs five machine learning algorithms to establish a practical platform for the prediction of short-term prognostic outcomes in individuals afflicted with sICH. Methods Within the framework of this retrospective analysis, the model underwent training utilizing data gleaned from 413 cases from the training center, with subsequent validation employing data from external validation center. Comprehensive clinical information, laboratory analysis results, and imaging features pertaining to sICH patients were harnessed as training features for machine learning. We developed and validated the model efficacy using all the selected features of the patients using five models: Support Vector Machine (SVM), Logistic Regression (LR), Random Forest (RF), XGboost and LightGBM, respectively. The process of Recursive Feature Elimination (RFE) was executed for optimal feature screening. An internal five-fold cross-validation was employed to pinpoint the most suitable hyperparameters for the model, while an external five-fold cross-validation was implemented to discern the machine learning model demonstrating the superior average performance. Finally, the machine learning model with the best average performance is selected as our final model while using it for external validation. Evaluation of the machine learning model's performance was comprehensively conducted through the utilization of the ROC curve, accuracy, and other relevant indicators. The SHAP diagram was utilized to elucidate the variable importance within the model, culminating in the amalgamation of the above metrics to discern the most succinct features and establish a practical prognostic prediction platform. Results A total of 413 patients with sICH patients were collected in the training center, of which 180 were patients with poor prognosis. A total of 74 patients with sICH were collected in the external validation center, of which 26 were patients with poor prognosis. Within the training set, the test set AUC values for SVM, LR, RF, XGBoost, and LightGBM models were recorded as 0.87, 0.896, 0.916, 0.885, and 0.912, respectively. The best average performance of the machine learning models in the training set was the RF model (average AUC: 0.906 +/- 0.029, P < 0.01). The model still maintains a good performance in the external validation center, with an AUC of 0.817 (95% CI 0.705-0.928). Pertaining to feature importance for short-term prognostic attributes of sICH patients, the NIHSS score reigned supreme, succeeded by AST, Age, white blood cell, and hematoma volume, among others. In culmination, guided by the RF model's variable importance weight and the model's ROC curve insights, the NIHSS score, AST, Age, white blood cell, and hematoma volume were integrated to forge a short-term prognostic prediction platform tailored for sICH patients. Conclusion We constructed a prediction model based on the results of the RF model incorporating five clinically accessible predictors with reliable predictive efficacy for the short-term prognosis of sICH patients. Meanwhile, the performance of the external validation set was also more stable, which can be used for accurate prediction of short-term prognosis of sICH patients.",Not About Sufficiency
Factors for the Future of Work and Their Impact on the European Economy and Labor Market,"The aim of the present paper is to gain a deeper understanding of the essence of the ""Future of work"" by exploring simultaneously its major drivers and their impact on the job market and employment on European level in the long run. The data used are based on the research of numerous international organizations, European Union institutions, and bodies, as well as on analytical data and expertise of reputable business consulting companies. The methodology includes qualitative research, including data analysis, comparative analysis, inductive and deductive approach, and others. This type of research helps to explore how and why the phenomenon ""Future of work"" occurred, what it represents and what the prospects for its development are. The main drivers of this phenomenon are analyzed as follows: globalization, digitalization, and demographic change. The paper explores and summarizes the expected economic and social effects of these three factors on the labor market in the EU. The main findings conclude that the ""Future of work"" is an interdisciplinary phenomenon covering the current and expected trends on the world labor market, provoked by the dynamic technological development and penetration of artificial intelligence technologies in the economic and business practice, including job automation (job loss and job creation), rising skills and qualification requirements, a big change in employment occupations, and rising diversity in working arrangements. Legislative, business-oriented, and educational actions on all levels are urgently required to face these new challenges successfully.",Not About Sufficiency
The Grants Pass v. Johnson Ruling: Decriminalizing Homelessness Is the First Step in Solving America's Housing Crisis,"The June 2024 City of Grants Pass, Oregon v. Johnson ruling by the U.S. Supreme Court marks a significant moment in the ongoing legal and societal debates about homelessness. The ruling allows cities to impose sanctions on homeless individuals who sleep in public spaces, raising profound questions about the intersection of urban planning and social justice. Through the theoretical frameworks of social justice and the right to the city, we critique the ruling as a setback in the fight for equitable housing solutions, arguing for the decriminalization of homelessness and a shift toward more compassionate and inclusive housing policies. We advocate for a reimagining of housing policies and urban planning practices that prioritize the universal right to housing, especially for marginalized populations.",Not About Sufficiency
The Healthy and Sustainable City-Influences of the Built Environment on Active Travel,"The city's built environment and functionality play a crucial role in shaping individual mobility patterns, impacting the overall health and quality of life of its population. Understanding these influences is an important research topic, making it a central focus of this paper. This study aims to identify the factors responsible for promoting healthy mobility behavior. To address this comprehensively, a multidisciplinary empirical survey was developed based on the ""Triad""-a model consisting of the built environment, mobility(-behavior), and public health. In addition to the evaluation of socio-demographic factors and activity radius mapping, statistical analyses like multiple linear regression were used. These statistical analyses allow the assessment of the impact of various independent variables on the promotion of healthy mobility behavior within urban settings. The multiple regression shows that the satisfaction with the accessibility of public transport and the sense of safety as a cyclist contribute to explaining the variation of healthy mobility. Furthermore, the satisfaction with walking in the neighborhood and the inhalation of exhaust fumes while walking also seem to have an impact. The results show the link between the Triad and make it clear that mobility planning and urban planning must take a more integrated approach to promote health and simultaneously protect the climate.",Not About Sufficiency
The Role of Machine Learning in Enhancing Particulate Matter Estimation: A Systematic Literature Review,"As urbanization and industrial activities accelerate globally, air quality has become a pressing concern, particularly due to the harmful effects of particulate matter (PM), notably PM2.5 and PM10. This review paper presents a comprehensive systematic assessment of machine learning (ML) techniques for estimating PM concentrations, drawing on studies published from 2018 to 2024. Traditional statistical methods often fail to account for the complex dynamics of air pollution, leading to inaccurate predictions, especially during peak pollution events. In contrast, ML approaches have emerged as powerful tools that leverage large datasets to capture nonlinear, intricate relationships among various environmental, meteorological, and anthropogenic factors. This review synthesizes findings from 32 studies, demonstrating that ML techniques, particularly ensemble learning models, significantly enhance estimation accuracy. However, challenges remain, including data quality, the need for diverse and balanced datasets, issues related to feature selection, and spatial discontinuity. This paper identifies critical research gaps and proposes future directions to improve model robustness and applicability. By advancing the understanding of ML applications in air quality monitoring, this review seeks to contribute to developing effective strategies for mitigating air pollution and protecting public health.",Not About Sufficiency
Cost-Effective Vibration Analysis through Data-Backed Pipeline Optimisation,"Vibration analysis is an active area of research, aimed, among other targets, at an accurate classification of machinery failure modes. The analysis often leads to complex and convoluted signal processing pipeline designs, which are computationally demanding and often cannot be deployed in IoT devices. In the current work, we address this issue by proposing a data-driven methodology that allows optimising and justifying the complexity of the signal processing pipelines. Additionally, aiming to make IoT vibration analysis systems more cost- and computationally efficient, on the example of MAFAULDA vibration dataset, we assess the changes in the failure classification performance at low sampling rates as well as short observation time windows. We find out that a decrease of the sampling rate from 50 kHz to 1 kHz leads to a statistically significant classification performance drop. A statistically significant decrease is also observed for the 0.1 s time window compared to the 5 s one. However, the effect sizes are small to medium, suggesting that in certain settings lower sampling rates and shorter observation windows might be worth using, consequently making the use of the more cost-efficient sensors feasible. The proposed optimisation approach, as well as the statistically supported findings of the study, allow for an efficient design of IoT vibration analysis systems, both in terms of complexity and costs, bringing us one step closer to the widely accessible IoT/Edge-based vibration analysis.",Not About Sufficiency
Agent-Based Simulations Using Genetic Algorithm Calibration: A Children's Services Application,"With increased pressures and tightening budgets within English Children's Services in the UK, seeking more effective operational and financial management is becoming a more significant topic of discussion. In other sectors, complex data analysis methods provide the aforementioned management improvements through better understanding of current situations leading to better decision making. Currently, investment remains at a slow pace in English Local Authorities due to budget restrictions. In this paper, a potential opportunity is explored with existing publicly available data related to this area. With the help of industry experts, an Agent-Based Model is created to emulate basic Children's Services operations and optimised to fit existing data using NSGA-III. With relatively close matches being achieved with sample authorities, this approach demonstrates promise in advancing analytics capabilities for Children's Services and practical solutions are discussed. With this presented work, it is shown that further expansion and exploration into real-world applications is warranted.",Not About Sufficiency
The impact of digital technologies on public life: a critical examination of perspectives from spatial design experts,"This study explores the influence of digital technologies on public life and evaluates spatial design professionals' perceptions of these impacts. The research involved surveying architects, urban planners, and landscape architects to comprehend the ramifications of digital technology on public life. In total, 356 spatial design experts participated in the study. The analysis utilised descriptive statistical methods, t-tests and ANOVA tests for independent samples. The findings reveal that social isolation was rated highest among the spatial design experts' scale, whereas social anxiety scored lowest among the elements comprising the spatial design experts' scale. A comparison between participants' socio-demographic characteristics and their scores on the impact of digital technologies in public life showed variances in gender, employment status, profession, and daily usage of digital technology, but found no significant differences in marital status, level of education, or professional experience. Consequently, this research sheds light on the repercussions of digital technology usage in public life, offering invaluable insights for all stakeholders, particularly spatial design professionals and social scientists. By understanding these impacts, the study highlights the importance of adopting more informed and conscious approaches to fostering positive social interactions and enhancing community well-being in public life. GRAPHICAL ABSTRACT",Not About Sufficiency
"Cooperative Game-Based Digital Twin Drives Decision Making: Overall Framework, Basic Formalization and Application Case","The emerging progress brought about by Industry 4.0 generates great opportunities for better decision making to cope with increasingly uncertain and complex industrial production. From the perspective of game theory, methods based on computational simulations and methods based on physical entities have their intrinsic drawbacks, such as partially accessible information, uncontrollable uncertainty and limitations of sample data. However, an insight that inspired us was that the digital twin modeling method induced interactive environments to allow decision makers to cooperatively learn from the immediate feedback from both cyberspace and physical spaces. To this end, a new decision-making method was put forward using game theory to autonomously ally the digital twin models in cyberspace with their physical counterparts in the real world. Firstly, the overall framework and basic formalization of the cooperative game-based decision making are presented, which used the negotiation objectives, alliance rules and negotiation strategy to ally the planning agents from the physical entities with the planning agents from the virtual simulations. Secondly, taking the assembly planning of large-scale composite skins as a proof of concept, a cooperative game prototype system was developed to marry the physical assembly-commissioning system with the virtual assembly-commissioning system. Finally, the experimental work clearly indicated that the coalitional game-based twinning method could make the decision making of composite assembly not only predictable but reliable and help to avoid stress concentration and secondary damage and achieve high-precision assembly. Obviously, this decision-making methodology that integrates the physical players and their digital twins into the game space can help them take full advantage of each other and make up for their intrinsic drawbacks, and it preliminarily demonstrates great potential to revolutionize the traditional decision-making methodology.",Not About Sufficiency
"Covid-19 Response and Protracted Exclusion of Informal Settlement Residents in Freetown, Sierra Leone","Freetown has over 1 million residents, many of whom live in about 68 crowded informal settlements. Residents of these settlements struggle daily to access basic services such as water, sanitation, and health-care services. We found that the government's Covid-19 response measures (curfews, lockdowns, and travel restrictions) excluded informal residents from contributing to its design, and the implementation of these measures prevented these residents from accessing basic services. Like the urban planning processes in Freetown, the Covid-19 response planning was done with the limited inclusion of informal residents, and not considering how these response measures would affect their livelihood priorities. The economic conditions of already vulnerable people such as those living with disabilities, beggars, and women heads of households worsened as a result of these measures. While these challenges were dire, communities were resilient in reversing the spread of Covid-19 through tailor-made messaging and by supporting the most vulnerable with food and basic needs. In this article we argue that the inclusion of the urban poor in decision-making and urban planning processes can improve service delivery and their ability to cope with health shocks.",Not About Sufficiency
A natural gas consumption forecasting system for continual learning scenarios based on Hoeffding trees with change point detection mechanism,"Forecasting natural gas consumption, considering seasonality and trends, is crucial in planning its supply and consumption and optimizing the cost of obtaining it, mainly by industrial entities. However, in times of threats to its supply, it is also a critical element that guarantees the supply of this raw material to meet individual consumers' needs, ensuring society's energy security. This article introduces a novel multistep forecasting of natural gas consumption with change point detection integration for model collection selection with continual learning capabilities using data stream processing. The performance of the forecasting models based on the proposed approach is evaluated in a complex real-world use case of natural gas consumption forecasting. Furthermore, the methodology generability was verified in an electricity load forecasting task. We employed Hoeffding tree predictors as forecasting models and the Pruned Exact Linear Time (PELT) algorithm for the change point detection procedure. The change point detection integration enables the selection of a different model collection for successive time frames. Thus, three model collection selection procedures are defined and evaluated for forecasting scenarios with various densities of detected change points. These models were compared with change point agnostic baseline approaches and deep learning models. Our experiments show that the proposed approach provides superior results to deep learning models for both datasets and that fewer change points result in a lower forecasting error regardless of the model collection selection procedure employed.",Not About Sufficiency
Machine Learning Optimized Graphene and MXene-Based Surface Plasmon Resonance Biosensor Design for Cyanide Detection,"Cyanide, a highly toxic chemical compound, presents severe risks to both human health and the environment. Its presence is particularly concerning in various industrial sectors, including mining, electroplating and chemical manufacturing, as well as in natural water bodies due to industrial discharge. This study introduces a graphene-based metasurface sensor designed for highly sensitive cyanide detection within the terahertz frequency range. The sensor's design was refined through comprehensive electromagnetic modelling and analysis. Performance characterization demonstrates optimal sensitivity of 929 GHz RIU-1, coupled with a figure of merit of 14.286 RIU-1 between 0.806 and 0.856 THz frequencies. The detection limit achieved is 0.053 RIU. Adjustments to graphene's chemical potential and structural dimensions demonstrated the device's adaptability. Additionally, the application of machine learning techniques, specifically 1D-CNN regression, proved effective in optimizing sensor performance. The predictive model demonstrated remarkable accuracy, with an optimal R2 score exceeding 95%, indicating that over 94.9% of the variance in the data was accounted for. This high precision enables accurate estimation of absorption values for wavelengths between measured points, underscoring the model's reliability in spectroscopic analysis. This work highlights a versatile platform for rapid, label-free cyanide detection, with significant potential for applications in environmental monitoring, industrial safety and public health protection.",Not About Sufficiency
PANDEMICS AND DOMESTIC VIOLENCE DURING COVID-19,"The year 2020 met us with the COVID-19 pandemic. The covid-19 pandemic has gone past a mere health challenge. Its effect can be felt in the economy and society in general. Women form a large chunk of the response efforts geared at flattening the curve of the COVID-19 scourge. As the first point of contact, caregivers, medical personnel, volunteers, logistics facilitators, researchers and scientists and other professionals critical to the fight against the virus, women are making profound contributions in the fight against the spread of the outbreak. Most of the caregivers found in our homes and communities today are women. Furthermore, women stand a higher risk of infection and loss of their sources of livelihood, and as the outbreak continues to spread, there is all likelihood that they may not be able to access programs vital to their reproductive and sexual health. There is also a rise in cases of domestic violence against women in this crisis period. This study will be exploring a wide range of literature about pandemics that have happened in the past and previous public health emergencies and crisis, to enable it to ascertain patterns by which pandemics can further heighten the different kinds of violence against women. Evidence gathered from this study will be used to make recommendations to governments, civil society organizations, community-based agencies, and international donor agencies to help make women and children's health priority, keeping them safe and preparing them adequately for another possible pandemic.",Not About Sufficiency
SeqImprove: Machine-Learning-Assisted Curation of Genetic Circuit Sequence Information,"The progress and utility of synthetic biology is currently hindered by the lengthy process of studying literature and replicating poorly documented work. Reconstruction of crucial design information through post hoc curation is highly noisy and error-prone. To combat this, author participation during the curation process is crucial. To encourage author participation without overburdening them, an ML-assisted curation tool called SeqImprove has been developed. Using named entity recognition, called entity normalization, and sequence matching, SeqImprove creates machine-accessible sequence data and metadata annotations, which authors can then review and edit before submitting a final sequence file. SeqImprove makes it easier for authors to submit sequence data that is FAIR (findable, accessible, interoperable, and reusable).",Not About Sufficiency
Single cell morphology distinguishes genotype and drug effect in Hereditary Spastic Paraplegia,"A central need for neurodegenerative diseases is to find curative drugs for the many clinical subtypes, the causative gene for most cases being unknown. This requires the classification of disease cases at the genetic and cellular level, an understanding of disease aetiology in the subtypes and the development of phenotypic assays for high throughput screening of large compound libraries. Herein we describe a method that facilitates these requirements based on cell morphology that is being increasingly used as a readout defining cell state. In patient-derived fibroblasts we quantified 124 morphological features in 100,000 cells from 15 people with two genotypes (SPAST and SPG7) of Hereditary Spastic Paraplegia (HSP) and matched controls. Using machine learning analysis, we distinguished between each genotype and separated them from controls. Cell morphologies changed with treatment with noscapine, a tubulin-binding drug, in a genotype-dependent manner, revealing a novel effect on one of the genotypes (SPG7). These findings demonstrate a method for morphological profiling in fibroblasts, an accessible non-neural cell, to classify and distinguish between clinical subtypes of neurodegenerative diseases, for drug discovery, and potentially for biomarkers of disease severity and progression.",Not About Sufficiency
Spatio-temporal modeling of PM2.5 risk mapping using three machine learning algorithms,"Urban air pollution is one of the most critical issues that affect the environment, community health, economy, and management of urban areas. From a public health perspective, PM2.5 is one of the primary air pollutants, especially in Tehran's metropolis. Owing to the different patterns of PM2.5 in different seasons, Spatio-temporal modeling and identification of high-risk areas to reduce its effects seems necessary. The purpose of this study was Spatio-temporal modeling and preparation of PM2.5 risk mapping using three machine learning algorithms (random forest (RF), AdaBoost, and stochastic gradient descent (SGD)) in the metropolis of Tehran, Iran. Therefore, in the first step, to prepare the dependent variable data, the PM2.5 average was used for the four seasons of spring, summer, autumn, and winter. Then, using remote sensing (RS) and a geographic information system (GIS), independent data such as temperature, maximum temperature, minimum temperature, wind speed, rainfall, humidity, normalized difference vegetation index (NDVI), population density, street density, and distance to industrial centers were prepared as a seasonal average. To Spatio-temporal modeling using machine learning algorithms, 70% of the data were used for training and 30% for validation. The frequency ratio (FR) model was used as input to machine learning algorithms to calculate the spatial relationship between PM2.5 and the effective parameters. Finally, Spatio-temporal modeling and PM2.5 risk mapping were performed using three machine learning algorithms. The receiver operating characteristic (ROC) area under the curve (AUC) results showed that the RF algorithm had the greatest modeling accuracy, with values of 0.926, 0.94, 0.949, and 0.949 for spring, summer, autumn, and winter, respectively. According to the RF model, the most important variable in spring and autumn was NDVI. Temperature and distance to industrial centers were the most important variables in the summer and winter, respectively. The results showed that autumn, winter, summer, and spring had the highest risk of PM2.5, respectively.",Not About Sufficiency
A domino-based Virtual Observatory service for the HI Parkes All Sky Survey,"The current development of globally accessible astrophysical data systems is increasingly embracing Grid computing concepts, with data description and formatting standards such as VOTable and Uniform Content Descriptors providing a basis for system-system interoperability. To date, a diverse set of database management systems have been used for catalogue storage within these systems. We present a Virtual Observatory service for the HI Parkes All Sky Survey, implemented on an IBM Lotus Domino R6 database management system. Domino's distributed computing architecture, with in-built support for replication and clustering, sets it apart from more general database systems as being inherently suitable for Grid computing applications.",Not About Sufficiency
Connecting the Unconnected,"Connecting the Unconnected or under-connected (CTU) is the holy grail of transforming the lives of over 3 billion people around the globe with wireless Internet who are yet to experience its value in multiple ways. If this could be accomplished, its impact on the society would be enormous. This white paper from the IEEE Future Networks CTU Working Group endeavors to highlight the need to consider the CTU requirements in 5G and B5G networks in the standardization process, in the development of the use cases, and affordable solutions. In its Vision 2030 SDG (Sustainability Development Goals) the United Nations has proclaimed access to Internet as basic human right and has said that these goals cannot be achieved without affordable access to Internet by everyone on this planet. While there are numerous projects and initiatives ongoing around the world, these are fragmented and lack the critical mass and coordination to be able to impact the future standards, product development, and cost of deployment otherwise achievable by volume. Although difficult to pin down, to define a threshold for basic connectivity for all is important. But it would need to be flexible to adapt to changing times. It is the goal of the CTU group to create an open platform where the experts can bring their ideas, solutions, and potentially collaborate to create large global projects and influence the network service providers, manufacturers and their governments. This white paper defines the CTU working group's charter, scope, and provides a brief overview of the relevant stakeholders and linkages between them. Then the paper goes into the current status of the CTU landscape and where we want to reach to accomplish the vision of connecting everybody, especially those living in rural and remote areas. We present the various standards and industry fora and how they are interlinked. While technologies are available today, they need to be customized and optimized at the systems level to bring down the cost of the network in order to be affordable. In addition, the content needs to be relevant and in local languages to be useful, not to mention the need to offer innovative human computer interaction (HCI) solutions (that are not text based) so that people who are not literate or are digitally disadvantaged can easily use the devices and consume services. Another important area is that of flexible spectrum allocation regime at the lower range of the spectrum to increase reach and coverage. Use of renewable energy sources will enable deployment in remote areas where there is lack of power grid, or it is intermittent. Thus, this white paper identifies a number of technology gaps to be filled in by 5G and B5G networks, such that access is affordable and content and services are actually consumed by the targeted set of users. Technology aside, the need to develop innovative business models is a must to be commercially sustainable in the long-term. A number of such models, especially designed for the rural population, are proposed, such as Village Level Entrepreneur (VLE) Freemium (Free + Premium), revenue sharing among the chain of service providers, subsidized billing by USOF (Universal Service Obligation Funds). Finally, the white paper presents a 10-year roadmap starting from the current state to 3 years, 5 years and 10 years.",Not About Sufficiency
A new generation of trade policy: potential risks to diet-related health from the trans pacific partnership agreement,"Trade poses risks and opportunities to public health nutrition. This paper discusses the potential food-related public health risks of a radical new kind of trade agreement: the Trans Pacific Partnership agreement (TPP). Under negotiation since 2010, the TPP involves Australia, Brunei, Canada, Chile, Japan, Malaysia, Mexico, New Zealand, Peru, Singapore, the USA, and Vietnam. Here, we review the international evidence on the relationships between trade agreements and diet-related health and, where available, documents and leaked text from the TPP negotiations. Similar to other recent bilateral or regional trade agreements, we find that the TPP would propose tariffs reductions, foreign investment liberalisation and intellectual property protection that extend beyond provisions in the multilateral World Trade Organization agreements. The TPP is also likely to include strong investor protections, introducing major changes to domestic regulatory regimes to enable greater industry involvement in policy making and new avenues for appeal. Transnational food corporations would be able to sue governments if they try to introduce health policies that food companies claim violate their privileges in the TPP; even the potential threat of litigation could greatly curb governments' ability to protect public health. Hence, we find that the TPP, emblematic of a new generation of 21st century trade policy, could potentially yield greater risks to health than prior trade agreements. Because the text of the TPP is secret until the countries involved commit to the agreement, it is essential for public health concerns to be articulated during the negotiation process. Unless the potential health consequences of each part of the text are fully examined and taken into account, and binding language is incorporated in the TPP to safeguard regulatory policy space for health, the TPP could be detrimental to public health nutrition. Health advocates and health-related policymakers must be proactive in their engagement with the trade negotiations.",Not About Sufficiency
Label Transfer for Drug Disease Association in Three Meta-Paths,"The identification of potential interactions and relationships between diseases and drugs is significant in public health care and drug discovery. As we all know, experimenting to determine the drug-disease interactions is very expensive in both time and money. However, there are still many drug-disease associations that are still undiscovered and potential. Therefore, the development of computational methods to explore the relationship between drugs and diseases is very important and essential. Many computational methods for predicting drug-disease associations have been developed based on known interactions to learn potential interactions of unknown drug-disease pairs. In this paper, we propose 3 new main groups of meta-paths based on the heterogeneous biological network of drug-protein-disease objects. For each meta-path, we design a machine learning model, then an integrated learning method is formed by these models. We evaluated our approach on 3 standard datasets which are DrugBank, OMIM, and Gottlieb's dataset. Experimental results demonstrate that the proposed method is better than some recent methods such as EMP-SVD, LRSSL, MBiRW, MPG-DDA, SCMFDD,. . . in some measures such as AUC, AUPR, and F1-score.",Not About Sufficiency
Design of health statistics intelligent education system based on Internet,"This Health statistics is an important basic course for medical students, and it is also an important tool for administration, public health and scientific research. Health statistics teaching plays an important role in training medical students' scientific research thinking mode. However, due to its abstract concept, strong logic, it is difficult to understand and lack of diversified learning materials, the effect of practical application is unsatisfactory. On the other hand, being limited by the face to face teaching method, the high-quality teaching resources has not been effectively expanded and fully utilized. This paper put forward the idea of establishing intelligent education system based on Internet + health statistics, and applied new technologies such as mobile internet and artificial intelligence, auxiliary teaching APP, WeChat public number, and intelligent question bank software, to build a diversified, multidimensional, hierarchical and systematic health statistics knowledge system, which can benefit medical students, medical researchers with various forms of teaching resources and learning support services, promote the standardization and development of health statistics teaching, through the sharing of quality teaching resources.",Not About Sufficiency
Dealing with uncertainty in flood risk management and land use planning decisions: Insights from Aotearoa New Zealand,"Flooding with increasing intensity and frequency is presenting significant challenges for risk management and land use planning in urban areas. This is further exacerbated by uncertainties regarding how flood patterns are changing because of climate change. However, how such uncertainties are considered to inform flood risk management and land use planning decisions can vary largely from place to place and remain unclear in the literature. This paper contributes to this by examining how uncertainty is dealt with in flood risk management and land use planning in Aotearoa New Zealand. Drawing on empirical data at the local level, findings indicate that Aotearoa New Zealand's decision-makers face challenges in considering and communicating uncertainty due to the prevalence of outdated approaches and regulatory constraints, fragmented risk governance, and lack of appropriate understanding of different perceptions and assumptions regarding flood risk between different stakeholders. Based on findings, the paper discusses the critical role of a national-level adaptive flood risk governance in helping to ensure consistency and coherency across different jurisdictions and levels of government, regarding the incorporation of uncertainty into flood risk management and land use planning. This includes the provision of national directives for incorporating uncertainty in decision-making whilst leaving room for innovation and targeted variability at the local level.",Not About Sufficiency
THE ECONOMIC ASSESSMENT OF HEALTH BENEFITS OF ACTIVE TRANSPORT,"Purpose - This chapter addresses the economic assessment of health benefits of active transport and presents most recent valuation studies with an overview of progresses made towards the inclusion of health benefits in the cost-benefit analysis (CBA) of active transport. Methodology/approach - It is built upon the contracted study for the World Health Organization (WHO) on the economic appraisal of health benefits of walking and cycling investments at the city of Viana do Castelo, the former pilot study in Portugal for evaluating the health benefits of non-motorized transport using the WHO Health Economic Assessment Tool (HEAT). The relative risk values adopted in the HEAT for walking refer to adult population of the age group 20 - 74 years and the assessment focus in on average physical activity/regular behaviour of groups of pedestrians and all-cause mortality health impacts. During the case study, it was developed and implemented a mobility survey which aimed to collect behavioural data before and after a street intervention in the historic centre. Findings - Most recent appraisal guidance of walking and cycling and health impact modelling studies reviewed confirm that further research is expected before a more comprehensive appraisal procedure can be adopted in Europe, able to integrate physical activity effects along with other health risks such as those related to road traffic injuries and exposure to air pollution. Social implications - The health benefits assessment of walking investments helped local decision-makers to progress towards sustainable mobility options in the city. Making the population aware of the potential health benefits of regular walking can encourage more people to uptake active transport as part of their daily activities. Originality/value - This study provides a useful review of the health benefits of active transport with a comprehensive analysis of valuation studies, presenting value-added information. It then reports a former assessment of the health effects of active transport in the Portuguese context (case study) using the state-of-the-art economic analysis tool (HEAT) of the World Health Organization which is believed to contribute to a paradigm shift in the transport policy and appraisal practice given the need of shaping future cities (and their citizens) for health through more investments in active transport.",Not About Sufficiency
Problems Encountered Using Fungal Extracts as Test Solutions for Fungal Allergy Diagnosis,"Fungal allergy is a worldwide public health burden, and problems associated with a reliable allergy diagnosis are far from being solved. Especially, the lack of high-quality standardized fungal extracts contributes to the underdiagnosis of fungal allergy. Compared to the manufacturing processes of extracts from other allergen sources, the processes used to manufacture extracts from fungi show the highest variability. The reasons for the high variability are manifold as the starting material, the growth conditions, the protein extraction methods, and the storage conditions all have an influence on the presence and quantity of individual allergens. Despite the vast variety of studies that have analyzed the impact of the different production steps on the allergenicity of fungal allergen extracts, much remains unknown. This review points to the need for further research in the field of fungal allergology, for standardization and for generally accepted guidelines on the preparation of fungal allergen extracts. In particular, the standardization of fungal extracts has been and will continue to be difficult, but it will be crucial for improving allergy diagnosis and therapy.",Not About Sufficiency
Personalizing Large-Scale Assessment in Practice,"The article describes practical suggestions for measurement researchers and psychometricians to respond to calls for social responsibility in assessment. The underlying assumption is that personalizing large-scale assessment improves the chances that assessment and the use of test scores will contribute to equity in education. This article describes a spectrum of standardization and personalization in large-scale assessment. Informed by a review of existing theories, models, and frameworks in the context of current and developing technologies and with a social justice lens, we propose steps to take, as part of assessment research and development, to contribute to the science of personalizing large-scale assessment in technically defensible ways.",Not About Sufficiency
Efficient Intrusion Detection System for SDN Orchestrated Internet of Things,"Internet of Things (IoT) can simply be defined as an extension of the current Internet system. It extends the human to human interconnection and intercommunication scenario of the Internet by including things, to bring anytime, anywhere, and anything communication. A discipline in networking evolving in parallel with IoT is Software Defined Networking (SDN). It is an important technology that is aimed to solve the different problems existing in the traditional network systems. It provides a new convenient home to address the different challenges existing in different network-based systems including IoT. One important security challenge prevailing in such SDN-based IoT (SDIoT) systems is guarantying service availability. The ever-increasing denial of service (DoS) attacks are responsible for such service denials. A centralized signature-based intrusion detection system (IDS) is proposed and developed in this work. Random Forest (RF) classifier is used for training the model. A very popular and recent benchmark dataset, CICIDS2017, has been used for training and validating the machine learning (ML) models. An accuracy result of 99.968% has been achieved by using only 12 features on Wednesday's release of the dataset. This result is higher than the achieved accuracy results of related works considering the original CICIDS2017 dataset. A maximum cross-validated accuracy result of 99.713% has been achieved on the same release of the dataset. These developed models meet the basic requirement of a supervised IDS system developed for smart environments and can effectively be used in different IoT service scenarios.",Not About Sufficiency
A machine learning-based overlay technique for improving the mechanism of road traffic prediction using global positioning system,"Global Positioning System (GPS)-based road traffic prediction is one of the predominating technologies in the modern technological era, which facilitates smooth navigation and reduces mobility time. Various Navigation Maps are used worldwide for traffic congestion and delay prediction, which relies upon the GPS location of the individual's smartphone to predict traffic congestion and delay utilizing stored data and current GPS locations. However, this method sometimes malfunctions due to the uneven distribution of passengers in different vehicle types on the roadway as there are far more passengers in buses as compared with trucks, if few buses are present in the traffic stream then it will show congestion and delay in traffic. Existing mapping techniques possess limits to incorporate the classified vehicle count and categories of vehicles. To mitigate such limitations, this work overlays the information of GPS localization, using existing Maps, with classified vehicle count and vehicle categories to estimate better road traffic congestion and delay. We consider two mid-sized Indian cities in the state of Uttar Pradesh (Varanasi and Gorakhpur) due to the diverse nature of mixed road traffic. For classified vehicle count data, video recording was carried out by using video recording cameras at various sites in both considered cities. Next, different handcrafted features are extracted from the collected traffic volume data prior to the training of the machine learning-based forecasting models (ARIMA and SVM) to predict traffic volume. The classified road traffic vehicle utilizes previously observed values for prediction, thereby helps in making a good decision about route selection and traffic management. Further, this work annotates the forecasted data overlay with GPS value as per the traffic condition to build an XGBoost-based classification model. Finally, the results demonstrated the effectiveness of the proposed technique and highlighted the importance of integrating classified vehicle count and categories of vehicles with GPS. We achieve forecasting accuracy of more than 93% for both ARIMA and SVM forecasting models, followed by more than 95% accuracy of prediction via XGBoost.",Not About Sufficiency
ICT Tools to Foster Small-and-Medium- Enterprise Collaboration in the Energy-Retrofitting Sector,"Since decades, the European Commission has turned the spotlight on energy efficiency in the building sector. While the technological domain has been investigated achieving interesting results, on the organizational and financial sides there is still a lot of room for new advancements. Especially in certain countries, the construction sector has to face many challenges. The highly fragmented markets, the cumbersome organizational models adopted by big enterprises on the one hand and the lack of knowledge and skills of Small and Medium Enterprises (SMEs) on the other hand, the perpetual variability of supporting schemes, and the plethora of regulation frameworks represent huge barriers in leveraging new ways to collaborate. Focused on SMEs, the NewBEE EU-project sheds light on innovative methodologies to set-up new collaborative business models in the energy-retrofitting sector that may accelerate the transition towards more sustainable buildings and cities. SMEs currently face two main problems: (a) the availability of easy-to-access knowledge and (b) the ineffectiveness of existing organizational and business models. To tackle these issues, NewBEE provides a comprehensive ICT platform to foster innovative methodologies facilitating the collaboration of actors in the energy-retrofitting chain, enabling the adoption of the business models. The paper briefly introduces the NewBEE-project approach followed by the description of the core modules of the tools: a. Prompt and accessible information about emerging technologies and business model are collected in the information repository. b. The pre-assessment tool enables buildings' owners to roughly estimate the energy-saving potentials of common renovation processes, receiving in return an order of magnitude of the investment's costs. c. The virtual breeding environment is the main module of the tool: it is where SMEs collaborate, putting in place virtual collaborative networks to make a proposal. d. The financial simulator enables building owners, investors, and SMEs to understand the effects of different financing schemes and the implications of energy-cost variations on the profitability of the investment. e. The energy assessment module provides a professional tool to simulate the building performance before and after the refurbishment process. The application of the NewBEE methodology has been tested in four real business cases: Spain, Slovenia, Germany and Finland. Recommendations raised during the demonstration phase are reported. In a mature market like the building one, the NewBEE project shows how innovative ICT technologies may help SMEs to fine-tune their business model, creating opportunities to collaborate both in a virtual and a real way.",Not About Sufficiency
Overview of Virus Metagenomic Classification Methods and Their Biological Applications,"Metagenomics poses opportunities for clinical and public health virology applications by offering a way to assess complete taxonomic composition of a clinical sample in an unbiased way. However, the techniques required are complicated and analysis standards have yet to develop. This, together with the wealth of different tools and workflows that have been proposed, poses a barrier for new users. We evaluated 49 published computational classification workflows for virus metagenomics in a literature review. To this end, we described the methods of existing workflows by breaking them up into five general steps and assessed their ease-of-use and validation experiments. Performance scores of previous benchmarks were summarized and correlations between methods and performance were investigated. We indicate the potential suitability of the different workflows for (1) time-constrained diagnostics, (2) surveillance and outbreak source tracing, (3) detection of remote homologies (discovery), and (4) biodiversity studies. We provide two decision trees for virologists to help select a workflow for medical or biodiversity studies, as well as directions for future developments in clinical viral metagenomics.",Not About Sufficiency
Promotion of a Sustainable Urban Lifestyle: Three Projects for Belgrade,"A Sustainable Urban Lifestyle refers to individuals and societies that attempt to reduce their carbon footprint by altering methods of transportation (Winter, M. (2007).) and energy consumption. Sustainability is expressed as understanding and meeting present ecological and economic needs without compromising these factors for future generations. The large populations of most Serbian towns and particularly that of Belgrade show a marked lack of understanding of the necessity of changing their living habits and this consequently has a negative impact on the environment. At the same time even professionals and decision makers involved in urban planning are poorly educated about the possibilities that sustainable development has to offer. This paper presents a strategy initiated by an NGO as a first step towards creating a Quality Living Environment, for the general public in Belgrade. A new approach in planning is discussed related to the idea of sustainable development. Research undertaken so far has produced studies, explorations and plans which provide quality material for creating a necessary database, as one of the steps proposed in the methodology for the realization of the Sustainable Urban Life Style. Besides the theoretical background, this paper presents Projects as a part of the Strategy for the Promotion of a Sustainable Urban Lifestyle.",Not About Sufficiency
Modelling Digital Twins as a recursive Multi-Agent Architecture: application to energy management of communicating materials,"In recent years, there has been an increasing interest in intelligent product especially for industrial applications. A special intelligent product was proposed ten years ago, and this paradigm was named ""communicating material"", i.e. materials equipped with micro components able to sense, store and communicate data. Communicating material needs a recursive architecture to be managed with a digital twin. A literature analysis is proposed concerning recursive multi-agent systems and architectures regarding communicating material applicability. The most promising reference architecture was adapted for modelling digital twin of a communicating material under energy constraint. The composition and decomposition mechanisms are detailed to improve energy performance by generating an aggregated product with a global view. An applicative scenario relative to the different aggregation modes between two communicating materials, illustrates the proposed architecture ability. Copyright (C) 2021 The Authors.",Not About Sufficiency
A hybrid machine learning model of depression estimation in home-based older adults: a 7-year follow-up study,"BackgroundOur aim was to explore whether a two-step hybrid machine learning model has the potential to discover the onset of depression in home-based older adults. MethodsDepression data (collected in the year 2011, 2013, 2015 and 2018) of home-based older Chinese (n = 2,548) recruited in the China Health and Retirement Longitudinal Study were included in the current analysis. The long short-term memory network (LSTM) was applied to identify the risk factors of participants in 2015 utilizing the first 2 waves of data. Based on the identified predictors, three ML classification algorithms (i.e., gradient boosting decision tree, support vector machine and random forest) were evaluated with a 10-fold cross-validation procedure and a metric of the area under the receiver operating characteristic curve (AUROC) to estimate the depressive outcome. ResultsTime-varying predictors of the depression were successfully identified by LSTM (mean squared error =0.8). The mean AUCs of the three predictive models had a range from 0.703 to 0.749. Among the prediction variables, self-reported health status, cognition, sleep time, self-reported memory and ADL (activities of daily living) disorder were the top five important variables. ConclusionsA two-step hybrid model based on ""LSTM+ML "" framework can be robust in predicting depression over a 5-year period with easily accessible sociodemographic and health information.",Not About Sufficiency
Dynamic thermal simulation based on building information modeling: A review,"With the emergence of the fourth industrial revolution and extensive promotion of information communication tools, it is on the construction industry to reconsider the current linear design approach and adopt more integrated design approaches that encompass dynamic thermal simulation. Building information modeling (BIM), without a doubt, is a technology, which proposes an integrated practice for building design. Design team using BIM cooperatively work on each activity loop during the design procedure and present a three-dimensional (3D) design model with all data from various disciplines attached at the final stage. The inclusion of a data-rich 3D shape into the design procedure marks the dynamic thermal simulation as a simple, accessible, and, more importantly, decisive stage. This article aims to draw the role of BIM in facilitating the inclusion of dynamic thermal simulation in building design procedures. BIM in this article is introduced as an effective remedy for dealing with the linear nature of conventional building design that is the major impediment to the inclusion of dynamic thermal simulations in the design efforts. This article goes through the conventional method's deficiencies and challenges with the BIM-based method to portray each approach's advantages and disadvantages. Renowned BIM software, interoperable simulation tools, and their distinguishing characteristics are introduced in summary to give a better insight into the application of BIM for dynamic thermal simulation purposes. Additionally, a literature survey on applying the introduced tools is conducted to derive the possible BIM-based mechanisms for thermal performance and comfort analysis. This article concludes that, regardless of interoperability issues, BIM by coordination of building data and information in a consistent framework significantly facilitates data collection procedure for conducting dynamic thermal simulations. In combination with data collection and interoperable simulation tools, BIM tools provide novel mechanisms for dynamic thermal performance and comfort analysis. Highlights Traditional and BIM-based building design procedures have been reviewed. The role of iterative simulation in BIM-based simulation has been elaborated. A BIM-based framework for building design has been provided. BIM-based approaches for thermal performance and comfort analysis have been reviewed.",Not About Sufficiency
Cybersecurity in Water Sector: Stakeholders Perspective,"Cyberattacks on critical infrastructure systems are becoming a significant concern. Sabotage and destruction of critical infrastructures may cause devastating impacts on physical systems, economic security, public health, and safety. The water sector is one of the most critical infrastructures, and as such, identifying and managing cyber threats on the water sector's facilities is crucial to providing a continuous and safe supply of water. This study reports on a stakeholder engagement process conducted in the form of an active workshop that aims to show different stakeholder's perspective on cyber threats, knowledge gaps, and barriers to implementations of cybersecurity procedures and technologies in the water sector. The workshop, demonstrated on the Israeli water and cyber sectors, brought together 45 professionals from the water and cyber sectors (government, academia, utilities, consultants, and commercial suppliers). In the cybersecurity domain, many studies focused on developing algorithms, but only a few considered the organizational and policy aspects of the problem. The present study addresses this gap and presents an example demonstrating the importance of integrating stakeholder engagement into decision making in the water domain. The workshop's findings are summarized and analyzed to highlight top priority activity areas and suggest required actions according to the stakeholders' perspective, which can help shape the landscape of the water sector's cybersecurity.",Not About Sufficiency
Analysis of Hospitalizing Behaviors Based on Big Trajectory Data,"With the improvement of living standards, people pay more attention to health, which is significant to analyze people's hospitalizing behaviors. The wide use of mobile devices generates a great deal of data, which contains a lot of travel information about residents. Many people would like to see a doctor through calling an online car hailing for its convenience. Thus, based on big trajectory data generated by the online car hailing, the hospitalizing behaviors of residents are analyzed in this paper. The hospitalizing behaviors are analyzed from two aspects. One is performed from the temporal aspect, in which the daily numbers of trips of hospitalizing behaviors under different modes are analyzed. The other one is performed from the spatial aspect, in which the hot hospitals, popularity, and gravity distribution of hospitals are analyzed. Based on the spatial analysis, the network constructed by the hot hospitals is also analyzed. The results show that the hospitalizing behavior analysis can reflect the hospitalizing behaviors in detail, which can make contributions to the decision-making of infrastructure configuration for institutions, such as urban planning departments and hospitals.",Not About Sufficiency
Hematoxylin and Eosin Architecture Uncovers Clinically Divergent Niches in Pancreatic Cancer,"Pancreatic ductal adenocarcinoma (PDAC) represents one of the only cancers with an increasing incidence rate and is often associated with intra- and peri-tumoral scarring, referred to as desmoplasia. This scarring is highly heterogeneous in extracellular matrix (ECM) architecture and plays complex roles in both tumor biology and clinical outcomes that are not yet fully understood. Using hematoxylin and eosin (H&E), a routine histological stain utilized in existing clinical workflows, we quantified ECM architecture in 85 patient samples to assess relationships between desmoplastic architecture and clinical outcomes such as survival time and disease recurrence. By utilizing unsupervised machine learning to summarize a latent space across 147 local (e.g., fiber length, solidity) and global (e.g., fiber branching, porosity) H&E-based features, we identified a continuum of histological architectures that were associated with differences in both survival and recurrence. Furthermore, we mapped H&E architectures to a CO-Detection by indEXing (CODEX) reference atlas, revealing localized cell- and protein-based niches associated with outcome-positive versus outcome-negative scarring in the tumor microenvironment. Overall, our study utilizes standard H&E staining to uncover clinically relevant associations between desmoplastic organization and PDAC outcomes, offering a translatable pipeline to support prognostic decision-making and a blueprint of spatial-biological factors for modeling by tissue engineering methods. Impact Statement Pancreatic ductal adenocarcinoma (PDAC) may become the second-leading cause of cancer mortality in the next decade and exhibits scarring with complex, yet unclear, roles in differentiating patient outcomes. We utilized hematoxylin and eosin, a standard histological stain used in clinical workflows, to quantify the extracellular matrix (ECM) architecture of PDAC-associated scarring. Unsupervised machine learning identified a continuum of histological architectures based on divergence in 147 ECM fiber features, which correlated with survival time, disease recurrence, and divergent cellular/protein niches. These findings offer a clinically accessible prognostic signature based on patient-specific histology and suggest spatial-biological targets for tumor modeling.",Not About Sufficiency
"Evolution of urban vitality drivers from 2014 to 2022: a case study of Kunming, China","Urban vitality is a critical indicator of sustainable urban development and long-term growth. This study investigates the spatial-temporal evolution of urban vitality drivers in Kunming, China, across three periods: 2014, 2018, and 2022. Using geospatial data, normalized densities of catering points of interest and nighttime light were analyzed through spatial autocorrelation, standard deviation ellipse, geographic detector, and geographically weighted regression methods to reveal dynamic patterns and mechanisms. Results show that urban vitality exhibited clustering characteristics and directional shifts over time. In 2014, transportation accessibility, including time to bus stops and subway stations, was the dominant factor, emphasizing foundational infrastructure. By 2018, the city's rapid northeastward expansion highlighted parking accessibility and public facilities density as key drivers, reflecting increasing reliance on private vehicles. By 2022, urban vitality rebalanced spatially, with built environment factors such as building density and park accessibility playing a more significant role. The findings highlight the spatial heterogeneity and dynamic interplay among transportation systems, population density, and built environment indicators. While transportation factors declined in influence, built environment and green space accessibility gained prominence over time. This study underscores the necessity for multidimensional, adaptive urban planning to sustain vitality and suggests integrating additional factors, such as environmental quality and social equity, in future research. These insights provide valuable guidance for achieving balanced and sustainable urban development in Kunming and beyond.",Not About Sufficiency
Research on the System of Infrastructure Investment and Financing in Rural Areas,"Rural infrastructure construction has always been the weak link and no system of investing and financing in China. To a great degree, it embarrasses the rural areas be get rid of state of isolated and backward effectively. Recently, China has established economic and social goals as 'building a well-off society in an all-round' and 'accelebrating building a harmonious society'. At present, the critical 'issues concerning agriculture, countryside and famers' and the city has formed the system of urban infrastructure construction with the use of market function and the trend of diversification. We have the proposal that the rural infrastructure system of investment and financing should us the trend of the city.",Not About Sufficiency
Motion vectors and deep neural networks for video camera traps,"Commercial camera traps are usually triggered by a Passive Infra-Red (PIR) motion sensor necessitating a delay between triggering and the image being captured. This often seriously limits the ability to record images of small and fast moving animals. It also results in many ""empty"" images, e.g., owing to moving foliage against a background of different temperature. In this paper we detail a new triggering mechanism based solely on the camera sensor. This is intended for use by citizen scientists and for deployment on an affordable, compact, lowpower Raspberry Pi computer (RPi). Our system introduces a video frame filtering pipeline consisting of movement and image-based processing. This makes use of Machine Learning (ML) feasible on a live camera stream on an RPi. We describe our free and open-source software implementation of the system; introduce a suitable ecology efficiency measure that mediates between specificity and recall; provide ground-truth for a video clip collection from camera traps; and evaluate the effectiveness of our system thoroughly. Overall, our video camera trap turns out to be robust and effective.",Not About Sufficiency
Evolution of Phenotypic and Molecular Drug Susceptibility Testing,"Drug Resistant Tuberculosis (DRTB) is an emerging problem world-wide. In order to control the disease and decrease the number of cases overtime a prompt diagnosis followed by an appropriate treatment should be provided to patients. Phenotypic DST based on liquid automated culture has greatly reduced the time needed to generate reliable data but has the drawback to be expensive and prone to contamination in the absence of appropriate infrastructures. In the past 10 years molecular biology tools have been developed. Those tools target the main mutations responsible for DRTB and are now globally accessible in term of cost and infrastructures needed for the implementation. The dissemination of the Xpert MTB/rif has radically increased the capacity to perform the detection of rifampicin resistant TB cases. One of the main challenges for the large scale implementation of molecular based tests is the emergence of conflicting results between phenotypic and genotypic tests. This mines the confidence of clinicians in the molecular tests and delays the initiation of an appropriate treatment. A new technique is revolutionizing the genotypic approach to DST: the WGS by Next-Generation Sequencing technologies. This methodology promises to become the solution for a rapid access to universal DST, able indeed to overcome the limitations of the current phenotypic and genotypic assays. Today the use of the generated information is still challenging in decentralized facilities due to the lack of automation for sample processing and standardization in the analysis. The growing knowledge of the molecular mechanisms at the basis of drug resistance and the introduction of high-performing user-friendly tools at peripheral level should allow the very much needed accurate diagnosis of DRTB in the near future.",Not About Sufficiency
Olive Oils Classification via Laser-Induced Breakdown Spectroscopy,"The classification of olive oils and the authentication of their geographic origin are important issues for public health and for the olive oil market and related industry. The development of fast, easy to use, suitable for on-line, in-situ and remote operation techniques for olive oils classification is of high interest. In the present work, 36 olive oils from different places in Crete, Greece, are studied using a laser-based technique, Laser-Induced Breakdown Spectroscopy (LIBS), assisted by machine learning algorithms, aiming to classify them in terms of their geographical origin. The excellent classification results obtained demonstrate the great potential of LIBS, which is further extended by the use of machine learning.",Not About Sufficiency
Exporting labour services and market access commitments under GATS in the World Trade Organization - An analysis from the perspective of developing countries,,Not About Sufficiency
A scoping review and index of body stimuli in psychological science,"Naturalistic body stimuli are necessary for understanding many aspects of human psychology, yet there are no centralized databases of body stimuli. Furthermore, there are a high number of independently developed stimulus sets lacking in standardization and reproducibility potential, and a general lack of organization, contributing to issues of both replicability and generalizability in body-related research. We conducted a comprehensive scoping review to index and explore existing naturalistic whole-body stimuli. Our research questions were as follows: (1) What sets of naturalistic human whole-body stimuli are present in the literature? And (2) On what factors (e.g., demographics, emotion expression) do these stimuli vary? To be included, stimulus sets had to (1) include human bodies as stimuli; (2) be photographs, videos, or other depictions of real human bodies (not computer generated, drawn, etc.); (3) include the whole body (defined as torso, arms, and legs); and (4) could include edited images, but still had to be recognizable as human bodies. We identified a relatively large number of existing stimulus sets (N = 79) which offered relative variability in terms of main manipulated factors and the degree of visual information included (i.e., inclusion of heads and/or faces). However, stimulus sets were demographically homogenous, skewed towards White, young adult, and female bodies. We identified significant issues in reporting and availability practices, posing a challenge to the generalizability, reliability, and reproducibility of body-related research. Accordingly, we urge researchers to adopt transparent and accessible practices and to take steps to diversify body stimuli.",Not About Sufficiency
Examining Smart Neighborhood Platforms: A Qualitative Exploration of Features and Applications,"Smart neighborhood platforms have emerged as innovative solutions to address the challenges of urbanization and as smart living approaches to create sustainable, interconnected communities. This study aims to provide a qualitative exploration of multiple smart neighborhood platform initiatives in the German speaking region. The research examines the diverse characteristics, features and capabilities offered by these platforms, as well as their potential application in various aspects of smart community living. Qualitative research methods are employed to identify key parameters. Additionally, the study investigates the practical application of these platforms, ranging from optimizing resource consumption and improving quality of life to enhancing social interactions and fostering social cohesion and sustainable behaviors. The findings contribute to a deeper understanding of the design and applications of smart neighborhood platforms, providing valuable insights for further research as well as urban planners, policymakers, and technology developers seeking to create smarter and more livable communities.",Not About Sufficiency
Differentiating intentions and competence: Exploring living environments and health outcomes among older adults in diverse residential (im-) mobility scenarios,"The growing mobility and academic emphasis have challenged the traditional link between remaining in one place and successful aging, emphasizing dwelling conditions that fit older adults and their preferences for relocating or staying. Using 5034 older-participant data in the CFPS 2012/2014 waves, we categorized them into six categories based on their intended or unintended (im-)mobility. Descriptive statistics revealed diverse housing and community characteristics and their health consequences. Logistic regression and paired-sample Wilcoxon signed-rank tests examined health changes across categories, while panel-ordered logit models explored the influence of housing and community features on health. The results indicate no significant correlation between (im-)mobility types and health outcomes, although certain subgroups exhibit meaningful relationships. Intended immobility seems to provide more psychological benefits than substantive health advantages. Whether intentional or not, relocation often leads to improved housing conditions, better aligning them with physical needs. The study also outlines the (im-)mobility process among older adults, from intention and competence to action, and establishes an intention/competence model for their (im-)mobility. Enhanced comprehension in this field would be beneficial for reassessing whether older individuals should stay in place or relocate, and for emphasizing the significance of tailored strategies for the living arrangements of the older population.",Not About Sufficiency
Intra-topic latency as an automated behavioral marker of treatment response in autism spectrum disorder,"Data science advances in behavioral signal processing and machine learning hold the promise to automatically quantify clinically meaningful behaviors that can be applied to a large amount of data. The objective of this study was to identify an automated behavioral marker of treatment response in social communication in children with autism spectrum disorder (ASD). First, using an automated computational method, we successfully derived the amount of time it took for a child with ASD and an adult social partner (N pairs = 210) to respond to each other while they were engaged in conversation bits (""latency"") using recordings of brief, natural social interactions. Then, we measured changes in latency at pre- and post-interventions. Children with ASD who were receiving interventions showed significantly larger reduction in latency compared to those who were not receiving interventions. There was also a significant group difference in the changes in latency for adult social partners. Results suggest that the automated measure of latency derived from natural social interactions is a scalable and objective method to quantify treatment response in children with ASD.",Not About Sufficiency
Study on Parameter Inversion Model Construction and Evaluation Method of UAV Hyperspectral Urban Inland Water Pollution Dynamic Monitoring,"The problem of environmental water pollution is becoming increasingly important. Inland rivers and lakes form interconnected water networks with fragile water ecosystems, and urban water pollution problems occur frequently. Chemical oxygen demand (COD), dissolved oxygen (DO), total phosphorus (TP), total nitrogen (TN), and ammonia nitrogen (NH3-N) in inland rivers are important indicators to evaluate water health quality. Timely and accurate reflection of dynamic changes to the key indices of urban river health status are of vital practical significance to adjust water treatment policy and ensure the stability of the aquatic environment and people's health. This study used COD, DO, TP, TN and NH3-N as typical water quality parameters for a reservoir in Guangxi Province, China and established a set of standardized processes covering UAV hyperspectral sampling and ground spectral correction, spectral data preprocessing, and modeling. In combination with machine learning and statistical analysis, an inversion method for measuring urban inland water pollution from UAV hyperspectral imaging with different dynamic monitoring parameters was proposed. And we compared the different combinations of preprocessing algorithm-regression algorithm and dimensionality reduction algorithm to get a unified model for quantitative estimation of water quality parameter concentration. We evaluated the performance of the proposed model according to root mean square error (RMSE), mean absolute error (MAE), mean absolute percentage error (MAPE), and coefficient of determination (R2). The experimental results showed that our model was superior to other algorithms in RMSE, MAE, MAPE, and R2. The MAPE of this model ranged from 0.01 to 0.12 and R2 ranged from 0.84 to 0.98 in all water quality parameters. In general, this study provides an effective tool for decision-makers to investigate the source and physical mechanism of water pollution and establish a graded water quality evaluation model.",Not About Sufficiency
Detecting risk level in individuals misusing fentanyl utilizing posts from an online community on Reddit,"Introduction: Opioid misuse is a public health crisis in the US, and misuse of synthetic opioids such as fentanyl have driven the most recent waves of opioid-related deaths. Because those who misuse fentanyl are often a hidden and high-risk group, innovative methods for identifying individuals at risk for fentanyl misuse are needed. Machine learning has been used in the past to investigate discussions surrounding substance use on Reddit, and this study leverages similar techniques to identify risky content from discussions of fentanyl on this platform. Methods: A codebook was developed by clinical domain experts with 12 categories indicative of fentanyl misuse risk, and this was used to manually label 391 Reddit posts and comments. Using this data, we built machine learning classification models to identify fentanyl risk. Results: Our machine learning risk model was able to detect posts or comments labeled as risky by our clinical experts with 76% accuracy and 76% sensitivity. Furthermore, we provide a vocabulary of community-specific, colloquial words for fentanyl and its analogues. Discussion: This study uses an interdisciplinary approach leveraging machine learning techniques and clinical domain expertise to automatically detect risky discourse, which may elicit and benefit from timely intervention. Moreover, our vocabulary of online terms for fentanyl and its analogues expands our understanding of online ""street"" nomenclature for opiates. Through an improved understanding of substance misuse risk factors, these findings allow for identification of risk concepts among those misusing fentanyl to inform outreach and intervention strategies tailored to this at-risk group.",Not About Sufficiency
Urban development and pedestrian thermal comfort in Melbourne,"In October 2013, ""Plan Melbourne"" was released by the Victorian government to outline the vision for Melbourne's growth to the year 2050. The City of Melbourne's draft municipal strategic statement identified ""City North"" as a great urban renewal area that can accommodate a significant part of the growth. Structure plans provide guidance to the community, planners, business, government and developers about the appropriate directions and opportunities for future changes in City North. Proposing street hierarchy, increasing the building heights, expanding the urban forest by increasing tree canopy coverage, implementing green roofs and overall transition from a low-rise to medium rise urban area are some of the strategies presented in structural plans. This study investigates the effect of future structural plans presented in ""Plan Melbourne"" on pedestrian thermal comfort in City North for extreme hot summer days. A three-dimensional microclimatic modelling tool ENVI-met 3.1 was used to evaluate the outdoor human thermal environment for the existing and future scenarios proposed by the Victorian government. Field measurements were carried out to validate ENVI-met and examine its ability in addressing the research objectives. Structural plans were modelled t in three stages; increased building height, adding tree canopy coverage and adding green roofs. The study showed that deeper canyons, higher aspect ratios and lower sky view factors in future scenario contribute to lower level of mean radiant temperatures (42 degrees C-64 degrees C), compared to the existing scenario (49 degrees C-60 degrees C). Physiological equivalent temperature (PET) was improved by 1 degrees C-4 degrees C as a result of ""Increased building height"" scenario. Increasing the tree canopy coverage caused 1 degrees C-2 degrees C reduction on PET level and adding a green roof did not show any improvement on PET at pedestrian level. Although the study showed a slight improvement in PET after implementing future structural plans, it was necessary to further improve PET level, particularly during certain hours of the day. Therefore long term planning strategies (integrating public realms with small urban parks and increasing the tree canopy coverage from 40% to 50%) were proposed and modelled to examine their effectiveness in further improving thermal comfort in an extremely hot summer day. Implementing future structural plans and proposed scenarios together resulted in 5.1 degrees C improvement in the PET in an extremely hot summer day. The study also indicated that aspect ratio (HAW) is the most efficient strategy in decreasing Tmrt and PET during the day. Integrating climatic knowledge into planning practices in Melbourne metropolitan area would lead to less vulnerability to the extreme heat events and reduce the adverse impacts of increased air temperature on public health. (C) 2017 Elsevier Ltd. All rights reserved.",Not About Sufficiency
Bayesian Deep Decline Curve Analysis: A New Approach for Well Oil Production Modeling and Forecasting,"Following the rapid growth of unconventional resources, many models and methods have been proposed for forecasting the performances of unconventional wells. Several studies have attempted to use machine learning (ML) for improving the forecasting. However, owing to limitations of ML in regard to long-term forecasts (e.g., the occurrence of unphysical results), most of these ML forecasts are not satisfactory. In this work, we propose, demonstrate, and discuss a new ML approach able to rapidly provide probabilistic, long-term forecasts of oil production rates from individual wells in a decline curve analysis (DCA) manner. The novelties of the proposed approach are as follows: (1) it combines an automated ML (AutoML) method for supervised learning and a Bayesian neural ordinary differential equation (BN-ODE) framework for time-series modeling; (2) it uses the DCA model to inform the B-ODE framework of ""physics"" and regulate the BN-ODE forecasts; and (3) several completion parameters (such as locations, lengths, and slickwater volume) of individual wells are analyzed and included as the inputs of model building, in addition to measured oil production rate data. Specifically, AutoML method is first used to model the relationship between the well location, completion parameters, and the DCAs parameters, and the BN-ODE framework is then used to model the relationship between the DCAs parameters and the time--series oil production rates. A publicly accessible data set, consisting of completion parameters and oil production rates, of 396 horizontal wells in the Bakken Shale Formation is used to train and test the model of the proposed approach. The results lead to the conclusion that the proposed approach is practical for providing probabilistic, long--term forecasts of oil production from individual wells, given data of existing wells in the reservoir.",Not About Sufficiency
A voting gray wolf optimizer-based ensemble learning models for intrusion detection in the Internet of Things,"The Internet of Things (IoT) has garnered considerable attention from academic and industrial circles as a pivotal technology in recent years. The escalation of security risks is observed to be associated with the growing interest in IoT applications. Intrusion detection systems (IDS) have been devised as viable instruments for identifying and averting malicious actions in this context. Several techniques described in academic papers are thought to be very accurate, but they cannot be used in the real world because the datasets used to build and test the models do not accurately reflect and simulate the IoT network. Existing methods, on the other hand, deal with these issues, but they are not good enough for commercial use because of their lack of precision, low detection rate, receiver operating characteristic (ROC), and false acceptance rate (FAR). The effectiveness of these solutions is predominantly dependent on individual learners and is consequently influenced by the inherent limitations of each learning algorithm. This study introduces a new approach for detecting intrusion attacks in an IoT network, which involves the use of an ensemble learning technique based on gray wolf optimizer (GWO). The novelty of this study lies in the proposed voting gray wolf optimizer (GWO) ensemble model, which incorporates two crucial components: a traffic analyzer and a classification phase engine. The model employs a voting technique to combine the probability averages of the base learners. Secondly, the combination of feature selection and feature extraction techniques is to reduce dimensionality. Thirdly, the utilization of GWO is employed to optimize the parameters of ensemble models. Similarly, the approach employs the most authentic intrusion detection datasets that are accessible and amalgamates multiple learners to generate ensemble learners. The hybridization of information gain (IG) and principal component analysis (PCA) was employed to reduce dimensionality. The study utilized a novel GWO ensemble learning approach that incorporated a decision tree, random forest, K-nearest neighbor, and multilayer perceptron for classification. To evaluate the efficacy of the proposed model, two authentic datasets, namely, BoT-IoT and UNSW-NB15, were scrutinized. The GWO-optimized ensemble model demonstrates superior accuracy when compared to other machine learning-based and deep learning models. Specifically, the model achieves an accuracy rate of 99.98%, a DR of 99.97%, a precision rate of 99.94%, an ROC rate of 99.99%, and an FAR rate of 1.30 on the BoT-IoT dataset. According to the experimental results, the proposed ensemble model optimized by GWO achieved an accuracy of 100%, a DR of 99.9%, a precision of 99.59%, an ROC of 99.40%, and an FAR of 1.5 when tested on the UNSW-NB15 dataset.",Not About Sufficiency
Strabismus Detection in Monocular Eye Images for Telemedicine Applications,"This study presents a novel method for the early detection of strabismus, a common eye misalignment disorder, with an emphasis on its application in telemedicine. The technique leverages synchronized eye movements to estimate the pupil location of one eye based on the other, achieving close alignment in non-strabismic cases. Regression models for each eye are developed using advanced machine learning algorithms, and significant discrepancies between estimated and actual pupil positions indicate the presence of strabismus. This approach provides a non-invasive, efficient solution for early detection and bridges the gap between basic research and clinical care by offering an accessible, machine learning-based tool that facilitates timely intervention and improved outcomes in diverse healthcare settings. The potential for pediatric screening is discussed as a possible direction for future research.",Not About Sufficiency
"A mixed methods study to inform fatal overdose prevention in San Diego, California: Perspectives from people who use drugs","Background: In the United States, community overdose education and naloxone distribution (OEND) programs have demonstrated efficacy in reducing opioid-related mortality. OEND programs have expanded across San Diego County, California, but differential naloxone accessibility among people who use drugs (PWUD) has not been assessed. We examined factors that shape individual naloxone accessibility in San Diego. Methods: We employed a convergent parallel mixed methods design using surveys (n = 194) and qualitative interviews (n = 20). Ordinal logistic regression examined factors associated with individual naloxone accessibility (i.e., the frequency with which participants could access naloxone within five minutes, categorized as never, sometimes, or always). Qualitative interviews explored participant perceptions of naloxone accessibility and whether and how they maintained naloxone. We organized multilevel findings into a modified social-ecological model. Results: In quantitative and qualitative samples, participants were majority male (72 % and 70 % respectively), non-White race/ethnicity (55 % and 75 %), with an average age around 42 years. In the quantitative sample, 24 % never had personally accessible naloxone, 52 % sometimes did, and 24 % always did. Factors independently associated with individual naloxone accessibility were female gender (Adjusted Odds Ratio [AdjOR]: 2.51, 95 % Confidence Interval [CI]: 1.31-4.85), monthly income <$500 (AdjOR: 0.42, 95 %CI:0.19, 0.90), witnessing an overdose (AdjOR: 3.51, 95 %CI:1.67-7.55), and knowing where to get free naloxone (AdjOR: 3.44, 95 %CI: 1.79-6.75). Qualitative data suggested that naloxone was generally easy to acquire in San Diego due to community harm reduction outreach and mutual aid among peers, albeit community barriers including distance to harm reduction providers and frequent relocation/displacement for those experiencing homelessness. Individual attitudes toward overdose risk, naloxone, and community responsibility contributed to varied individual naloxone accessibility. Conclusions: This study highlights multilevel factors influencing individual naloxone accessibility among people who use drugs in San Diego, emphasizing the importance of harm reduction outreach and peer-to-peer support. We identified opportunities for interventions that address both individual attitudes and community-level barriers to improve naloxone accessibility.",Not About Sufficiency
Urban greenery distribution and its link to social vulnerability,"Urban greenery plays a pivotal role in urban environments, impacting the environmental well-being and people's comfort. Several studies have demonstrated a strong link between urban greenery and socioeconomic status but still lack an analysis of greenery on uneven distribution in social vulnerability. This study assesses how multilevel greenery rates distribute and associate the social vulnerability of people in 429 census tracts in the Seattle metropolitan area. It integrates multi-source urban informatics data, including remote sensing data and street view imagery, to identify various vegetation types. Then, it uses the interpretable machine learning model to explore the relationship between street-level green space distribution and community vulnerability. The results show a serious problem of uneven distribution of green spaces in urban centers since urban areas are built up and fragmented the landscape. Areas with low urban greening in the Seattle area have higher rates of poverty, unemployment, racial segregation, and housing overcrowding. Besides, greening features like street green views, which are more related to human perception, have a great association with social vulnerability. These findings contribute to the urban green spaces to better promote community equity and vulnerability.",Not About Sufficiency
Spatiotemporal Variation Assessment and Improved Prediction Of Cyanobacteria Blooms in Lakes Using Improved Machine Learning Model Based on Multivariate Data,"Cyanobacterial blooms in shallow lakes pose a significant threat to aquatic ecosystems and public health worldwide, highlighting the urgent need for advanced predictive methodologies. As impounded lakes along the Eastern Route of the South-to-North Water Diversion Project, Lakes Hongze and Luoma play a key role in water resource management, making the prediction of cyanobacterial blooms in these lakes particularly important. To address this, satellite remote sensing data were utilized to analyze the spatiotemporal dynamics of cyanobacterial blooms in these lakes. Subsequently, a precise machine learning model, integrating the Projection Pursuit Model and Random Forest (PP-RF) algorithms, was developed to predict the extent of cyanobacterial blooms, considering a range of influencing factors, including physical, chemical, climatic, and hydrologic variables. The findings indicated pronounced seasonal fluctuations in cyanobacterial blooms, with higher levels in summer than in other seasons. Key determinants for cyanobacterial blooms prediction included solar radiation, temperature and total nitrogen for Lake Hongze, while for Lake Luoma, significant predictors were identified as temperature, water temperature, and solar radiation. Compared with traditional data preprocessing methods, PP-RF model has advantages in addressing multicollinearity. This study provides a feasible method for predicting cyanobacterial blooms in impounded lakes within inter-basin water transfer projects. By inputting region-specific data, this model could be applied broadly, contributing to against the adverse effects of cyanobacterial blooms and provide scientific guidance for the protection and management of aquatic ecosystems.",Not About Sufficiency
A MULTIDISCIPLINARY ANALYSIS OF STANDARDIZATION IN PUBLIC HEALTH FACILITY ARCHITECTURE IN TURKEY,"Health facilities are highly complex structures in terms of their design, construction and management processes. A great number of multi-disciplinary approaches and applications are required in the sustaining of these processes in order to compensate with this complexity. In this context, being also related with the fast developing character of health services all over the world, health facility design and construction criteria change continuously with all their sides including their architectural aspects. Accordingly, health facility architecture has also witnessed a transformation period in Turkey especially in the last ten years together with the essential developments in other sides of health services in the country. It was basically related with the changing politics of the state about the issue with all its sides. New design criteria and technologies came into effect in the construction stages of health facilities within the framework of the Transformation in Health Project carried out by the Ministry of Health. In this process, the Ministry aimed to arrange the field of health facility construction works for public by bringing some rules and standards despite the existence of some organizational and technical problems. Accordingly, the standards and arrangements related with health facility constructions which were implemented by the Ministry of Health will generally be examined in this study with a great emphasis on the architectural standards issue. This analysis is connectedly made with an examination of the approaches and concrete applications of the Ministry of Health so as to have a contextual point of view to the realization of these standards in health facility constructions. Consequently, the basic aim of this study is to express the multidisciplinary development of standardization related with public health facility architecture and constructions of the last 10 years in Turkey. In this framework, the public health facilities of this period being in project or construction stage will be taken into consideration in this study.",Not About Sufficiency
"Optimizing the environmental design and management of public green spaces: Analyzing urban infrastructure and long-term user experience with a focus on streetlight density in the city of Las Vegas, NV","In Las Vegas and many other desert cities, the unique climatic conditions, marked by high daytime temperatures, naturally encourage residents to seek outdoor recreational activities during the cooler evening hours. However, the approach to streetlight management has been less than optimal, leading to inadequate illumination in public parks after dark. This lack of proper lighting compromises not only the safety but also the enjoyment opportunity of these spaces during the night, a time when they could offer a much-needed respite during summer heat. Recent scholarship has highlighted the deterrence of park usage due to poor design of the street lighting, pointing to a broader issue in urban planning that requires attention to adapt infrastructures to local climates for the benefit of public health and well-being. This study seeks to contribute to the existing scholarship on park lighting by utilizing diverse data sources and creating longitudinal measures to examine how population behaviors in urban parks vary over time in different locations. It seeks to explore the impact of park users' demographics, particularly variations across race and income levels, and the density of street lighting on the nighttime usage of public green spaces by using the time fixed effect method. It aims to understand how demographic diversity among park users and the physical environment, specifically street lighting density, influences patterns of nighttime activities in public parks. Using this analysis, we develop an improved predictive model for determining the density of street lighting in public green spaces by comparing multiple types of machine learning models. This model will consider the demographic diversity of users and the observed patterns of nighttime usage, with the goal of enhancing accessibility, safety, and utilization of these spaces during nighttime hours. The significance of this research contributes to the broader objective of creating resilient, healthy, and inclusive cities that cater to the well-being of their residents.",Not About Sufficiency
A Fraudster in a Haystack: Crafting a Classifier for Non-delivery Fraud Prediction at Online Auction Sites,"Non-delivery fraud is a recurring problem at online auction sites: false sellers that list inexistent products just to receive payments and disappear, possibly repeating the swindle with another identity. The high transaction volume of these sites calls for the use of machine learning techniques in fraud prediction systems, at least for the identification of suspect sellers which deserve further expert analysis. In our work we identified a set of features related to listings, sellers and product categories, and built a system for fraud prediction taking into account the high class imbalance of real data, since fraud is a relatively rare event. The identified features are all based on publically accessible data, opening the possibility of developing fraud prediction systems independent of site operators. We tested the proposed system with data collected from a major online auction site, obtaining encouraging results on identification of fraudsters before they strike, while keeping the number of false positives low.",Not About Sufficiency
Socio-Economic Feasibility Analysis for Sustainable Mass Rapid Transit Project in Western India,"Sustainable public transport systems may be achieved by adopting electric bus locomotion. The problem being addressed by this research is the development and case application of a computation methodology of the social benefit cost ratio of an electric bus transport project in India and identification of whether it can significantly favour the situation of environment friendly transport. The underlying theory behind this approach is if the environmental and social dimensions of an infrastructure project are considered in addition to the financial dimensions for the purpose of project appraisal, a holistic evaluation can be achieved and such an evaluation can give an edge to the approval of environmentally friendly projects. The evaluation has been performed using the present worth analysis of various types of benefits and costs associated with the implementation of the electric bus rapid transit system in a city. The variables considered in the researched methodology are benefits which are revenue, savings in vehicle operating costs (VOC), environmental benefits, savings in travel time, reduction in accidents and non-consumption of fossil fuel, and costs which are infrastructure investment costs, cost of the bus fleet, maintenance cost, replacement costs, cost of system operation and maintenance and additional electric power generation. The outcome as indicated by the value of the social benefit-cost ratio (SBCR) illustrates that such projects can be positively justified from point of view of the benefits gained by the society as well as fruitful returns and value addition of infrastructure investment in the long run. The research contributes by validating that social benefit-cost analysis (SBCA) can be used for the evaluation of sustainable transport system appraisals in order to make their realisation more favourable.",Not About Sufficiency
Topic modeling and social network analysis approach to explore diabetes discourse on Twitter in India,"Introduction The utilization of social media presents a promising avenue for the prevention and management of diabetes. To effectively cater to the diabetes-related knowledge, support, and intervention needs of the community, it is imperative to attain a deeper understanding of the extent and content of discussions pertaining to this health issue. This study aims to assess and compare various topic modeling techniques to determine the most effective model for identifying the core themes in diabetes-related tweets, the sources responsible for disseminating this information, the reach of these themes, and the influential individuals within the Twitter community in India.Methods Twitter messages from India, dated between 7 November 2022 and 28 February 2023, were collected using the Twitter API. The unsupervised machine learning topic models, namely, Latent Dirichlet Allocation (LDA), non-negative matrix factorization (NMF), BERTopic, and Top2Vec, were compared, and the best-performing model was used to identify common diabetes-related topics. Influential users were identified through social network analysis.Results The NMF model outperformed the LDA model, whereas BERTopic performed better than Top2Vec. Diabetes-related conversations revolved around eight topics, namely, promotion, management, drug and personal story, consequences, risk factors and research, raising awareness and providing support, diet, and opinion and lifestyle changes. The influential nodes identified were mainly health professionals and healthcare organizations.Discussion The study identified important topics of discussion along with health professionals and healthcare organizations involved in sharing diabetes-related information with the public. Collaborations among influential healthcare organizations, health professionals, and the government can foster awareness and prevent noncommunicable diseases.",Not About Sufficiency
"Integrating machine learning and network analytics to model project cost, time and quality performance","This study aims to connect project management, network science and machine learning in an accessible overview applied to a real original dataset. Based on an initial literature review of applicable project performance measures and attributes, relevant project data were collected through an online survey. The information was split into three categories, including the basic project measures (five attributes), project stakeholder network measures (seven attributes), and project complexity measures (seven attributes). In total, 70 responses were collected, and five machine learning approaches (i.e. support vector machine, logistic regression, k-nearest neighbour, random forest and extreme gradient boosting) were applied to model the relationships between project attributes, networks and the Iron Triangle of project cost, time and quality. The results confirm the expected trends affecting project performance and provide an example for the discussion of the applicability of integrated machine learning and network analytics approaches to modelling project performance. The article demonstrates in an accessible way a real case of integration of machine learning, network science and project management and suggests avenues for further research and applications in practice.",Not About Sufficiency
Investigating the Power of LSTM-Based Models in Solar Energy Forecasting,"Solar is a significant renewable energy source. Solar energy can provide for the world's energy needs while minimizing global warming from traditional sources. Forecasting the output of renewable energy has a considerable impact on decisions about the operation and management of power systems. It is crucial to accurately forecast the output of renewable energy sources in order to assure grid dependability and sustainability and to reduce the risk and expense of energy markets and systems. Recent advancements in long short-term memory (LSTM) have attracted researchers to the model, and its promising potential is reflected in the method's richness and the growing number of papers about it. To facilitate further research and development in this area, this paper investigates LSTM models for forecasting solar energy by using time-series data. The paper is divided into two parts: (1) independent LSTM models and (2) hybrid models that incorporate LSTM as another type of technique. The Root mean square error (RMSE) and other error metrics are used as the representative evaluation metrics for comparing the accuracy of the selected methods. According to empirical studies, the two types of models (independent LSTM and hybrid) have distinct advantages and disadvantages depending on the scenario. For instance, LSTM outperforms the other standalone models, but hybrid models generally outperform standalone models despite their longer data training time requirement. The most notable discovery is the better suitability of LSTM as a predictive model to forecast the amount of solar radiation and photovoltaic power compared with other conventional machine learning methods.",Not About Sufficiency
"Predicting the potential nationwide distribution of the snail vector, Oncomelania hupensis quadrasi, in the Philippines using the MaxEnt algorithm","Schistosomiasis remains a major public health concern affecting approximately 12 million people in the Philippines due to inadequate information about the disease and limited prevention and control efforts. Schistosoma japonicum, one of the causative agents of the disease, requires an amphibious snail Oncomelania hupensis quadrasi (O. h. quadrasi) to complete its life cycle. Using the geographical information system (GIS) and maximum entropy (MaxEnt) algorithm, this study aims to predict the potential high-risk habitats of O. h. quadrasi driven by environmental factors in the Philippines. Based on the bioclimatic determinants, a very high-performance model was generated (AUC = 0.907), with the mean temperature of the driest quarter (25.3%) contributing significantly to the prevalence of O. h. quadrasi. Also, the snail vector has a high focal distribution, preferring areas with a pronounced wet season and high precipitation throughout the year. However, the findings provided evidence for snail adaptation to different environmental conditions. High suitability of snail habitats was found in Quezon, Camarines Norte, Camarines Sur, Albay, Sorsogon, Northern Samar, Eastern Samar, Leyte, Bohol, Surigao del Norte, Surigao del Sur, Agusan del Norte, Davao del Norte, North Cotabato, Lanao del Norte, Misamis Occidental, and Zamboanga del Sur. Furthermore, snail habitat establishment includes natural and man-made waterlogged areas, with the progression of global warming and climate change predicted to be drivers of increasing schistosomiasis transmission zones in the country.",Not About Sufficiency
ncRDense: A novel computational approach for classification of non-coding RNA family by deep learning,"With the rapidly growing importance of biological research, non-coding RNAs (ncRNA) attract more attention in biology and bioinformatics. They play vital roles in biological processes such as transcription and translation. Classification of ncRNAs is essential to our understanding of disease mechanisms and treatment design. Many approaches to ncRNA classification have been developed, several of which use machine learning and deep learning. In this paper, we construct a novel deep learning-based architecture, ncRDense, to effectively classify and distinguish ncRNA families. In a comparative study, our model produces comparable results with existing state-of-the-art methods. Finally, we built a freely accessible web server for the ncRDense tool, which is available at http://nsclbio.jbnu.ac.kr/tools/ncRDense/.",Not About Sufficiency
Sentence-BERT Distinguishes Good and Bad Essays in Cross-prompt Automated Essay Scoring,"Automated Essay Scoring (AES) refers to a set of processes that automatically assigns grades to student-written essays with machine learning models. Existing AES models are mostly trained prompt-specifically with supervised learning, which requires the essay prompt to be accessible to the system vendor at the time of model training. However, essay prompts for high-stakes testing should usually be kept confidential before the test date, which demands the model to be cross-promptly trainable with pre-scored essay data already in hands. Document embeddings obtained from pretrained language models such as Sentence-BERT (SBERT) are primarily expected to represent the semantic content of the text. We hypothesize SBERT embeddings also contain assessment-relevant elements that are extractable by document embedding decomposition through Principal Component Analysis (PCA) enhanced with Normalized Discounted Cumulative Gain (nDCG) measurement. The identified evaluative elements in the entire embedding space of the source essays are then cross-promptly transferred to the target essays written on different prompts for binary clustering task of dividing high/low-scored groups. The result implies non-finetuned SBERT already contains evaluative elements to distinguish good and bad essays.",Not About Sufficiency
Current status of DNA methylation profiling in neuro-oncology as a diagnostic support tool: A review,"Over the last 2 decades, high throughput genome-wide molecular profiling has revealed characteristic genetic and epigenetic alterations associated with different types of central nervous system (CNS) tumors. DNA methylation profiling has emerged as an important molecular platform for CNS tumor classification with improved diagnostic accuracy and patient risk stratification in comparison to the standard of care histopathological analysis and any single molecular tests. The emergence of DNA methylation arrays have also played a crucial role in refining existing types and the discovery of new tumor types or subtypes. The adoption of methylation data into neuro-oncology has been greatly aided by the development of a freely accessible machine learning-based classifier. In this review, we discuss methylation workflow, address the utility of DNA methylation profiling in CNS tumors in a routine diagnostic setting, and provide an overview of the methylation-based tumor types and new types or subtypes identified with this platform.",Not About Sufficiency
DISTRIBUTION OF CARBON STORAGE AND POTENTIAL STRATEGIES TO ENHANCE CARBON SEQUESTRATION CAPACITY IN SINGAPORE,"The expansion of urbanization leads to significant changes in land use, consequently affecting carbon storage. This research aims to investigate the carbon loss due to land use alterations and proposes strategies for mitigation. Utilizing existing land use data from 2017 and 2022, along with simulated data for 2025 generated by an ANN model and Cellular Automata, we identified changes in land use. These changes were then correlated with variations in carbon storage, both gains and losses. Our findings reveal a significant loss of 36,859 metric tons of carbon storage from 2017 to 2022. The projection for 2025 estimates a further reduction, reaching a total loss of 83,409 metric tons. By employing the LISA method, we identified that low-carbon storage zones are concentrated in the southeast region of the research site. By overlaying these zones with areas of carbon storage loss, we pinpointed regions severely affected by carbon depletion. Consequently, we propose that mitigation strategies should be imperatively implemented in these identified areas to counteract the trend of carbon storage loss. This approach offers urban planners a solution to identify areas experiencing carbon storage decline. Moreover, our research methodology provides a novel framework for scholars studying similar carbon issues.",Not About Sufficiency
Aberrant splicing prediction across human tissues,"AbSplice predicts aberrant splicing for 50 human tissues by integrating sequence-based deep learning models, DNA variation and RNA-seq obtained from accessible tissues. Aberrant splicing is a major cause of genetic disorders but its direct detection in transcriptomes is limited to clinically accessible tissues such as skin or body fluids. While DNA-based machine learning models can prioritize rare variants for affecting splicing, their performance in predicting tissue-specific aberrant splicing remains unassessed. Here we generated an aberrant splicing benchmark dataset, spanning over 8.8 million rare variants in 49 human tissues from the Genotype-Tissue Expression (GTEx) dataset. At 20% recall, state-of-the-art DNA-based models achieve maximum 12% precision. By mapping and quantifying tissue-specific splice site usage transcriptome-wide and modeling isoform competition, we increased precision by threefold at the same recall. Integrating RNA-sequencing data of clinically accessible tissues into our model, AbSplice, brought precision to 60%. These results, replicated in two independent cohorts, substantially contribute to noncoding loss-of-function variant identification and to genetic diagnostics design and analytics.",Not About Sufficiency
Automatic Object Detection using DBSCAN for Counting Intoxicated Flies in the FLORIDA Assay,"In this paper, we propose an instrumentation and computer vision pipeline that allows automatic object detection on images taken from multiple experimental set ups. We demonstrate the approach by autonomously counting intoxicated flies in the FLORIDA assay. The assay measures the effect of ethanol exposure onto the ability of a vinegar fly Drosophila melanogaster to right itself. The analysis consists of a three-step approach. First, obtaining an image of a large set of individual experiments, second, identify areas containing a single experiment, and third, discover the searched objects within the experiment. For the analysis we facilitate well-known computer vision and machine learning algorithms-namely color segmentation, threshold imaging and DBSCAN. The automation of the experiment enables an unprecedented reproducibility and consistency, while significantly decreasing the manual labor.",Not About Sufficiency
Design Criteria and Implementation of Link Layer Protocol in Inter-satellite Link Networking,"Inter-satellite link networking is an important way to manage global satellite navigation system. Inter-satellite link, TT&C link and upload link share the link layer protocol can achieve interoperability and mutual support. Therefore coverage of TT&C system and integrity of the satellite navigation system has been improved. By analyzing and modeling the requirements of different traffics transmission and the characteristics of different physical links, this paper proposes a set of criteria for designing link layer protocol in inter -satellite link networking. A link layer protocol prototype is designed by using these design criteria. Theoretical analysis and simulation results show that the protocol prototype has a good performance of multi-service access, transmission throughput, multi-link adaptability and interoperability.",Not About Sufficiency
"Explanatory predictive model for COVID-19 severity risk employing machine learning, shapley addition, and LIME","The rapid spread of SARS-CoV-2 threatens global public health and impedes the operation of healthcare systems. Several studies have been conducted to confirm SARS-CoV-2 infection and examine its risk factors. To produce more effective treatment options and vaccines, it is still necessary to investigate biomarkers and immune responses in order to gain a deeper understanding of disease pathophysiology. This study aims to determine how cytokines influence the severity of SARS-CoV-2 infection. We measured the plasma levels of 48 cytokines in the blood of 87 participants in the COVID-19 study. Several Classifiers were trained and evaluated using Machine Learning and Deep Learning to complete missing data, generate synthetic data, and fill in any gaps. To examine the relationship between cytokine storm and COVID-19 severity in patients, the Shapley additive explanation (SHAP) and the LIME (Local Interpretable Model-agnostic Explanations) model were applied. Individuals with severe SARS-CoV-2 infection had elevated plasma levels of VEGF-A, MIP-1b, and IL-17. RANTES and TNF were associated with healthy individuals, whereas IL-27, IL-9, IL-12p40, and MCP-3 were associated with non-Severity. These findings suggest that these cytokines may promote the development of novel preventive and therapeutic pathways for disease management. In this study, the use of artificial intelligence is intended to support clinical diagnoses of patients to determine how each cytokine may be responsible for the severity of COVID-19, which could lead to the identification of several cytokines that could aid in treatment decision-making and vaccine development.",Not About Sufficiency
FedAir: Towards Multi-hop Federated Learning Over-the-Air,"Federated learning (FL) has emerged as a key technology for enabling next-generation AI at scale. The classical FL systems use single-hop cellular links to deliver the local models from mobile workers to edge routers that then reach the remote cloud servers via high-speed Internet core for global model averaging. Due to the cost-efficiency, wireless multi-hop networks have been widely exploited to build communication backbones. Therefore, enabling FL over wireless multi-hop networks can make it accessible in a low-cost manner to everyone (e.g., under-developed areas and disaster sites). Wireless multi-hop FL, however, suffers from profound communication constraints including noisy and interference-rich wireless links, which results in slow and nomadic FL model updates. To address this, we suggest novel machine learning-enabled wireless multi-hop FL framework, namely FedAir, that can greatly mitigate the adverse impact of wireless communications on FL performance metrics such as model convergence time. This will allow us to fast prototype, deploy, and evaluate FL algorithms over ML-enabled, programmable wireless router (ML-router). The experiments on the deployed testbed validate and show that wireless multi-hop FL framework can greatly accelerate the runtime convergence speed of the de-facto FL algorithm, FedAvg.",Not About Sufficiency
Machine Learning Algorithms for Breast Cancer Prediction,"There are numerous subtypes of breast cancer, each with its own unique outlook. The evaluation of the expression of small gene sets is the primary focus of the current stratification methods. In the upcoming years, Next Generation Sequencing (NGS) is anticipated to generate a significant amount of genomic data. We investigate the application of deep learning, or machine learning, to the subtyping of breast cancer in this case study. We used pan-cancer and non-cancer data to create semi-supervised settings because there weren't any publicly accessible data. A wide range of supervised and semisupervised designs are investigated with the help of Integrative omics data like microRNA expression and copy number variations.On our gene expression data challenge, accuracy results indicate that simpler models perform better than deep semi-supervised approaches. Deep model performance improves only marginally (if at all) when integrated combining several omics data types emphasises the need for additional research on bigger datasets of multi-omics data as they become accessible. In terms of biology, our linear model typically confirms. earlier classifications of gene subtypes. The development of a more varied and unexplored set of representative omics traits that may be helpful for subtyping breast cancer has resulted from deep methods, which imitate non-linear interactions.",Not About Sufficiency
Lack of standardization and faculty development in pediatric colonoscopy: A qualitative study,"A standard curriculum for pediatric colonoscopy training has neither been required nor universally implemented in North American fellowship programs. This qualitative study assessed the needs of colonoscopy training in pediatric gastroenterology to determine the standardized components of procedural teaching. Focus groups with pediatric gastroenterology attendings, fellows, procedural nurses, and interviews with advanced endoscopists, all practicing at a single institution, were conducted between March and June 2018. Data were analyzed using thematic analysis principles. Four themes emerged: (1) lack of standardization of colonoscopy performance, (2) lack of professional development of procedure teaching skills, (3) need for teaching behaviors that promote learner's performance, and (4) barriers to effective teaching and learning. A conceptual framework was created for developing a standardized ""train-the-trainer"" curriculum. Our needs assessment supports expansion of efforts to make this comprehensive training available to all pediatric gastroenterologists involved in procedure teaching. image Lack of standardization in colonoscopy performance compromises teaching. Few pediatric gastroenterology faculty members have participated in a formal ""train-the-trainer"" curriculum. A proposed model for developing a universal curriculum for colonoscopy training accessible to all pediatric gastroenterologists involved in procedure teaching.What is Known? Few pediatric gastroenterology training programs in North America have adopted formal curricula for colonoscopy training. Almost all pediatric gastroenterology training programs in North America lack formal curricula for colonoscopy training, beyond a simple apprenticeship model. Few programs have evaluated the current training they provide for fellows.",Not About Sufficiency
"Spatial-temporal patterns and driving mechanism of rural vulneraiblity at county level:A case study of 117 counties in Heilongjiang Province, China","Rural vulnerability is used to understand the potential multi-hazard threats and describe the fragile state of rural areas, which engage decision-makers in developing policies and strategies to reduce vulnerability. Existing studies about rural vulnerability focused on the single exogenous disturbance but paid inadequate attention to the multi-disturbances of rural coupled human-environment system. In addition, current studies mainly analyzed the spatial differentiation of vulnerability degree, ignoring the temporal evolution and the internal elements' relationships of rural systems. In this study, we structured the cognition of rural vulnerability with a framework for understanding coupled human-environment system, evaluated rural vulnerability with the dimensions of exposure, sensitivity and adaptability, and analyzed the driving mechanism based on spatial-temporal heterogeneity. Taking 117 county units in Heilongjiang Province as study cases, we found that (1) rural vulnerability was indeed significant, as the area of county units with extreme or high vulnerability levels accounts for 50.4% of the total area, indicating a trend of high vulnerability in the counties on the north and south sides and low vulnerability in the center. (2) The spatial-temporal heterogeneity of rural vulnerability presented a clustering trend, shifting from a relatively balanced spatial distribution from 2010 to 2013 to a state of vulnerability aggregation at all levels from 2016 to 2019. (3) Rural vulnerability was mainly affected by changes in the principal factors of sensitivity and adaptability, and driving sources mainly generated by human activities, which was largely derived from rural construction activities and government policy guidance on rural regulation. Based on the results, we classified county units into different rural vulnerability types, put forward a rural resilience planning mode of ""General + Special"" with planning strategies for each type, which can be used as a reference for rural planning positioning of county and township level land spatial planning in the national territory spatial planning.",Not About Sufficiency
Identifying bridges prone to instream wood accumulation: insights from bridges across the UK,"Accumulation of instream large wood (i.e., fallen trees, trunks, branches, and roots) at bridges during floods may exacerbate flooding, scour and cause structural failure. Yet, explaining and predicting the likelihood of a bridge trapping wood remains challenging. Quantitative data regarding wood accumulation at bridges are scarce, and most equations proposed to estimate the accumulation probability were derived from laboratory experiments, and include variables such as flow velocity, Froude number, and approaching wood volume or size which are difficult to obtain. Other evaluations based on technical reports and information regarding wood removal have been proposed but are mostly qualitative. Until now, a data-driven approach combining multiple quantitative accessible variables at the river reach and catchment scales remains lacking. As a result, the controlling parameters explaining whether a bridge is prone to trap wood are still unclear. This work aims to fill this gap by analysing a database of 49 bridges across the United Kingdom (UK) classified as prone and not prone to wood accumulation. The database contained information regarding the geometry of the bridge (i.e., number of piers and pier shape) and we added parameters describing the upstream river channel morphology, the riparian landcover, and high-flow characteristics. We applied multivariate statistics and a machine learning approach to identify the variables that explained and predicted the predisposition of bridges to wood accumulation. Results showed that the number of bridge piers, the unit stream power, the pier shape, and the riparian forested area explained 87% of the total variability for the training dataset (0.87 training accuracy), and the selected model had a testing accuracy of 0.60 (60%). Although limited by the sample size, this study sheds light on the identification of bridges prone to wood accumulation and can inform bridge design and management to mitigate wood-related hazards.",Not About Sufficiency
"International Jurisprudence on Trade and Environmental Health: One Step Forward, Two Steps Back?","Since the creation of the World Trade Organization (WTO), there has been considerable debate regarding the impact of its rules on public health. By contrast, the role of the WTO dispute settlement mechanism has received little attention, even though the bodies responsible for settling disputes are the ultimate interpreters of WTO rules and agreements. To date, three WTO disputes that relate to occupational and/or environmental health have been fully litigated. A review of the decisions and reasoning in these cases indicates that WTO jurisprudence is evolving, as Panels and the Appellate Body try-with varying degrees of success-to balance countries' rights and obligations under international trade agreements with their right to protect occupational and environmental health. Disputes between nations can have an impact beyond the parties concerned, and raise questions about the relationship between trade agreements and other international agreements, especially multilateral environmental agreements (MEAs).",Not About Sufficiency
MSCI: A multistate dataset for colposcopy image classification of cervical cancer screening,"Background: Cervical cancer is the second most common female cancer globally, and it is vital to detect cervical cancer with low cost at an early stage using automated screening methods of high accuracy, especially in areas with insufficient medical resources. Automatic detection of cervical intraepithelial neoplasia (CIN) can effectively prevent cervical cancer. Objectives: Due to the deficiency of standard and accessible colposcopy image datasets, we present a dataset containing 4753 colposcopy images acquired from 679 patients in three states (acetic acid reaction, green filter, and iodine test) for detection of cervical intraepithelial neoplasia. Based on this dataset, a new computer-aided method for cervical cancer screening was proposed. Methods: We employed a wide range of methods to comprehensively evaluate our proposed dataset. Hand-crafted feature extraction methods and deep learning methods were used for the performance verification of the multistate colposcopy image (MSCI) dataset. Importantly, we propose a gated recurrent convolutional neural network (C-GCNN) for colposcopy image analysis that considers time series and combined multistate cervical images for CIN grading. Results: The experimental results showed that the proposed C-GCNN model achieves the best classification performance in CIN grading compared with hand-crafted feature extraction methods and classic deep learning methods. The results showed an accuracy of 96.87 %, a sensitivity of 95.68 %, and a specificity of 98.72 %. Conclusion: A multistate colposcopy image dataset (MSCI) is proposed. A CIN grading model (C-GCNN) based on the MSCI dataset is established, which provides a potential method for automated cervical cancer screening.",Not About Sufficiency
Genome-Based Prediction of Bacterial Antibiotic Resistance,"Clinical microbiology has long relied on growing bacteria in culture to determine antimicrobial susceptibility profiles, but the use of whole-genome sequencing for antibiotic susceptibility testing (WGS-AST) is now a powerful alternative. This review discusses the technologies that made this possible and presents results from recent studies to predict resistance based on genome sequences. We examine differences between calling antibiotic resistance profiles by the simple presence or absence of previously known genes and single-nucleotide polymorphisms (SNPs) against approaches that deploy machine learning and statistical models. Often, the limitations to genome-based prediction arise from limitations of accuracy of culture-based AST in addition to an incomplete knowledge of the genetic basis of resistance. However, we need to maintain phenotypic testing even as genome-based prediction becomes more widespread to ensure that the results do not diverge over time. We argue that standardization of WGS-AST by challenge with consistently phenotyped strain sets of defined genetic diversity is necessary to compare the efficacy of methods of prediction of antibiotic resistance based on genome sequences.",Not About Sufficiency
RUN-AS: a novel approach to annotate news reliability for disinformation detection,"The development of the internet and digital technologies has inadvertently facilitated the huge disinformation problem that faces society nowadays. This phenomenon impacts ideologies, politics and public health. The 2016 US presidential elections, the Brexit referendum, the COVID-19 pandemic and the Russia-Ukraine war have been ideal scenarios for the spreading of fake news and hoaxes, due to the massive dissemination of information. Assuming that fake news mixes reliable and unreliable information, we propose RUN-AS (Reliable and Unreliable Annotation Scheme), a fine-grained annotation scheme that enables the labelling of the structural parts and essential content elements of a news item and their classification into Reliable and Unreliable. This annotation proposal aims to detect disinformation patterns in text and to classify the global reliability of news. To this end, a dataset in Spanish was built and manually annotated with RUN-AS and several experiments using this data set were conducted to validate the annotation scheme by using Machine Learning (ML) and Deep Learning (DL) algorithms. The experiments evidence the validity of the annotation scheme proposed, obtaining the best F(1)m, 0.948, with the Decision Tree algorithm.",Not About Sufficiency
How can we deliver on the promise of precision medicine in oncology and beyond? A practical roadmap for action,"BackgroundPrecision medicine (PM) is a form of personalized medicine that recognizes that individuals with the same condition may have different underlying factors and uses molecular information to provide tailored treatments. This approach can improve treatment outcomes and transform lives through favorable risk/benefit ratios, avoidance of ineffective interventions, and possible cost savings, as evidenced in the field of lung cancer and other oncology/therapeutic settings, including cardiac disease, diabetes, and rare diseases. However, the potential benefits of PM have yet to be fully realized. DiscussionThere are many barriers to the implementation of PM in clinical practice, including fragmentation of the PM landscape, siloed approaches to address shared challenges, unwarranted variation in availability and access to PM, lack of standardization, and limited understanding of patients' experience and needs throughout the PM pathway. We believe that a diverse, intersectoral multistakeholder collaboration, with three main pillars of activity: generation of data to demonstrate the benefit of PM, education to support informed decision-making, and addressing barriers across the patient pathway, is necessary to reach the shared goal of making PM an accessible and sustainable reality. Besides healthcare providers, researchers, policymakers/regulators/payers, and industry representatives, patients in particular must be equal partners and should be central to the PM approach?from early research through to clinical trials and approval of new treatments?to ensure it represents their entire experience and identifies barriers, solutions, and opportunities at the point of delivery. ConclusionWe propose a practical and iterative roadmap to advance PM and call for all stakeholders across the healthcare system to employ a collaborative, cocreated, patient-centered methodology to close gaps and fully realize the potential of PM.",Not About Sufficiency
A Two-Stage Restoration Resource Allocation Model for Enhancing the Resilience of Interdependent Infrastructure Systems,"Infrastructure systems play a critical role in delivering essential services that are important to the economy and welfare of society. To enhance the resilience of infrastructure systems after a large-scale disruptive event, determining where and when to invest restoration resources is a challenge for decision makers. Comprehensively considering the recovery time of infrastructure systems and the overall losses resulting from a disaster, this study proposes a two-stage restoration resource allocation model for enhancing the resilience of interdependent infrastructure systems. First, to evaluate the effect of resource allocation during the recovery process, dynamic resilience is selected as the criterion for the recovery of infrastructure systems. Second, taking into consideration the decision makers' point of view, a two-stage resource allocation model is proposed. The objective of the first stage is to quickly recover the infrastructure systems' dynamic resilience to meet the basic needs of the users. The second stage is aimed at minimizing the overall losses in the following recovery process. The effects of infrastructure interdependencies on resource allocation are incorporated in the model using the dynamic inoperability input-output model. Through a case study, the proposed approach is compared with other resource allocation strategies. The results show that: (1) the restoration resource allocation strategy obtained from the proposed approach balances the recovery time and the overall losses to infrastructure systems; and (2) the value of the usage cost of the unit restoration resource has a significant impact on the recovery time and the overall losses under different strategies. The proposed model is both effective and efficient in solving the post-disaster resource allocation problem and can provide decision makers with scientific decision support.",Not About Sufficiency
Building Material Counting in Warehouse Scenario Based on Multi-Dilation Rates and Attention Mechanism,"Automated counting is an essential procedure in the inventory management of building materials. However, existing methods primarily cater to construction site scenarios, leaving counting in building material warehouses reliant on manual labor. The challenges posed by variations in scale, complex backgrounds, occlusion, and mixed placement of different types of materials result in counting difficulties. Furthermore, current density-map-based models in the field cannot simultaneously detect and count multiple types of building materials in the same image. In this paper, we introduce multi-category object detection and counting methods to the building material warehouse scenario and propose a network architecture that effectively addresses these challenges. Our approach incorporates multiple attention mechanisms to enhance foreground features and reduce inter-class interferences. Additionally, the employment of dilated-convolution augments our architecture's capability in extracting multi-scale information, enhancing its prowess in discernment. Experimental results demonstrated that our model outperformed other models in detecting and counting different types of building materials.",Not About Sufficiency
Close to the Madding Crowd: Waterbird Responses to Land Use Conversion in and Around a Mediterranean Urban Wetland,"Investigating how Mediterranean wetlands respond to adjacent land use conversion, is an important first step in mitigating the impact of human encroachment and other environmental stressors. We monitored the composition and structure of waterbird assemblages, in a Mediterranean urban marsh, subjected to severe anthropogenic pressures. Remote sensing indicated that in the last two decades Boussedra Pond was subjected to landfill, resulting in a substantial reduction (similar to 50 %) of the marsh, while due to a lack of urban planning urban built-up and agriculture areas expanded considerably in its surroundings. This precipitous reduction of the size of this urban pond threatens the diversity of its resident waterbirds which include the globally Endangered (EN) White-headed Duck Oxyura leucocephala, the Near-Threatened (NT) Ferruginous Duck Aythya nyroca, and many other staging and wintering migratory species. The long-term study suggested that breeding waterbirds species responded differentially to the loss and degradation of the marsh, as highlighted by the apparent resilience of the synanthropic Moorhen Gallinula chloropus and the disappearance of several breeding marsh specialists, such as the Little Bittern Ixobrychus minutus and the Western Marsh Harrier Circus aeruginosus. The study also points out the need for both a coordinated cross-sectorial land use planning and an immediate, affordable and sustainable wetland conservation action.",Not About Sufficiency
Machine Learning Prediction of Hypoglycemia and Hyperglycemia From Electronic Health Records: Algorithm Development and Validation,"Background: Acute blood glucose (BG) decompensations (hypoglycemia and hyperglycemia) represent a frequent and significant risk for inpatients and adversely affect patient outcomes and safety. The increasing need for BG management in inpatients poses a high demand on clinical staff and health care systems in addition. Objective: This study aimed to generate a broadly applicable multiclass classification model for predicting BG decompensation events from patients' electronic health records to indicate where adjustments in patient monitoring and therapeutic interventions are required. This should allow for taking proactive measures before BG levels are derailed. Methods: A retrospective cohort study was conducted on patients who were hospitalized at a tertiary hospital in Bern, Switzerland. Using patient details and routine data from electronic health records, a multiclass prediction model for BG decompensation events (<3.9 mmol/L [hypoglycemia] or >10, >13.9, or >16.7 mmol/L [representing different degrees of hyperglycemia]) was generated based on a second-level ensemble of gradient-boosted binary trees. Results: A total of 63,579 hospital admissions of 38,250 patients were included in this study. The multiclass prediction model reached specificities of 93.7%, 98.9%, and 93.9% and sensitivities of 67.1%, 59%, and 63.6% for the main categories of interest, which were nondecompensated cases, hypoglycemia, or hyperglycemia, respectively. The median prediction horizon was 7 hours and 4 hours for hypoglycemia and hyperglycemia, respectively. Conclusions: Electronic health records have the potential to reliably predict all types of BG decompensation. Readily available patient details and routine laboratory data can support the decisions for proactive interventions and thus help to reduce the detrimental health effects of hypoglycemia and hyperglycemia.",Not About Sufficiency
Leveraging natural language processing to augment structured social determinants of health data in the electronic health record,"Objective Social determinants of health (SDOH) impact health outcomes and are documented in the electronic health record (EHR) through structured data and unstructured clinical notes. However, clinical notes often contain more comprehensive SDOH information, detailing aspects such as status, severity, and temporality. This work has two primary objectives: (1) develop a natural language processing information extraction model to capture detailed SDOH information and (2) evaluate the information gain achieved by applying the SDOH extractor to clinical narratives and combining the extracted representations with existing structured data. Materials and Methods We developed a novel SDOH extractor using a deep learning entity and relation extraction architecture to characterize SDOH across various dimensions. In an EHR case study, we applied the SDOH extractor to a large clinical data set with 225 089 patients and 430 406 notes with social history sections and compared the extracted SDOH information with existing structured data. Results The SDOH extractor achieved 0.86 F1 on a withheld test set. In the EHR case study, we found extracted SDOH information complements existing structured data with 32% of homeless patients, 19% of current tobacco users, and 10% of drug users only having these health risk factors documented in the clinical narrative. Conclusions Utilizing EHR data to identify SDOH health risk factors and social needs may improve patient care and outcomes. Semantic representations of text-encoded SDOH information can augment existing structured data, and this more comprehensive SDOH representation can assist health systems in identifying and addressing these social needs.",Not About Sufficiency
"Urban Form, Sustainability and Health: The Case of Greater Oslo","Several studies have shown dense urban structures to be favourable in order to reduce greenhouse gas emissions from transport, limit energy consumption in buildings and protect farmland and natural areas in the surroundings of the city. There may, however, be some tensions between such a compact urban developmental strategy and considerations of public health in urban planning. This paper reviews findings from international research on the relationships between urban form and health and illustrates some of these effects by comparing statistics on life expectancy and the frequency of heart attacks among inhabitants of different urban districts in the Norwegian capital Oslo. Since we have only had access to aggregate figures at the level of urban districts, the results must be interpreted with caution. The currently available results do, however, suggest that the densification strategies often recommended for reducing the ecological footprints of cities might be encumbered with some important drawbacks, seen from a public health perspective. Based on the findings, some dilemmas and perspectives for sustainability- and health-oriented urban planning are discussed.",Not About Sufficiency
Exploring the social and spatial potential of an intermodal approach to transport planning,"Gilbert and Perl (2007) established how different epochs in human history came with specific transport revolutions. Our research suggests that intermodal approaches could constitute an invisible transport planning revolution.Based on a review defining social sustainability as it applies to urban passenger transportation, we consider the potential of an intermodal focus to better integrate non-motorized (walking-cycling) and public transportation. We start from the premise that rather than being a primary mono-mode such as buses or trains, sustainable transport is best understood as an ecology of modes with specific strengths and complementarities that can be mobilized through planning.Using data from Metropolitan Santiago (Chile), we explore this potential, from a conceptual, mainly social, and spatial perspective. We find that paying more attention to different formats of cycling, public transport integration could significantly improve low-cost alternatives for individual and feeder trips. Moreover, adjusting land use to non-work trip purposes could yield substantial benefits. This approach also offers the possibility of developing relatively simple tools helpful for improving the deliberative aspects of participatory planning, thereby increasing buy-in as well as improving health, efficiency, safety, and other benefits of sustainable transport.",Not About Sufficiency
Towards low-carbon cities: Patch-based multi-objective optimization of land use allocation using an improved non-dominated sorting genetic algorithm-II,"The rational land use allocation is of great significance to the construction of low-carbon cities. The optimization model of land use allocation is an important tool that helps urban planners to quantitatively trade-off among the multi-objectives and achieve optimal land use schemes. For multi-objective optimization of low-carbon land use allocation, the models conducted by existing studies generally tend to be based on gridded data, lack of comprehensive consideration of quantitative and spatial objectives, and efficient algorithms to execute the optimization process. Therefore, this paper proposed a patch-based low carbon multi-objective land use allocation (LC-MLUA) optimization model involving both quantitative and spatial optimization targets. The LCMLUA optimization model was solved with an improved non-dominated sorting genetic algorithm-II (NSGAII), and the weighted-sum method was used to make the final selection under different preferences. The LCMLUA optimization model was then applied to a case study of Changxing, a county-level city in east China, and there were three key results. (1) The LC-MLUA optimization model had a remarkable outperform of the land use allocation than the original land use plan, and the optimized values of economic benefit, emission reduction, and accessibility increased by 27.0%, 6.2% and 8.3%, respectively. (2) The LC-MLUA optimization model generated a series of optimal schemes to support suggestion-making for the low-carbon adjustment of the land use structure and spatial layout. (3) The LC-MLUA optimization model based on vector land patch data was proved more efficient as the unit number was reduced by 5 times than gridded data and better reflected the land use planning practice. (4) Compared with other algorithms, the improved NSGA-II had better performance in the number of solutions, target optimization rate, and comprehensive performance. Based on these results, it suggests that the patch-based LC-MLUA optimization model method can provide good technical support for lowcarbon land use planning, and can be flexibly applied to other cities.",Not About Sufficiency
"SAFE ENVIRONMENT MANAGEMENT IN ACUTE PSYCHIATRIC WARDS IN THE CZECH REPUBLIC, FOUNDATION FOR RECOMMENDATIONS FOR PREVENTIVE PRACTICE","Objectives: Suicides of hospitalized patients present rare but very serious adverse events in healthcare settings. The aim of this article is to describe and analyse the facilities and material equipment of acute psychiatric settings in the Czech Republic and contrast these with recommenda-tions for effective prevention of suicidal behaviour. Since there are currently no universally accepted protocols for risk assessment and prevention of suicides in hospital settings in the Czech Republic, these recommendations draw on international guidelines. Based on the outcomes of our study we provide recommendations for risk management and effective prevention of suicidal behaviour of patients hospitalized in acute care settings. Methods: In order to describe and analyse the environment of acute psychiatric wards in the Czech Republic we have developed a questionnaire based on international recommendations for risk management and prevention of suicidal behaviour. We also collected data on the prevalence of attempted and completed suicides and their respective methods in these hospitals. Results: We have established that acute psychiatric wards in the Czech Republic operate within insufficient safety regimes, especially with respect to the prevention of suicide by hanging and the accessibility of objects for cutting. Our findings demonstrate that only 75% of the wards are equipped with safety glass, and only 50% of the wards with safety mirrors. Only just over 40% of hospitals have safety door handles and shower heads. Conclusion: While it is impossible to entirely eliminate the risk of suicidal behaviour it is possible to manage it. The risk reduction is attainable by providing a safe-proofed environment and minimizing opportunities of suicide attempts by hanging and cutting. In order to effectively prevent suicides, it is essential to increase the awareness of the possibilities of safe proofing of the environment as well as standardization of risk assess-ment of potential suicidal behaviour of patients.",Not About Sufficiency
Predicting human mobility with activity changes,"Human mobility patterns can provide valuable information in understanding the impact of human behavioral regularities in urban systems, usually with a specific focus on traffic prediction, public health or urban planning. While existing studies on human movement have placed huge emphasis on spatial location to predict where people go next, the time dimension component is usually being treated with oversimplification or even being neglected. Time dimension is crucial to understanding and detecting human activity changes, which play a negative role in prediction and thus may affect the predictive accuracy. This study aims to predict human movement from a spatio-temporal perspective by taking into account the impact of activity changes. We analyze and define changes of human activity and propose an algorithm to detect such changes, based on which a Markov chain model is used to predict human movement. The Microsoft GeoLife dataset is used to test our methodology, and the data of two selected users is used to evaluate the performance of the prediction. We compare the predictive accuracy (R-2) derived from the data with and without implementing the activity change detection. The results show that the R-2 is improved from 0.295 to 0.762 for the user with obvious activity changes and from 0.965 to 0.971 for the user without obvious activity changes. The method proposed by this study improves the accuracy in analyzing and predicting human movement and lays the foundation for related urban studies.",Not About Sufficiency
The Water Sector Was Born and Raised with Big-Impact Water Data,"For thousands of years, data allowed societies to firstunderstandthe connection between water quality and public health and later torefine water and waste management strategies. In this commentary,we look at the past, present, and future of 'big-impact'data in thewater sector, from ancient water systems to Paris' valorizationof wastes, London's tribulations with cholera, and modern environmentalstandards for the protection of public and environmental health. Wecontinue the journey of water data with emerging applications in understandingour water resources, including blue-green city planning, understandingwater use trends in remote areas, water resource monitoring, and wastewaterepidemiology. We conclude with a foray into the future of the watersector, where proactively managing water resources to ensure equitableand resilient water access can be informed by remote sensing, satellite-basedmonitoring, and machine learning. By understanding how water datarevolutionized our societies in the past, we aim to inspire the useof big water data to open possibilities for managing urban and environmentalwater in our future.",Not About Sufficiency
"Population-Level Exposure to Particulate Air Pollution during Active Travel: Planning for Low-Exposure, Health-Promoting Cities","BACKGROUND: Providing infrastructure and land uses to encourage active travel (i.e., bicycling and walking) are promising strategies for designing health-promoting cities. Population-level exposure to air pollution during active travel is understudied. OBJECTIVES: Our goals were a) to investigate population-level patterns in exposure during active travel, based on spatial estimates of bicycle traffic, pedestrian traffic, and particulate concentrations; and b) to assess how those exposure patterns are associated with the built environment. METHODS: We employed facility-demand models (active travel) and land use regression models (particulate concentrations) to estimate block-level (n=13,604) exposure during rush-hour (1600-1800 hours) in Minneapolis, Minnesota. We used the model-derived estimates to identify land use patterns and characteristics of the street network that are health promoting. We also assessed how exposure is correlated with indicators of health disparities (e.g., household income, proportion of nonwhite residents). Our work uses population-level rates of active travel (i. e., traffic flows) rather than the probability of walking or biking (i. e., ""walkability"" or ""bikeability"") to assess exposure. RESULTS: Active travel often occurs on high-traffic streets or near activity centers where particulate concentrations are highest (i. e., 20-42% of active travel occurs on blocks with high population-level exposure). Only 2-3% of blocks (3-8% of total active travel) are ""sweet spots"" (i.e., high active travel, low particulate concentrations); sweet spots are located a) near but slightly removed from the city-center or b) on off-street trails. We identified 1,721 blocks (similar to 20% of local roads) where shifting active travel from high-traffic roads to adjacent low-traffic roads would reduce exposure by similar to 15%. Active travel is correlated with population density, land use mix, open space, and retail area; particulate concentrations were mostly unchanged with land use. CONCLUSIONS: Public health officials and urban planners may use our findings to promote healthy transportation choices. When designing health-promoting cities, benefits (physical activity) as well as hazards (air pollution) should be evaluated.",Not About Sufficiency
An Efficient Covid19 Epidemic Analysis and Prediction Model Using Machine Learning Algorithms,"The whole world is experiencing a novel infection called Coronavirus brought about by a Covid since 2019. The main concern about this disease is the absence of proficient authentic medicine The World Health Organization (WHO) proposed a few precautionary measures to manage the spread of illness and to lessen the defilement in this manner decreasing cases. In this paper, we analyzed the Coronavirus dataset accessible in Kaggle. The past contributions from a few researchers of comparative work covered a limited number of days. Our paper used the covid19 data till May 2021. The number of confirmed cases, recovered cases, and death cases are considered for analysis. The corona cases are analyzed in a daily, weekly manner to get insight into the dataset. After extensive analysis, we proposed machine learning regressors for covid 19 predictions. We applied linear regression, polynomial regression, Decision Tree Regressor, Random Forest Regressor. Decision Tree and Random Forest given an r-square value of 0.99. We also predicted future cases with these four algorithms. We can able to predict future cases better with the polynomial regression technique. This prediction can help to take preventive measures to control covid19 in near future. All the experiments are conducted with python language.",Not About Sufficiency
LAND-USE PLANNING: HISTORICAL ASPECTS,"Urban development is the research and practical activities of planning, development and management of settlements. These settlements may include cities, towns, rural settlements, and the systems of settlements. The urban development aims to form the material and territorial environment. Such development is used to locate territorial and functional facilities, which change their parameters over time. The term ""land-use planning"" is referred to decision-making concerning the settlement development. Territorial entities that include countries, regions, and their integral parts are land-use planning objects. The article deals with the issues of historical development of the land-use planning system and changes in urban development policies of the Russian Federation. The issue of ideal towns of the Renaissance Era, which were most comfortable and safe places, is touched upon. The methodical development plan of Saint Petersburg is described. Besides, the article describes a scheme of land-use planning of Irkutsk oblast. Research results can improve the urban planning system of land-use. (c) 2018 Published by Future Academy www.FutureAcademy.org.UK",Not About Sufficiency
Quality of Medical Care for Minor Surgery in Urban Units of the First Level of Care,"Background: minor surgery includes a series of surgical procedures, with simple and brief techniques, performed on superficial tissues or easily accessible structures, under local anesthesia and with little risk of post-surgical complications.Objective: to describe the quality of medical care for minor surgery in the urban units of the first level of care in the city of Tulcan, Ecuador.Methods: a descriptive, cross-sectional study was carried out in the urban units of the first level of care in the city of Tulcan, aimed at medical and nursing staff, which together total 42 professionals. These professionals underwent a survey related to the infrastructure, supplies, knowledge and performance of health personnel in minor surgery procedures.Results: health units do not have infrastructure, equipment, necessary supplies and a protocol that allows these procedures to be carried out safely, so patients have to be referred to the second level of care.Conclusions: the systematization of the action protocols, the implementa tion and the standardization of the minor surgery procedures guarantee the safety, relevance and opportunity of the action of the health personnel in favor of the users in the first level of care.",Not About Sufficiency
Deep reinforcement learning in World-Earth system models to discover sustainable management strategies,"Increasingly complex nonlinear World-Earth system models are used for describing the dynamics of the biophysical Earth system and the socioeconomic and sociocultural World of human societies and their interactions. Identifying pathways toward a sustainable future in these models for informing policymakers and the wider public, e.g., pathways leading to robust mitigation of dangerous anthropogenic climate change, is a challenging and widely investigated task in the field of climate research and broader Earth system science. This problem is particularly difficult when constraints on avoiding transgressions of planetary boundaries and social foundations need to be taken into account. In this work, we propose to combine recently developed machine learning techniques, namely, deep reinforcement learning (DRL), with classical analysis of trajectories in the World-Earth system. Based on the concept of the agent-environment interface, we develop an agent that is generally able to act and learn in variable manageable environment models of the Earth system. We demonstrate the potential of our framework by applying DRL algorithms to two stylized World-Earth system models. Conceptually, we explore thereby the feasibility of finding novel global governance policies leading into a safe and just operating space constrained by certain planetary and socioeconomic boundaries. The artificially intelligent agent learns that the timing of a specific mix of taxing carbon emissions and subsidies on renewables is of crucial relevance for finding World-Earth system trajectories that are sustainable in the long term. Published under license by AIP Publishing.",Not About Sufficiency
Can Robots Understand Welfare? Exploring Machine Bureaucracies in Welfare-to-Work,"The exercise of administrative discretion by street-level workers plays a key role in shaping citizens' access to welfare and employment services. Governance reforms of social services delivery, such as performance-based contracting, have often been driven by attempts to discipline this discretion. In several countries, these forms of market governance are now being eclipsed by new modes of digital governance that seek to reshape the delivery of services using algorithms and machine learning. Australia, a pioneer of marketisation, is one example, proposing to deploy digitalisation to fully automate most of its employment services rather than as a supplement to face-to-face case management. We examine the potential and limits of this project to replace human-to-human with 'machine bureaucracies'. To what extent are welfare and employment services amenable to digitalisation? What trade-offs are involved? In addressing these questions, we consider the purported benefits of machine bureaucracies in achieving higher levels of efficiency, accountability, and consistency in policy delivery. While recognising the potential benefits of machine bureaucracies for both governments and jobseekers, we argue that trade-offs will be faced between enhancing the efficiency and consistency of services and ensuring that services remain accessible and responsive to highly personalised circumstances.",Not About Sufficiency
"Environmental conflict, urbanization and dissent: Perspectives on democratic practices in territorial planning","This article examines the notion of environmental conflict in order to contribute to contemporary discussions about just socio-ecological transitions in times of climate and social crisis and pandemics. Based on a conceptual review and information about cases of democratic practices in spatial planning processes and urbanization, the article proposes that the environmental conflict is a process of production and socio-ecological dispute of multiple territorialities. This process is in turn crossed by different socio-ecological inequalities. Therefore, in light of a recent interest in the depoliticization of the environmental issue, the article also proposes that conflicts can be understood as a rupture of a normalized order to force new arrangements towards environmental justice respecting the difference instead of controlling or eliminating dissent. This will allow promoting more democratic practices of spatial planning.",Not About Sufficiency
A new framework for multi-level territorial spatial zoning management: Integrating ecosystem services supply-demand balance and land use structure,"The supply -demand balance of ecosystem services (ESs balance) is a research hotspot. Previous studies have explored spatio-temporal patterns of ESs supply, demand, and balance, but have neglected the inherent relationship between ESs balance and land use structure, which impedes the development of direct and effective land use regulatory strategies. This study assessed the balance of six ESs (i.e., timber production (TP), food production (FP), water retention (WR), carbon sequestration (CS), soil retention (SR), and habitat quality (HQ)) by the index of ecological relative surplus ratio (ERSR) at 30 m x 30 m pixel scale, and identified ESs balance bundles (ESBBs) in Lancang County, a typical mountainous region in China. It also defined the land use proportion thresholds for achieving different objectives of ESs balance, and proposed land use regulatory strategies accordingly. The results showed that: (1) From 2010 to 2020, the supply of six ESs exceeded the demand in Lancang County, and the surplus of different ESs changed over time. ESs balance also exhibited spatial heterogeneity, and ESs imbalance aggravated in some areas. Three ESBBs were divided as FP-WR-SR integrated bundle, TP-CS-HQ integrated bundle, and Integrated promotion bundle at the township scale. (2) Cropland, built-up land, and forest were the key land use types that affected the balance of the six ESs. Moreover, the land use proportion thresholds under the objectives of ""ERSR >= 0 '' and ""ERSR >= 1 '' were determined, and they differed by ES type and objective. (3) For each ESBB, ESs comprehensive management zones were delineated at the multi -scales of the township and 1 km x 1 km grid. Regulatory strategies oriented by ""reducing/increasing/maintaining the proportion of certain land use types"" were suggested under different objectives. This study developed a framework for ""multi -level zoning management of 'county - ESs balance bundle - township - grid' for promoting ESs balance by land use structure regulation"" that aligned with China's territorial spatial planning hierarchy at the county and township levels. It could inform the design of a scientific and orderly territorial spatial pattern with a high-level ESs balance at various scales (e.g., counties, agglomerations, provinces, and cities) and regions. However, this framework could not be directly applied to land use spatial optimization at pixel scale. Further research is needed to integrate land use simulation models, such as the mixed -cell cellular automata (MCCA) model, to achieve ESs balance objectives at finer scales.",Not About Sufficiency
Using Artificial Intelligence to Identify Suitable Artificial Groundwater Recharge Areas for the Iranshahr Basin,"A water supply is vital for preserving usual human living standards, industrial development, and agricultural growth. Scarce water supplies and unplanned urbanization are the primary impediments to results in dry environments. Locating suitable sites for artificial groundwater recharge (AGR) could be a strategic priority for countries to recharge groundwater. Recent advances in machine learning (ML) techniques provide valuable tools for producing an AGR site suitability map (AGRSSM). This research developed an ML algorithm to identify the most appropriate location for AGR in Iranshahr, one of the major districts in the East of Iran characterized by severe drought and excessive groundwater consumption. The area's undue reliance on groundwater resources has resulted in aquifer depletion and socioeconomic problems. Nine digitized and georeferenced data layers have been considered for preparing the AGRSSM, including precipitation, slope, geology, unsaturated zone thickness, land use, distance from the main rivers, precipitation, water quality, and transmissivity of soil. The developed AGRSSM was trained and validated using 1000 randomly selected points across the study area with an accuracy of 97%. By comparing the results of the proposed sites with those of other methods, it was discovered that the artificial intelligence method could accurately determine artificial recharge sites. In summary, this study uses a novel approach to identify optimal AGR sites using machine learning algorithms. Our findings have practical implications for policymakers and water resource managers looking to address the problem of groundwater depletion in Iranshahr and other regions facing similar challenges. Future research in this area could explore the applicability of our approach to other regions and examine the potential economic benefits of using AGR to recharge groundwater.",Not About Sufficiency
Predicting Bus Travel Time in Cheonan City through Deep Learning Utilizing Digital Tachograph Data,"Urban transportation systems are increasingly burdened by traffic congestion, a consequence of population growth and heightened reliance on private vehicles. This congestion not only disrupts travel efficiency but also undermines productivity and urban resident's overall well-being. A critical step in addressing this challenge is the accurate prediction of bus travel times, which is essential for mitigating congestion and improving the experience of public transport users. To tackle this issue, this study introduces the Hybrid Temporal Forecasting Network (HTF-NET) model, a framework that integrates machine learning techniques. The model combines an attention mechanism with Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) layers, enhancing its predictive capabilities. Further refinement is achieved through a Support Vector Regressor (SVR), enabling the generation of precise bus travel time predictions. To evaluate the performance of the HTF-NET model, comparative analyses are conducted with six deep learning models using real-world digital tachograph (DTG) data obtained from intracity buses in Cheonan City, South Korea. These models includes various architectures, including different configurations of LSTM and GRU, such as bidirectional and stacked architectures. The primary focus of the study is on predicting travel times from the Namchang Village bus stop to the Dongnam-gu Public Health Center, a crucial route in the urban transport network. Various experimental scenarios are explored, incorporating overall test data, and weekday and weekend data, with and without weather information, and considering different route lengths. Comparative evaluations against a baseline ARIMA model underscore the performance of the HTF-NET model. Particularly noteworthy is the significant improvement in prediction accuracy achieved through the incorporation of weather data. Evaluation metrics, including root mean squared error (RMSE), mean absolute error (MAE), and mean squared error (MSE), consistently highlight the superiority of the HTF-NET model, outperforming the baseline ARIMA model by a margin of 63.27% in terms of the RMSE. These findings provide valuable insights for transit agencies and policymakers, facilitating informed decisions regarding the management and optimization of public transportation systems.",Not About Sufficiency
The Hitchhiker's Guide to Fused Twins: A Review of Access to Digital Twins In Situ in Smart Cities,"Smart Cities already surround us, and yet they are still incomprehensibly far from directly impacting everyday life. While current Smart Cities are often inaccessible, the experience of everyday citizens may be enhanced with a combination of the emerging technologies Digital Twins (DTs) and Situated Analytics. DTs represent their Physical Twin (PT) in the real world via models, simulations, (remotely) sensed data, context awareness, and interactions. However, interaction requires appropriate interfaces to address the complexity of the city. Ultimately, leveraging the potential of Smart Cities requires going beyond assembling the DT to be comprehensive and accessible. Situated Analytics allows for the anchoring of city information in its spatial context. We advance the concept of embedding the DT into the PT through Situated Analytics to form Fused Twins (FTs). This fusion allows access to data in the location that it is generated in in an embodied context that can make the data more understandable. Prototypes of FTs are rapidly emerging from different domains, but Smart Cities represent the context with the most potential for FTs in the future. This paper reviews DTs, Situated Analytics, and Smart Cities as the foundations of FTs. Regarding DTs, we define five components (physical, data, analytical, virtual, and Connection Environments) that we relate to several cognates (i.e., similar but different terms) from existing literature. Regarding Situated Analytics, we review the effects of user embodiment on cognition and cognitive load. Finally, we classify existing partial examples of FTs from the literature and address their construction from Augmented Reality, Geographic Information Systems, Building/City Information Models, and DTs and provide an overview of future directions.",Not About Sufficiency
Developing Secure Privacy Preserving and Causal Genetic Alteration Research in Building an Innovative Systematic Pedagogy for Integrated Research and Education - The INSPIRE Model,"To meet current keen demand for producing next-generation workforce equipped with skills and expertise in big-data analytics, we developed an innovative systematic pedagogy for integrated research-education (INSPIRE model) that is centered around two great challenges: (1) Transforming multidisciplinary STEM training so that it enhances emerging problem-solving capacity and (2) Training STEM students how to have a bigger hand in performing large-scale scientific work. To help strengthen the problem-solving skills and leadership abilities of STEM graduates, we reform the current STEM research training in Bioinformatics and CIS (Computer and Information Sciences) so that it helps us reach the goal of catalyzing science and research training. Our main research hypothesis is that critical improvement in the way big-data scientists are trained comes not solely from large-scale data mining but, in addition, comes from developing useful machine learning and artificial intelligence techniques that automate intelligent learning derived from big-data. The INSPIRE model was built by enablers in the scientific community, and indeed, by the community at large, to help resolve the scarcity of those Professionally Skilled / Trained in Big Data analytics (PSTBD) issue by equipping students with a versatile cross-disciplinary skill set. There is a dire need for those of us in the scientific and academic community to be able to transfer our own successes into perfecting the feedback-based machine learning - cognitive science INSPIRE model, one that places a heavy emphasis on providing individualized training to individuals from all walks of life, including large populations of minorities and women, so that all efforts are made as collaboratively as possible, and the benefits of the sewn seeds may be reaped by everyone. We integrate our secure privacy preserving and causal genetic alteration research at single-cell resolution to demonstrate the model. On an even grander scale, we enhance the PSTBD research by developing the INSPIRE model so that broader social impacts can be made by such newly created fields as Systems Genomics at single-cell level and fields fostered by creative cross-disciplinary genomic big-data analytics (http://americancse.org/events/csce2017/keynotes_lectures/yang_talk) with catalyzed learning-research synergies.",Not About Sufficiency
Sensor Evaluation for Autonomous Surface Vehicles in Inland Waterways,"In this paper, we evaluate and discuss various sensors that may be used by an autonomous surface vehicle (ASV) to detect and classify objects. These sensors were each evaluated quantitatively and qualitatively for effectiveness (range, accuracy, resolution, field of view, height information) and practicality (marinization, effectiveness in various weather conditions, use in darkness, cost). In brief, LIDAR was found to be extremely accurate, although the maximum range (100m for our unit) is too short for most marine applications, the hardware is expensive, and most units are not marinized. Although marine radar is properly marinized and works well in all-weather at acceptable ranges, returns are often reduced to amorphous blobs with little fine detail to aid identification or even proper boundary definition of obstacles. Additionally, marine radar has no concept of height in its return, collapsing all vertical returns to the ground plane, thus obscuring covered passageways, such as those under bridge archways. Visible light cameras perform well at high resolution and long ranges, but only operate in daylight during good weather and are blinded by glare on the water or by the sun itself. Although stereocameras suffer the same drawbacks as visible light cameras, they have potential for short range obstacle detection. Thermal cameras operate day or night and provide an interesting supplement to other sensors, because water is a different temperature and has different emissivity characteristics than most objects. This enables buoys, vessels, structures, etc. to be discerned from the background using a thresholding method. Thermal cameras are limited by weather conditions and are often most affordable in low resolution, low field of view models.",Not About Sufficiency
MECHANISMS FOR ENSURING ESTONIA'S PERMANENT DEVELOPMENT: THE ENVIRONMENTAL ASPECT,"The latest trends in global development actualize the problem of ecological modernization in order to ensure the sustainability of socio- economic and political functioning of modern states, creating additional opportunities for an appropriate response to possible threats. The issues of ecological modernization are key on the agenda of the EU, which seeks to demonstrate leadership in addressing it. Ecological modernization and the subsequent digitalization of the economic sphere are considered one of the most important components of sustainable development of the EU and each member state. These processes and mechanisms for their practical implementation are approved in the program The European Green Deal. The European Commission created the Recovery and Resilience Facility, which provides for the coordination of efforts of EU member states to ensure resilience and sustainability of development. The subject of the study is the content and characteristics of sustainable development in Estonia. The purpose of the scientific exploration is to identify environmental aspects and to specify mechanisms for sustainable development in Estonia. Research methodology: systematic approach, methods of analysis and generalization. Estonia has developed a long-term strategy ""Estonia 2035"", which defines the basic principles of sustainability and sustainable development of the state. The tools for implementing the set sustainable development goals are legislative, managerial, educational and awarenessraising. In this strategy, the priorities of sustainable development of the state are innovation and sustainability of the economy, its competitiveness in world markets, energy efficiency, digitalization of management processes, safe environment for citizens, stimulating their activity in different spheres of society. Environmental issues are a key aspect of the ""Estonia 2035"" strategy, which meets such sustainable development goals as rational consumption, sustainable cities and communities, combating climate change, and preserving the ecosystem. A recovery and sustainability plan, correlated with the ""Estonia 2035"" strategy, was presented to enhance the ability to implement the basic principles of sustainable development and obtain additional sources of funding. Estonian officials are actively engaging financial and political mechanisms to implement sustainable development. According to the plan approved by the European Commission, Estonia will receive 969.3 million euros in funding from the Recovery and Resilience Facility. They will be distributed along the following lines: ecological modernization, digitalization, economic and social sustainability. In addition, political mechanisms, in particular public diplomacy, are actively used. They are aimed at positioning environmental initiatives according to the state branding strategy and intensifying cooperation with stakeholders in the implementation of international environmental projects. Research results: sustainability and resilience of Estonia's development correlate with the need to implement environmental modernization of the economy and digitalization of management processes; implementation of the ""Estonia 2035"" strategy, national recovery and sustainability plan is facilitated by economic and political decision- making mechanisms at the supranational and national levels of government.",Not About Sufficiency
Properties of Fairness Measures in the Context of Varying Class Imbalance and Protected Group Ratios,"Society is increasingly relying on predictive models in fields like criminal justice, credit risk management, and hiring. To prevent such automated systems from discriminating against people belonging to certain groups, fairness measures have become a crucial component in socially relevant applications of machine learning. However, existing fairness measures have been designed to assess the bias between predictions for protected groups without considering the imbalance in the classes of the target variable. Current research on the potential effect of class imbalance on fairness focuses on practical applications rather than dataset-independent measure properties. In this article, we study the general properties of fairness measures for changing class and protected group proportions. For this purpose, we analyze the probability mass functions of six of the most popular group fairness measures. We also measure how the probability of achieving perfect fairness changes for varying class imbalance ratios. Moreover, we relate the dataset-independent properties of fairness measures described in this work to classifier fairness in real-life tasks. Our results show that measures such as Equal Opportunity and Positive Predictive Parity are more sensitive to changes in class imbalance than Accuracy Equality. These findings can help guide researchers and practitioners in choosing the most appropriate fairness measures for their classification problems.",Not About Sufficiency
The Impact of COVID-19 and Public Health Emergencies on Consumer Purchase of Scarce Products in China,"Objectives: During public health emergencies, people often scramble to buy scarce goods, which may lead to panic behavior and cause serious negative impacts on public health management. Due to the absence of relevant research, the internal logic of this phenomenon is not clear. This study explored whether and why public health emergencies such as the COVID-19 pandemic stimulate consumers' preference for scarce products. Methods: Applying the questionnaire survey method, two online surveys were conducted on the Credamo data platform in China. The first survey was launched in February and collected psychological and behavioral data from 1,548 participants. Considering the likelihood of population relocation due to the pandemic, a follow-up survey was conducted in August with 463 participants who had participated in the first survey and had not relocated to other cities between February and August. The hypotheses were tested with these data through stepwise regression analysis, bootstrapping, and robustness testing. Results: Pandemic severity was found to positively affect scarce consumption behavior and the effect was found to be situational; this indicates that the impact of the pandemic on scarce consumption was only significant during the pandemic. Further, it was found that materialism plays a mediating role in the relationship between pandemic severity and scarce consumption. Finally, the need to belong was found to play a moderating role between pandemic severity and materialism. Conclusion: This study findings imply that the scarce consumption behavior during public health emergencies can be reduced by decreasing materialism and increasing the need to belong. These findings may aid government leaders in managing public health emergencies.",Not About Sufficiency
"Trends and Directions in Environmental Justice: From Inequity to Everyday Life, Community, and Just Sustainabilities","This article begins with a review and synthesis of some of the key theories, scholars, case examples, debates, methods, and (multiple) interpretations of environmental justice (EJ), as well as its expansion and globalization. We then look to some newly emerging themes, actions, and strategies for EJ and just sustainabilities. First, we look at the practices and materials of everyday life, illustrated by food and energy movements; second, the ongoing work on community and the importance of identity and attachment, informed by urban planning, food, and climate concerns; third, the growing interest in the relationship between human practices and communities and nonhuman nature. We also expand on the longstanding interest in just sustainabilities within this movement, illustrated by a wide range of concerns with food, energy, and climate justice. These new areas of work illustrate both recent developments and a set of paths forward for both the theory and practice of EJ.",Not About Sufficiency
The mediation effect of land surface temperature in the relationship between land use-cover change and energy consumption under seasonal variations,"Increasing temperatures in urban areas have brought about a series of prevalent environmental and energy shortage problems. Many studies have focused on the relationship between land use-cover change and land surface temperature (LST), and the effect of LST on energy consumption. The objective of this study was to identify the relationship among land use-cover change, LST, and building electricity consumption from 1990 to 2018 in the city of Nanjing, China. This investigation used land cover data and satellite imagery to obtain the land use-cover change data and LST data for each study year, and the mediation effect analysis method was then employed to examine the effect of land use-cover change and LST on building electricity consumption during different seasons. The results revealed that from 1990 to 2018 in Nanjing, the area of arable land decreased significantly, while the area of urban, rural, residential, industrial, and mining land increased significantly. During this period, the LST of Nanjing increased considerably. This study also found that the LST mediates the relationship between land use-cover change and electricity consumption, and the indirect effect in summer is particularly significant. Furthermore, the results revealed that the direct effect of land use-cover change on electricity consumption is intriguingly stronger than the indirect effect. These major findings can help urban planners to reasonably arrange and use land in order to alleviate LST increases and electricity consumption.",Not About Sufficiency
"Urban Sorcery, Segregation, and Ethnographic Spectacle in Twentieth-Century Rio de Janeiro","In twentieth-century Rio de Janeiro, police and local authorities addressed ""black magic practices"" through surveillance and regulation that were related to new cartographic and discursive imaginaries of urban reform and segregation. Police raids were a common occurrence, and journalists who wrote about sorcerers and sorcery participated in a discursive mapping of Rio de Janeiro's new urban imaginaries. This article examines a set of public health laws and policing tactics that monitored the activities of poor women and Afro-Brazilian spirituality under the assumption that their practices constituted black magic. Accusations of witchcraft represented a spectacle in which ideas adapted from eugenics and racial science to urban planning and capitalist modernity were enacted. Equally important, sorcery scenes present an important set of counter-narratives that demonstrate the ways in which urban residents deployed strategic performances as sorcerers and fortunetellers to counter police narratives that considered their bodies and activities to be heterodox and inadequate for secular urban modernity.",Not About Sufficiency
A Public Health Knowledge Management Repository that Includes Grey Literature,"Problem: Public health professionals rely heavily on resources that are often only available in grey literature format. However, while grey literature may contain comprehensive, concrete, and up-to-date information, the fugitive nature of this material makes access problematic. The public health community needs a knowledge management repository of grey literature and tools for easy and rapid access, so time spent searching across and through materials can be reduced. Goal: Design a customizable prototype public health knowledge management repository system and end-user interface with optimal interoperability and the capability to provide timely access to public health Information in support of decision making at the point and time of need. Specification of an appropriate metadata schema, which Identifies in a standardized way the elements needed to describe a resource, are a critical part of the system. The long-term goal is a system that delivers answers to public health questions, not a list of pointers to resources that may or may not contain information to answer those questions Evaluation Procedure: We are utilizing user needs analysis, user profiling, and resource assessment to inform understanding the information needs of public health professionals in the context of their everyday workflow and enable identification of key grey literature knowledge resources for incorporation into the knowledge management system. Rapid prototyping Is being used to translate these findings into system specifications and interface design of a small-scale prototype system. The prototype defines system components and interactivity both among components and with relevant external knowledge resources for example, the New York Academy of Medicine's Grey Literature collection, web resources from the Centers for Disease Control and Prevention, Department of Health materials, etc. The collection of materials will be organized utilizing resource metadata (high level formal, standards-based descriptions of documents) to improve location of relevant grey literature and other information sources. Results: Testing and evaluation will result in enhancements to the user interface, information resources, presentation of those resources, etc. We also anticipate that the metadata schema employed in a public health knowledge management system will improve the efficacy and efficiency of locating answers to public health questions from the grey literature. Conclusions: As the amount and breadth of public health information resources continue to expand it is critical that we find ways to provide direct access to the contents of these rich and complex resources. We believe that a public health grey literature knowledge management system with a collection of resources driven by the information needs of public health practitioners and organized using an appropriate metadata scheme will reduce time spent searching across and through materials, enhance public health decision making and ultimately improve the overall quality of public health services.",Not About Sufficiency
Natural Language Processing in Biomedicine : A Unified System Architecture Overview,"In contemporary electronic medical records much of the clinically important data-signs and symptoms, symptom severity, disease status, etc.-are not provided in structured data fields but rather are encoded in clinician-generated narrative text. Natural language processing (NLP) provides a means of unlocking this important data source for applications in clinical decision support, quality assurance, and public health. This chapter provides an overview of representative NLP systems in biomedicine based on a unified architectural view. A general architecture in an NLP system consists of two main components: background knowledge that includes biomedical knowledge resources and a framework that integrates NLP tools to process text. Systems differ in both components, which we review briefly. Additionally, the challenge facing current research efforts in biomedical NLP includes the paucity of large, publicly available annotated corpora, although initiatives that facilitate data sharing, system evaluation, and collaborative work between researchers in clinical NLP are starting to emerge.",Not About Sufficiency
Urban Green-Blue Space Utilization and Public Perceptions Amid the COVID-19 Pandemic: Insights from Northwest China,"The COVID-19 pandemic has reshaped our daily lives and the way we interact with urban green-blue spaces (UGBS), particularly in the economically challenged regions of Northwest China. Our study, utilizing surveys and social media, delves into the pandemic's impact on UGBS engagement in this area, offering critical insights for urban planning amidst a global health crisis. We found a gender-balanced but preference-specific engagement in UGBS, with women and married couples in the Chengguan District of Lanzhou city showing affinity. Moreover, educational levels and proximity to academic institutions emerged as key factors influencing UGBS use, pointing to the importance of educational attainment in engagement diversity. Enhancing safety, creating child-friendly and leisure facilities for families, and designing vibrant spaces for socializing are vital, and placing UGBS near educational districts could also promote environmental awareness and scientific learning. Furthermore, the pandemic has reshaped public priorities, elevating the value of accessible, safe UGBS. This shift is evidenced by varied motivations for UGBS visits, with an emphasis on health, nature connectivity, and leisure. Women, older adults, and families, each with their distinct reasons, were drawn to UGBS for activities ranging from recreation to relaxation. Our findings advocate for the creation of multifunctional UGBS that cater to these varied interests, incorporating features such as air-purifying plants, scenic pathways, and zones for family activities, all underpinned by enhanced safety and accessibility. The study also highlights distinct transportation preferences among residents of Chengguan's northern and southern parts, suggesting a tailored approach to urban infrastructure that accommodates pedestrian access and public transit use. To prevent overcrowding, adjusting facility hours and event timings based on peak visitation times is recommended. Moreover, improving walkways and public transport connectivity is essential not just for convenience but also for ensuring that these green spaces are equitable and financially accessible, fostering inclusive access to these essential urban areas. During the pandemic, social media revealed a growing search for spiritual fulfillment within UGBS, highlighting their importance in societal well-being and coping mechanisms. In response, there's a compelling opportunity for UGBS to evolve by incorporating designated areas for spiritual relaxation, along with mental health support services. By actively monitoring social media feedback and trends, these spaces can adapt and refine their offerings, ensuring that they meet the community's changing needs more effectively. Our study highlights the importance of tailoring UGBS to meet diverse community needs, especially during crises. It emphasizes creating multifunctional, accessible UGBS that reflect demographic trends, transportation habits, and public preferences, aiming to boost community resilience and well-being. Drawing from research conducted amidst a worldwide crisis, our study provides key recommendations for the future evolution of UGBS, urging the creation of inclusive environments that bolster the health and well-being of urban populations.",Not About Sufficiency
An incentive compatible ZD strategy-based data sharing model for federated learning: A perspective of iterated prisoner's dilemma,"Federated learning has been increasingly adopted as an effective means to cope with the significant increase in the volume of training data needed for machine learning and address the privacy concerns in using these data. However, moral hazard may occur when individual data providers (IDPs) use smaller amounts or low -quality data to train their local models and submit these low -quality results (gradients) to free -ride on the benefits of the federated learning. Therefore, federated learning operators often face the dilemma of encouraging more IDPs to participate in data sharing and ensuring truthful contributions from IDPs to obtain high -quality global training results. This article proposes a spontaneous cooperative data -sharing model to address this dilemma. Through an iterated prisoner's dilemma model solved by the zero -determinant (ZD) strategy, we show that the optimal ZD strategies of all IDPs are to maximize their training efforts when participating in federated learning. According to the comparisons with other approaches through simulations, we demonstrate that either the two-IDP with binary strategies case or the multi-IDP with continuous strategies case could result in the optimal individual utility and social welfare. Therefore, the proposed spontaneous cooperative model effectively avoids the existing moral hazard problem in federated learning and provides a viable instrument for the federated learning operator to maximize the performance of the global model without the need to evaluate the quality of local gradients.",Not About Sufficiency
"Assessment and optimization of urban spatial resilience from the perspective of life circle: A case study of Urumqi, NW China","This paper presents an original framework for assessing and enhancing urban resilience from the perspective of the life circle that emphasizes the daily activity spaces of residents, using Urumqi, a major city in northwest China 's arid oasis region, as a case study. Based on multi -source urban geographic big data, we introduce Gini coefficient and Lorenz curve to analyze the spatial matching relationship between urban spatial pressure and urban spatial resilience, and divided the priority of planning intervention by priority index. Our findings for the case study Urumqi reveal a strong ""center -periphery"" structure of urban resilience and spatial pressures. Based on the findings, we proposed targeted planning interventions prioritized by a resilience index that considers the life circle to address disparities and improve urban resilience. Our study advocates for urban planning to focus on the life circle, aiming to develop a multi -center and cluster -type resilient urban spatial structure to foster a ""peopleoriented"" resilient city.",Not About Sufficiency
Plasma proteomics identify biomarkers predicting Parkinson's disease up to 7 years before symptom onset,"Parkinson's disease is increasingly prevalent. It progresses from the pre-motor stage (characterised by non-motor symptoms like REM sleep behaviour disorder), to the disabling motor stage. We need objective biomarkers for early/pre-motor disease stages to be able to intervene and slow the underlying neurodegenerative process. Here, we validate a targeted multiplexed mass spectrometry assay for blood samples from recently diagnosed motor Parkinson's patients (n = 99), pre-motor individuals with isolated REM sleep behaviour disorder (two cohorts: n = 18 and n = 54 longitudinally), and healthy controls (n = 36). Our machine-learning model accurately identifies all Parkinson patients and classifies 79% of the pre-motor individuals up to 7 years before motor onset by analysing the expression of eight proteins-Granulin precursor, Mannan-binding-lectin-serine-peptidase-2, Endoplasmatic-reticulum-chaperone-BiP, Prostaglaindin-H2-D-isomaerase, Interceullular-adhesion-molecule-1, Complement C3, Dickkopf-WNT-signalling pathway-inhibitor-3, and Plasma-protease-C1-inhibitor. Many of these biomarkers correlate with symptom severity. This specific blood panel indicates molecular events in early stages and could help identify at-risk participants for clinical trials aimed at slowing/preventing motor Parkinson's disease. Parkinson's disease is lacking easily accessible biomarkers. Here the authors show, that targeted blood proteomics is feasible to identify the patients and to predict the phenoconvertion in prodromal subjects up to 7 years before symptom onset.",Not About Sufficiency
Data-Driven Incentive Design in the Medicare Shared Savings Program,"The Medicare Shared Savings Program (MSSP) was created under the Patient Protection and Affordable Care Act to control escalating Medicare spending by incentivizing providers to deliver healthcare more efficiently. Medicare providers that enroll in the MSSP earn bonus payments for reducing spending to below a risk-adjusted financial benchmark that depends on the provider's historical spending. To generate savings, a provider must invest to improve efficiency, which is a cost that is absorbed entirely by the provider under the current contract. This has proven to be challenging for the MSSP, with a majority of participating providers unable to generate savings owing to the associated costs. In this paper, we propose a predictive analytics approach to redesigning the MSSP contract with the goal of better aligning incentives and improving financial outcomes from the MSSP. We formulate the MSSP as a principal-agent model and propose an alternate contract that includes a performance-based subsidy to partially reimburse the provider's investment. We prove the existence of a subsidy-based contract that dominates the current MSSP contract by producing a strictly higher expected payoff for both Medicare and the provider. We then propose an estimator based on inverse optimization for estimating the parameters of our model. We use a data set containing the financial performance of providers enrolled in the MSSP, which together accounts for 7 million beneficiaries and more than $70 billion in Medicare spending. We estimate that introducing performance-based subsidies to the MSSP can boost Medicare savings by up to 40% without compromising provider participation in the MSSP. We also find that the subsidy-based contract performs well in comparison with a fully flexible nonparametric contract.",Not About Sufficiency
Application of <SUP>18</SUP>F-fluorodeoxyglucose PET/CT radiomic features and machine learning to predict early recurrence of non-small cell lung cancer after curative-intent therapy,"ObjectiveTo predict the recurrence of non-small cell lung cancer (NSCLC) within 2 years after curative-intent treatment using a machine-learning approach with PET/CT-based radiomics. Patients and methodsA total of 77 NSCLC patients who underwent pretreatment F-18-fluorodeoxyglucose PET/CT were retrospectively analyzed. Five clinical features (age, sex, tumor stage, tumor histology, and smoking status) and 48 radiomic features extracted from primary tumors on PET were used for binary classifications. These were ranked, and a subset of useful features was selected based on Gini coefficient scores in terms of associations with relapsed status. Areas under the receiver operating characteristics curves (AUC) were yielded by six machine-learning algorithms (support vector machine, random forest, neural network, naive Bayes, logistic regression, and gradient boosting). Model performances were compared and validated via random sampling. ResultsA PET/CT-based radiomic model was developed and validated for predicting the recurrence of NSCLC during the first 2 years after curation. The most important features were SD and variance of standardized uptake value, followed by low-intensity short-zone emphasis and high-intensity zone emphasis. The naive Bayes model with the 15 best-ranked features displayed the best performance (AUC: 0.816). Prediction models using the five best PET-derived features outperformed those using five clinical variables. ConclusionThe machine learning model using PET-derived radiomic features showed good performance for predicting the recurrence of NSCLC during the first 2 years after a curative intent therapy. PET/CT-based radiomic features may help clinicians improve the risk stratification of relapsed NSCLC.",Not About Sufficiency
An Investigation of Automatic Food Logging for Health Applications Based on Video Camera Glasses,"Dietary monitoring can provide valuable information for individuals, while aggregated information from diet logs is important for public health. Although several food logging systems have been presented in the scientific literature, a small number have employed smartglasses. In this paper, we introduce eyewear-based software architecture for tag extraction from video by using several machine learning models. We demonstrate our software architecture with a system designed to help smart eyewear users to perform food logging in an automatic, passive way. Also, we conduct an experiment to report the performance of our system in various conditions.",Not About Sufficiency
Kinetically-driven reactivity of sulfinylamines enables direct conversion of carboxylic acids to sulfinamides,"Sulfinamides are some of the most centrally important four-valent sulfur compounds that serve as critical entry points to an array of emergent medicinal functional groups, molecular tools for bioconjugation, and synthetic intermediates including sulfoximines, sulfonimidamides, and sulfonimidoyl halides, as well as a wide range of other S(iv) and S(vi) functionalities. Yet, the accessible chemical space of sulfinamides remains limited, and the approaches to sulfinamides are largely confined to two-electron nucleophilic substitution reactions. We report herein a direct radical-mediated decarboxylative sulfinamidation that for the first time enables access to sulfinamides from the broad and structurally diverse chemical space of carboxylic acids. Our studies show that the formation of sulfinamides prevails despite the inherent thermodynamic preference for the radical addition to the nitrogen atom, while a machine learning-derived model facilitates prediction of the reaction efficiency based on computationally generated descriptors of the underlying radical reactivity.",Not About Sufficiency
Autoencoders for sample size estimation for fully connected neural network classifiers,"Sample size estimation is a crucial step in experimental design but is understudied in the context of deep learning. Currently, estimating the quantity of labeled data needed to train a classifier to a desired performance, is largely based on prior experience with similar models and problems or on untested heuristics. In many supervised machine learning applications, data labeling can be expensive and time-consuming and would benefit from a more rigorous means of estimating labeling requirements. Here, we study the problem of estimating the minimum sample size of labeled training data necessary for training computer vision models as an exemplar for other deep learning problems. We consider the problem of identifying the minimal number of labeled data points to achieve a generalizable representation of the data, a minimum converging sample (MCS). We use autoencoder loss to estimate the MCS for fully connected neural network classifiers. At sample sizes smaller than the MCS estimate, fully connected networks fail to distinguish classes, and at sample sizes above the MCS estimate, generalizability strongly correlates with the loss function of the autoencoder. We provide an easily accessible, code-free, and dataset-agnostic tool to estimate sample sizes for fully connected networks. Taken together, our findings suggest that MCS and convergence estimation are promising methods to guide sample size estimates for data collection and labeling prior to training deep learning models in computer vision.",Not About Sufficiency
Human milk composition and infant anthropometrics: overview of a systematic review with clinical and research implications,"Background Despite global public health organizations endorsing breastfeeding or human milk (HM) as the optimal source of nutrition for infants, detailed knowledge of how HM composition influences infant growth is lacking. In this commentary we summarize and interpret the key findings of a large systematic review on HM components and child growth (N = 141 articles included). We highlight the most consistent associations, discuss study quality issues, explore socio-economic and time trends in this body of research, and identify gaps and future research directions. Key Findings of Systematic Review We grouped HM components into three categories: micronutrients (28 articles), macronutrients (57 articles), and bioactives (75 articles). Overall, we struggled to find consistent associations between HM components and infant growth. The majority of studies (85%) were of moderate or low-quality, with inconsistent HM collection and analysis strategies being identified as the most substantial quality concerns. Additional quality issues included failing to account for potential confounding by factors such as breastfeeding exclusivity and maternal body mass index. Considerations for Future Human Milk Research Many opportunities exist for the future of HM research. Using untargeted metabolomics will expand our understanding of HM components beyond previously defined and well-understood components. Machine learning will allow researchers to investigate HM as an integrated system, rather than a collection of individual components. Future research on HM composition should incorporate evidence-based HM sampling strategies to encompass circadian variation as well as infant consumption. Additionally, researchers need to focus on developing high quality growth data using consistent growth metrics and definitions. Building multidisciplinary research teams will help to ensure that outcomes are meaningful and clinically relevant. Conclusion Despite a large body of literature, there is limited quality evidence on the relationship between HM composition and infant growth. Future research should engage in more accurate collection of breastfeeding data, use standardized HM collection strategies and employ assays that are validated for HM. By systematically evaluating the existing literature and identifying gaps in existing research methods and practice, we hope to inspire standardized methods and reporting guidelines to support robust strategies for examining relationships between HM composition and child growth.",Not About Sufficiency
Data-driven analysis for the characterization of household appliance ownership and use in Sub-Saharan Africa,"To grant reliable and affordable electricity provision to non-electrified communities, proper system sizing, based on accurate demand estimation is crucial. However, the absence of historical data, and scarce, scattered, and often unreliable pre-electrification surveys, make this process particularly prone to errors. Acquiring data, especially with high quality and detail, is difficult, time-consuming and expensive. Even though, in a few sitespecific cases the limited data collected has allowed researchers to develop methodologies to generate synthetic demand profiles based on variegated site-specific socio-economic information and appliance adoption patterns. However, given the lack of comprehensive datasets of such information, the use of synthetic methodologies has been circumscribed to limited regional and socio-economic scopes. This research proposes the development of a data-driven machine-learning framework for estimating appliance adoption patterns with a subset of relevant socio-economic indicators, identified throughout a comprehensive literature analysis and data collection across various sources. To successfully train the model, a novel open-access database has been created and populated with socio-economic information combined with appliance data collected from public and private sources. Finally, a structured logistic regression analysis has been performed, not only to capture the nexus of socio-economic factors with appliance adoption but also to estimate the most relevant ones. The methodology calibrated with the proposed open-access database has shown 71.7 % accuracy, which represents an important achievement in the field. The study's findings lay the foundations for simplifying the estimation of appliance adoption, which can facilitate the demand estimation for sizing rural energy systems and rural electrification approaches.",Not About Sufficiency
Key Applications of State and Health Estimation,"In the previous two chapters, we have developed theoretical foundation of data-driven methods for lithium-ion batteries. In this chapter, we present test cases to demonstrate applicability and capabilities of these methods for lithium-ion battery state estimation. First, we explore the recursive Bayesian framework for state of charge estimation. In this chapter, we compare the unscented Kalman filter and particle filter for state of charge estimation. Functionality of these algorithms is demonstrated for a commercial NCA/C cell state estimation at different operating conditions including constant current discharge at room and low temperatures, hybrid power pulse characterization (HPPC), and urban driving schedule (UDDS) protocols. In addition to accurate voltage prediction, the electrochemical nature of ROM enables drawing of physical insights into the cell behavior. Advantages of using electrode concentrations over conventional Coulomb counting for accessible capacity estimation are discussed. In addition to the mean state estimation, the framework also provides estimation of the associated confidence bounds that are used to establish predictive capability of the proposed framework. Next, we demonstrate applicability of the machine learning algorithms for lithium-ion battery state of health estimation. For this, we present a novel method that utilizes both the classification and regression flavors of the machine learning algorithms. For demonstration purpose, we consider SVM for classification and regression; however, other approaches can be similarly used. For this demonstration, we have used a publicly available battery life testing dataset for training the SVM/R algorithm and subsequently tested our approach on a different subset of the dataset.",Not About Sufficiency
Identification of gene expression signature for drought stress response in barley (Hordeum vulgare L.) using machine learning approach,"Barley (Hordeum vulgare L.) is an important cereal crop, playing a pivotal role in global agriculture and food systems. Drought has a significant impact on barley growth and yield productivity. In the current study, core drought stress responsive genes were investigated using an integrative approach. First, we determined the core differentially expressed genes (DEGs) in multiple RNA-seq experiments using a p-value combination approach. Then, machine learning approaches including four weighting algorithms were harnessed for prioritization and determination of signature genes. Moreover, predictive models were optimized using tree induction and naive Bayes algorithms. Finally, the functional importance of the core DEGs and signature genes and pathways were dissected using gene ontology, KEGG enrichment, and protein-protein interaction network analysis. Results showed that the core DEGs participate in carbon metabolism, biosynthesis of secondary metabolites, glyoxylate and dicarboxylate metabolism, carbon fixation, biosynthesis and degradation of amino acids, glycolysis/gluconeogenesis, pyruvate metabolism, starch and sucrose metabolism, glycerolipid metabolism, beta-alanine metabolism, ascorbate and aldarate metabolism, taurine and hypotaurine metabolism. Notably, the C4.5 algorithm, boasting a remarkable 100 % accuracy, pinpointed two genes of particular importance including HORVU. MOREX.R3.1HG0063740, encoding the endo-1, 3-1, 4-beta-D-glucanase, and HORVU.MOREX.R3.1HG0083720, which encodes the bifunctional inhibitor/lipid-transfer protein. This comprehensive analysis contributes significantly to understanding of the core drought responsive genes and pathways. Moreover, these findings lay the groundwork for further research aimed at developing drought-resistant barley varieties and utilizing predictive models in field screening programs.",Not About Sufficiency
Associations between urban metrics and mortality rates in England,"Background: Seventy-five percent of the population in Europe live in urban areas and analysing the effects of urban form on the health of the urban population is of great public health interest. Not much is known, however, on the effects of urban form on the health of city dwellers. This study uses a novel approach to investigate whether associations exist between different measures of urban form and mortality risks in cities in England. Methods: We conducted an ecological, cross-sectional study for urban areas in England with more than 100,000 residents (n = 50) and included all registered premature deaths (<65 years) between 1st January 2002 and 31st December 2009. To describe and categorise urban form we quantified the distribution and density of population, land cover and transport networks and measures of geographical characteristics. We used Poisson regression models to examine associations between the measures of urban form and age-standardised risks of deaths from all causes, cardiovascular disease, and traffic accidents after adjustment for socioeconomic status and smoking. Analysis was stratified by gender to explore differential associations between females and males. Results: There were a total of 200,200 premature deaths during the study period (Females: 37 %; Males: 63 %). Transport network patterns were associated with overall and cardiovascular mortality rates in cities. We saw 12 % higher mortality risk after adjustment in cities with high junction density compared to cities with low density [Females: RR 1.12 (95 % CI 1.10 - 1.15); Males: RR 1.12 (95 % CI 1.10-1.14)]; the risk was slightly higher for cardiovascular mortality [Females: RR 1.16 (95 % CI 1.10 - 1.22); Males: RR 1.12 (95 % CI 1.09 - 1.16)]. Associations between mortality and population patterns were of similar magnitude [Females: RR 1.10 (95 % CI 1.09 - 1.13); Males: RR 1.09 (95 % CI 1.07-1.10)]; associations between mortality and land cover patterns were inconclusive. Conclusions: We found an association between transport patterns and risk of premature mortality. Associations between urban form and mortality observed in this study suggest that characteristics of city structure might have negative effects on the overall health of urban communities. Future urban planning and regeneration strategies can benefit from such knowledge to promote a healthy living environment for an increasing urban population.",Not About Sufficiency
Chem2Side: A Deep Learning Model with Ensemble Augmentation (Conventional,"Drug side effects (DSEs) or adverse drug reactions (ADRs) are a major concern in the healthcare industry, accounting for a significant number of annual deaths in Europe alone. Identifying and predicting DSEs early in the drug development process is crucial to mitigate their impact on public health and reduce the time and costs associated with drug development. Objective: In this study, our primary objective is to predict multiple drug side effects using 2D chemical structures, especially for COVID-19, departing from the conventional approach of relying on 1D chemical structures. We aim to develop a novel model for DSE prediction that leverages the CNN-based transfer learning architecture of ResNet152V2. Motivation: The motivation behind this research stems from the need to enhance the efficiency and accuracy of DSE prediction, enabling the pharmaceutical industry to identify potential drug candidates with fewer adverse effects. By utilizing 2D chemical structures and employing data augmentation techniques, we seek to revolutionize the field of drug side-effect prediction. Novelty: This study introduces several novel aspects. The proposed study is the first of its kind to use 2D chemical structures for predicting drug side effects, departing from the conventional 1D approaches. Secondly, we employ data augmentation with both conventional and diffusion-based models (Pix2Pix), a unique strategy in the field. These innovations set the stage for a more advanced and accurate approach to DSE prediction. Results: Our proposed model, named CHEM2SIDE, achieved an impressive average training accuracy of 0.78. Moreover, the average validation and test accuracy, precision, and recall were all at 0.73. When evaluated for COVID-19 drugs, our model exhibited an accuracy of 0.72, a precision of 0.79, a recall of 0.72, and an F1 score of 0.73. Comparative assessments against established transfer learning and machine learning models (VGG16, MobileNetV2, DenseNet121, and KNN) showcased the exceptional performance of CHEM2SIDE, marking a significant advancement in drug side-effect prediction. Conclusions: Our study introduces a groundbreaking approach to predicting drug side effects by using 2D chemical structures and incorporating data augmentation. The CHEM2SIDE model demonstrates remarkable accuracy and outperforms existing models, offering a promising solution to the challenges posed by DSEs in drug development. This research holds great potential for improving drug safety and reducing the associated time and costs.",Not About Sufficiency
Battery capacity trajectory prediction by capturing the correlation between different vehicles,"Advances in battery technology and dwindling oil resources have greatly boosted the popularity of electric ve-hicles (EVs). Accurate prediction of battery capacity trajectory is critical to ensure safety and timely maintenance of EVs. However, present studies only based on laboratory data. To bridge this gap, this paper proposes a data -driven capacity prediction framework using the vehicle field data, which can automatically match the aging pattern (AP) of the vehicle and fully utilizes the correlation between vehicles and does not need to extract features. The real capacity of battery pack in vehicle is calculated by ampere-hour integration method combined with open circuit voltage correction. To assess the overall effectiveness of this diagnostic methodology, the target vehicle's accessible data is divided into three stages: early, middle, and late. It is demonstrated that after automatic matching of AP, the average mean absolute percentage error (MAPE) of capacity prediction based on the multioutput spectral mixture Gaussian process (MOSMGP) are reduced by 9.82%, 21.25%, and 26.92% respectively in three different aging stages. Compared with the other six machine learning methods, the MOSMGP has the highest accuracy in early prediction. Its average MAPE and average root mean squared error (RMSE) are only 1.39% and 1.92Ah, respectively.",Not About Sufficiency
"Including the urban heat island in spatial heat health risk assessment strategies: a case study for Birmingham, UK","Background: Heatwaves present a significant health risk and the hazard is likely to escalate with the increased future temperatures presently predicted by climate change models. The impact of heatwaves is often felt strongest in towns and cities where populations are concentrated and where the climate is often unintentionally modified to produce an urban heat island effect; where urban areas can be significantly warmer than surrounding rural areas. The purpose of this interdisciplinary study is to integrate remotely sensed urban heat island data alongside commercial social segmentation data via a spatial risk assessment methodology in order to highlight potential heat health risk areas and build the foundations for a climate change risk assessment. This paper uses the city of Birmingham, UK as a case study area. Results: When looking at vulnerable sections of the population, the analysis identifies a concentration of ""very high"" risk areas within the city centre, and a number of pockets of ""high risk"" areas scattered throughout the conurbation. Further analysis looks at household level data which yields a complicated picture with a considerable range of vulnerabilities at a neighbourhood scale. Conclusions: The results illustrate that a concentration of ""very high"" risk people live within the urban heat island, and this should be taken into account by urban planners and city centre environmental managers when considering climate change adaptation strategies or heatwave alert schemes. The methodology has been designed to be transparent and to make use of powerful and readily available datasets so that it can be easily replicated in other urban areas.",Not About Sufficiency
To pay or not to pay? Understanding public acceptance of congestion pricing: A case study of Nanjing,"Congestion pricing has been unquestionably recognized as an efficient strategy for managing traffic demand following the successful introduction of such schemes in a number of cities. However, the lack of political and public acceptance can be blamed for the nonexecution of congestion pricing projects in numerous cities around the world. This paper sheds light on the impacts of congestion pricing and the factors influencing its public acceptance. Our research was aimed to answer the following questions: (i) What are the factors that can influence public acceptance of congestion pricing? (ii) What are the impacts of implementing a congestion pricing scheme? (iii) How can we overcome the barriers that currently stand in the way of public acceptance of congestion pricing? To answer these questions, we developed a case study combining stated preference and revealed preference data collected in Nanjing, China. The study analyzes the acceptance of congestion pricing and the factors influencing it, such as socioeconomics, the perceived impact, fairness and public transit-related factors. We compare logistic regression and artificial neural network models to gain a deeper knowledge of the important factors and investigate the respondents' attitudes. The results revealed that the perceived impacts on congestion, the environment, trips to the city center, revenue allocation, public transportation price satisfaction, annual income, fairness, car ownership and travel frequency, along with the efficiency and capacity of public transport systems, need to be included when evaluating individuals' acceptance of congestion pricing. Among these, the perceived impacts on congestion and the environment, fairness and revenue allocation to public transportation are the most significant factors. Moreover, we offer further qualitative insight into the individual, economic and social impacts of congestion pricing. This paper provides decision- and policy-makers with important advice on how to promote public acceptance when considering the implementation of a congestion pricing scheme.",Not About Sufficiency
Making SHAP Rap: Bridging Local and Global Insights Through Interaction and Narratives,"The interdisciplinary field of explainable artificial intelligence (XAI) aims to foster human understanding of black-box machine learning models through explanation-generating methods. In practice, Shapley explanations are widely used. However, they are often presented as visualizations and thus leave their interpretation to the user. As such, even ML experts have difficulties interpreting them appropriately. On the other hand, combining visual cues with textual rationales has been shown to facilitate understanding and communicative effectiveness. Further, the social sciences suggest that explanations are a social and iterative process between the explainer and the explainee. Thus, interactivity should be a guiding principle in the design of explanation facilities. Therefore, we (i) briefly review prior research on interactivity and naturalness in XAI, (ii) designed and implemented the interactive explanation interface SHAPRap that provides local and global Shapley explanations in an accessible format, and (iii) evaluated our prototype in a formative user study with 16 participants in a loan application scenario. We believe that interactive explanation facilities that provide multiple levels of explanations offer a promising approach for empowering humans to better understand a model's behavior and its limitations on a local as well as global level. With our work, we inform designers of XAI systems about human-centric ways to tailor explanation interfaces to end users.",Not About Sufficiency
Search for H ? c(c)over-bar at a Multi-TeV Muon Collider,"A Multi-TeV (root s = 1.5 - 10 TeV) Muon Collider providing O(ab(-1)) integrated luminosity will be a great opportunity to probe the most intimate nature of the Standard Model (SM) and the Electroweak Symmetry Breaking mechanism, allowing the precise measurement of the Higgs couplings to several SM particles. The study of the Higgs boson couplings to the second generations of fermions is of particular interest due to sensitivity to a whole class of new physics models. It is also true that this measurement is extremely challenging, because of the small branching ratio. Indeed, it is currently not accessible at LHC, where the quantum chromodynamics processes are overwhelming. In this paper it is explored, for the first time, the search for H -> c (c) over bar at a Multi-TeV Muon Collider. The mu(+)mu(-) -> H nu(nu) over bar -> c (c) over bar nu(nu) over bar signal process has been fully simulated and reconstructed at root s = 1.5 TeV with a preliminary detector design, along with the main physics backgrounds. The machine background originated from the decay of beam muons, the so-called Beam Induced Background (BIB), is not included in this preliminary study. A c quark-tagging algorithm has been developed, combining several observables in a single discriminator using Machine Learning techniques, with the goal to improve the rejection of jets coming from b-quark and u-d-s-g hadronization. A first estimate of the precision on the Higgs coupling with c-quark reachable with a Muon Collider machine is presented. The relative uncertainty on the coupling at root s = 1.5 TeV is estimated to be 5.5%. A projection to root s = 3 TeV shows that the precision improves with increasing energy, reaching the value of 2.6%.",Not About Sufficiency
Sustainable Framework for Smart Transportation System: A Case Study of Karachi,"In this paper, a framework of smart transportation system is proposed, aiming to address the transportation problem in Karachi city. In modern day world, the mega cities and urban areas are on the edge of transformation into smart cities. With the advancement of engineering and technology, smart cities are designed to integrate and utilize these scientific innovations to provide smart solutions and social innovations for sustainable infrastructure, thus they are able to provide its resident highest quality of life by utilizing its resources effectively. One of the major application of smart cities is the Smart Transportation System, which provides safer, quick, environment friendly service to the residents. Thus, this study highlights the current traffic situation of Karachi and propose a framework to transform it into a smart transportation system. In order to have a smart transportation system, it is necessary to have in-depth knowledge and information about the city dynamics and its traffic related issues. Therefore, this study also highlights current traffic situation of Karachi, its road conditions and capacity, vehicles condition, alternate mean of transport (other than road-based system) and its present condition, and finally proposes a framework to develop a smart transportation system while keeping in mind the aforesaid traffic problems.",Not About Sufficiency
Managing supply chain performance using a real time Microsoft Power BI dashboard by action design research (ADR) method,"Supply chain (SC) performance is being advanced through digitalization, automation, and real-time visibility, leading to improved operational efficiency, optimized inventory management, and proactive solutions for problems. This study focuses on managing SC performance by developing a real-time Microsoft Power BI dashboard using the ADR method. The aim is to help small business owners effectively and affordably manage their supply chains. The research analyzed data from Ly Foods Ltd. and utilized Microsoft Power BI to create the dashboard. The study highlights how this state-of-the-art dashboard enhances the measurement of operational and decision-making processes by providing efficient, visible, accessible, and shareable information. The findings demonstrate the potential benefits of leveraging advanced technologies to enhance supply chain management (SCM) practices. This research makes a significant contribution to the field of SCM by illustrating how SC performance can be managed using the ADR method and the SC key performance indicators (KPIs). The results of this study have important implications for businesses of all sizes, as they enhance SC efficiency and profitability.",Not About Sufficiency
Changing Cities in the Perspective of Religious Tourism - A case of Allahabad,"Urban planning is a tool, which enables in managing changes within a spatial approach. It helps in conducting planning activities where the economic, environmental, and social goals are taken care of holistically. Planning process involves studying past trends, learning from past and present, and then preparing a proposal for future. This process mostly assesses the trend of growth and plans for development. A trend follows change in a defined rate. An indefinite or unexpected change spoils the planning process, by breaking the trend of growth. City is known for its resources, geography, population, social-culture structure, economy etc. Whenever there is an unexpected change in any of these parameters, the city undergoes remarkable changes. Disaster also brings hasty changes in all these, which is indeed a challenge to urban planning and design. The fast modernization and adoption of western culture leads to significant changes in socio-cultural and economic spheres. City also changes, when rapid movement of people and resources occur. Tourism industry in this decade brought many changes is Indian cities. It diversifies social and cultural identity of the city and generates tremendous opportunity for economic development. Therefore cities having tourism potential are facing urban planning problems. Exploring these issues and reasons behind it will be helpful to minimize its impact. Tourism cities have tremendous potential of research in this direction. The need of the decade for urban planners and designers is to be prepared for the changes in a tourist city. (C) 2016 The Authors. Published by Elsevier Ltd.",Not About Sufficiency
Developing an Observing Air-Sea Interactions Strategy (OASIS) for the global ocean,"The Observing Air-Sea Interactions Strategy (OASIS) is a new United Nations Decade of Ocean Science for Sustainable Development programme working to develop a practical, integrated approach for observing air-sea interactions globally for improved Earth system (including ecosystem) forecasts, CO2 uptake assessments called for by the Paris Agreement, and invaluable surface ocean information for decision makers. Our ""Theory of Change"" relies upon leveraged multi-disciplinary activities, partnerships, and capacity strengthening. Recommendations from >40 OceanObs'19 community papers and a series of workshops have been consolidated into three interlinked Grand Ideas for creating #1: a globally distributed network of mobile air-sea observing platforms built around an expanded array of long-term time-series stations; #2: a satellite network, with high spatial and temporal resolution, optimized for measuring air-sea fluxes; and #3: improved representation of air-sea coupling in a hierarchy of Earth system models. OASIS activities are organized across five Theme Teams: (1) Observing Network Design & Model Improvement; (2) Partnership & Capacity Strengthening; (3) UN Decade OASIS Actions; (4) Best Practices & Interoperability Experiments; and (5) Findable-Accessible-Interoperable-Reusable (FAIR) models, data, and OASIS products. Stakeholders, including researchers, are actively recruited to participate in Theme Teams to help promote a predicted, safe, clean, healthy, resilient, and productive ocean.",Not About Sufficiency
Weather-aware energy management for unmannedaerial vehicles: a machine learning application with global data integration,"This study introduces a machine learning (ML) framework to predict unmanned aerial vehicle (UAV) energy requirements under diverse environmental conditions. The framework correlates UAV flight patterns with publicly accessible weather data, to yield an energy management tool applicable to a wide range of UAV configurations. The model employs the Cross-industry standard process for data mining and advanced feature engineering, offering an in-depth analysis of meteorological factors and UAV energy demands. The study assesses several multi-regression linear and ML models, whereby ensemble models gradient boosting (GB) and eXtreme gradient boosting demonstrate superior performance and accuracy. Specifically, the GB model achieved a test mean absolute error (MAE) of 0.0395 V (V) for voltage, 0.808 A (A) for current, and 9.758 mA-hours (mAh) for discharge, with prediction accuracy of over 99.9% for voltage and discharge, and 97% for current, derived from the coefficient of determination (R2). A novel integration of real-world UAV logs and weather data underpins the development of a weather-aware ML prediction model for UAV energy consumption. Our framework is capable of concurrently predicting three components of energy and power with almost uniform accuracy, a feature not found in contemporary models. Empirical test flights show a discrepancy of only 0.005 W-hour (Wh) between total predicted and actual energy consumption. This work enhances both efficiency and safety in UAV operations. The resulting energy-predictive flight planning tool sets a new benchmark for artificial intelligence (AI) applications in intelligent automation for UAVs.",Not About Sufficiency
Why do we continue to use standardized mortality ratios for small area comparisons?,"Public health practitioners are often faced with the necessity to compare the mortality experience of different geographical areas. indirect standardization, producing a 'standardized mortality ratio' (SMR) is the most commonly used technique for doing this. However, as we show, indirect standardization is inappropriate for such comparisons, as SMRs for different geographical areas have different denominators. The fact that in direct standardization is usually chosen for th is type of comparison is probably based on two beliefs: (1)that direct standardization yields only a rate rather than a more easily interpreted ratio or index; (2) that direct standardization cannot he carried out in many cases because the sub-group specific mortality rates in the groups to be compared are not available or, in at least some age classes, are based upon such small numbers as to be completely unreliable. In this paper we show that a simple index (the comparative mortality figure) can be calculated from the directly standardized rate in most cases. Using a comparison of the overall mortality experience of electoral wards in Sheffield between 1980 and 1987 we demonstrate also that the advantage gained by the smaller standard error of the SMR is outweighed by the bias inherent in its construction. We recommend that the SMR is used only when absolutely necessary, that is, in the rare circumstance when data are not available for the calculation of age- and sex-specific subgroup rates in the study population.",Not About Sufficiency
A dual-branch weakly supervised learning based network for accurate mapping of woody vegetation from remote sensing images,"Mapping woody vegetation from aerial images is an important task bluein environment monitoring and management. A few studies have shown that semantic segmentation methods involving deep learning achieve significantly better performance in mapping than methods involving field-based measurement and handcrafted features. However, current deep networks used for mapping vegetation require labour-intensive pixel-level annotations. Thus, this paper proposes the use of image-level annotations and a weakly supervised semantic segmentation (WSSS) network for mapping woody vegetation based on Unmanned Aerial Vehicle (UAV) imagery. The network comprises a Localization Branch (LB) and an Attention Relocation Branch (ARB). The LB is trained in stage 1 of the mapping to identify regions with the most discriminative vegetation, while the ARB is introduced to better mine semantic information, which enhances the ability of the class activation maps (CAMs) to represent useful information. The ARB inherits the weights from the LB in stage 2 and uses a Multi-layer Attention Refocus Structure (MARS) into the network to expand the receptive field to enable the model to process global features. Thus, same-category regions that are located farther apart are better captured. Finally, the region focused by the dual branches are integrated to more accurately cover the areas to be segmented. Using UAV imagery datasets, namely UOPNOA and MiniFrance, along with quantitative metrics and qualitative results, the network demonstrates performance better than existing state-of-the-art related methods. The effectiveness and generalization of each module of the network are validated by ablation experiments. The code for implementing the network will be accessible on https://github.com/Mr-catc/DWSLNet.",Not About Sufficiency
"""I Felt Safe"": The Role of the Rapid Rehousing Program in Supporting the Security of Families Experiencing Homelessness in Salt Lake County, Utah","Homelessness is a public health issue that many organizations are addressing through a Housing First Model. One such organization is The Road Home (TRH), which provides services to homeless individuals and families in Salt Lake County. TRH is perhaps best known for their emergency shelters, but the organization also administers the Rapid Rehousing Program (RRHP), designed to help families experiencing homelessness transition back into stable housing. Those experiencing homelessness tend to have high rates of chronic mental/physical disabilities as well as issues related to substance abuse. Having a home is the first step toward achieving some kind of stability in their lives. The RRHP allows families to find housing in the private rental market and will cover the initial costs and several months of rent for clients. While the program has been praised by policymakers and social service providers for helping homeless families find rental housing, there is no empirical research about participant perspectives regarding their residential (in)security. The research question of this article is: what is the role of the RRHP in supporting the security of families experiencing homelessness? Researchers collected qualitative data through focus groups and interviews with 31 participants, 23 families experiencing homelessness, two landlords, six case managers, and service providers. Lastly, we identify recommendations for program improvements based on information gathered from research participants. It is our hope that the information presented in this article can and will be used in a way that improves public health by increasing the residential security of families experiencing homelessness.",Not About Sufficiency
Integrating Complex Adaptive System Simulation and Evolutionary Computation to Support Water Infrastructure Threat Management,"Threat management for water distribution infrastructure systems should develop plans for responding to hazards, including the presence of contaminants in the system and failure of physical components. In the event that the delivery of clean and reliable drinking water is threatened, the complex interactions among managers' operational decisions, consumers' water consumption choices, and the hydraulics and contaminant transport in the water distribution system will influence public health consequences. A Complex Adaptive System (CAS) approach is developed here to couple process models of the pipe network with agent-based models of consumers and public officials. Development of threat management strategies, which would prescribe a set of actions to mitigate the situation, is enabled through a simulation-optimization framework that couples heuristic search methods with the CAS model. The framework is demonstrated for an illustrative case study to identify efficient threat management strategies to achieve public health protection and maintain acceptable levels of service.",Not About Sufficiency
Assessment of Flood Vulnerability Through a Multidimensional Index,"To overcome the flood challenge, urban planning must consider innovative and simple strategies to achieve resilient and sustainable cities. Vulnerability assessment is considered key to reducing flood risk, and the most widespread methodology for assessing vulnerability is the creation of indexes. However, the indexes are often fragmented or follow a very complicated methodology, making their interpretation difficult for stakeholders, limiting their operational uses for decision-making. This paper addresses these gaps by proposing a methodology through the development of a multidimensional index based on indicators that take into account five dimensions, namely: social, economic, physical, environmental, and institutional dimensions. This method uses easy-to-use interactive maps to assess flooding and multidimensional vulnerability. These maps help decision-makers to understand and effectively address these challenges by allowing them to intuitively visualize risks and assess vulnerability in different areas. This index is comprehensive and rigorous, as well as complementary in its ease of use and application for urban planning decision-makers. The proposed methodology will allow to identify sources of vulnerability to floods and formulate strategies to reduce risk and promote urban resilience, allowing us to face present and future challenges in urban planning. Due to its simplicity of construction and interpretation, the added value of this vulnerability index is to allow comprehensive support for the implementation of adaptation and mitigation strategies in each of the dimensions evaluated.",Not About Sufficiency
Transit-oriented autonomous vehicle operation with integrated demand-supply interaction,"Autonomous vehicles (AVs) represent potentially disruptive and innovative changes to public transportation (PT) systems. However, the exact interplay between AV and PT is understudied in existing research. This paper proposes a systematic approach to the design, simulation, and evaluation of integrated autonomous vehicle and public transportation (AV + PT) systems. Two features distinguish this research from the state of the art in the literature: the first is the transit oriented AV operation with the purpose of supporting existing PT modes; the second is the explicit modeling of the interaction between demand and supply. We highlight the transit-orientation by identifying the synergistic opportunities between AV and PT, which makes AVs more acceptable to all the stakeholders and respects the social-purpose considerations such as maintaining service availability and ensuring equity. Specifically, AV is designed to serve first-mile connections to rail stations and provide efficient shared mobility in low-density suburban areas. The interaction between demand and supply is modeled using a set of system dynamics equations and solved as a fixed-point problem through an iterative simulation procedure. We develop an agent-based simulation platform of service and a discrete choice model of demand as two subproblems. Using a feedback loop between supply and demand, we capture the interaction between the decisions of the service operator and those of the travelers and model the choices of both parties. Considering uncertainties in demand prediction and stochasticity in simulation, we also evaluate the robustness of our fixed-point solution and demonstrate the convergence of the proposed method empirically. We test our approach in a major European city, simulating scenarios with various fleet sizes, vehicle capacities, fare schemes, and hailing strategies such as in-advance requests. Scenarios are evaluated from the perspectives of passengers, AV operators, PT operators, and urban mobility system. Results show the trade off between the level of service and the operational cost, providing insight for fleet sizing to reach the optimal balance. Our simulated experiments show that encouraging ride-sharing, allowing in-advance requests, and combining fare with transit help enable service integration and encourage sustainable travel. Both the transit-oriented AV operation and the demand-supply interaction are essential components for defining and assessing the roles of the AV technology in our future transportation systems, especially those with ample and robust transit networks.",Not About Sufficiency
Equilibrium and Temporal Evolution of Asphaltene Nanoaggregates during Dynamic Heteroaggregation with Polysaccharides,"Asphaltene blockage and microbiologically induced corrosion (MIC) in oil pipes pose substantial flow assurance challenges, but their interrelationship remains poorly understood despite extensive individual research efforts. This study utilized molecular dynamics simulation and machine learning techniques to elucidate the effects of alginate, a representative biofilm polysaccharide associated with MIC, on asphaltene aggregation. Results underscored the ability of alginates to disperse large asphaltene nanoaggregates into smaller ones with more pronounced dispersion effects at higher concentrations. This was achieved through forming asphaltene-alginate heteroaggregates wherein asphaltene nanoaggregates were tightly wrapped by cross-linked alginate chains stabilized by Na+ bridging. The presence of alginates significantly increased the solvent accessible surface area (SASA) of asphaltenes, while the heteroaggregation process resulted in abundant hydroxyl and carboxyl groups coating the surface of the asphaltene nanoaggregates. Furthermore, this study showed that the time-varying asphaltene aggregation parameters such as nanoaggregate number, intermolecular contact number, and SASA could be accurately predicted by alginate structural properties using optimized random forest models built from 3636 data sets extracted from molecular trajectories (R 2 = 0.9628-0.9827). This enabled fast prediction of asphaltene aggregation when the polysaccharide composition and concentration changed during the MIC process. These findings provided theoretical support for developing strategies to address asphaltene-related issues in the presence of a biofilm polysaccharide.",Not About Sufficiency
Guidelines for the Implementation of Mass Customization in Affordable House-Building Projects,"Mass customization (MC) is a business strategy that stands for the ability to develop high value-added products within short time frames and at relatively low costs. It emerged from the manufacturing industry and has been applied to several industrial sectors, including housing. However, the segment of affordable housing programs in developing countries has been criticized for having a high degree of product standardization, and failure to meet customers' specific needs. The aim of this paper is to propose guidelines for implementing mass customization in affordable house-building projects. It is based on a design science research study carried out in a small-sized company, which explored the possibility of adopting mass customization ideas to offer some flexibility for customers while maintaining low costs. The main contributions of this research study are concerned with expanding the current understanding of the components that enable MC to be implemented in the context of house-building companies that adopt traditional construction technologies. From a practical perspective, this investigation outlines several practices that can be introduced for the implementation of MC strategies at a relatively low cost by small-sized house-building companies.",Not About Sufficiency
Recent advances in energy demand for residential space heating,"Energy consumption for residential space heating has experienced a dramatic increase, driven by contin-uous income growth and demand for thermal comfort. Technology innovation, demand-side manage-ment, and system transition have been promoted to meet these needs while reducing negative impacts on the environment and public health. This special issue on ""Energy Demand for Residential Space Heating: History and Outlook "" improves our understanding of energy consumption for space heating. The purpose of this introductory article is to present research advances by summarizing recent liter-ature as well as new findings and insights from this special issue. This research shows that heating energy systems show increasing disparity due to various local resources, building characteristics, climatic con-ditions, technology adoption, and economic status. Thus, more advanced algorithms and more detailed classification of these factors can more accurately measure and predict the energy demand for space heating. The goal of heating energy systems is to reduce energy consumption and carbon emissions without sacrificing thermal comfort. Most research focuses on how technology innovation, demand-side management strategies, or public policies can help achieve this goal. Current findings suggest that a comprehensive evaluation, considering technological feasibility, economic affordability, and environ-mental sustainability, is needed to promote the sustainable transition of heating energy systems. (c) 2022 Elsevier B.V. All rights reserved.",Not About Sufficiency
Robust model for tunnel squeezing using Bayesian optimized classifiers with partially missing database,"Accurately predicting and estimating the squeezing and ground response to tunneling remains challenging. Moreover, tunnel -squeezing hazards are much more likely to occur in deeply buried long tunnels with complex engineering-geological environments. There-fore, a high-performance predictive model for tunnel squeezing is necessary. A superior ensemble classifier is put forward in this study, which is composed of four individual classifiers (gradient boosting classifier, extra-trees classifier, AdaBoost classifier, and Logistic regression classifier) and two optimization algorithms (Bayesian optimization (BO) and sparrow search algorithm (SSA)). The training database covers five parameters: tunnel depth (H), rock tunneling quality index (Q), tunnel diameter (D), support stiffness (K), and strength stress ratio (SSR), about which the basic information is accessible at the early design phases. However, the dataset compiled from the literature is insufficient. Thus, the ten proposed methods are used to replace the missing values. During the model training pro-cess, BO shows its strong ability to optimize seventeen hyperparameters. When applied to tune the classifiers' weights, SSA achieves a fast and efficient performance. The novel Shapley Additive Explanations-LightGBM method indicates that the K is the most important input feature, followed by SSR, Q, H, and D, respectively. The ensemble classifier is then validated using the test set and additional his-torical case projects. The validation shows that the model can achieve an accuracy of 98% (i.e., the error rate is 2%) on the test set, higher than those achieved by previous prediction models. Moreover, the predicted probability could provide warning information for timely support measures. Finally, the application results are illustrated through tests on the tunnel sections that have not yet been excavated in the line of the Sichuan-Tibet railway project. The applied predictive tendencies and laws are in line with the practical experience. In sum-mary, the proposed model's prediction results are reasonable, and its prediction will be more accurate as more data is collected and trained for prewarning the tunnel squeezing hazard.",Not About Sufficiency
Efficient detection of data entry errors in large-scale public health surveys: an unsupervised machine learning approach,"Data entry errors in large-scale public health surveys can undermine the effectiveness of data-driven interventions. Therefore, identifying these data entry errors is crucial for public health experts. In large-scale public health surveys, manually verifying the accuracy of every data point by domain experts is nearly impossible. This study evaluates unsupervised machine learning algorithms for detecting these errors, focusing on the 'weight' parameter in the Annual Health Survey (AHS) dataset. The AHS, conducted by the Ministry of Health and Family Welfare, Government of India, in collaboration with the Registrar General of India, is a large-scale, stratified, household-level survey targeting maternal and child health across nine states in India. The dataset is freely available on the Open Government Data (OGD) Platform of India for public health research. In this study, five algorithms-DBSCAN, K-Means, Gaussian Mixture Model (GMM), Isolation Forest (IF), and One-Class SVM (1C-SVM) were applied to detect erroneous data entries. The evaluation process involved comprehensive preprocessing and feature engineering to optimize detection capabilities. Performance metrics such as precision, recall, accuracy, false anomaly, and missed anomaly rates were used to assess each algorithm. Among these, DBSCAN demonstrated superior performance, achieving a recall of 94.7% and a precision of 81.9%, making it highly effective for this task. The findings underscore the potential of unsupervised machine learning in automating the detection of data entry errors, thereby improving the integrity of public health data. This research contributes to the advancement of precision public health, supporting more accurate and reliable evidence-based decision-making and policy formulation.",Not About Sufficiency
Evaluating Behavioral Biometrics for Continuous Authentication: Challenges and Metrics,"In recent years, behavioral biometrics have become a popular approach to support continuous authentication systems. Most generally, a continuous authentication system can make two types of errors: false rejects and false accepts. Based on this, the most commonly reported metrics to evaluate systems are the False Reject Rate (FRR) and False Accept Rate (FAR). However, most papers only report the mean of these measures with little attention paid to their distribution. This is problematic as systematic errors allow attackers to perpetually escape detection while random errors are less severe. Using 16 biometric datasets we show that these systematic errors are very common in the wild. We show that some biometrics (such as eye movements) are particularly prone to systematic errors, while others (such as touchscreen inputs) show more even error distributions. Our results also show that the inclusion of some distinctive features lowers average error rates but significantly increases the prevalence of systematic errors. As such, blind optimization of the mean EER (through feature engineering or selection) can sometimes lead to lower security. Following this result we propose the Gini Coefficient (GC) as an additional metric to accurately capture different error distributions. We demonstrate the usefulness of this measure both to compare different systems and to guide researchers during feature selection. In addition to the selection of features and classifiers, some nonfunctional machine learning methodologies also affect error rates. The most notable examples of this are the selection of training data and the attacker model used to develop the negative class. 13 out of the 25 papers we analyzed either include imposter data in the negative class or randomly sample training data from the entire dataset, with a further 6 not giving any information on the methodology used. Using real-world data we show that both of these decisions lead to significant underestimation of error rates by 63% and 81%, respectively. This is an alarming result, as it suggests that researchers are either unaware of the magnitude of these effects or might even be purposefully attempting to over-optimize their EER without actually improving the system.",Not About Sufficiency
Assessing the environmental and social co-benefits and disbenefits of natural risk management measures,"Risk management measures (RMM) participate in the sustainability of cities and communities through the protection of these socio-eco-environmental systems against threatening events, and by ensuring system recovery. They include structural measures that are grey or green/blue solutions, or hybrid solutions combining the two former types. These measures can provide environmental and social co-benefits (e.g., improved biodiversity, recreational services) and disbenefits (e.g., the development of unwanted flora, concentrations of pollutants). The aim of this article is to provide an approach to assess and compare RMMs by considering these different dimensions. An application to three natural hazards - floods, coastal floods and wildfires - is proposed. The approach takes the form of a procedure to assess the co-benefits/disbenefits of the various RMMs and some technical specifications. It allows comparing the performances of one RMM against another and collectively discussing the choice of RMMs that takes into account a wide range of dimensions. The approach is based on the formulation of eight sustainability criteria and thirty-one indicators. The results were graphically displayed as several types of diagram: one radar chart per RMM, compiling all the indicators; one radar chart by type of risk studied (flood, wildfire and coastal flooding) based on averages of indicators per criterion; a table of the global score assigned to each RMM calculated with an arithmetic mean or a weighted mean. The approach relies on an interdisciplinary research team and involves end-users in a focus group for the validation step. This approach constitutes a transparent base for decision-making processes in the context of sustainable spatial planning against natural risks.",Not About Sufficiency
Visual detection and tracking of toxic mercury in biological cells using biocompatible mesoporous silica nanospheres,"A novel, cost-effective, eco-friendly, and straightforward approach is urgently required to assess the distribution, localization, and concentration of toxins in biological cells, as their accumulation poses significant health risks. Herein, we report innovative fabrication of biocompatible silica nanospheres for real-time tracking, visualization, quantification, and clinical diagnosis of Hg(II) poisoning in cells, employing ultra-sensitive and ultra-selective assays. The creation of a biocompatible yolk-like nanosphere with a highly accessible hollow interior cavity resulted in a distinctive porous structure, providing ultra-precise recognition of Hg(II) at trace levels. The low limits of detection (LOD) and quantification (LOQ) for fluorometric monitoring of Hg(II) ions were determined as 0.11 ppb and 0.33 ppb respectively. Simple, portable, and batch analyses were employed for the rapid and continuous visualization, quantification, and inhibition of ultra-trace Hg(II) concentrations (up to ppb) in HeLa cells, aiming to assess the suitability of the innovative nano- tracker hybrid. The results showed exceptional biocompatibility of the nano-tracker hybrid. The straightforward penetration and binding pathways of the nanospheres through the cell membrane were examined. Moreover, outstanding emission enhancement was observed in the intracellular area under biological conditions. The adorned nanotrackers could serve as valuable tools for visualizing and monitoring Hg(II) poisoning during in vitro assays for clinical diagnosis in HeLa cells.",Not About Sufficiency
Public Regulation and People's Welfare Orientations of Indonesia's Corporate Social Responsibility (CSR) in the Mining Industry,"Corporate Social Responsibility (CSR) is an important topic in the intersections between businesses and the society, being used to address the negative effects of corporate and business activities on the society and environment. Regardless of the variants of its definition and forms, CSR has been used as a form of regulation in many countries including Indonesia. Indonesia's mining industry has a unique CSR regulation in the sense that it is backed by sectoral mining regulations used to protect certain uniquely defined societal interests. Since Indonesia's mining regulation prescribes certain manners of mineral production, mandatory CSR obligations have been created for industry players that will have to be read in the light of public regulation and people's welfare orientations. This paper finds that the expansive use of CSR particularly in the mining regulatory framework will create implications for the international legal commitments undertaken by Indonesia including those under the World Trade Organization (WTO).",Not About Sufficiency
Precision Diagnosis in Pandemics: Machine Learning and Cough Sound Analysis for COVID-19,"The unprecedented global crisis caused by the COVID-19 pandemic has significantly impacted public health, economies, and daily routines. A critical challenge in managing the pandemic is promptly and precisely identifying COVID-19 cases. As a result, there is an urgent need for alternative, non-invasive, and swift diagnostic techniques to identify infected individuals and mitigate the spread of the virus. In this study, we introduce a machine-learning method for early COVID-19 diagnosis by analyzing cough sounds. The study utilizes Mel-frequency cepstral coefficients, extracted from cough audio. Extensive experiments using the COUGHVID cough dataset demonstrate the feasibility and effectiveness of the approach which uses the Extra-Tree classifier. The classification model outperforms other models with a testing accuracy of 90.05%, sensitivity of 86.01%, and an area under the curve of 90.08%, outperforming other similar works, in terms of performance and computation.",Not About Sufficiency
Source apportionment of environmental combustion sources using excitation emission matrix fluorescence spectroscopy and machine learning,"The link between particulate matter (PM) air pollution and negative health effects is well-established. Air pollution was estimated to cause 4.9 million deaths in 2017 and PM was responsible for 94% of these deaths. In order to inform effective mitigation strategies in the future, further study of PM and its health effects is important. Here, we present a method for identifying sources of combustion generated PM using excitationemission matrix (EEM) fluorescence spectroscopy and machine learning (ML) algorithms. PM samples were collected during a health effects exposure assessment panel study in Seattle. We use archived field samples from the exposure study and the associated positive matrix factorization (PMF) source apportionment based on X-ray fluorescence and light absorbing carbon measurements to train convolutional neural network and principal component regression algorithms. We show EEM spectra from cyclohexane extracts of the archived filter samples can be used to accurately apportion mobile and vegetative burning sources but were unable to detect crustal dust, Cl-rich, secondary sulfate and fuel oil sources. The use of this EEM-ML approach may be used to conduct PM exposure studies that include source apportionment of combustion sources.",Not About Sufficiency
Policy brief: Improving national vaccination decision-making through data,"Life course immunisation looks at the broad value of vaccination across multiple generations, calling for more data power, collaboration, and multi-disciplinary work. Rapid strides in artificial intelligence, such as machine learning and natural language processing, can enhance data analysis, conceptual modelling, and real-time surveillance. The GRADE process is a valuable tool in informing public health decisions. It must be enhanced by real-world data which can span and capture immediate needs in diverse populations and vaccination administration scenarios. Analysis of data from multiple study designs is required to understand the nuances of health behaviors and interventions, address gaps, and mitigate the risk of bias or confounding presented by any single data collection methodology. Secure and responsible health data sharing across European countries can contribute to a deeper understanding of vaccines.",Not About Sufficiency
Predicting Metamorphic Changes In Parkinson's Disease Patients Using Machine Learning Algorithms,"Parkinson's disease is a nervous disorder mainly it affects the motor activities of the human body. Manifestations start step by step; at later point it becomes the greatest obstacle to do our day to today activities. Individuals influenced with Parkinson's ailment should go through lifestyle changes and enthusiastic changes like dozing issues, disposition swings, stultification, and skin issues. The proposed methodology is to analyse the proportion of metamorphic changes of a person affected by Parkinson's disease using machine learning techniques. Principal Component Analysis (PCA), recurrent neural network and logistic regression algorithms are used for prediction. The accuracy, precision, recall and F I measure is used to assess the performance of the prediction algorithms. The dataset includes activities of daily living which from PPMI (Parkinson's Progression Markers Initiative) was taken for experimentation. Logistic regression can predict metamorphic changes with a higher accuracy of 92% for sleep dataset and 95% for Olfactory(smell) dataset when compared to other two algorithms.",Not About Sufficiency
Comparing automated and non-automated machine learning for autism spectrum disorders classification using facial images,"Autism spectrum disorder (ASD) is a developmental disorder associated with cognitive and neurobehavioral disorders. It affects the person's behavior and performance. Autism affects verbal and non-verbal communication in social interactions. Early screening and diagnosis of ASD are essential and helpful for early educational planning and treatment, the provision of family support, and for providing appropriate medical support for the child on time. Thus, developing automated methods for diagnosing ASD is becoming an essential need. Herein, we investigate using various machine learning methods to build predictive models for diagnosing ASD in children using facial images. To achieve this, we used an autistic children dataset containing 2936 facial images of children with autism and typical children. In application, we used classical machine learning methods, such as support vector machine and random forest. In addition to using deep-learning methods, we used a state-of-the-art method, that is, automated machine learning (AutoML). We compared the results obtained from the existing techniques. Consequently, we obtained that AutoML achieved the highest performance of approximately 96% accuracy via the Hyperpot and tree-based pipeline optimization tool optimization. Furthermore, AutoML methods enabled us to easily find the best parameter settings without any human efforts for feature engineering.",Not About Sufficiency
Advanced Lung Disease in Patients with Cystic Fibrosis Is Associated with Low Diffusion capacity,"Background: The single-breath diffusing capacity of the lungs (DLCOSB) test measures the extent to which carbon monoxide (CO) passes from the lung air sacs into the blood. The accessible alveolar volume (VA(SB)) is measured by inert gas during a 10-second period. The single-breath transfer coefficient of the lung for carbon monoxide (KCOSB) is the DLCOSB divided by VA(SB) Cystic fibrosis (CF) disease comprises progressive airway obstruction with bronchiectasis and parenchyma fibrosis. Yet, the KCOSB appears insignificant in the assessment of pulmonary function in CF. Objectives: To challenge the precision of normal KCOSB in CF. Methods: The authors collected pulmonary function tests (PFT) data from 74 confirmed CF patients (mean age 26 +/- 10 years) with various levels of pulmonary disease severity. PFTs included spirometry, DLCOSB and lung volumes calculated via body plethysmography (BP). Alveolar volume (VA(BP)) was calculated by deducting ""anatomical dead space"" from total lung capacity (TLCBP) and KCOBP was then determined. We also included individual data of arterial pCO(2) blood-gas level. Results: KCOSB values were normal or higher than normal in most patients, regardless of patient FEV1 value (R-2 = 0.2204; P < 0.02) or their trapped-air levels. In contrast, the measurements of KCOBP were low parallel with low FEV1 values, and negatively correlated with the elevation of trapped air and pCO(2) levels (R-2 = 0.1383; P = 0.0133, P > 0.05, respectively). Conclusions: The measurement of VA(SB) using the short, 10-second perfusion time of the inert gas, represent only the communicative alveolar volume in CF patients with moderate to severe airway obstruction. The findings justify the use of VA(BP), measured with DLCOSB which correlate with the deterioration of FEV1 and elevation of pCO(2) level.",Not About Sufficiency
Prediction of network public opinion features in urban planning based on geographical case-based reasoning,"As a significant part of sustainable urban development proposed by the United Nations, urban planning is related to the ecological environment and transportation, especially affecting quality of life and social well-being. In the process of urban planning, the public express their opinions on open network platforms, resulting in large quantities of network public opinion data, which has important implications for evaluating urban planning. Based on the idea of geographical case-based reasoning (CBR), this paper constructs an expression framework for urban planning cases in the form of a 'case problem-case attribute-case result' triad. On this basis, this paper proposes a similarity calculation method of urban planning cases that integrates case attribute. Finally, based on an improvement to the traditional k-nearest neighbors method, the proposed public opinion feature calculation model considers similarity weights, which allow us to predict network public opinion features, including viewpoint-level emotional tendency and concerned groups of urban planning cases. The experimental result shows similarity weights (SWs) model could effectively improve the prediction accuracy, where the average MIC-F1 score reached more than 74%. Based on CBR, the proposed method can predict the development trends of public opinion in future planning cases, and provide scientific and reasonable decision support for urban planning.",Not About Sufficiency
IoT based Passenger Information System Optimized for Indian Metros,"Public transport plays a crucial role in meeting a city's transportation needs. Moreover, it helps reduce traffic congestion and vehicular pollution. In India, buses are the preferred mode of transit due to their cheap fare and extensive routes. The major obstacles to their usage are: 1) overcrowding and 2) uncertainty in arrival time. Crowd estimation techniques using cameras, infrared lasers at entrances are not suitable for Indian metros. Hence, our paper proposes a solution leveraging the Internet of Things. We have simulated the handheld Electronic Ticketing Machines (ETMS) used by Tamil Nadu State Transport Corporation (TNSTC) conductors via an android application. Crowd estimation is done by maintaining a list of ""live"" tickets (on-board passengers) of each bus at the server side. The ETMs communicate with the server via an Application Programming Interface (API). The Global Positioning System (GPS) receiver built into the ETM is used for real time tracking of the buses thereby notifying passengers about the bus arrival time. The crowd information and location of the bus are displayed in the client side android application embedded with Google Maps. This solution empowers passengers to take better decisions by bridging the information gap between passengers and bus operators.",Not About Sufficiency
Exploring the potential of machine learning for leaf angle distribution type identification from leveled digital photography: A case study for broadleaf tree and shrub species,"Leaf angle distribution (LAD) is an important plant structural trait that determines radiation interception, biomass production, rainfall interception, and evapotranspiration. LAD has remained one of the most poorly constrained parameters and a major source of uncertainty in ecological models due to difficulty in quantification. In this study, we look for a simple, affordable way to estimate leaf angle distribution type by anyone, anywhere. For the first time, we test the possibility of determining LAD type from photographs (816 in total) of single plants from various broadleaf tree and shrub species taken from field works. We used Google's TensorFlow and built models using a convolutional neural network to classify these images by LAD type. The best results, training accuracy of 95% and validation accuracy of 91%, were acquired using the two most distinct LAD types - pla-nophile and erectophile. The accuracy decreased with the addition of categories, as expected. However, our results indicate that the involvement of machine learning may indeed hold the potential to remove the current bottleneck in retrieving the information on LAD.",Not About Sufficiency
Human Oral Mucosa Stem Cells Increase Survival of Neurons Affected by In Vitro Anoxia and Improve Recovery of Mice Affected by Stroke Through Time-limited Secretion of miR-514A-3p,"The success rate of regenerative medicine largely depends on the type of stem cells applied in such procedures. Consequently, to achieve the needed level for clinical standardization, we need to investigate the viability of accessible sources with sufficient quantity of cells. Since the oral region partly originates from the neural crest, which naturally develops in niche with decreased levels of oxygen, the main goal of this work was to test if human oral mucosa stem cells (hOMSC) might be used to treat neurons damaged by anoxia. Here we show that hOMSC are more resistant to anoxia than human induced pluripotent stem cells and that they secrete BDNF, GDNF, VEGF and NGF. When hOMSC were added to human neurons damaged by anoxia, they significantly improved their survival. This regenerative capability was at least partly achieved through miR-514A-3p and SHP-2 and it decreased in hOMSC exposed to neural cells for 14 or 28 days. In addition, the beneficial effect of hOMSC were also confirmed in mice affected by stroke. Hence, in this work we have confirmed that hOMSC, in a time-limited manner, improve the survival of anoxia-damaged neurons and significantly contribute to the recovery of experimental animals following stroke.",Not About Sufficiency
Spatial and Temporal Dynamics of Social Vulnerability in the United States from 1970 to 2010: A County Trajectory Analysis,"Social vulnerability has been an important concept to characterize the extent to which human society is vulnerable to hazards. Although it is well known that social vulnerability varies across space and over time, there is only a paucity of studies to examine the basic patterns of the spatial and temporal dynamics of the social vulnerability in the United States. This study examines the spatial and temporal dynamics of social vulnerability of the U.S. counties from 1970 to 2010. For each decade, social vulnerability of counties is quantified by the social vulnerability index (SoVI) using county-level social, economic, demographic, and built environment characteristics. The SoVI is mainly designed to quantify the cross-sectional variation of social vulnerability and is not conducive to direct comparison over time. This study implements a methodology that integrates quantile standardization, sequence alignment analysis, and cluster analysis to investigate how social vulnerability of U.S. counties has changed over time. The authors find that U.S. counties exhibit distinctive spatial and longitudinal patterns, and there are counties/areas which have persistent high or low social vulnerability as well as frequent change in their social vulnerability over time. The results can be useful for policymakers, disaster managers, planning officials, and social scientists in general.",Not About Sufficiency
"Cultivating belonging: refugees, urban gardens, and placemaking in the Midwest, USA","In the aftermath of failed urban renewal projects and the decline of central cities, community gardens have become increasingly popular in urban planning, public health, and environmental circles. However, gardens still occupy a tenuous and contradictory position in the city. While urban gardens are bounded spaces, they are also dynamic places where different understandings of (agri)culture, land use, and belonging are enacted and contested. In this paper, we identify three distinct ways in which gardens in a small Midwestern city are used and experienced by refugee gardeners and local officials: the material garden, the imagined garden, and the community's garden. The material garden, embodied in the biophysical aspects of the soil, seeds, and resources needed to cultivate plants, shapes what can grow in the garden and the transformations by refugee agricultural practices. While planners tend to see urban gardens as temporary spaces that can promote limited pathways of migrant incorporation, gardeners practice, and imagine gardening differently through social, cultural, and economic interactions. We argue that these practices challenge traditional understandings of nature and urban planning, and can promote inclusive understandings of agriculture, cities, and sustainability, embodied in the ideal of the community's garden.",Not About Sufficiency
Multi-objective intelligent algorithm model design for housing environment optimization,"With the improvement of the national living standard, the buyers have higher and higher requirements for the rationality and aesthetics of the spatial planning and layout of the residential area. The traditional residential space planning method is purely manual design, which is inefficient, and the design effect will be greatly affected by the designer's work experience and personal aesthetics. Therefore, this research attempts to combine Pareto solution set and piecewise prediction idea into genetic algorithm, propose an algorithm for solving multi-objective optimization problems, and build an intelligent housing environment planning system based on this. The statistical results of simulation experiments show that the system can output more design schemes with better overall quality than the comparison system and manual planning results, and the stability of multiple operations is higher. When the number of iterations reaches 200, the average value of Pareto optimal solution number and optimal solution quality index QPS of the former is 44 and 0.41, respectively. The expert group analyzed the design results of this method and the manual method for an actual case, and found that the results designed by this method met the requirements and the calculation efficiency was much faster than manual processing. From the simulation test data and the actual case analysis, it can be seen that the intelligent housing environment planning system designed in this study is helpful to improve the efficiency of residential space design and the stability of residential space scheme style.",Not About Sufficiency
"New Villages on the Urban Fringe: Spatial Planning, Lifestyle Changes, and Health Implications","This chapter provides an analysis of how peasants' daily lives (space, time, and lifestyle) have been radically restructured in the process of China's rural relocation program. By looking into the planning and design of the new villages, their effects on villagers' everyday lives, and residents' perceptions of the effects of these changes, the chapter offers a unique angle from which to view the changing rural lifestyle in the context of rapid Chinese urbanization. The chapter provides a discussion on the health implications of the new village life.",Not About Sufficiency
Modeling Tick Populations: An Ecological Test Case for Gradient Boosted Trees,"General linear models have been the foundational statistical framework used to discover the ecological processes that explain the distribution and abundance of natural populations. Analyses of the rapidly expanding cache of environmental and ecological data, however, require advanced statistical methods to contend with complexities inherent to extremely large natural data sets. Modern machine learning frameworks such as gradient boosted trees efficiently identify complex ecological relationships in massive data sets, which are expected to result in accurate predictions of the distribution and abundance of organisms in nature. However, rigorous assessments of the theoretical advantages of these methodologies on natural data sets are rare. Here we compare the abilities of gradient boosted and linear models to identify environmental features that explain observed variations in the distribution and abundance of blacklegged tick ( Ixodesscapularis ) populations in a data set collected across New York State over a ten-year period. The gradient boosted and linear models use similar environmental features to explain tick demography, although the gradient boosted models found non-linear relationships and interactions that are difficult to anticipate and often impractical to identify with a linear modeling framework. Further, the gradient boosted models predicted the distribution and abundance of ticks in years and areas beyond the training data with much greater accuracy than their linear model counterparts. The flexible gradient boosting framework also permitted additional model types that provide practical advantages for tick surveillance and public health. The results highlight the potential of gradient boosted models to discover novel ecological phenomena affecting pathogen demography and as a powerful public health tool to mitigate disease risks.",Not About Sufficiency
Application of machine learning algorithms to predict the thyroid disease risk: an experimental comparative study,"Thyroid disease is the general concept for a medical problem that prevents one's thyroid from producing enough hormones. Thyroid disease can affect everyone-men, women, children, adolescents, and the elderly. Thyroid disorders are detected by blood tests, which are notoriously difficult to interpret due to the enormous amount of data necessary to forecast results. For this reason, this study compares eleven machine learning algorithms to determine which one produces the best accuracy for predicting thyroid risk accurately. This study utilizes the Sick-euthyroid dataset, acquired from the University of California, Irvine's machine learning repository, for this purpose. Since the target variable classes in this dataset are mostly one, the accuracy score does not accurately indicate the prediction outcome. Thus, the evaluation metric contains accuracy and recall ratings. Additionally, the F1-score produces a single value that balances the precision and recall when an uneven distribution class exists. Finally, the F1-score is utilized to evaluate the performance of the employed machine learning algorithms as it is one of the most effective output measurements for unbalanced classification problems. The experiment shows that theANNClassifier with an F1-score of 0.957 outperforms the other nine algorithms in terms of accuracy.",Not About Sufficiency
A dynamic caching strategy for CCN-based MANETs,"Mobile environments are known for frequent disconnections among different parts of the network. This characteristic may lead to unavailability of data in some parts of the network, as the data source is not accessible due to partitions. Content Centric Networks (CCN) use in-network caching; this inevitably improves performance in mobile networks by decreasing the impact of dynamic topology. At the same time, limited storage space available at mobile nodes further signifies the need for an optimized decision regarding (1) which content to cache and (2) where to cache. With this in mind, we propose a Caching Strategy in CCN-MANET (CSCM) that dynamically adapts its caching decisions for each content, and selects optimal nodes for relocation of cached contents in case the old cache node moves to a disconnected region. In this paper, CSCM is implemented as a generic framework to work with existing cache approaches for improving data availability, data access time, and for reducing traffic and bandwidth consumption. Extensive simulation results show that CSCM improves performance in terms of content retrieval time, network traffic, and content relocation compared to traditional approaches. (C) 2018 Published by Elsevier B.V.",Not About Sufficiency
Assessing the capabilites to manage risks in energy systems-analytical perspectives and frameworks with a starting point in Swedish experiences,"In this study we conceptualise how a capability approach could be used for analysing the conditions for preserving security of energy supply. We derive a concept, 'energy system capability', to describe how well the system is designed for this purpose. Based on a socio-technical systems perspective, we suggest that this capability is composed of five categories of 'building blocks': technical structures, natural resources, economic resources, institutions and actors. The configurations of these building blocks and the interactions between them provide the system with a certain level of reliability, robustness, flexibility, adaptivity and capacity for swift recovery and handling. These building blocks and system characteristics form various capabilities along the event chain (prevent, withstand, recover, handle, prepare, detect etc.) that together build the energy system capability. (C) 2016 Elsevier Ltd. All rights reserved.",Not About Sufficiency
Global missions and the critical needs of food science and technology,"Background: Achievement of many Sustainable Development Goals has a critical reliance on the food chain at both a global and local level. The future contribution of the biological sciences to agriculture and human health has been widely recognised, but the enabling science and engineering underpinning food manufacturing and distribution has not been so thoroughly examined. Scope and approach: The challenges confronting Food Science and Technology are considered from a global perspective and routes to their solutions are expressed as Mission Statements requiring multidisciplinary collaboration. These encompass the introduction of novel raw materials; changes in Manufacturing, including process and systems engineering; waste reduction and product safety and traceability. Approaches to better health via improved diets are presented, including ?Hidden Hunger? and affordable foods for the poor as well as the increasing role of advanced digital technologies such as Machine Learning and Artificial Intelligence. Key findings and conclusions: Our analysis demonstrates that FS&T is a crucial knowledge base that will allow advances in Primary Production to be sustainably converted to better control of Health through Diet. The missions involve new greater interdisciplinary collaboration, and require development of new measurement science. The need for continuing investment in food science and technology is global, but its application will require different approaches in local regions. It is vital that continuous education and training is increased by both public and private sector investment. Consumers must also be engaged to increase their awareness of new technologies and their consequent benefits to food supply and healthier diets.",Not About Sufficiency
Deep structured learning with vision intelligence for oral carcinoma lesion segmentation and classification using medical imaging,"Oral carcinoma (OC) is a toxic illness among the most general malignant cancers globally, and it has developed a gradually significant public health concern in emerging and low-to-middle-income states. Late diagnosis, high incidence, and inadequate treatment strategies remain substantial challenges. Analysis at an initial phase is significant for good treatment, prediction, and existence. Despite the current growth in the perception of molecular devices, late analysis and methods near precision medicine for OC patients remain a challenge. A machine learning (ML) model was employed to improve early detection in medicine, aiming to reduce cancer-specific mortality and disease progression. Recent advancements in this approach have significantly enhanced the extraction and diagnosis of critical information from medical images. This paper presents a Deep Structured Learning with Vision Intelligence for Oral Carcinoma Lesion Segmentation and Classification (DSLVI-OCLSC) model for medical imaging. Using medical imaging, the DSLVI-OCLSC model aims to enhance OC's classification and recognition outcomes. To accomplish this, the DSLVI-OCLSC model utilizes wiener filtering (WF) as a pre-processing technique to eliminate the noise. In addition, the ShuffleNetV2 method is used for the group of higher-level deep features from an input image. The convolutional bidirectional long short-term memory network with a multi-head attention mechanism (MA-CNN-BiLSTM) approach is utilized for oral carcinoma recognition and identification. Moreover, the Unet3 + is employed to segment abnormal regions from the classified images. Finally, the sine cosine algorithm (SCA) approach is utilized to hyperparameter-tune the DL model. A wide range of simulations is implemented to ensure the enhanced performance of the DSLVI-OCLSC method under the OC images dataset. The experimental analysis of the DSLVI-OCLSC method portrayed a superior accuracy value of 98.47% over recent approaches.",Not About Sufficiency
"Ultra-long-distance transport of supercritical natural gas (SNG) at very-high mass flow rates via pipelines through land, underground, water bodies, and ocean","A detailed study of SNG transport using a computational model that accounts for compressibility and Joule-Thomson effects is presented, for the first time. It shows that the transport distance and mass flow rate of supercritical natural gas (SNG) - above the cricondenbar and cricondentherm, and beyond the anomalous state - can exceed far beyond that achieved or proposed under high pressure, dense phase conditions, thus far. As an example, SNG (average composition of USA/Canada gas), at 800 kg/s can travel, without recompression, to 4801 km. It is revealed that the pressure-drop and pumping power per unit length decrease asymptotically as the inlet pressure increases beyond 20 MPa; orders-of-magnitude lower than that at low pressures. The increase in inlet pressure, pipe diameter, and/or heat conductance of the pipe wall increases the distance travelled by SNG whereas the increase in mass flow rate and surrounding temperature has a negative effect, including the strengthening of Joule-Thomson cooling near the exit. SNG pipelines at ocean bottom offer many advantages, including shorter distances, isothermal flows (similar to 4 degrees C), and balance between the outer and inner pressures. Also, SNG delivery at 6 MPa can allow regional distribution without immediate recompression. SNG pipelines therefore offer enormous possibilities of energy-efficient transport of natural gas to far-distant intra- and inter-continental destinations, not feasible thus far, which is urgently needed for uninterrupted supply of natural gas and worldwide energy security.",Not About Sufficiency
"Long-term energy consumptions of urban transportation: A prospective simulation of ""transport-land uses"" policies in Bangalore","The current trends of urban dynamics in the Third World are alarming with regard to climate change, because they are giving an increasingly important role to cars-to the detriment of public and nonmotorized transportation. Yet this is the type of energy consumption that is expected to grow the fastest, in business-as-usual scenarios. How can these market-based urban trends be influenced? What level of emissions reduction can be achieved? This article shows that first, there is a relevant and urgent need to tackle the urban dynamics of cities in developing countries focusing on the ""transport-land uses"" couple, and second, that existing transport technologies and decision-helping tools are already available to take up the climate change challenge. Through the application of an integrated ""transport-land uses"" model, TRANUS, this study demonstrates that transit technologies affordable to an emerging city like Bangalore can significantly curb the trajectories of energy consumption and the ensuing carbon dioxide emissions, if and only if they are implemented in the framework of appropriate urban planning. Furthermore, this study establishes that there are tools which are available to facilitate the necessary policy-making processes. These tools allow stakeholders to discuss different political alternatives integrating energy issues, based on quantitative assessments. (C) 2008 Elsevier Ltd. All rights reserved.",Not About Sufficiency
"A Spatiotemporal Pattern Analysis of High-Frequency Land-Use Changes in the Guangdong-Hong Kong-Macao Greater Bay Area, from 1990 to 2018","With continuous rises in GDP, land cover in the Guangdong-Hong Kong-Macao Greater Bay Area (GBA) has undergone a drastic change over the period 1990-2018. In this study, land use in the GBA was divided into six types: farmland, forestland, grassland, wetland, construction land, and unused land. We analyzed changes in spatiotemporal patterns according to region and type by using statistical analysis, spatial clustering, and hotspot analysis, focusing on the spatial characteristics of areas where land-use types changed with high frequency. The high-frequency land use in the GBA has strategic guidance for further urban planning and management. With discussions on urban planning, the natural environment, and social and economic development, we found the following: (1) Urban construction land in the GBA showed a unipolar growth mode, increasing from 5.63% to 14.34% from 1990 to 2018. Accordingly, the degree of urban concentration and contiguity rose continuously. (2) Hotspots with frequent land-use changes were concentrated mainly in areas with economic intensity. (3) Plots with high-frequency land-use changes (Flc > 2) were concentrated primarily in the waters and rivers of the GBA within 10 km of the administrative boundaries of prefecture-level cities. (4) Nearly 80% of the land has been or will be transformed into ecological land over the period 1990-2018. On the basis of these findings, we suggest further improving land-use efficiency, and ecological land damage and the over-occupation of sea space should be avoided while maintaining economic growth. Thus, linking increases and decreases in construction land is an excellent land-consolidation mechanism to transform inefficient urban land into ecological land.",Not About Sufficiency
LeanDojo: Theorem Proving with Retrieval-Augmented Language Models,"Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection-a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's program analysis capability to identify accessible premises and hard negative examples, which makes retrieval much more effective. Furthermore, we construct a new benchmark consisting of 98,734 theorems and proofs extracted from Lean's math library. It features challenging data split requiring the prover to generalize to theorems relying on novel premises that are never used in training. We use this benchmark for training and evaluation, and experimental results demonstrate the effectiveness of ReProver over non-retrieval baselines and GPT-4. We thus provide the first set of open-source LLM-based theorem provers without any proprietary datasets and release it under a permissive MIT license to facilitate further research.",Not About Sufficiency
GIpred: a computational tool for prediction of GIGANTEA proteins using machine learning algorithm,"In plants, GIGANTEA (GI) protein plays different biological functions including carbon and sucrose metabolism, cell wall deposition, transpiration and hypocotyl elongation. This suggests that GI is an important class of proteins. So far, the resource-intensive experimental methods have been mostly utilized for identification of GI proteins. Thus, we made an attempt in this study to develop a computational model for fast and accurate prediction of GI proteins. Ten different supervised learning algorithms i.e., SVM, RF, JRIP, J48, LMT, IBK, NB, PART, BAGG and LGB were employed for prediction, where the amino acid composition (AAC), FASGAI features and physicochemical (PHYC) properties were used as numerical inputs for the learning algorithms. Higher accuracies i.e., 96.75% of AUC-ROC and 86.7% of AUC-PR were observed for SVM coupled with AAC + PHYC feature combination, while evaluated with five-fold cross validation. With leave-one-out cross validation, 97.29% of AUC-ROC and 87.89% of AUC-PR were respectively achieved. While the performance of the model was evaluated with an independent dataset of 18 GI sequences, 17 were observed as correctly predicted. We have also performed proteome-wide identification of GI proteins in wheat, followed by functional annotation using Gene Ontology terms. A prediction server ""GIpred'' is freely accessible at http://cabgrid.res.in:8080/gipred/ for proteome-wide recognition of GI proteins.",Not About Sufficiency
"Creating Smart Energy Cities for Sustainability through Project Implementation: A Case Study of Bolzano, Italy","Globally, cities are experiencing an unprecedented socio-economic, demographic, and environmental transition that brings with it negative environmental externalities, such as climate change and environmental degradation. Smart energy city (SEC) has emerged as the latest urban development strategy in European countries. It can vastly accelerate cities' decarbonisation efforts, which can have several co-benefits. Capitalising on advances in information and communication technologies (ICT), various SEC projects have been initiated and seek to integrate ICT into the urban system. Although there are a significant number of SEC demonstration projects, there has been a scant examination of the performance and overall effect of these projects in addressing core sustainability endogenous principles. This research provides an in-depth analysis of the components of the SEC projects, highlighting the strengths and shortcomings. By highlighting the ""Smart INitiative of cities Fully cOmmitted to iNvest In Advanced large-scale energy solutions"" (SINFONIA) project in Bolzano, we aimed to determine the extent to which the current practical processes are achieved in reality. Our methodology entailed a structured literature review of the different components of SEC projects, followed by a content analysis based on the 5W1H (why, what, who, how, where, and when) conceptual framework. The data collection methods adopted included a review of specific open-access sources and project deliverables of each of the selected SEC projects, personal observation of the case study city, and interviews with SEC project experts. We analysed the SEC projects components against fundamental sustainability principles of integration, implementation, equity (intra-generational, procedural, and inter-generational), and scalability and replicability to assess their potential for creating sustainable urban settlements. The study found that the critical components of SEC projects are strategically aligned with sustainability dimensions. However, although ICT integration in urban planning and development provides the core determinants of the components, stakeholder engagement and dynamic business models are critical for successful implementation. Furthermore, through scalability and replicability of SEC projects, diversification of sustainability principles can be actualised throughout the entire city and in other geographical regions to ensure long-term intra-generational and inter-generational equity.",Not About Sufficiency
INVESTIGATING HUMAN BEHAVIORS IN SELECTING PERSONAL PHOTOS TO PRESERVE MEMORIES,"Photos are excellent means for keeping and refreshing memories. Digital photography, however, imposes new challenges for keeping photos accessible on the long run due to threats such as hard disk crashes, format changes, or storage medium decay. Safe long-term preservation, ensuring the longevity of photos, comes at a cost, suggesting a restriction of this investment to the most valuable photos. Therefore, understanding how people behave when selecting their most valuable photos from large collections is an important first step towards the development of automatic preservation approaches. In this paper, we conduct a user study of 35 participants in selecting personal photos for long-term preservation. The results of the study provide insights for the photo selection process. In addition, we propose a photo selection method based on machine learning and compared human selections with leading edge clustering techniques, highlighting significant issues in emulating human decision patterns.",Not About Sufficiency
Not all soy products are created equal: Caution needed in interpretation of research results,"Interest in the health benefits of soy foods has been intense among the research community, health professionals, and the public. At the same time, potential concerns associated with soy consumption, especially as related to soy isoflavones, have tempered the enthusiasm for making public health recommendations. On both accounts, the primary soybean isoflavone, genistein, has received the most attention. Because consumers are becoming increasingly confused by the often conflicting dietary messages, a balanced and accurate view of the risks and benefits of soy foods and soy food components is essential. Even among health professionals, confusion exists about proper nomenclature and about the precise composition of the agents under investigation. Levels of isoflavones are frequently assumed to be constant within categories of soy foods, and intakes are estimated rather than being directly analyzed. Furthermore, all too often research dealing singularly with genistein is interpreted by both health professionals and the media as equating directly with soy. Researchers often fail to fully understand the implications of their research outcomes and the context in which those outcomes should be placed. With the hundreds of publications yearly on soy and isoflavones, it is especially important to consider the literature in its entirety when making pronouncements about health effects. Efforts are needed by all to reduce the public confusion by adapting standardized approaches to the reporting of data. This paper provides a framework for both standardization of nomenclature and appropriate interpretation of data.",Not About Sufficiency
Discovering public attitudes and emotions toward educational robots through online reviews: a comparative analysis of Weibo and Twitter,"PurposePublic reviews on educational robots are of great importance for the design, development and management of the most advanced robots with an educational purpose. This study explores the public attitudes and emotions toward educational robots through online reviews on Weibo and Twitter by using text mining methods.Design/methodology/approachOur study applied topic modeling to reveal latent topics about educational robots through online reviews on Weibo and Twitter. The similarities and differences in preferences for educational robots among public on different platforms were analyzed. An enhanced sentiment classification model based on three-way decision was designed to evaluate the public emotions about educational robots.FindingsFor Weibo users, positive topics tend to the characteristics, functions and globalization of educational robots. In contrast, negative topics are professional quality, social crisis and emotion experience. For Twitter users, positive topics are education curricula, social interaction and education supporting. The negative topics are teaching ability, humanistic care and emotion experience. The proposed sentiment classification model combines the advantages of deep learning and traditional machine learning, which improves the classification performance with the help of the three-way decision. The experiments show that the performance of the proposed sentiment classification model is better than other six well-known models.Originality/valueDifferent from previous studies about attitudes analysis of educational robots, our study enriched this research field in the perspective of data-driven. Our findings also provide reliable insights and tools for the design, development and management of educational robots, which is of great significance for facilitating artificial intelligence in education.",Not About Sufficiency
Engineering the electronic structure of high performance FeCo bimetallic cathode catalysts for microbial fuel cell application in treating wastewater,"The development of high-performance, strong-durability and low-cost cathode catalysts toward oxygen reduction reaction (ORR) is of great significance for microbial fuel cells (MFCs). In this study, a series of bimetallic catalysts were synthesized by pyrolyzing a mixture of g-C3N4 and Fe, Co-tannic complex with various Fe/Co atomic ratios. The initial Fe/Co atomic ratio (3.5:0.5, 3:1, 2:2, 1:3) could regulate the electronic state, which effectively pro-moted the intrinsic electrocatalytic ORR activity. The alloy metal particles and metal-Nx sites presented on the catalyst surface. In addition, N-doped carbon interconnected network consisting of graphene-like and bamboo -like carbon nanotube structure derived from g-C3N4 provided more accessible active sites. The resultant Fe3Co1 catalyst calcined at 700 degrees C (Fe3Co1-700) exhibited high catalytic performance in neutral electrolyte with a half-wave potential of 0.661 V, exceeding that of the commercial Pt/C (0.6 V). As expected, the single chamber microbial fuel cell (SCMFC) with 1 mg/cm2 loading of Fe3Co1-700 catalyst as the cathode catalyst afforded a maximum power density of 1425 mW/m2, which was 10.5% higher than commercial Pt/C catalyst with the same loading (1290 mW/m2) and comparable to the Pt/C catalyst with 2.5 times higher loading ( 1430 mW/m2). Additionally, the Fe3Co1-700 also displayed better long-term stability over 1100 h than the Pt/C. This work provides an effective strategy for regulating the surface electronic state in the bimetallic electro-catalyst.",Not About Sufficiency
A state-of-the-art decision-support environment for risk-sensitive and pro-poor urban planning and design in Tomorrow?s cities,"In this Special Issue introductory paper, we present the Tomorrow's Cities Decision Support Environment (TCDSE). As the negative impacts of natural hazards continue to escalate around the world due to increasing populations, climate change, and rapid urbanisation (among other factors and processes), there is an urgent requirement to develop structured and operational approaches towards multi-hazard risk-informed decision making on urban planning and design. This is a particularly pressing issue for low-to-middle income countries, which are set to be impacted ever more disproportionately during future natural-hazard events if the ""business as usual"" urban -development approach continues unabated. Urban poor residents of these countries will signifi-cantly suffer under risk-insensitive development trajectories. The proposed TCDSE addresses this crucial challenge. It facilitates a participatory, people -centred approach to risk-informed decision making, using state-of-the-art procedures for physics-based hazard and engineering impact modelling, integrating physical and social vulner-ability in a unified framework, and expressing the consequences of future disasters across an array of stakeholder-weighted impact metrics that facilitate democratisation of the risk concept. The purpose of this introductory paper is to provide a detailed description of each component of the TCDSE, characterising related data inflows and outflows between modules. We conclude with a short operational end-to-end demonstration of the TCDSE, using the Tomorrowville virtual urban testbed. Individual components of the TCDSE are further dealt with in detail within subsequent papers of this Special Issue.",Not About Sufficiency
Trade and Health: From Ancient Pandemics to the World Trade Organization and Beyond,"Trade is as old as human societies, but treaties governing trade between nations are relatively new. Negotiations for international trade rules began after World War II, culminating in the creation of the World Trade Organization (WTO) in 1995. The WTO established principles to govern trade, schedules for tariff reductions, rules for dispute settlement, and agreements covering services, intellectual properties, agriculture and 'non-tariff' trade barriers. The sweeping scope of WTO agreements raised public health concerns that their rules could affect governments' ability to regulate for health, environmental and social protection purposes. New 'WTO-Plus' bilateral and regional trade and investment agreements add to the complexity of these rules and create more options for foreign investors to challenge new government health measures intended to protect public or environmental health.",Not About Sufficiency
Mobile geographic information handling technologies to support disaster management,"Geographic information, easily accessible in real time and capable of being shared amongst users through different emerging technologies (particularly mobile technologies), is essential for a wide range of public and privately-owned agencies responsible for disaster management tasks. Such tasks cannot be successfully undertaken without the real-time ability to visualise map locations, determine the scale of emergency, identify and evacuate at-risk populations, and expedite and direct rescue efforts. Considerable progress has been made in developing geographic information systems (GIS) over the past 30 years, such that the current state of geographic information technology can provide decision makers with the information they need to confront a wide variety of operations. However, little of this effort has gone into researching the unique requirements of field (mobile) geographic information management, or into developing relevant technologies that support integration and interoperability among databases and GIS to enable real-time mobile data processing. This article investigates the potential of mobile geographic information handling for disaster management, particularly oil spill emergency response. It considers measures to improve geographic information handling and to support rescue efforts during emergency response.",Not About Sufficiency
Economic Incentives in Sub-Saharan African Urban Planning A Ghanaian Case Study Preface,,Not About Sufficiency
Hypoxia vulnerability in the salmon watersheds of Southeast Alaska,"The frequency of dissolved oxygen depletion events (hypoxia) in coastal aquatic ecosystems has risen dramatically since the late 20th century, yet the causes and consequences of hypoxia for some culturally and economically impor-tant species remain poorly understood. In rivers, oxygen depletion can be caused by high densities of spawning Pacific salmon (Oncorhynchus spp.) consuming oxygen faster than can be replaced by reaeration. This process may be exacer-bated when salmon densities are artificially inflated, such as when hatchery-origin salmon stray into rivers instead of returning to hatcheries. In Southeast Alaska, hatchery salmon production has increased rapidly since the 1970s, with over 553 million chum salmon (O. keta) and 64 million pink salmon (O. gorbuscha) released in 2021 alone. Straying is pervasive in streams with outlets <25 km from nearshore marine hatchery release sites. Using a previously ground-truthed mechanistic model of dissolved oxygen dynamics, we examined how water temperature and low-flow channel hydraulics contribute to hypoxia vulnerability. We then applied the model to predict hypoxia vulnerability for watersheds within 25 km of hatchery salmon release points, where straying salmon spawner densities are expected to be higher and promote dissolved oxygen depletion. Our model predicted that low-gradient stream reaches, regardless of water temper-ature, are the most prone to hypoxia due to low reaeration rates. Our spatial analysis determined that nearly 17,000 km of anadromous-accessible stream reaches are vulnerable to high densities of hatchery-origin salmon based on 2021 release sites. To our knowledge, this study is the first to map the spatial variation of hypoxia vulnerability in anadromous water-sheds, identify habitat conditions most likely to promote hypoxia, and provide a repeatable analytical approach to identify hypoxia-prone stream reaches that can be updated as empirical data sets improve.",Not About Sufficiency
Service quality evaluation of integrated health and social care for older Chinese adults in residential settings based on factor analysis and machine learning,"Objective To evaluate the service quality of integrated health and social care institutions for older adults in residential settings in China, addressing a critical gap in the theoretical and empirical understanding of service quality assurance in this rapidly expanding sector.Methods This study employs three machine learning algorithms-Backpropagation Neural Networks (BPNN), Feedforward Neural Networks (FNN), and Support Vector Machines (SVM)-to train and validate an evaluative item system. Comparative indices such as Mean Squared Error, Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), and predictive performance metrics were employed to assess the models.Results The service quality evaluation model, enhanced by factor analysis and fuzzy BPNN, demonstrated reduced error rates and improved predictive performance metrics. Key factors influencing service quality included daily care, medical attention, recreational activities, rehabilitative services, and psychological well-being, listed in order of their impact.Conclusion The BPNN-based model provides a comprehensive and unified framework for assessing service quality in integrated care settings. Given the pressing need to match service supply with the complex demands of older adults, refining the service delivery architecture is essential for enhancing overall service quality.",Not About Sufficiency
Local values and fairness in climate change adaptation: Insights from marginal rural Australian communities,"A key criterion of successful adaptation to climate change is that it avoids potential inequalities arising from climate impacts or from adaptation strategies themselves. Recent research on adaptation in developing and developed countries argues that the measures of such fairness cannot be captured by standard metrics of vulnerability and should be situated in the milieu of people's daily lives and temporalities. Yet there is little empirical evidence to support this theoretical argument. This paper describes a method, and presents findings from research that aimed to understand and classify the lived values of four marginal rural communities at risk of sea-level rise in Australia to inform adaptation planning and implementation. Our research finds that there are at least five types of primary residents and second homeowners attached to these four low-lying coastal communities. Some of these residents are more likely to be amenable to relocation if their needs for affordable living and belonging are met. For others, there may be little that can be done to compensate for the loss of place attachment, and implementing a measured approach that provides them time to adapt to the idea of change and form connections to new places is the best that could be achieved. We discuss the implications of place-specific and people centric values for achieving fair adaptation. (C) 2017 Elsevier Ltd. All rights reserved.",Not About Sufficiency
Innovating human chemical hazard and risk assessment through an holistic approach,"This paper advocates a human-based holistic approach to chemical safety assessment, based on an in silico description of human biology, the derivation of the adverse outcome pathway network from that description, its translation into batteries of in vitro and in silico assays to monitor critical key events in the pathway network, and the integration of the re-sults by modern computational tools to predict health effects. Several ongoing international research projects are described which take on this challenge aiming at providing proofs of principle for the feasibility of this approach. This advance is supported by successes in the application of computational approaches, such as machine learning in clinical diagnostics and treatment.",Not About Sufficiency
Use of a Machine Learning Algorithm to Predict Individuals with Suicide Ideation in the General Population,"Objective In this study, we aimed to develop a model predicting individuals with suicide ideation within a general population using a machine learning algorithm. Methods Among 35,116 individuals aged over 19 years from the Korea National Health & Nutrition Examination Survey, we selected 11,628 individuals via random down-sampling. This included 5,814 suicide ideators and the same number of non-suicide ideators. We randomly assigned the subjects to a training set (n=10,466) and a test set (n=1,162). In the training set, a random forest model was trained with 15 features selected with recursive feature elimination via 10-fold cross validation. Subsequently, the fitted model was used to predict suicide ideators in the test set and among the total of 35,116 subjects. All analyses were conducted in R. Results The prediction model achieved a good performance [area under receiver operating characteristic curve (AUC)=0.85] in the test set and predicted suicide ideators among the total samples with an accuracy of 0.821, sensitivity of 0.836, and specificity of 0.807. Conclusion This study shows the possibility that a machine learning approach can enable screening for suicide risk in the general population. Further work is warranted to increase the accuracy of prediction.",Not About Sufficiency
Identification of Risk Factors for Suicidal Ideation and Attempt Based on Machine Learning Algorithms: A Longitudinal Survey in Korea (2007-2019),"Investigating suicide risk factors is critical for socioeconomic and public health, and many researchers have tried to identify factors associated with suicide. In this study, the risk factors for suicidal ideation were compared, and the contributions of different factors to suicidal ideation and attempt were investigated. To reflect the diverse characteristics of the population, the large-scale and longitudinal dataset used in this study included both socioeconomic and clinical variables collected from the Korean public. Three machine learning algorithms (XGBoost classifier, support vector classifier, and logistic regression) were used to detect the risk factors for both suicidal ideation and attempt. The importance of the variables was determined using the model with the best classification performance. In addition, a novel risk-factor score, calculated from the rank and importance scores of each variable, was proposed. Socioeconomic and sociodemographic factors showed a high correlation with risks for both ideation and attempt. Mental health variables ranked higher than other factors in suicidal attempts, posing a relatively higher suicide risk than ideation. These trends were further validated using the conditions from the integrated and yearly dataset. This study provides novel insights into suicidal risk factors for suicidal ideations and attempts.",Not About Sufficiency
Federated learning meets remote sensing,"Remote sensing (RS) imagery provides invaluable insights into characterizing the Earth's land surface within the scope of Earth observation (EO). Technological advances in capture instrumentation, coupled with the rise in the number of EO missions aimed at data acquisition, have significantly increased the volume of accessible RS data. This abundance of information has alleviated the challenge of insufficient training samples, a common issue in the application of machine learning (ML) techniques. In this context, crowd-sourced data play a crucial role in gathering diverse information from multiple sources, resulting in heterogeneous datasets that enable applications to harness a more comprehensive spatial coverage of the surface. However, the sensitive nature of RS data requires ensuring the privacy of the complete collection. Consequently, federated learning (FL) emerges as a privacy-preserving solution, allowing collaborators to combine such information from decentralized private data collections to build efficient global models. This paper explores the convergence between the FL and RS domains, specifically in developing data classifiers. To this aim, an extensive set of experiments is conducted to analyze the properties and performance of novel FL methodologies. The main emphasis is on evaluating the influence of such heterogeneous and disjoint data among collaborating clients. Moreover, scalability is evaluated for a growing number of clients, and resilience is assessed against Byzantine attacks. Finally, the work concludes with future directions and serves as the opening of a new research avenue for developing efficient RS applications under the FL paradigm. The source code is publicly available at https://github.com/hpc-unex/FLmeetsRS.",Not About Sufficiency
Music preferences as an instrument of emotional self-regulation along the business cycle,"This paper studies the influence of macroeconomic conditions on subjective well-being and music preferences. The macroeconomic cycle exerts an effect on happiness and well-being that consumers counterbalance by modifying music consumption. We use machine learning techniques to make a weekly classification of the top 100 songs of Billboard Hot 100 into positive and negative lyrics over the period 1958-2019. When unemployment is high, society generally prefers more positive songs. Other macroeconomic indicators such as high inflation, high interest rates or low stock market prices also affect musical preferences. These results provide initial evidence regarding the use of cultural consumption to offset business cycle oscillations.",Not About Sufficiency
Obesity in Qatar: A Case-Control Study on the Identification of Associated Risk Factors,"Obesity is an emerging public health problem in the Western world as well as in the Gulf region. Qatar, a tiny wealthy county, is among the top-ranked obese countries with a high obesity rate among its population. Compared to Qatar's severity of this health crisis, only a limited number of studies focused on the systematic identification of potential risk factors using multimodal datasets. This study aims to develop machine learning (ML) models to distinguish healthy from obese individuals and reveal potential risk factors associated with obesity in Qatar. We designed a case-control study focused on 500 Qatari subjects, comprising 250 obese and 250 healthy individuals- the later forming the control group. We obtained the most extensive collection of clinical measurements for the Qatari population from the Qatar Biobank (QBB) repertoire, including (i) Physio-clinical Biomarkers, (ii) Spirometry, (iii) VICORDER, (iv) DXA scan composition, and (v) DXA scan densitometry readings. We developed several machine learning (ML) models to distinguish healthy from obese individuals and applied multiple feature selection techniques to identify potential risk factors associated with obesity. The proposed ML model achieved over 90% accuracy, thereby outperforming the existing state of the art models. The outcome from the ablation study on multimodal clinical datasets revealed physio-clinical measurements as the most influential risk factors in distinguishing healthy versus obese subjects. Furthermore, multiple feature ranking techniques confirmed known obesity risk factors (c-peptide, insulin, albumin, uric acid) and identified potential risk factors linked to obesity-related comorbidities such as diabetes (e.g., HbA1c, glucose), liver function (e.g., alkaline phosphatase, gamma-glutamyl transferase), lipid profile (e.g., triglyceride, low density lipoprotein cholesterol, high density lipoprotein cholesterol), etc. Most of the DXA measurements (e.g., bone area, bone mineral composition, bone mineral density, etc.) were significantly (p-value < 0.05) higher in the obese group. Overall, the net effect of hypothesized protective factors of obesity on bone mass seems to have surpassed the hypothesized harmful factors. All the identified factors warrant further investigation in a clinical setup to understand their role in obesity.",Not About Sufficiency
InSupport: Proxy Interface for Enabling Efficient Non-Visual Interaction with Web Data Records,"Interaction with web data records typically involves accessing auxiliary webpage segments such as filters, sort options, search form, and multi-page links. As these segments are usually scattered all across the screen, it is arduous and tedious for blind users who rely on screen readers to access the segments, given that content navigation with screen readers is predominantly one-dimensional, despite the available support for skipping content via either special keyboard shortcuts or selective navigation. The extant techniques to overcome inefficient web screen reader interaction have mostly focused on general web content navigation, and as such they provide little to no support for data record-specific interaction activities such as filtering and sorting - activities that are equally important for enabling quick and easy access to the desired data records. To fill this void, we present InSupport, a browser extension that: (i) employs custom-built machine learning models to automatically extract auxiliary segments on any webpage containing data records, and (ii) provides an instantly accessible proxy one-stop interface for easily navigating the extracted segments using basic screen reader shortcuts. An evaluation study with 14 blind participants showed significant improvement in usability with InSupport, driven by increased reduction in interaction time and the number of key presses, compared to state-of-the-art solutions.",Not About Sufficiency
Machine learning enables improved runtime and precision for bio-loggers on seabirds,"Unravelling the secrets of wild animals is one of the biggest challenges in ecology, with bio-logging (i.e., the use of animal-borne loggers or bio-loggers) playing a pivotal role in tackling this challenge. Bio-logging allows us to observe many aspects of animals' lives, including their behaviours, physiology, social interactions, and external environment. However, bio-loggers have short runtimes when collecting data from resource-intensive (high-cost) sensors. This study proposes using AI on board video-loggers in order to use low-cost sensors (e.g., accelerometers) to automatically detect and record complex target behaviours that are of interest, reserving their devices' limited resources for just those moments. We demonstrate our method on bio-loggers attached to seabirds including gulls and shearwaters, where it captured target videos with 15 times the precision of a baseline periodic-sampling method. Our work will provide motivation for more widespread adoption of AI in bio-loggers, helping us to shed light onto until now hidden aspects of animals' lives. Joseph Korpela et al. demonstrate the use of machine-learning assisted bio-loggers on black-tailed gulls and streaked shearwaters. As video recording is only activated through variations in movement detected by low-cost accelerometers, this method represents improvements to runtime and precision over existing bio-logging technology.",Not About Sufficiency
Optimization of CO2 absorption rate for environmental applications and effective carbon capture,"Reliable predictions of CO2 absorption are essential for designing effective carbon capture and sequestration (CCS) systems, which are paramount in combating climate change and its health impacts. Artificial Intelligence (AI) and CCS modelling significantly enhance the efficiency and accuracy of CCS technologies, optimizing operations and reducing greenhouse gas emissions. This study presents several standalone machine learning (ML) models for predicting CO2 absorption rates, a critical component in CCS technologies. Subsequently, ensemble models such as Gaussian Process Regression (GPR-SA), Support Vector Machine (SVM-SA), and Random Forest (RF-SA) were evaluated against single models during both training and testing phases. The predicted outcomes were evaluated using various statistical performance indices and 2D-comparative visualization. The study also employed feature engineering to understand the dominancy of the input variables. The prediction skills justified GPR-M4 (94%) emerged as satisfactory and outperformed other model combinations (RF and SVM). The predictive skill during the testing phase, the ensemble models' robustness, was further featured, with GPR-SA achieving 91% goodness-of-fit despite a slight increase in MAE = 0.0392 and PBIAS = -0.0527. Further quantitative analysis indicated that SVM-SA and RF-SA showed improvements in the testing phase, indicating a strong generalization capability, with SVM-SA reporting an MAE = 0.0065 and a PBIAS = 0.0213 and RF-SA attaining an impressive MAE = 0.0013 and a PBIAS = -0.0703, proposed a slight tendency to underpredict the target variable. Rigorous reliability testing was conducted using the Augmented Dickey-Fuller (ADF), Phillips-Perron (PP), and Jarque-Bera tests to ensure data integrity. The ADF and PP tests confirmed the stationarity of variables, while the Jarque-Bera test indicated that the data followed a normal distribution across all parameters. These findings have significant environmental and public health implications. The study emphasizes the potential of ensemble models in enhancing the accuracy and reliability of CO2 absorption predictions, emphasizing their importance in informing policy decisions and operational strategies for sustainable environmental management.",Not About Sufficiency
The End of Managerial Control?,"This article has the aim of considering whether managerial control, no matter its form, has outlived, its usefulness in postbureaucratic society, and if so, whether it can be replaced by a more emancipatory discourse among local practitioners as they confront the immanent requirements of social interaction within their own practices. The article initially reviews the limitations of bureaucratic control and considers postbureaucratic or ""soft"" alternatives as sources of empowerment. Seen as ideals, postbureaucratic options are not thought to overturn the imposition of control in organizing, but the relocation of the positioning of work to the practice setting offers management a new role as the facilitator of the critical discourse required to sustain and enhance local activity.",Not About Sufficiency
"Percutaneous Coronary Intervention Mortality, Cost, Complications, and Disparities after Radiation Therapy: Artificial Intelligence-Augmented, Cost Effectiveness, and Computational Ethical Analysis","The optimal cardio-oncology management of radiation therapy and its complications are unknown despite the high patient and societal costs. This study is the first known nationally representative, multi-year, artificial intelligence and propensity score-augmented causal clinical inference and computational ethical and policy analysis of percutaneous coronary intervention (PCI) mortality, cost, and disparities including by primary malignancy following radiation therapy. Bayesian Machine learning-augmented Propensity Score translational (BAM-PS) statistics were conducted in the 2016-2020 National Inpatient Sample. Of the 148,755,036 adult hospitalizations, 2,229,285 (1.50%) had a history of radiation therapy, of whom, 67,450 (3.00%) had an inpatient AMI, and of whom, 18,400 (28.69%) underwent PCI. Post-AMI mortality, costs, and complications were comparable with and without radiation across cancers in general and across the 30 primary malignancies tested, except for breast cancer, in which PCI significantly increased mortality (OR 3.70, 95%CI 1.10-12.43, p = 0.035). In addition to significant sex, race, and insurance disparities, significant regional disparities were associated with nearly 50 extra inpatient deaths and over USD 500 million lost. This large clinical, cost, and pluralistic ethical analysis suggests PCI when clinically indicated should be provided to patients regardless of sex, race, insurance, or region to generate significant improvements in population health, cost savings, and social equity.",Not About Sufficiency
Assessing the capabilities of ChatGPT to improve additive manufacturing troubleshooting,"This paper explores the potential of using Chat Generative Pre-trained Transformer (ChatGPT), a Large Language Model (LLM) developed by OpenAI, to address the main challenges and improve the efficiency of the Gcode generation process in Additive Manufacturing (AM), also known as 3D printing. The Gcode generation process, which controls the movements of the printer's extruder and the layer-by-layer build process, is a crucial step in the AM process and optimizing the Gcode is essential for ensuring the quality of the final product and reducing print time and waste. ChatGPT can be trained on existing Gcode data to generate optimized Gcode for specific polymeric materials, printers, and objects, as well as analyze and optimize the Gcode based on various printing parameters such as printing temperature, printing speed, bed temperature, fan speed, wipe distance, extrusion multiplier, layer thickness, and material flow. Here the capability of ChatGPT in performing complex tasks related to AM process optimization was demonstrated. In particular performance tests were conducted to evaluate ChatGPT's expertise in technical matters, focusing on the evaluation of printing parameters and bed detachment, warping, and stringing issues for Fused Filament Fabrication (FFF) methods using thermoplastic polyurethane polymer as feedstock material. This work provides effective feedback on the performance of ChatGPT and assesses its potential for use in the AM field. The use of ChatGPT for AM process optimization has the potential to revolutionize the industry by offering a user-friendly interface and utilizing machine learning algorithms to improve the efficiency and accuracy of the Gcode generation process and optimal printing parameters. Furthermore, the real-time optimization capabilities of ChatGPT can lead to significant time and material savings, making AM a more accessible and cost-effective solution for manufacturers and industry.(c) 2023 Kingfa Scientific and Technological Co. Ltd. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY license (http://creativecommons. org/licenses/by/4.0/).",Not About Sufficiency
CYBER RISKS IN THE AVIATION ECOSYSTEM: AN APPROACH THROUGH A TRUST FRAMEWORK,"The evolution in the way data and information are stored, processed and exchanged using up to date technologies, open and off-the-shelf, brought efficiencies not possible before with the use of aviation dedicated traditional technologies. Systems and technologies not specifically developed for aviation purposes are now in widespread use by the aviation industry for aircraft operations and air traffic management. Digital connectivity for data exchange in support of decision-making processes are common across the whole industry. However, the use of these new technologies and processes brought also new challenges to the confidentiality, integrity and availability of data and information due to cyber vulnerabilities. This affects at the end the agreed levels of safety practiced by the aviation community and the continuity of operations with the associated socio-economic impacts that interruptions in air operations can bring to all members of the aviation community in particular and the society in general. This paper proposes a global approach to reduce the cyber-attack surface and increase the cyber resilience of the aviation ecosystem through specific process and procedures as the basis for an international aviation trust framework.",Not About Sufficiency
Turkey-Pharmaceuticals: The First WTO Arbitration for Appellate Review,"On 25 July 2022, the World Trade Organization (WTO) issued an arbitration award under Article 25 of the Dispute Settlement Understanding (DSU) on the appeal from the Panel Report in Turkey-Pharmaceuticals. This is the first use of the DSU Article 25 arbitration proceedings for the review of a WTO panel decision. As such, the case is highly significant in that it demonstrates how this alternative dispute settlement method can be successfully utilized to fulfil the function of appellate review after the Appellate Body (AB) became paralysed. Moreover, this innovative use of Article 25 arbitration may suggest a long-term solution in the reform of WTO dispute settlement. With respect to substantive legal issues, this case is also noteworthy, for it dealt with the question of public health and WTO law, a particularly sensitive topic in the post-pandemic era. This essay provides a critical review of the findings of the Panel and the Arbitrators on the substantive issues, while appraising the systemic significance of this first appeal arbitration.",Not About Sufficiency
Deciphering small business community disaster support using machine learning,"With the increase in severity and frequency of natural hazards due to climate change, developing a holistic understanding of community resilience factors is critically important to disaster response and community support. Our investigation of small business survey responses about COVID-19 impacts finds that they are conduits of national support to their local communities. Small businesses that have demonstrated high levels of pre-disaster local involvement are more likely to take an active role in community resilience during a disaster, regardless of their own financial security. In addition, businesses with natural hazard experience before or during COVID-19 provided help to more community groups than hazard inexperienced businesses. While community resilience models often characterize small businesses as passive actors using variables such as employment or financial security, this research suggests that small businesses take an active role in community resilience by providing critical local support. The pandemic presented an opportunity to consider small business' role in community resilience nationally, which was utilized here to identify the multi-dimensional factors that predict small business operators' community disaster support. This study improves upon previous research by studying the small business-community resilience interface at both regional (n = 184) and national (n = 6,121) scales. We predict small business' active involvement in community resilience using random forest machine learning, and find that adding social capital predictors greatly increases model performance (F1 score of 0.88, Matthews Correlation Coefficient of 0.67).",Not About Sufficiency
Towards IoT Anomaly Detection with Tsetlin Machines,"Anomaly detection is pivotal in strengthening the security and reliability of Internet of Things (IoT) devices. This study delves into the capabilities of the Tsetlin Machine, an innovative machine learning algorithm, in the context of anomaly detection for IoT devices. Expanding upon existing research, we conduct comprehensive empirical investigations using five prominent network traffic datasets, including CIC-IDS2017, KDD99, NSL-KDD, UNSW-NB15, and UNSW Bot-IoT. Moreover, we assess the efficacy of Tsetlin Machines compared to an extensive array of algorithms that currently hold prominence in state-of-the-art network intrusion detection. Our research reveals that Tsetlin Machines emerged as a highly competitive and potent approach for anomaly detection in IoT devices, consistently delivering superior or comparable results compared to the previous methods. Notably, Tsetlin Machines afford on-device training capabilities, a distinct advantage not readily accessible with other methods. These insights suggest that the Tsetlin Machine holds significant promise as a robust and efficient tool for detecting anomalies in IoT devices, thereby strengthening IoT systems' overall security and reliability. In summary, our paper highlights the potential of the Tsetlin Machine to address pressing IoT security challenges, paving the way for future advancements in this critical domain.",Not About Sufficiency
Non-invasive technique to detect diabetic retinopathy based on Electrooculography signal using machine learning classifiers,"Single-channel Electrooculogram (EOG) is proposed for detecting diabetic retinopathy. The Corneal-retinal potential of the eyes plays a vital role in the acquisition of Electrooculography. Diabetes is the most prevalent disease and for one out of three people with diabetes above 40 years, diabetic retinopathy occurs. It is necessary for the early detection of diabetic retinopathy as it is one of the primary reasons for blindness in the population. The potential difference between cornea and retina leads to the acquisition of EOG signal. The proposed study aims to design a low-cost miniaturized hardware circuit to obtain EOG signal using second order filters without compromising in accuracy of the outcome signal and to classify the signal into normal and diabetic retinopathy subjects by extracting the statistical features like kurtosis, mean, median absolute deviation, standard deviation, and range from software filtered EOG signal. Among the classifiers used, Support vector machine (SVM) shows a higher accuracy of 93.33%. The sensitivity, specificity and Area Under Curve (AUC) values of SVM are 96.43%, 90.625%, 0.93% is considered as more favorable outcome for the proposed method and it supports the developed prototype and processing methodology. The novelty of the research is based on proposing and exploring a non-invasive methodology for Diabetic retinopathy diagnosis based on EOG signal. Thus, the designed hardware is simple in operation and cost effective, provides an affordable and non-invasive diagnostic tool for diabetic retinopathy patients",Not About Sufficiency
The influence of perceived aesthetic and acoustic quality on outdoor thermal comfort in urban environment,"Thermal comfort in outdoor space is essential for human health and human wellbeing. The comfortable outdoor space enhances urban livability and sustainability. Currently, the influence of environmental quality on human thermal comfort is not conclusive. Research on the interrelation between perceived environmental quality and subjective human thermal comfort is needed to have a concrete argument. This paper examines the relationship between perceived aesthetics, perceived acoustics, and the outdoor thermal comfort in Hong Kong during the hot summer, by conducting questionnaires and on-site meteorological measurement. Thermal sensation vote (TSV) showed a strong, negative association with the perceived aesthetics vote and acoustics vote as calculated for 1 degrees C UTCI bin. It was also revealed that the groups with satisfactory of perceived acoustic and aesthetics have a significantly higher comfort vote than that of unsatisfactory groups. Findings suggest that humans in a perceptually quiet and beautiful outdoor environment have a significantly higher thermal tolerance, and lower thermal sensitivity. This work provides valuable data on the effects of perceived environmental quality on outdoor thermal comfort in subtropical hot summers in high density urban settings. These findings help the urban development in the outdoor urban environment in our changing climate. Urban planner and designer can create a more satisfactory aesthetic and acoustic environment to improve the thermal tolerance and adaptation of individuals in the outdoor urban environment.",Not About Sufficiency
Modeling the Determinants of Subjective Well-Being in Schizophrenia,"Background The ultimate goal of successful schizophrenia treatment is not just to alleviate psychotic symptoms, but also to reduce distress and achieve subjective well-being (SWB). We aimed to identify the determinants of SWB and their interrelationships in schizophrenia.Methods Data were obtained from 637 patients with schizophrenia enrolled in multicenter, open-label, non-comparative clinical trials. The SWB under the Neuroleptic Treatment Scale (SWN) was utilized; a cut-off score of 80 indicated a high level of SWB at baseline and 6 months. Various machine learning (ML) algorithms were employed to identify the determinants of SWB. Furthermore, network analysis and structural equation modeling (SEM) were conducted to explore detailed relationship patterns.Results The random forest (RF) model had the highest area under the curve (AUC) of 0.794 at baseline. Obsessive-compulsive symptoms (OCS) had the most significant impact on high levels of SWB, followed by somatization, cognitive deficits, and depression. The network analysis demonstrated robust connections among the SWB, OCS, and somatization. SEM analysis revealed that OCS exerted the strongest direct effect on SWB, and also an indirect effect via the mediation of depression. Furthermore, the contribution of OCS at baseline to SWB was maintained 6 months later.Conclusions OCS, somatization, cognition, and depression, rather than psychotic symptoms, exerted significant impacts on SWB in schizophrenia. Notably, OCS exhibited the most significant contribution not only to the current state of well-being but also to follow-up SWB, implying that OCS was predictive of SWB. The findings demonstrated that OCS management is critical for the treatment of schizophrenia.",Not About Sufficiency
Microscopic (Dis)order and Dynamics of Cations in Mixed FA/MA Lead Halide Perovskites,"Recent developments in the field of high efficiency perovskite solar cells are based on stabilization of the perovskite crystal structure of FAPbI(3) while preserving its excellent optoelectronic properties. Compositional engineering of, for example, MA or Br mixed into FAPbI(3) results in the desired effects, but detailed knowledge of local structural features, such as local (dis)order or cation interactions of formamidinium (FA) and methylammonium (MA), is still limited. This knowledge is, however, crucial for their further development. Here, we shed light on the microscopic distribution of MA and FA in mixed perovskites MA(1-x)FA(x)PbI(3) and MA(0.15)FA(0.85)PbI(2.55)Br(0.45 )by combining high-resolution double-quantum H-1 solid-state nuclear magnetic resonance (NMR) spectroscopy with state-of-the-art near-first-principles accuracy molecular dynamics (MD) simulations using machine-learning force-fields (MLFFs). We show that on a small local scale, partial MA and FA clustering takes place over the whole MA/FA compositional range. A reasonable driving force for the clustering might be an increase of the dynamical freedom of FA cations in FA-rich regions. While MA(0.15)FA(0.85)PbI(2.55)Br(0.45 )displays similar MA and FA ordering as the MA(1-x)FA(x)PbI(3) systems, the average cation-cation interaction strength increased significantly in this double mixed material, indicating a restriction of the space accessible to the cations or their partial immobilization upon Br- incorporation. Our results shed light on the heterogeneities in cation composition of mixed halide perovskites, helping to exploit their full optoelectronic potential.",Not About Sufficiency
Expatriate adjustment: The role of justice and conflict in intimate relationships,"Framing expatriation as family relocation, this research examines the influence of perceived justice and conflict on the psychological adjustment of 103 expatriate couples. Based on the actor-partner interdependence model, the proposed model simultaneously addresses effects of justice and conflict on own and partner's outcomes. Supporting the current model, and based on the self-interest model, distributive justice influenced work-related task conflict among expatriates and household-related task conflict among expatriate spouses. Among expatriate spouses, and in line with the group-value model, fairness perceptions regarding interpersonal treatment influenced both parties' personal conflict. Unanticipated, both parties' distributive justice also influenced personal conflict. Personal conflict negatively affected both parties' psychological adjustment and acted as a mediator in the relationship between distributive justice and psychological adjustment.",Not About Sufficiency
Synthesis of Household Yard Area Dynamics in the City of San Juan Using Multi-Scalar Social-Ecological Perspectives,"Urban sustainability discourse promotes the increased use of green infrastructure (GI) because of its contribution of important ecosystem services to city dwellers. Under this vision, all urban green spaces, including those at the household scale, are valued for their potential contributions to a city's social-ecological functioning and associated benefits for human well-being. Understanding how urban residential green spaces have evolved can help improve sustainable urban planning and design, but it requires examining urban processes occurring at multiple scales. The interaction between social structures and ecological structures within the subtropical city of San Juan, the capital and the largest city of Puerto Rico, has been an important focus of study of the San Juan ULTRA (Urban Long-Term Research Area) network, advancing understanding of the city's vulnerabilities and potential adaptive capacity. Here we provide a synthesis of several social-ecological processes driving residential yard dynamics in the city of San Juan, Puerto Rico, through the evaluation of empirical findings related to yard management decisions, yard area, and yard services. We emphasize the role of factors occurring at the household scale. Results are discussed within the context of shrinking cities using an integrated, multi-scalar, social-ecological systems framework, and consider the implications of household green infrastructure for advancing urban sustainability theory.",Not About Sufficiency
Characterization and Automatic Updates of Deprecated Machine-Learning API Usages,"Due to the rise of AI applications, machine learning (ML) libraries, often written in Python, have become far more accessible. ML libraries tend to be updated periodically, which may deprecate existing APIs, making it necessary for application developers to update their usages. In this paper, we build a tool to automate deprecated API usage updates. We first present an empirical study to better understand how updates of deprecated ML API usages in Python can be done. The study involves a dataset of 112 deprecated APIs from Scikit-Learn, TensorFlow, and PyTorch. Guided by the findings of our empirical study, we propose MLCatchUp, a tool to automate the updates of Python deprecated API usages, that automatically infers the API migration transformation through comparison of the deprecated and updated API signatures. These transformations are expressed in a Domain Specific Language (DSL). We evaluate MLCatchUp using a dataset containing 267 files with 551 API usages that we collected from public GitHub repositories. In our dataset, MLCatchUp can detect deprecated API usages with perfect accuracy, and update them correctly for 80.6% of the cases. We further improve the accuracy of MLCatchUp in performing updates by adding a feature that allows it to accept an additional user input that specifies the transformation constraints in the DSL for context-dependent API migration. Using this addition, MLCatchUp can make correct updates for 90.7% of the cases.",Not About Sufficiency
Dasymetric mapping of urban population in China based on radiance corrected DMSP-OLS nighttime light and land cover data,"High spatial resolution urban population dataset is increasingly required for sustainable urban planning and management. Dasymetric mapping is an effective approach to create such dataset. However, the created gridded total population datasets usually have limitation for urban analysis in developing countries as they usually underestimate urban population because of the strong urban-rural difference. In this study, we aimed to create a dataset of gridded urban population with 1 km resolution in China in year 2000 and 2010. We proposed an index of urban nighttime light (UNTL) by integrating radiance corrected DMSP nighttime light (RcNTL) and urban land, which is then used as weight to disaggregate county-level urban population. The validation using township population in Beijing as references shows reasonable accuracy with a mean relative error of 38% and a R-2 of 68%. Using only two widely available datasets (RcNTL and urban land), the proposed method is simple and computing efficient compared with methods using multiple geospatial data (e.g., land use and land cover, distance to city center, slope) and that combined with remote sensing imagery. As the used two auxiliary datasets are accessible globally, the method has great potential to produce similar urban population dataset for other developing countries where fine scale census population datasets are scarce. The produced urban population dataset is valuable for enriching our understanding of the urbanization process and designing sustainable urban planning and management strategies in China. (C) 2018 Elsevier B.V. All rights reserved.",Not About Sufficiency
Quality and Operations Management in Food Supply Chains: A Literature Review,"We present a literature review on quality and operations management problems in food supply chains. In food industry, the quality of the food products declines over time and should be addressed in the supply chain operations management. Managing food supply chains with operations management methods not only generates economic benefit, but also contributes to environmental and social benefits. The literature on this topic has been burgeoning in the past few years. Since 2005, more than 100 articles have been published on this topic in major operations research and management science journals. In this literature review, we concentrate on the quantitative models in this research field and classify the related articles into four categories, that is, storage problems, distribution problems, marketing problems, and food traceability and safety problems. We hope that this review serves as a reference for interested researchers and a starting point for those who wish to explore it further.",Not About Sufficiency
Lessons learned from the applicatfon of machine learning to studies on plant response to radio-frequency,"This paper applies Machine Learning (ML) algorithms to peer-reviewed publications in order to discern whether there are consistent biological impacts of exposure to non-thermal low power radio-frequency electromagnetic fields (RF-EMF). Expanding on previous analysis that identified sensitive plant species, we extracted data from 45 articles published between 1996 and 2016 that included 169 experimental case studies of plant response to RF-EMF. Raw-data from these case studies included six different attributes: frequency, specific absorption rate (SAR), power flux density, electric field strength, exposure time and plant type (species). This dataset has been tested with two different classification algorithms: k-Nearest Neighbor (kNN) and Random Forest (RF). The outputs are estimated using k-fold cross-validation method to identify and compare classifier mean accuracy and computation time. We also developed an optimization technique to distinguish the trade-off between prediction accuracy and computation time based on the classification algorithm. Our analysis illustrates kNN (91.17%) and RF (89.41%) perform similarly in terms of mean accuracy, nonetheless, kNN takes less computation time (3.38?s) to train a model compared to RF (248.12 s). Very strong correlations were observed between SAR and frequency, and SAR with power flux density and electric field strength. Despite the low sample size (169 reported experimental case studies), that limits statistical power, nevertheless, this analysis indicates that ML algorithms applied to bioelectromagnetics literature predict impacts of key plant health parameters from specific RF-EMF exposures. This paper addresses both questions of the methodological importance and relative value of different methods of ML and the specific finding of impacts of RF-EMF on specific measures of plant growth and health. Recognizing the importance of standardizing nomenclature for EMF-RF, we conclude that Machine Learning provides innovative and efficient RF-EMF exposure prediction tools, and we propose future applications in occupational and environmental epidemiology and public health.",Not About Sufficiency
ANALYSIS OF THE SIGNIFICANCE OF PROTECTED AREAS IN FINANCING LOCAL DEVELOPMENT IN CROATIA,"The aim of the research, whose results are presented in this paper, is to analyse the level and point out the possibilities of more active involvement of local communities in the management of protected areas. Involving local communities in the management of protected areas - national and nature parks - will allow local stakeholders to understand the possibilities, significance and role of protected natural areas and ecosystem services in enhancing their living standards and overall quality of life. The study was conducted by a method of semi-structured interviews combined with survey on a sample of 43 subjects. Multi-sectorial analysis included the sectors of nature protection, economy, agriculture, fisheries, tourism, forestry, health, education, employment, spatial planning and sports. The results of the study of attitudes of different stakeholders and their relationship to protected natural areas at national, regional and local level as well as international organizations in the Republic of Croatia show that the protected areas are an important resource of the Republic of Croatia and the potential to launch socio-economic development and sustainable development financing in local communities where they are established. However, there are few development ideas and business opportunities between various stakeholders from national and nature parks. The overall conclusion is that the protected natural areas are still largely viewed through the protection, control and conservation of nature, and therefore they are not open to potential investors and the business sector in general. Stronger integration of protected natural areas in the development of local communities is a long process that requires the implementation of a series of activities, and one of the key activities for the forthcoming period is to strengthen the knowledge and capacity of people about the value of ecosystem services and utilization of economic potential of protected natural areas in a sustainable manner.",Not About Sufficiency
A hybrid AI approach for supporting clinical diagnosis of attention deficit hyperactivity disorder (ADHD) in adults,"Attention deficit hyperactivity disorder (ADHD) is a neurodevelopmental disorder that includes symptoms such as inattentiveness, hyperactivity and impulsiveness. It is considered as an important public health issue and prevalence of, as well as demand for diagnosis, has increased as awareness of the disease grew over the past years. Supply of specialist medical experts has not kept pace with the increasing demand for assessment, both due to financial pressures on health systems and the difficulty to train new experts, resulting in growing waiting lists. Patients are not being treated quickly enough causing problems in other areas of health systems (e.g. increased GP visits, increased risk of self-harm and accidents) and more broadly (e.g. time off work, relationship problems). Advances in AI make it possible to support the clinical diagnosis of ADHD based on the analysis of relevant data. This paper reports on findings related to the mental health services of a specialist Trust within the UK's National Health Service (NHS). The analysis studied data of adult patients who underwent diagnosis over the past few years, and developed a hybrid approach, consisting of two different models: a machine learning model obtained by training on data of past cases; and a knowledge model capturing the expertise of medical experts through knowledge engineering. The resulting algorithm has an accuracy of 95% on data currently available, and is currently being tested in a clinical environment.",Not About Sufficiency
Machine learning in mental health: a scoping review of methods and applications,"BackgroundThis paper aims to synthesise the literature on machine learning (ML) and big data applications for mental health, highlighting current research and applications in practice.MethodsWe employed a scoping review methodology to rapidly map the field of ML in mental health. Eight health and information technology research databases were searched for papers covering this domain. Articles were assessed by two reviewers, and data were extracted on the article's mental health application, ML technique, data type, and study results. Articles were then synthesised via narrative review.ResultsThree hundred papers focusing on the application of ML to mental health were identified. Four main application domains emerged in the literature, including: (i) detection and diagnosis; (ii) prognosis, treatment and support; (iii) public health, and; (iv) research and clinical administration. The most common mental health conditions addressed included depression, schizophrenia, and Alzheimer's disease. ML techniques used included support vector machines, decision trees, neural networks, latent Dirichlet allocation, and clustering.ConclusionsOverall, the application of ML to mental health has demonstrated a range of benefits across the areas of diagnosis, treatment and support, research, and clinical administration. With the majority of studies identified focusing on the detection and diagnosis of mental health conditions, it is evident that there is significant room for the application of ML to other areas of psychology and mental health. The challenges of using ML techniques are discussed, as well as opportunities to improve and advance the field.",Not About Sufficiency
Machine learning for real-time detection of local heat accumulation in metal additive manufacturing,"Metal additive manufacturing is associated with thermal cycles of high rates of heating, melting, cooling, and solidification. Some areas within the build experience thermal cycles that depend on the paths of the energy source. Additionally, geometrical features, such as thin walls and overhangs, can lead to heat accumulation, potentially affecting the microstructure, fatigue life, and induced residual stresses that may lead to dimensional distortion and cracking. The identification of significant heat accumulation can be used for part quality monitoring to inform the design process, enhance the quality of printed parts, and optimize the process parameters. This study aims to efficiently identify heat accumulation with affordable in-situ infrared imaging for further characterization and mitigation to enhance the quality of printed parts. A computational framework employing machine learning is developed to identify zones of local heat accumulation in real time. The effectiveness of this approach is demonstrated by experiments conducted on a build with a wide variety of geometrical features. In addition, characterization and detailed analyses of detected local heat accumulation zones are provided.",Not About Sufficiency
Energy Forecasting with Building Characteristics Analysis,"With the installation of smart meters, high resolution building-level energy consumption data become increasingly accessible, which not only provides more accurate data for energy forecasting at the aggregated level but also enables data-driven energy forecasting for individual buildings. On the one hand, individual buildings exhibit high randomness, making the forecasting problem at the building-level more challenging. On the other hand, buildings usually have their own characteristics, therefore such valuable information needs to be considered in the forecast models at the aggregation level. In this paper we investigate how unique characteristics of buildings could affect the performance of forecasting models and aim to identify defining patterns of buildings. The usefulness of the proposed approach is demonstrated using data from three real-world buildings.",Not About Sufficiency
A novel infrasound and audible machine-learning approach to the diagnosis of COVID-19,"Background The coronavirus disease 2019 (COVID-19) outbreak has rapidly spread around the world, causing a global public health and economic crisis. A critical limitation in detecting COVID-19-related pneumonia is that it is often manifested as a ""silent pneumonia"", i.e. pulmonary auscultation that sounds ""normal"" using a standard stethoscope. Chest computed tomography is the gold standard for detecting COVID-19 pneumonia; however, radiation exposure, availability and cost preclude its utilisation as a screening tool for COVID-19 pneumonia. In this study we hypothesised that COVID-19 pneumonia, ""silent"" to the human ear using a standard stethoscope, is detectable using a full-spectrum auscultation device that contains a machine-learning analysis. Methods Lung sound signals were acquired, using a novel full-spectrum (3-2000 Hz) stethoscope, from 164 COVID-19 pneumonia patients, 61 non-COVID-19 pneumonia patients and 141 healthy subjects. A machine-learning classifier was constructed and the data were classified into three groups: 1) normal lung sounds, 2) COVID-19 pneumonia and 3) non-COVID-19 pneumonia. Results Standard auscultation found that 72% of the non-COVID-19 pneumonia patients had abnormal lung sounds compared with only 25% of the COVID-19 pneumonia patients. The classifier's sensitivity and specificity for the detection of COVID-19 pneumonia were 97% and 93%, respectively, when analysing the sound and infrasound data, and they were reduced to 93% and 80%, respectively, without the infrasound data ( p<0.01 difference in receiver operating characteristic curves with and without infrasound). Conclusions This study reveals that useful clinical information exists in the infrasound spectrum of COVID-19-related pneumonia and machine-learning analysis applied to the full spectrum of lung sounds is useful in its detection.",Not About Sufficiency
Health Impacts of COVID-19 through the Changes in Mobility,"Understanding the wider effects of the COVID-19 pandemic on public health is needed to respond sufficiently to the impacts and facilitate recovery. We studied the secondary health impacts of COVID-19 through the changes in transportation using a ripple effect mode. Three ripples are defined to reflect the impacts of COVID-19 on (1) transportation and the systems behind it, (2) transportation-related health risk factors, and (3) public health. COVID-19 impacts on transportation are synthesized through six areas: transportation demand, transportation mode, traffic safety, land use and built environment, transportation jobs, and transportation equity. These changes are further associated with decreased transportation-related air pollution, greenhouse gases, noise, heat, and stress. Higher rates of road casualties were observed in the area of COVID-19. Social exclusion and limitations in accessibility to healthcare and healthy food were identified as negative consequences of changes in transportation. There are uncertainties in the rate of active transportation (i.e., walking and cycling) and related crashes that require further investigation. The findings of this study uncover the complex and relatively unknown impacts of COVID-19 on public health through changes in transportation.",Not About Sufficiency
InterSpin: Integrated Supportive Webtools for Low- and High-Field NMR Analyses Toward Molecular Complexity,"InterSpin (http://dmar.riken.jp/interspin/) comprises integrated, supportive, and freely accessible preprocessing webtools and a database to advance signal assignment in low-and high-field NMR analyses of molecular complexities ranging from small molecules to macromolecules for food, material, and environmental applications. To support handling of the broad spectra obtained from solid-state NMR or low-field benchtop NMR, we have developed and evaluated two preprocessing tools: sensitivity improvement with spectral integration, which enhances the signal-to-noise ratio by spectral integration, and peaks separation, which separates overlapping peaks by several algorithms, such as non-negative sparse coding. In addition, the InterSpin Laboratory Information Management System (SpinLIMS) database stores numerous standard spectra ranging from small molecules to macromolecules in solid and solution states (dissolved in polar/nonpolar solvents), and can be searched under various conditions using the following molecular assignment tools. SpinMacro supports easy assignment of macromolecules in natural mixtures via solid-state C-13 peaks and dimethyl sulfoxide-dissolved H-1-C-13 correlation peaks. InterAnalysis improves the accuracy of molecular assignment by integrated analysis of H-1-C-13 correlation peaks and H-1-J correlation peaks of small molecules dissolved in D2O or deuterated methanol, which supports easy narrowing down of metabolite candidates. Finally, by enabling database interoperability, SpinLIMS's client software will ultimately support scientific discovery by facilitating sharing and reusing of NMR data.",Not About Sufficiency
A SIMULATOR TESTBED FOR MT-CONNECT BASED MACHINES IN A SCALABLE AND FEDERATED MULTI-ENTERPRISE ENVIRONMENT,"The emergence and steady adoption of machine communication protocols like the MTConnect are steering the manufacturing sector towards greater machine interoperability, higher operational productivity, substantial cost savings with advanced decision-making capabilities at the shop-floor level. MTConnect GitHub repository and NIST Smart Manufacturing Systems (SMS) Test Bed are two major resources for collecting data from CNC machines. However, these tools would be insufficient and protractive in Modeling & Simulation (M&S) scenarios where spawning hundreds of MTConnect agents and thousands of adapters with real-time virtual machining is necessary for advancing research in the digital supply chain. This paper introduces a flexible simulator testbed of multiple MTConnect agents and adapters for simulating Levels 0 & 1 of the ISA-95 framework and help support R&D activities in complex multi-enterprise supply chain scenarios. To the best knowledge of the authors, there is no publicly accessible multi-enterprise MTConnect testbed yet.",Not About Sufficiency
A proposal for a White Paper on Geoethics in Forensic Geology,"This paper outlines the construction of a White Paper on Geoethics in Forensic Geology. It focuses on forensic geology, although it also relates to the wider sphere of the forensic geosciences. Forensic geology is rapidly evolving to provide assistance in police investigations and in criminal and civil courts with providing scientific advice and evidence, but there also should be associated clear guidelines to benefit both the practitioner and the justice system. Examples of where forensic geology delivers to society in a vital way are required and also where potential malpractice could happen. The paper discusses where forensic geology should pursue social justice in compliance with current legal systems. In order to achieve this goal, it outlines the main areas that we suggest should be developed within the discipline: the competence of the scientist in forensic geology; the creation of best practice guidelines; the establishing of clear duties of the expert in forensic geology; and consideration of ethical aspects in forensic geological activities and ethical aspects in communicating geoscience evidence. When developing geoethics within forensic geology, the following practices were identified as of prime importance: improved standardization of methods; the use of appropriate methods and/or combination of complementary methods; greater clarity of approach used for the location of areas of interest; collection and recovery of evidence; scene examination and sample collection evaluation of data; construction and appropriate use of databases, background information, documentation, cartography and communication of forensic data; and summary of evidence and acknowledgement and consideration of uncertainty and bias. Honesty, integrity, respect, transparency, competence and reliability are vital for the forensic geoscientist to adhere to. Raising the ethical profile of the forensic geoscience profession aims to pave the way for forensic geoscientists to be empowered now and into the future to serve society: acting responsibly and adopting effective ethical codes is vitally important for a safe society. This paper highlights the necessity to hold urgent discussions on the ethical and social implications of forensic geology and their potential repercussions on societal justice. Forensic geology is a very useful tool, but like any other tool in human hands, it presupposes responsibility in its application. Professionalism and honesty in forensic geology are fundamental to assure the public that geoscientists involved have the highest scientific respectability, social credibility and community respect for their role to help pursue judicial truth. The aim of this draft White Paper is to stimulate an open and informed debate on geoethics.",Not About Sufficiency
The elephant in the room is really a cow: using consumption corridors to define sustainable meat consumption in the European Union,"Implementing the European Green Deal requires a consistent food systems' policy that involves not only targeting the supply side but also conducting extensive changes in diets at the consumer level. Reducing meat consumption is an obvious strategy to put the European food system on track to meet the Green Deal's goals. This cannot be achieved by focusing solely on consumer choice and individual responsibility. Stronger governance is required to reduce the scale of meat consumption to sustainable levels. Such governance needs to be informed by a holistic definition of ""sustainable meat consumption"", designed to ensure that important sustainability priorities are not neglected, and to account for all emissions associated with EU consumption, regardless of where production takes place. This article presents a conceptual framework to define ""sustainable meat consumption"" based on the concept of consumption corridors (CCs). A CC is the space between a minimum (the floor) and maximum (the ceiling) consumption level, which allows everybody to satisfy their needs without compromising others' ability to meet their own. Embedded in a powerful set of principles (recognizing universal needs; tackling both over and under-consumption; framing food as a common good; promoting public participation; and addressing environmental justice and planetary sustainability), CCs are attuned to the Green Deal's ambition to ""leave no one behind"", in the EU and beyond. CCs provide a demand-side solution encompassing a more equitable alternative to discuss what is actually a ""fair share"" of the world's limited resources when it comes to meat consumption.",Not About Sufficiency
TCGEx: a powerful visual interface for exploring and analyzing cancer gene expression data,"Analyzing gene expression data from the Cancer Genome Atlas (TCGA) and similar repositories often requires advanced coding skills, creating a barrier for many researchers. To address this challenge, we developed The Cancer Genome Explorer (TCGEx), a user-friendly, web-based platform for conducting sophisticated analyses such as survival modeling, gene set enrichment analysis, unsupervised clustering, and linear regression-based machine learning. TCGEx provides access to preprocessed TCGA data and immune checkpoint inhibition studies while allowing integration of user-uploaded data sets. Using TCGEx, we explore molecular subsets of human melanoma and identify microRNAs associated with intratumoral immunity. These findings are validated with independent clinical trial data on immune checkpoint inhibitors for melanoma and other cancers. In addition, we identify cytokine genes that can be used to predict treatment responses to various immune checkpoint inhibitors prior to treatment. Built on the R/Shiny framework, TCGEx offers customizable features to adapt analyses for diverse research contexts and generate publication-ready visualizations. TCGEx is freely available at https://tcgex.iyte.edu.tr, providing an accessible tool to extract insights from cancer transcriptomics data.",Not About Sufficiency
Forecasting air quality index considering socioeconomic indicators and meteorological factors: A data granularity perspective,"Forecasting air quality index (AQI) is critically important to provide a basis for government policy makers, especially in public health, smart transportation, energy management, economic development, and sustainable environments. In reality, AQI consists of various components, such as PM2.5, PM10, CO, NO2, and SO2. Although numerous methods have been presented, few studies concurrently considered the causalities of socioeconomic indicators and meteorological factors and different data granularities. The aggregate AQI of Taiwan comprises five representative cities: Taipei, Hsinchu, Taichung, Tainan, and Kaohsiung. Research findings identify seasonal factors, carbon power generation, steel and metal production, highway cargo load, the number of registered cars, and retail and manufacturing employment population as the key indicators to predict the monthly AQI of Taiwan. For the daily AQI of Hsinchu and the hourly AQI of Kaohsiung, PM2.5, PM10, O-3, ambient temperature, humidity, wind speed, wind direction, and pollutants (CO, NO2, and SO2) are recognized. Deep learning significantly outperforms machine learning in the hourly AQI while it performs slightly better in the daily AQI. With the presented framework, governments can balance the trade-offs between economic development and environmental sustainability.",Not About Sufficiency
Gender differences in the effects of urban neighborhood on depressive symptoms in Jamaica,"Objective. To explore the mental health effects of the urban neighborhood on men and women in Jamaica and the implications for urban planning and social development. Methods. A cross-sectional household sample of 2 848 individuals 15-74 years of age obtained from the Jamaica Health and Lifestyle Survey 2007-2008 was analyzed. Secondary analysis was undertaken by developing composite scores to describe observer recorded neighborhood features, including infrastructure, amenities/services, physical conditions, community socioeconomic status, and green spaces around the home. Depressive symptoms were assessed using the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV). Bivariate and multivariate methods were used to explore the associations among gender, neighborhood factors, and risk of depressive symptoms. Results. While no associations were found among rural residents, urban neighborhoods were associated with increased risk of depressive symptoms. Among males, residing in a neighborhood with poor infrastructure increased risk; among females, residing in an informal community/unplanned neighborhood increased risk. Conclusions. The urban neighborhood contributes to the risk of depression symptomatology in Jamaica, with different environmental stressors affecting men and women. Urban and social planners need to consider the physical environment when developing health interventions in urban settings, particularly in marginalized communities.",Not About Sufficiency
Genome-Enabled Molecular Subtyping and Serotyping for Shiga Toxin-Producing Escherichia coli,"Foodborne pathogens are a major public health burden in the United States, leading to 9.4 million illnesses annually. Since 1996, a national laboratory-based surveillance program, PulseNet, has used molecular subtyping and serotyping methods with the aim to reduce the burden of foodborne illness through early detection of emerging outbreaks. PulseNet affiliated laboratories have used pulsed-field gel electrophoresis (PFGE) and immunoassays to subtype and serotype bacterial isolates. Widespread use of serotyping and PFGE for foodborne illness surveillance over the years has resulted in the accumulation of a wealth of routine surveillance and outbreak epidemiological data. This valuable source of data has been used to understand seasonal frequency, geographic distribution, demographic information, exposure information, disease severity, and source of foodborne isolates. In 2019, PulseNet adopted whole genome sequencing (WGS) at a national scale to replace PFGE with higher-resolution methods such as the core genome multilocus sequence typing. Consequently, PulseNet's recent shift to genome-based subtyping methods has rendered the vast collection of historic surveillance data associated with serogroups and PFGE patterns potentially unusable. The goal of this study was to develop a bioinformatics method to associate the WGS data that are currently used by PulseNet for bacterial pathogen subtyping to previously characterized serogroup and PFGE patterns. Previous efforts to associate WGS to PFGE patterns relied on predicting DNA molecular weight based on restriction site analysis. However, these approaches failed owing to the non-uniform usage of genomic restriction sites by PFGE restriction enzymes. We developed a machine learning approach to classify isolates to their most probable serogroup and PFGE pattern, based on comparisons of genomic k-mer signatures. We applied our WGS classification method to 5,970 Shiga toxin-producing Escherichia coli (STEC) isolates collected as part of PulseNet's routine foodborne surveillance activities between 2003 and 2018. Our machine learning classifier is able to associate STEC WGS to higher-level serogroups with very high accuracy and lower-level PFGE patterns with somewhat lower accuracy. Taken together, these classifications support the ability of public health investigators to associate currently generated WGS data with historical epidemiological knowledge linked to serogroups and PFGE patterns in support of outbreak surveillance for food safety and public health.</p>",Not About Sufficiency
"Identification of Use Cases, TargetGroups, and Motivations Around Adopting SmartSpeakers forHealth Care and Social Care Settings: Scoping Review","Background: Conversational agents (CAs) are finding increasing application in health and social care, not least due to their growing use in the home. Recent developments in artificial intelligence, machine learning, and natural language processing have enabled a variety of new uses for CAs. One type of CA that has received increasing attention recently is smart speakers. Objective: The aim of our study was to identify the use cases, user groups, and settings of smart speakers in health and social care. We also wanted to identify the key motivations for developers and designers to use this particular type of technology. Methods: We conducted a scoping review to provide an overview of the literature on smart speakers in health and social care. The literature search was conducted between February 2023 and March 2023 and included 3 databases (PubMed, Scopus, and Sociological Abstracts), supplemented by Google Scholar. Several keywords were used, including technology (eg, voice assistant), product name (eg, Amazon Alexa), and setting (health care or social care). Publications were included if they met the predefined inclusion criteria: (1) published after 2015 and (2) used a smart speaker in a health care or social care setting. Publications were excluded if they met one of the following criteria: (1) did not report on the specific devices used, (2) did not focus specifically on smart speakers, (3) were systematic reviews and other forms of literature-based publications, and (4) were not published in English. Two reviewers collected, reviewed, abstracted, and analyzed the data using qualitative content analysis. Results: A total of 27 articles were included in the final review. These articlescovered a wide range of use cases in different settings, such as private homes, hospitals, long-term care facilities, and outpatient services. The main target group was patients, especially older users, followed by doctors and other medical staff members. Conclusions: The results show that smart speakers have diverse applications in health and social care, addressing different contexts and audiences. Their affordability and easy-to-use interfaces make them attractive to various stakeholders. It seems likely that, due to technical advances in artificial intelligence and the market power of the companies behind the devices, there will be more use cases for smart in the near future.",Not About Sufficiency
Machine learning-based energy consumption clustering and forecasting for mixed-use buildings,"Mixed-use buildings contribute to the sustainable development of cities by providing economic, environmental, and social benefits. Energy management of these buildings still remains a challenge due to their unpredictable energy consumption characteristics and the lack of design guidelines for energy efficiency and sustainability solutions. Energy consumption forecasting models have been crucial to the improvement of energy efficiency and sustainability of buildings but its application to mixed-use buildings are limited. Hence, this study aims to develop a prediction model to characterize and forecast the energy consumption of mixed-use buildings. Machine learning techniques are employed in the proposed prediction model which used k-means algorithm for clustering and support vector machines for forecasting. The prediction model was developed and demonstrated on simulated energy consumption of 30 mixed-use buildings from the open energy information database. The clustering results have found major differences in the consumption behavior of building clusters, especially on peaking characteristics. The differences were highlighted in terms of the domain knowledge on residential and commercial energy consumption behavior. The forecasting model results showed that the proposed integration of the clustering model was able to capture unique variations in the energy consumption of mixed-use buildings. This led to a 46% decrease in the mean bias error and a 10% decrease in the coefficient of variation root mean square error wherein both indicators are commonly used in building energy modeling standards.",Not About Sufficiency
From air to airway: Dynamics and risk of inhalable bacteria in municipal solid waste treatment systems,"Municipal solid waste treatment (MSWT) system emits a cocktail of microorganisms that jeopardize environmental and public health. However, the dynamics and risks of airborne microbiota associated with MSWT are poorly understood. Here, we analyzed the bacterial community of inhalable air particulates (PM10, n = 71) and the potentially exposed on-site workers' throat swabs (n = 30) along with waste treatment chain in Shanghai, the largest city of China. Overall, the airborne bacteria varied largely in composition and abundance during the treatment (P < 0.05), especially in winter. Compared to the air conditions, MSWT-sources that contributed to 15 similar to 70% of airborne bacteria more heavily influenced the PM10-laden bacterial communities (PLS-SEM, beta = 0.40, P < 0.05). Moreover, our year-span analysis found PM10 as an important media spreading pathogens (10(4) similar to 10(8) copies/day) into on-site workers. The machine-learning identified Lactobacillus and Streptococcus as pharynx niched featured biomarker in summer and Rhodococcus and Capnocytophaga in winter (RandomForest, ntree = 500, mtry = 10, cross = 10, OOB = 0%), which closely related to their airborne counterparts (Procrustes test, P < 0.05), suggesting that MSWT a dynamic hotspot of airborne bacteria with the pronounced inhalable risks to the neighboring communities.",Not About Sufficiency
"Quality of Life and Compact Development Policies in Bandung, Indonesia","The study explores whether Quality of Life (QOL) corresponds to the spatial pattern of urban system as a result of compact development policy practice in Bandung city, Indonesia. It examines the connection between QOL and selected attributes of compact development. A self-reported life satisfaction is used as a proxy for QOL based on a cross-sectional survey data from 400 respondents. The analysis shows that the changes in QOL significantly correspond to the change of different attributes of compact development. The result suggests that compact development policies in Bandung have not shown a desirable result in improving QOL of the urban residents. The result also indicates that the implementation of the policy is less beneficial in the context of developing countries. The study strengthens the existing argument that compact development policies need to be tailored to suit the context of developing countries, rather than just be taken for granted from the practices in developed countries.",Not About Sufficiency
Enabling Unobtrusive Gait Analysis for Self-Health Monitoring using a Smartphone,"Gait analysis states that the scientific study of body movements are responsible for locomotion in human beings. Many people can move about with abnormal gait patterns for years without knowing the odd symptoms. However, an abnormal walking style can lead to more significant health problems when someone's regular gait pattern is altered due to injury or illness. Therefore, one must self-assess and diagnose the early stages of abnormal gait. We propose an interdisciplinary implementation of the Human Gait Analysis System project (HGAS), a human pose estimation using the PoseNet machine learning model that can run on a smartphone browser. Gait analysis conducted in a laboratory environment is expensive and cannot be used to assess the person's gait daily. Therefore, it is desired to focus on developing a new gait analysis system that is affordable for personal use, user-friendly, and mobile. The present work explores machine learning techniques that can be used to determine basic gait parameters using observational gait analysis. The individual can utilize the developed system for a self-gait assessment, help doctors/nurses to understand the patient gait patterns for their treatment, or an individual pose assessment during yoga/exercises.",Not About Sufficiency
"Green infrastructure site selection in the Walnut Creek wetland community: A case study from southeast Raleigh, North Carolina","Recent findings have shown that minority communities are frequently underserved by green infrastructure developments relative to non-minority communities, as local installations of green infrastructure often follow patterns of gentrification. Antipathy from these communities toward existing environmental management efforts present further obstacles related to green infrastructure placement. While hydrologic modeling has been highly utilized in decision support for green infrastructure placement, this technique does not consider ownership, access concerns, or the importance of visibility. Alternatively, participatory geographic information systems (PPGIS) can provide a different perspective from hydrologic models, as they have the potential to forecast community perceptions of green infrastructure utility, rather than hydrological benefit. We use a mixed-methods approach to optimize green infrastructure site-selection that considers hydrologic vulnerabilities in the context of place-based knowledge and historical realities. Residents' perceptions of the locations of nuisance flooding were reported via participatory mapping within a paper-based survey (n = 95) conducted in the communities surrounding Walnut Creek, a historically African-American community in Raleigh, North Carolina. Hotspot analysis was used to identify statistically significant clustering, which was related to a correspondence between participant-indicated nuisance flooding sites and high flow accumulation cells. Comparison of the participatory and hydrologic hotspot analyses show some geospatial overlap for potential green infrastructure placement. We propose that, when undertaken with community input, green infrastructure installation in these downstream areas may help offset localized flooding patterns while facilitating greater trust with stormwater and environmental practitioners.",Not About Sufficiency
Small area disease mapping of cancer incidence in British Columbia using Bayesian spatial models and the smallareamapp R Package,"IntroductionThere is an increasing interest in small area analyses in cancer surveillance; however, technical capacity is limited and accessible analytical approaches remain to be determined. This study demonstrates an accessible approach for small area cancer risk estimation using Bayesian hierarchical models and data visualization through the smallareamapp R package. Materials and methodsIncident lung (N = 26,448), female breast (N = 28,466), cervical (N = 1,478), and colorectal (N = 25,457) cancers diagnosed among British Columbia (BC) residents between 2011 and 2018 were obtained from the BC Cancer Registry. Indirect age-standardization was used to derive age-adjusted expected counts and standardized incidence ratios (SIRs) relative to provincial rates. Moran's I was used to assess the strength and direction of spatial autocorrelation. A modified Besag, York and Mollie model (BYM2) was used for model incidence counts to calculate posterior median relative risks (RR) by Community Health Service Areas (CHSA; N = 218), adjusting for spatial dependencies. Integrated Nested Laplace Approximation (INLA) was used for Bayesian model implementation. Areas with exceedance probabilities (above a threshold RR = 1.1) greater or equal to 80% were considered to have an elevated risk. The posterior median and 95% credible intervals (CrI) for the spatially structured effect were reported. Predictive posterior checks were conducted through predictive integral transformation values and observed versus fitted values. ResultsThe proportion of variance in the RR explained by a spatial effect ranged from 4.4% (male colorectal) to 19.2% (female breast). Lung cancer showed the greatest number of CHSAs with elevated risk (N-women = 50/218, N-men = 44/218), representing 2357 total excess cases. The largest lung cancer RRs were 1.67 (95% CrI = 1.06-2.50; exceedance probability = 96%; cases = 13) among women and 2.49 (95% CrI = 2.14-2.88; exceedance probability = 100%; cases = 174) among men. Areas with small population sizes and extreme SIRs were generally smoothed towards the null (RR = 1.0). DiscussionWe present a ready-to-use approach for small area cancer risk estimation and disease mapping using BYM2 and exceedance probabilities. We developed the smallareamapp R package, which provides a user-friendly interface through an R-Shiny application, for epidemiologists and surveillance experts to examine geographic variation in risk. These methods and tools can be used to estimate risk, generate hypotheses, and examine ecologic associations while adjusting for spatial dependency.",Not About Sufficiency
Learning to Navigate The Synthetically Accessible Chemical Space Using Reinforcement Learning,"Over the last decade, there has been significant progress in the field of machine learning for de novo drug design, particularly in generative modeling of novel chemical structures. However, current generative approaches exhibit a significant challenge: they do not ensure that the proposed molecular structures can be feasibly synthesized nor do they provide the synthesis routes of the proposed small molecules, thereby seriously limiting their practical applicability. In this work, we propose a novel reinforcement learning (RL) setup for de novo drug design: Policy Gradient fothree HIV targets. Finally, we describe how the end-to-end training conceptualized in this study represents an important paradigm in radically expanding the synthesizable chemical space and automating the drug discovery process.r Forward Synthesis (PGFS), that addresses this challenge by embedding the concept of synthetic accessibility directly into the de novo drug design system. In this setup, the agent learns to navigate through the immense synthetically accessible chemical space by subjecting initial commercially available molecules to valid chemical reactions at every time step of the iterative virtual synthesis process. The proposed environment for drug discovery provides a highly challenging test-bed for RL algorithms owing to the large state space and high-dimensional continuous action space with hierarchical actions. PGFS achieves state-of-the-art performance in generating structures with high QED and clogP. Moreover, we validate PGFS in an in-silico proof-of-concept associated with three HIV targets. Finally, we describe how the end-to-end training conceptualized in this study represents an important paradigm in radically expanding the synthesizable chemical space and automating the drug discovery process.",Not About Sufficiency
Investigating the influence of total productive maintenance key success factors on the social sustainability dimension of manufacturing SMEs,"PurposeKey success factors (KSFs) of total productive maintenance (TPM) have historically played a vital role in attaining economic and ecological sustainability but have overlooked social sustainability. Hence, this study analyses and ranks the most significant TPM KSFs for attaining social sustainability in manufacturing small and medium enterprises (SMEs).Design/methodology/approachThe research employs a deductive methodology to identify the relevant TPM KSFs and social sustainability indicators and then uses Fuzzy Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) to rank the TPM KSFs in order to achieve social sustainability, followed by a sensitivity analysis to assess the methodological robustness.FindingsThe findings indicate that the top five TPM KSFs influencing social sustainability are employee health and safety, organizational culture, top management commitment, employee engagement and effective communication and effective workplace management. In addition, the results indicate that effective equipment utilization is the least significant TPM key factor affecting social sustainability.Research limitations/implicationsSME manufacturing managers do not need to worry about all of the TPM KSFs if they only concentrate on the ones that will have the most impact. If managers use the top 5 TPM KSFs as a starting point, they may create customized TPM training programs for their companies. As a result, this will facilitate the efforts of their personnel toward social sustainability.Originality/valueIn the existing literature, little emphasis has been paid to social sustainability and how SMEs may implement these practices. This research adds to the current theory of TPM and social sustainability and sheds light on how SMEs might use TPM to advance toward more socially sustainable operations.",Not About Sufficiency
Serum biomarker-based osteoporosis risk prediction and the systemic effects of Trifolium pratense ethanolic extract in a postmenopausal model,"Background Recent years, a soaring number of marketed Trifolium pratense (red clover) extract products have denoted that a rising number of consumers are turning to natural alternatives to manage postmenopausal symptoms. T. pratense ethanolic extract (TPEE) showed immense potential for their uses in the treatment of menopause complications including osteoporosis and hormone dependent diseases. Early diagnosis of osteoporosis can increase the chance of efficient treatment and reduce fracture risks. Currently, the most common diagnosis of osteoporosis is performed by using dual-energy x-ray absorptiometry (DXA). However, the major limitation of DXA is that it is inaccessible and expensive in rural areas to be used for primary care inspection. Hence, serum biomarkers can serve as a meaningful and accessible data for osteoporosis diagnosis. Methods The present study systematically elucidated the anti-osteoporosis and estrogenic activities of TPEE in ovariectomized (OVX) rats by evaluating the bone microstructure, uterus index, serum and bone biomarkers, and osteoblastic and osteoclastic gene expression. Leverage on a pool of serum biomarkers obtained from this study, recursive feature elimination with a cross-validation method (RFECV) was used to select useful biomarkers for osteoporosis prediction. Then, using the key features extracted, we employed five classification algorithms: extreme gradient boosting (XGBoost), random forest, support vector machine, artificial neural network, and decision tree to predict the bone quality in terms of T-score. Results TPEE treatments down-regulated nuclear factor kappa-B ligand, alkaline phosphatase, and up-regulated estrogen receptor beta gene expression. Additionally, reduced serum C-terminal telopeptides of type 1 collagen level and improvement in the estrogen dependent characteristics of the uterus on the lining of the lumen were observed in the TPEE intervention group. Among the tested classifiers, XGBoost stood out as the best performing classification model with the highest F1-score and lowest standard deviation. Conclusions The present study demonstrates that TPEE treatment showed therapeutic benefits in the prevention of osteoporosis at the transcriptional level and maintained the estrogen dependent characteristics of the uterus. Our study revealed that, in the case of limited number of features, RFECV paired with XGBoost model could serve as a powerful tool to readily evaluate and diagnose postmenopausal osteoporosis.",Not About Sufficiency
Enhancing SARS-CoV-2 Lineage Surveillance through the Integration of a Simple and Direct qPCR-Based Protocol Adaptation with Established Machine Learning Algorithms,"Emerging and evolving Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) lineages, adapted to changing epidemiological conditions, present unprecedented challenges to global public health systems. Here, we introduce an adapted analytical approach that complements genomic sequencing, applying a cost-effective quantitative polymerase chain reaction (qPCR)-based assay. Viral RNA samples from SARS-CoV-2 positive cases detected by diagnostic laboratories or public health network units in Ceara, Brazil, were tracked for genomic surveillance and analyzed by using paired-end sequencing combined with integrative genomic analysis. Validation of a key structural variation was conducted with gel electrophoresis for the presence of a specific open reading frame 7a(ORF7a) gene deletion within the ""BE.9"" lineages tracked. The analytical innovation of our method is the optimization of a simple intercalating dye-based qPCR assay through repositioning primers from the ARTIC v4.1 amplicon panel to detect large molecular patterns. This assay distinguishes between ""BE.9"" and ""non-BE.9"" lineages, particularly BQ.1, without the need for expensive probes or sequencing. The protocol was validated against lineage predictions from next-generation sequencing (NGS) using 525 paired samples, achieving 93.3% sensitivity, 95.1% specificity, and 92.4% agreement, as measured by Cohen's Kappa coefficient. Machine learning (ML) models were trained using the melting curves from intercalating dye-based qPCR of 1724 samples, enabling highly accurate lineage assignment. Among them, the support vector machine (SVM) model had the best performance and after fine-tuning showed similar to 96.52% (333/345) accuracy in comparison to the test data set. Our integrated approach provides an adapted analytical method that is both cost-effective and scalable, suitable for rapid assessment of emerging variants, especially in resource-limited settings. In this work, the protocol is applied to improve the monitoring of SARS-CoV-2 sublineages but can be extended to track any key molecular signature, including large insertions and deletions (indels) commonly observed in pathogenic agent subtypes. By offering a complement to traditional sequencing methods and utilizing easily trainable machine learning algorithms, our methodology contributes to enhanced molecular surveillance strategies and supports global efforts in pandemic control.",Not About Sufficiency
Smart Blood Bank System Using IOT,"Today, in the present scenario, many people were facing a lot of problems in getting blood for patients at right time. The needy does not know where to get the blood or how to get access to a required quantity of blood quickly in an emergency. Blood donation is a big issue and consumes a lot of time to the donor. The blood bank systems have no proper tool for managing the blood collected from the camps. The needy in an emergency could not find the compatible blood or platelets with the patient. There are many existing mobile apps for blood donation. Though smartphones are available it is not possible to find a right donor who has compatible blood group with the patient in right time. We propose an integrated solution which will connect blood banks, donors, and the needy. This solution provides efficiency and convenience for the needy to search for required blood group or platelets in your neighbourhood easy and simple. Get instant help form blood banks without signing up required. The solution works for the encouragement of blood donation and ensures the availability of blood or platelets accessible to the needy. It also assists quality management programs for blood platelets transfusion services. Finally, this solution is used to notify/communicate with the nearest blood bank organizations timely to ensure the availability of safe and quality blood accessible to the needy with minimum effort.",Not About Sufficiency
Advances on acrylamide in cocoa and its derivates: a challenge to control from postharvest to the industrialization,"This review summarizes the main advances reached in studying acrylamide (AA) in cocoa and its derivates from 2004 to 2022. This systematic analysis was based on bibliometric networks with the VOSviewer application run on scientific databases. Three focus areas were identified as relevant to analysis: i) concentration of AA, ii) analytical methods of quantification, and iii) risk-assessment evaluation. The results show the presence of AA from dry cocoa beans without roasting as well as processed chocolate and derivatives. The levels of AA are similar to the regulatory limits. The diversity of the analytical methods makes it difficult to unify effective standardized procedures, given the composition of the cocoa matrix, which has a high-fat content and the relatively low AA concentrations. Similarly, for risk- assessment evaluation, it is mandatory for the standardization of methods for the estimation of long-term exposition to acrylamide, as well as the development of strategies to counteract its adverse health effects.",Not About Sufficiency
Digital Gatekeepers,"If in William Blackstone's time we might have thought of a person's home as their castle, in Mark Zuckerberg's time we might say that their website is too. Under cyber-trespass laws like the Computer Fraud and Abuse Act, courts have treated online platforms as digital gatekeepers-as property owners that may permit and restrict access to websites much like landowners may do with private land in the real world. If platforms withhold their consent through words or inference, cyber-trespass laws let them enforce their preferences about who may access their services and gather information from the internet. Concerned about reputations and profits, platforms have deployed their gatekeeper rights to scare and sue those seeking to use their websites against their wishes. This legal regime affects all sorts of actors-from academics to journalists to businesses to consumers-who want to engage with the platforms ' websites, even when they're open to the public or when people permit complementary services to access their accounts. This Essay challenges the law's current embrace of gatekeeper rights. Applying cyber-trespass law across the entire internet has empowered private platforms to become public policymakers in unintended and unchecked ways. But it's not too late to adopt a different legal regime-one that defers far less to private companies to establish and enforce the internet's accessibility and informational rules. This Essay offers a three-part legislative framework to restrain the power of digital gatekeepers. To begin, Congress should clarify that cyber-trespass laws don't apply on websites that are accessible to the general public. Congress should then mandate and shield certain forms of interoperability between plafforms. Finally, Congress should pass targeted laws to regulate the collection and use of publicly accessible information on websites. Taking these steps will change the locus of governance in key internet policy choices, stripping private platforms of the unbounded and trans-substantive decisionmaking power they currently enjoy. Although this regulatory agenda is ambitious, these are the kinds of fundamental and structural changes needed to protect privacy, speech, and consumer interests in the digital age.",Not About Sufficiency
A Comparative Analysis for Air Quality Estimation from Traffic and Meteorological Data,"Air pollution in urban regions remains a crucial subject of study, given its implications on health and environment, where much effort is often put into monitoring pollutants and producing accurate trend estimates over time, employing expensive tools and sensors. In this work, we study the problem of air quality estimation in the urban area of Milan (IT), proposing different machine learning approaches that combine meteorological and transit-related features to produce affordable estimates without introducing sensor measurements into the computation. We investigated different configurations employing machine and deep learning models, namely a linear regressor, an Artificial Neural Network using Bayesian regularization, a Random Forest regressor and a Long Short Term Memory network. Our experiments show that affordable estimation results over the pollutants can be achieved even with simpler linear models, therefore suggesting that reasonably accurate Air Quality Index (AQI) measurements can be obtained without the need for expensive equipment.",Not About Sufficiency
"The relationships between forestry sector standardization, market evolution and sustainability approaches in the communist and post-communist economies: the case of Romania","This paper analyses how forestry standardization process interrelates with the national and sectoral economic characteristics and the evolution of sustainable forest management implementation in communist and post-communist Romania. The study used the database of Romanian Standardization Association for selecting forestry specific standards, which have been issued since 1949. The selected standards were grouped according to their scope, issuing period and international recognition, and the obtained distributions were analysed in the context of sectoral economic evolution. In the communist period, the long- term sectoral strategy, which was centred on sustainable forest management, added value products and export was accompanied by a sustained effort in standardizing the design and quality of forest products, as well as the needed processes. Based on standardization, the efficient and integrated forest industry acted in the framework of a prescriptively regulated sustainable forest management. Mandatory national standards from the communist period have been mostly replaced by post-communist consensual international standards. The opportunities of a market economy and EU trade supported a private forest industry that is increasingly efficient, productive and innovative. However, considering the high forestry sector environmental and social sustainability requirements, the state authorities must carefully address their mission of balancing different interests, for which standardization may provide very useful tools.",Not About Sufficiency
Local planning scenario for shading from trees as an urban nature-based solution,"With more than 75% of the European Union's population living in urban areas covering 21.5% of the EU territory, the importance of climate-resilient cities, towns and suburbs has increased dramatically. However, the rising impact of human-induced land-use changes on ecosystem services (ES) poses a major challenge to the urban environment. This study focuses on scenario development for nature-based solutions (NbS) in a European town with its intense development area. The concept is exemplified in a town in Croatia, Grad Velika Gorica (GVG), that like many others cities undergoes urbanisation processes with limited resources. It serves as a showpiece for the influence of NbS, in particular street trees along various paths. Using spatial analysis and modelling, the approach explores NbS for future urbanisation. The results, supported by quantitative analysis, show that 49% of cycle lanes and footpaths in GVG can be shaded by strategically planted street trees. The shading scenario analysis provides a nuanced perspective on the potential of NbS, offering insights into the key tasks for a climate-resilient city and opportunities towards equitable, green and healthy urban areas. In the context of urbanisation processes and climate adaptation, the study is in line with the overarching objectives of the European Commission which emphasises the need for sustainable NbS alternatives to address environmental challenges. The findings contribute to the framework of informed decision-making towards urban climate resilience. It also supports the pursuit of a sustainable local governance for climate-adjusted environmental quality in urban planning. As towns and cities grapple with the imperative of balancing urban development with environmental protection, this research highlights the central role of NbS, particularly street trees, in shaping climate-resilient and more sustainable urban environments for human well-being in cities.",Not About Sufficiency
Analysis of pharmaceutical inventory management based on ABC-VEN analysis in Rwanda: a case study of Nyamagabe district,"BackgroundPharmaceuticals account for a large portion of healthcare spending in healthcare organizations. Their effective inventory management is required to match the cost of stocks with the customer demand and avoid shortage of supplies at any health facility level. This study aimed to analyze pharmaceuticals' inventory management using ABC-VEN analysis.MethodsThe study was conducted at Rwanda Medical Supply (RMS) Ltd, Nyamagabe Branch for products distributed to health facilities in Nyamagabe District catchment area from the financial years 2017-2018 to 2019-2020. It consisted of a descriptive retrospective study of 457 items. The latter are generic essential medicines distributed to public health facilities during the study period. Products were arranged according to a descending order of importance, and we performed a breakdown of products according to the Pareto Principle. Following an ABC analysis of distribution data for such drugs billed to healthcare facilities, a VEN analysis was performed to identify high-value vital products that require more attention.ResultsDuring the ABC analysis, 76 products were classified in group A. These accounted for 19.84% and had a value of 74.91% of the total cost of all products. Group B included 116 products, representing 30.29% with a value of 20% of the total cost, while Group C had 191 products, representing 49.87% with a value of only 5.09% of the total cost. During the VEN analysis, 202 products (44.20%) were classified as vital, 231 (50.54%) as essential, and 24 products (75.26%) as non-vital. The analysis with ABC-VEN resulted in Class I representing 55.80% of all medicines that cost 87.88% of all total cost, Class II representing 40.70% with a total cost of 11.82%, and Class III representing 3.50% with a cost of 0.3%.ConclusionsThis study results show that inventory management of vital and expensive products, such as antibiotics, antihypertensive pharmaceuticals, consumables, and massive solutions would be carefully monitored to prevent a shortage of such products at health facility levels. The ABC-VEN analysis is one of the practical and affordable method to achieve their optimized supply chain.",Not About Sufficiency
Advancements in automated diagnosis of autism spectrum disorder through deep learning and resting-state functional mri biomarkers: a systematic review,"Autism Spectrum Disorder(ASD) is a type of neurological disorder that is common among children. The diagnosis of this disorder at an early stage is the key to reducing its effects. The major symptoms include anxiety, lack of communication, and less social interaction. This paper presents a systematic review conducted based on PRISMA guidelines for automated diagnosis of ASD. With rapid development in the field of Data Science, numerous methods have been proposed that can diagnose the disease at an early stage which can minimize the effects of the disorder. Machine learning and deep learning have proven suitable techniques for the automated diagnosis of ASD. These models have been developed on various datasets such as ABIDE I and ABIDE II, a frequently used dataset based on rs-fMRI images. Approximately 26 articles have been reviewed after the screening process. The paper highlights a comparison between different algorithms used and their accuracy as well. It was observed that most researchers used DL algorithms to develop the ASD detection model. Different accuracies were recorded with a maximum accuracy close to 0.99. Recommendations for future work have also been discussed in a later section. This analysis derived a conclusion that AI-emerged DL and ML technologies can diagnose ASD through rs-fMRI images with maximum accuracy. The comparative analysis has been included to show the accuracy range.",Not About Sufficiency
Doctor-patient bilateral matching considering diagnosis and treatment perception in the absence of public health resources,"IntroductionThe public health crisis is one of the main threats affecting the sustainable development of the economy and strengthening the rational allocation of medical resources is essential for building a strong public health system. Therefore, the study of the doctor-patient bilateral matching has important theoretical and practical significance and perception of diagnosis and treatment is taken as a key consideration in the research. MethodsBased on the current situation of the medical industry and the main contradiction between supply and demand of medical services, an evaluation index of doctor-patient satisfaction is constructed in this paper. Then, based on the different forms of evaluation, calculate the doctor's satisfaction and patient's satisfaction respectively. Taking maximizing the overall satisfaction of doctors and patients, maximizing the number of patients and minimizing the workload difference between doctors as the decision-making objectives, considering the upper limit of doctors' working hours as the constraint condition, a multi-objective decision-making model is constructed and solved by NSGA-II algorithm to realize the matching between doctors and patients. ConclusionFinally, through the comparison with NSGA-III algorithm in three dimensions: the degree of convergence to the reference set, the propagation range of the solution and the running time of the algorithm, it is proved that NSGA-II algorithm has good performance in solving the matching problem of medical service supply and demand.",Not About Sufficiency
"Machine Learning Approach To Estimate Hourly Exposure to Fine Particulate Matter for Urban, Rural, and Remote Populations during Wildfire Seasons","Exposure to wildfire smoke averaged over 24-hour periods has been associated with a wide range of acute cardiopulmonary events, but little is known about the effects of sub-daily exposures immediately preceding these events. One challenge for studying sub-daily effects is the lack of spatially and temporally resolved estimates of smoke exposures. Inexpensive and globally applicable tools to reliably estimate exposure are needed. Here we describe a Random Forests machine learning approach to estimate 1-hour average population exposure to fine particulate matter during wildfire seasons from 2010 to 2015 in British Columbia, Canada, at a 5 km x 5 km resolution. The model uses remotely sensed fire activity, meteorology assimilated from multiple data sources, and geographic/ecological information. Compared with observations, model predictions had a correlation of 0.93, root mean squared error of 3.2 mu g/m(3), mean fractional bias of 15.1%, and mean fractional error of 44.7%. Spatial cross-validation indicated an overall correlation of 0.60, with an interquartile range from 0.48 to 0.70 across monitors. This model can be adapted for global use, even in locations without air quality monitoring. It is useful for epidemiologic studies on sub-daily exposure to wildfire smoke and for informing public health actions if operationalized in near-real-time.",Not About Sufficiency
"XAI for learning: Narrowing down the digital divide between ""new"" and ""old"" experts","Regular eXplainable AI (XAI) approaches are often ineffective in supporting decision-makers across domains. In some instances, it can even lead to automation bias or algorithmic aversion or would simply be ignored as a redundant feature. Based on cognitive psychology literature we outline a strategy for how XAI interface design could be tailored to have a long-lasting educational value. We suggest the features that could support domain-related and technical skills development this way narrowing the digital divide between ""new"" and ""old"" experts. Lastly, we suggest an intermitted explainability approach that could help to find a balance between seamless and cognitively engaging explanations.",Not About Sufficiency
Tsunami Damage Detection with Remote Sensing: A Review,"Tsunamis are rare events compared with the other natural disasters, but once it happens, it can be extremely devastating to the coastal communities. Extensive inland penetration of tsunamis may cause the difficulties of understanding its impact in the aftermath of its generation. Therefore the social needs to technologies of detecting the wide impact of great tsunamis have been increased. Recent advances of remote sensing and technologies of image analysis meet the above needs and lead to more rapid and efficient understanding of tsunami affected areas. This paper provides a review of how remote sensing methods have developed to contribute to post-tsunami disaster response. The evaluations in the performances of the remote sensing methods are discussed according to the needs of tsunami disaster response with future perspective.",Not About Sufficiency
Hybrid generative regression-based deep intelligence to predict the risk of chronic disease,"Purpose Chronic diseases are considered as one of the serious concerns and threats to public health across the globe. Diseases such as chronic diabetes mellitus (CDM), cardio vasculardisease (CVD) and chronic kidney disease (CKD) are major chronic diseases responsible for millions of death. Each of these diseases is considered as a risk factor for the other two diseases. Therefore, noteworthy attention is being paid to reduce the risk of these diseases. A gigantic amount of medical data is generated in digital form from smart healthcare appliances in the current era. Although numerous machine learning (ML) algorithms are proposed for the early prediction of chronic diseases, these algorithmic models are neither generalized nor adaptive when the model is imposed on new disease datasets. Hence, these algorithms have to process a huge amount of disease data iteratively until the model converges. This limitation may make it difficult for ML models to fit and produce imprecise results. A single algorithm may not yield accurate results. Nonetheless, an ensemble of classifiers built from multiple models, that works based on a voting principle has been successfully applied to solve many classification tasks. The purpose of this paper is to make early prediction of chronic diseases using hybrid generative regression based deep intelligence network (HGRDIN) model. Design/methodology/approach In the proposed paper generative regression (GR) model is used in combination with deep neural network (DNN) for the early prediction of chronic disease. The GR model will obtain prior knowledge about the labelled data by analyzing the correlation between features and class labels. Hence, the weight assignment process of DNN is influenced by the relationship between attributes rather than random assignment. The knowledge obtained through these processes is passed as input to the DNN network for further prediction. Since the inference about the input data instances is drawn at the DNN through the GR model, the model is named as hybrid generative regression-based deep intelligence network (HGRDIN). Findings The credibility of the implemented approach is rigorously validated using various parameters such as accuracy, precision, recall, F score and area under the curve (AUC) score. During the training phase, the proposed algorithm is constantly regularized using the elastic net regularization technique and also hyper-tuned using the various parameters such as momentum and learning rate to minimize the misprediction rate. The experimental results illustrate that the proposed approach predicted the chronic disease with a minimal error by avoiding the possible overfitting and local minima problems. The result obtained with the proposed approach is also compared with the various traditional approaches. Research limitations/implications Usually, the diagnostic data are multi-dimension in nature where the performance of the ML algorithm will degrade due to the data overfitting, curse of dimensionality issues. The result obtained through the experiment has achieved an average accuracy of 95%. Hence, analysis can be made further to improve predictive accuracy by overcoming the curse of dimensionality issues. Practical implications The proposed ML model can mimic the behavior of the doctor's brain. These algorithms have the capability to replace clinical tasks. The accurate result obtained through the innovative algorithms can free the physician from the mundane care and practices so that the physician can focus more on the complex issues. Social implications Utilizing the proposed predictive model at the decision-making level for the early prediction of the disease is considered as a promising change towards the healthcare sector. The global burden of chronic disease can be reduced at an exceptional level through these approaches. Originality/value In the proposed HGRDIN model, the concept of transfer learning approach is used where the knowledge acquired through the GR process is applied on DNN that identified the possible relationship between the dependent and independent feature variables by mapping the chronic data instances to its corresponding target class before it is being passed as input to the DNN network. Hence, the result of the experiments illustrated that the proposed approach obtained superior performance in terms of various validation parameters than the existing conventional techniques.",Not About Sufficiency
"Quantification of prevalence, clinical characteristics, co-existence, and geographic variations of traditional Chinese medicine diagnostic patterns via latent tree analysis-based differentiation rules among functional dyspepsia patients","Background Traditional Chinese Medicine (TCM) treatment strategies are guided by pattern differentiation, as documented in the eleventh edition of the International Classification of Diseases (ICD). However, no standards for pattern differentiation are proposed to ensure inter-rater agreement. Without standardisation, research on associations between TCM diagnostic patterns, clinical features, and geographical characteristics is also not feasible. This diagnostic cross-sectional study aimed to (i) establish the pattern differentiation rules of functional dyspepsia (FD) using latent tree analysis (LTA); (ii) compare the prevalence of diagnostic patterns in Hong Kong and Hunan; (iii) discover the co-existence of diagnostic patterns; and (iv) reveal the associations between diagnostic patterns and FD common comorbidities. Methods A total of 250 and 150 participants with FD consecutively sampled in Hong Kong and Hunan, respectively, completed a questionnaire on TCM clinical features. LTA was performed to reveal TCM diagnostic patterns of FD and derive relevant pattern differentiation rules. Multivariate regression analyses were performed to quantify correlations between different diagnostic patterns and between diagnostic patterns and clinical and geographical variables. Results At least one TCM diagnostic pattern was differentiated in 70.7%, 73.6%, and 64.0% of the participants in the overall (n = 400), Hong Kong (n = 250), and Hunan (n = 150) samples, respectively, using the eight pattern differentiation rules derived. 52.7% to 59.6% of the participants were diagnosed with two or more diagnostic patterns. Cold-heat complex (59.8%) and spleen-stomach dampness-heat (77.1%) were the most prevalent diagnostic patterns in Hong Kong and Hunan, respectively. Spleen-stomach deficiency cold was highly likely to co-exist with spleen-stomach qi deficiency (adjusted odds ratio (AOR): 53.23; 95% confidence interval (CI): 21.77 to 130.16). Participants with severe anxiety tended to have liver qi invading the stomach (AOR: 1.20; 95% CI: 1.08 to 1.33). Conclusions Future updates of the ICD, textbooks, and guidelines should emphasise the importance of clinical and geographical variations in TCM diagnosis. Location-specific pattern differentiation rules should be derived from local data using LTA. In future, patients' pattern differentiation results, local prevalence of TCM diagnostic patterns, and corresponding TCM treatment choices should be accessible to practitioners on online clinical decision support systems to streamline service delivery.",Not About Sufficiency
"Blue Growth and its discontents in the Faroe Islands: an island perspective on Blue (De)Growth, sustainability, and environmental justice","Blue Growth is promoted as an important strategy for future food security, and sustainable harvesting of marine resources. This paper aims to identify dominating ideologies and strategies of Blue Growth in the Faroe Islands, mainly regarding salmon farming and industrial capture fisheries, and to investigate how these ideologies materialize in the social metabolism of Faroese society. The analysis approaches the Faroese Blue Economy from a holistic perspective using analytical concepts and frameworks of social (island) metabolism, environmental justice and degrowth to assess how current Blue Growth strategies pertain to long-term sustainability and human well-being. It offers a critical analysis of aquaculture in the Faroe Islands and shows that although the rhetoric around Blue Growth is framed within mainstreamed sustainability discourse, the ideologies and visions underpinning current Blue Growth strategies result in a continuation of conventional growth through the exploitation of new commodity frontiers. Finally, the negative consequences of Blue Growth are assessed and discussed through a mapping of recent and ongoing social and ecological distribution conflicts in the Faroes.",Not About Sufficiency
Nursing practice and global refugee migration: initial impressions from an Intergovernmental-Academic Partnership,"Aim This report from the field describes impressions of the initial impact of bilateral, multi-sectoral field-based activities undertaken to strengthen International Organization for Migration/United Nations Migration Agency and US-based nurses' capacity to address complex clinical, social and cultural challenges experienced by refugees in resettlement. Authors comment on the defined and thorough health assessment process that refugees go through prior to resettlement, and focus on the essential nursing role in the health assessment process and continuum of care. The development of the interdisciplinary and collaborative partnership is described as well as next steps to move the partnership forward. Background In 2017, International Organization for Migration/United Nations Migration Agency and the University of Minnesota, guided by experts from the United States Centers for Disease Control and Prevention, began a unique bilateral Intergovernmental-Academic partnership to enhance the health care of refugees. A key component was to strengthen nursing care of refugees through the standardization of clinical practice and nursing leadership. Sources of Evidence Listening sessions, direct interaction between International Organization for Migration/United Nations Migration Agency and US-based refugee resettlement stakeholders, patterns in resettlement. Conclusion and Implications for Nursing and Health Policy The report highlights the potential public health impact of a bilateral and collaborative initiative that develops and bridges key points in the migration and health trajectory of people with refugee status. Separated by geography, context and scope of work, health professionals in different roles in varied worldwide settings with a spectrum of resources may not fully understand the work of each other. Project activities were a platform through which US-based and internationally based nurses established mutuality, reciprocity and equity as partners. By strengthening systems and resources, the partnership reinforces the abilities of nurses who engage in this important work, to optimize health and wellbeing of people with refugee status.",Not About Sufficiency
Recent advances in availability and synthesis of the economic costs of biological invasions,"Biological invasions are a global challenge that has received insufficient attention. Recently available cost syntheses have provided policy- and decision makers with reliable and up-to-date information on the economic impacts of biological invasions, aiming to motivate effective management. The resultant InvaCost database is now publicly and freely accessible and enables rapid extraction of monetary cost information. This has facilitated knowledge sharing, developed a more integrated and multidisciplinary network of researchers, and forged multidisciplinary collaborations among diverse organizations and stakeholders. Over 50 scientific publications so far have used the database and have provided detailed assessments of invasion costs across geographic, taxonomic, and spatiotemporal scales. These studies have provided important information that can guide future policy and legislative decisions on the management of biological invasions while simultaneously attracting public and media attention. We provide an overview of the improved availability, reliability, standardization, and defragmentation of monetary costs; discuss how this has enhanced invasion science as a discipline; and outline directions for future development.",Not About Sufficiency
ProteinNet: a standardized data set for machine learning of protein structure,"BackgroundRapid progress in deep learning has spurred its application to bioinformatics problems including protein structure prediction and design. In classic machine learning problems like computer vision, progress has been driven by standardized data sets that facilitate fair assessment of new methods and lower the barrier to entry for non-domain experts. While data sets of protein sequence and structure exist, they lack certain components critical for machine learning, including high-quality multiple sequence alignments and insulated training/validation splits that account for deep but only weakly detectable homology across protein space.ResultsWe created the ProteinNet series of data sets to provide a standardized mechanism for training and assessing data-driven models of protein sequence-structure relationships. ProteinNet integrates sequence, structure, and evolutionary information in programmatically accessible file formats tailored for machine learning frameworks. Multiple sequence alignments of all structurally characterized proteins were created using substantial high-performance computing resources. Standardized data splits were also generated to emulate the difficulty of past CASP (Critical Assessment of protein Structure Prediction) experiments by resetting protein sequence and structure space to the historical states that preceded six prior CASPs. Utilizing sensitive evolution-based distance metrics to segregate distantly related proteins, we have additionally created validation sets distinct from the official CASP sets that faithfully mimic their difficulty.ConclusionProteinNet represents a comprehensive and accessible resource for training and assessing machine-learned models of protein structure.",Not About Sufficiency
Older adults' reasons for applying to a nursing home - a document analysis,"BackgroundAgeing in place is the social norm in Sweden, yet older adults apply for a nursing home on a daily basis which suggests that ageing in place needs further study as it is not suitable for everyone. Aim: to study descriptions of older adults' reasons for applying to a nursing home in documents of granted nursing home decisions.Materials and methodsOne hundred and sixty decisions were analyzed through document analysis with a deductive content analysis using the Canadian Model of Occupational Performance-Engagement (CMOP-E) as a framework.ResultsReasons for applying were represented in the three factors of the CMOP-E. In personal factors, reason for applying was e.g. connected to severe anxiety. In environmental factors, family culture had an influence in the application. In occupational factors, the ability to perform self-care and mobility greatly affected decisions to apply to nursing homes.ConclusionDescriptions of the older adults' activities in daily life were limited. If OTs were further involved in nursing home applications, adults ageing in place could be better supported and a move to a nursing home may be prevented. Significance: this study contributes to the understanding of why older adults chose to apply to a nursing home.",Not About Sufficiency
Using LSTM Cells for SIP Dialogs Mapping and Security Analysis,"Past decade has witnessed a rapid growth of a number of use cases employing the neural networks for many diverse purposes starting with user identification through anomaly detection and ending with speech synthesis. This growth is bolstered by the increasingly accessible massive computational power provided by both modern processors and graphic processing units (GPUs), community driven rapid development of machine learning frameworks such as Tensorflow or Theano, and the ability of neural networks to successfully find the relation that is difficult if not impossible to obtain analytically. In the field of modern communications, the anomaly detection algorithms have been a focus of many scientific and industrial studies since the late detection of attack/misconfiguration can result in massive economic losses in telecommunications industry. This paper follows the pattern of evolution of artificial intelligence algorithms employed in the telecommunications field and moves the field further by considering the signaling exchange as simple language protocol and creating a model that can learn a dialect of individual nodes in the network and detect if this dialect changes and thus alerting the network administrator of a possible break in. The abstraction of the signaling protocol used allows for usage of recurrent neural networks or more specifically Long-Short Term Memory (LSTM) Cells that have never been used for this purpose as far as the authors can tell.",Not About Sufficiency
Exploring Thermoset Fracture with a Quantum Chemically Accurate Model of Bond Scission,"A molecular understanding of thermoset fracture is crucial for enhancing the performance and durability across applications. However, achieving accurate atomistic modeling of thermoset fracture remains computationally prohibitive due to the high cost associated with quantum mechanical methods for describing bond breaking. In this work, we introduce an active learning (AL) framework for our recently developed machine learning-based adaptable bond topology (MLABT) model that uses data sets generated via density functional theory (DFT) calculations that are both minimalistic and informative. Employing MLABT integrated with AL and DFT, we explore fracture behavior in highly cross-linked thermosets, assessing the variations in fracture induced by system temperature, temperature fluctuations, strain rate, cooling rate, and degree of cross-linking. Notably, we discover that while fracture is minimally affected by temperature, it is strongly influenced by the strain rate. Furthermore, while the structural disparities introduced by different network annealing rates influence the elastic properties, they are inconsequential for thermoset fracture. In contrast, network topology emerges as the dominant determinant of fracture, influencing both the ultimate strain and stress. Particularly, MLABT with AL-DFT achieving near quantum-chemical bond breaking accuracy still leads to ductile failures, emphasizing the necessity of modeling polymer networks at larger length scales for bridging the gap between the experiment and simulation. Nevertheless, the integration of MLABT with the AL framework paves the way for efficient and DFT-accurate modeling of thermoset fracture, providing an affordable and accurate approach for calculating polymer network fracture across chemical space.",Not About Sufficiency
Grid-based spatiotemporal modeling of ambient ozone to assess human exposure using environmental big data,"Ozone (O-3) pollution in China is increasing. It is the primary pollutant in summer and ambient O-3 can lead to serious health problems for the public. Therefore, characterizing the spatiotemporal distribution of O-3 is required for better environmental management and human exposure assessment. Statistical models, especially those based on machine learning, can be more convenient to use than chemical transport models and have shown improved accuracy. However, the quality of data affects model precision especially at fine spatiotemporal scales. Web-based environmental data can improve the spatial and temporal resolution of modeling data. This study applied high spatiotemporal resolution source information and emission inventories based on point of interest and real-time traffic data in a fine scale grid network to predict the O-3 concentrations and assess the human exposure within Chengdu, China. The results showed that the web-based environmental data could be combined with statistical models such as random forest in air quality modeling. The model precision was high, especially at finer spatiotemporal scales. The R-2 of the hourly and daily maximum 8-h mean concentrations of O-3 models built in this study were 0.83 and 0.91 for sample-based cross-validation, and 0.79 and 0.90 for site-based cross-validation, respectively. Meteorological variables had the greatest impact on O-3 concentrations especially sea-level pressure, temperature, vapor pressure, and humidity. People within the research area had a relatively high exposure level to pollutants over a longer time scale in the summer and spring. Findings from this work provide a good reference for related research on modeling air quality, and human health risk assessment using environmental big data.",Not About Sufficiency
A systematic review of ethical challenges and opportunities of addressing domestic violence with AI-technologies and online tools,"Domestic violence remains a pressing complex social problem of people of any gender, age, socio-economic status, and ethno-cultural background, an issue that worsened worldwide during the COVID-19 pandemic. Digital, online, or artificial intelligence-based smart technological services, applications, and tools provide novel approaches in addressing domestic violence, including intimate partner violence. This systematic literature review analyses the ethical challenges and opportunities these (protective) digital and smart technologies provide to the stakeholders involved. Our results highlight that the public health and societal issue are the leading narratives of domestic violence, which is predominantly interpreted as gender-based violence. The review highlights an emerging trend of the role of machine learning-and artificial intelligence-based approaches in identifying and preventing domestic violence. However, we argue that little recommendation is available to professionals about how to use these approaches in a responsible way, and that the smartness of high-tech technologies is often challenged by basic-level technologies from perpetrators, creating an imbalance that also limits an impactful development of a comprehensive socio-technical regime that serves the safety and resilience of families in their communal setting.",Not About Sufficiency
Human Digital Twin: Enabling Human-Multi Smart Machines Collaboration,"AI solutions are becoming ubiquitous in private and professional Domains. Soon, humans using AI will be relying on recommendations and actions from multiple smart machines to coordinate and manage their financial, professional, or health objectives. As the human's life aspects and objectives are dependent and as her resources are limited, the paper argues that to avoid conflicts, all smart machines supporting a human need to be adaptively aligned with her objectives. The paper introduces the concept of a Human Digital Twin (HDT), which is a human-specific smart machine dedicated to aligning human objectives with the smart machines supporting her. The HDT monitors the entire human-AI space and, based on the human's responses to the various machine actions, the HDT identifies stable human-machine interaction patterns, which can be used to anticipate human responses in given contexts and thus ensure the alignment of the various machines with her objectives. The HDT learns the human-machine interaction patterns by using the structure of the information used in the interactions (metadata), and not by relying on the content of that information. This enables HDTs to overcome the constraint of existing approaches, which require all smart solutions to operate on the same platform. In addition, by relying only on information structures, HDTs would not compromise human data or proprietary provider information.",Not About Sufficiency
At least two accounting systems for Gross Ecosystem Product (GEP) are needed,"Gross Ecosystem Product (GEP) denotes the aggregate value of ecosystem services (ESs) supplied by all ecosystem types within an accounting area for a given accounting period in monetary units and represents one of the multiple types of measures of ES values. The value of each ES is typically calculated by multiplying the ES's quantity by the ES's unit value that may be reflected by various proxies, such as market price, replacement cost, and avoided damage cost. As an economic, ecological, or environmental indicator, GEP should be calculated based on certain standards, allowing for comparison over time and across regions. While many standards, including which ESs to account for and what proxies for ESs' values to use, should be unified to improve the comparability of GEP, this standardization does not mean a single GEP accounting system is sufficient to achieve multiple goals. Instead, at least two accounting systems with different applicability and levels of sophistication are needed. (1) To assess environmental status, ecosystems' capacities to provide ESs, and the performance of environmental governance, there should be a simplified system that accounts for both realized ESs (actually received by humans) and potential ESs (beyond actual humans demands) and adopts a nationwide unified constant unit value of each ES. (2) To assess ESs' actual contributions to socioeconomic development and human well-being, there should be a more sophisticated system that accounts for realized ESs only and adopts a dynamic unit value of each ES. This dynamic unit value should be adjusted to fit local socioeconomic and ecological contexts, consider the tipping point or threshold in each ES's quantity, and reflect the diminishment or increment in the marginal utility of each ES.",Not About Sufficiency
THE AURORA URBAN PLANNING SIMULATION Teaching About Class through Spatial Inequality in Secondary Social Studies,,Not About Sufficiency
"Evaluating the impact of proximity to reported toxic release facilities and flood events on chronic health outcomes in the city of Galena Park, Texas","Evidence has conclusively revealed that environmental justice communities experience poor environmental conditions compared to more affluent majority communities. However, there has been little research evaluating the health impacts of immediate proximity to industrial pursuits and flood events on a population compared to others living within the same community who are only marginally removed from these locations. This cross-sectional study (N = 130) utilized three approaches to assess health outcomes (1) the 12 item Short Form Health Survey, which creates a general physical component score, (2) self-reported noncancerous chronic conditions, and (3) self-reported diagnosis of twelve different cancers. Three risk levels were spatially created using a 5-scale ordinal score for each residential parcel based on the corresponding flood probability level and proximity to facilities which report to the United States Environmental Protection Agencies Toxic Release Inventory. Analysis revealed that general physical health scores were significantly lower (P-value < 0.001) in the medium and high-risk locations, Similarly chronic conditions witnessed a non-significant twofold increased risk in the highest-risk locations compared to the lowest (POR 1.91; 95 % CI 0.82-4.39) and a non-significant increased risk of cancer diagnosis (POR 1.51; 95 % CI 0.38-5.99). This research underscores the importance of place and health outcomes even within relatively geographically compact communities. Public health and urban planning interventions and designs should take into account fine grain approaches to respond to community needs while still being mindful of limited resources.",Not About Sufficiency
Unbalanced burden is escalating: Urban inequality landscape under sudden shocks?,"Sudden and unforeseen events, notably the COVID-19 pandemic has underscored and exacerbated pre-existing urban inequalities. These disruptions have disproportionately affected the most vulnerable sectors of society, thereby further aggravating existing inequalities. Addressing these inequality issues requires systematic research into academic community's response to significant global disturbances. Therefore, we have integrated bibliometric analysis with meta-analysis (quantitatively combines research results from systematic reviews) techniques, culminating in a research framework based on KNIME software (a software for creating data applications and services) and Python programming. Noteworthy for its open-source nature, scalability, and reproducibility, this framework tries to decode the developmental patterns and response characteristics of urban inequality research under sudden shocks. Additionally, it seeks to clarify its causal pathways, predict the emergence of new or strengthened inequalities, and provide effective suggestions to promote inclusive and sustainable urban development. Our findings indicate that the COVID-19 has exacerbated the existing urban inequality, attracting widespread attention in fields such as society and public health. Issues of fairness and justice of human health, resource allocation, urban resilience and vulnerability have become hot topics around COVID-19 related urban inequalities. Simultaneously, the sudden shocks have led to disparities in research areas and themes. Social, economic, and urban planning are the main drivers behind urban inequality, and particularly affecting vulnerable groups, including low-income populations and those living in remote regions. Before and after the COVID-19, the average global urban inequality are 0.57 and 0.61, respectively. Inequality issues are particularly severe in Africa and countries such as Afghanistan, Pakistan, and India. The pandemic changed the focus of urban inequality to health and social issues, worsening inequality performance in these regions and deepening global gaps. Additionally, countries with high urban inequality are more prone to large-scale outbreaks and higher mortality rates. To this end, we advocate academia for in-depth research and comprehensive evaluation, focusing on vulnerable groups, providing integrated policy recommendations, supporting interdisciplinary exchanges and cooperation, and committing to find long-term sustainable solutions. These efforts aim to propose more comprehensive and reasonable solutions to alleviate and address complex urban inequality issues under sudden shocks.",Not About Sufficiency
Unplanned urbanization and health risks of Dhaka City in Bangladesh: uncovering the associations between urban environment and public health,"BackgroundDhaka City, the capital of Bangladesh, has experienced rapid and unplanned urbanization over the past few decades. This process has brought significant challenges to public health as the urban environment has become a breeding ground for various health risks. Understanding the associations between unplanned urbanization, the urban environment, and public health in Dhaka City is crucial for developing effective interventions and policies.ObjectivesThis review paper aims to uncover the associations between unplanned urbanization and health risks in Dhaka City, with a specific focus on the urban environment and its impact on public health. The objectives of this study are to examine the health challenges faced by the city's population, explore the specific urban environmental factors contributing to health risks, analyze the socioeconomic determinants of health in unplanned urban areas, evaluate existing policies and governance structures, identify research and data gaps, and provide recommendations for future interventions.MethodsA comprehensive literature review was conducted to gather relevant studies, articles, reports, and policy documents related to unplanned urbanization, the urban environment, and public health in Dhaka City. Various databases and online resources were searched, and the selected literature was critically analyzed to extract key findings and insights.ResultsThe findings reveal that unplanned urbanization in Dhaka City has led to a range of public health risks, including air pollution, inadequate water and sanitation, poor waste management, overcrowding, slums, and substandard housing conditions. These environmental factors are strongly associated with respiratory diseases, waterborne illnesses, and other adverse health outcomes. Socioeconomic determinants such as poverty, income inequality, and limited access to healthcare further exacerbate the health risks faced by the urban population.ConclusionUnplanned urbanization in Dhaka City has significant implications for public health. Addressing the associations between unplanned urbanization, the urban environment, and public health requires comprehensive policies and interventions. Improved urban planning, enhanced infrastructure, and better policy governance are essential for mitigating health risks. Furthermore, addressing socioeconomic disparities and ensuring equitable access to healthcare services are crucial components of effective interventions.",Not About Sufficiency
Changing the Concept of Race: On UNESCO and Cultural Internationalism,"From 1945 and the following 25 years UNESCO - the United Nations Educational, Scientific and Cultural Organization - was, as a hub for cultural internationalism, at the heart of a dispute in international scientific circles over the correct definition of the concept of race. In the wake of World War II and the Holocaust this was the core of UNESCO's many post-war mental engineering initiatives and essentially a dispute about whether the natural sciences or the social sciences should take precedence in determining the origins of human difference, of social division, and of the attribution of value. The article provides an overview of the work on race carried out by UNESCO, examines the measures it took to combat racism and pays attention to their political and social impact. It demonstrates how UNESCO played a major part in imposing a new post-war view of man, but also that the impact differed from country to country and had a focus on problems in the U.S. and South Africa. Not before 1960 did it gradually begin to have a more global approach and impact.",Not About Sufficiency
Machine Learning Techniques in Predicting Bottom Hole Temperature and Remote Sensing for Assessment of Geothermal Potential in the Kingdom of Saudi Arabia,"The global demand for energy is increasing rapidly due to population growth, urbanization, and industrialization, as well as to meet the desire for a higher standard of living. However, environmental concerns, such as air pollution from fossil fuels, are becoming limiting factors for energy sources. Therefore, the appropriate and sustainable solution is to transition towards renewable energy sources to meet global energy demands by using environmentally friendly sources, such as geothermal. The Harrat Rahat volcanic field, located in the western region of the Kingdom of Saudi Arabia (KSA), gets more attention due to its geothermal potential as a viable site for geothermal energy exploration due to its high enthalpy. The prime objective of this study is to present up-to-date and comprehensive information on the utilization of borehole temperature and remote sensing data to identify the most prospective zones with significant geothermal activity favorable for exploration and drilling. A brief description of the selected wells and the methodology used to determine the petrophysical parameters relevant to the geothermal potential assessment are presented. Special emphasis is given to gamma-ray ray and temperature logs for calculating heat production and the geothermal gradient. The effectiveness of various machine learning techniques are assessed throughout this study for predicting the temperature-at-depth to evaluate the suitability of employing machine learning models for temperature prediction, and it is found that XG Boost provided excellent results. It can be observed that some linear anomalies can be traced in the NW, trending on the west side of the Harrat volcanic field based on magnetic data interpretation. The land surface temperature in 2021 exhibited higher temperatures compared to 2000, suggesting potential volcanic activity in the subsurface. It is concluded that the integration of remote sensing data with subsurface data provides the most reliable results.",Not About Sufficiency
Real-time prediction of <SUP>1</SUP>H and <SUP>13</SUP>C chemical shifts with DFT accuracy using a 3D graph neural network,"Nuclear magnetic resonance (NMR) is one of the primary techniques used to elucidate the chemical structure, bonding, stereochemistry, and conformation of organic compounds. The distinct chemical shifts in an NMR spectrum depend upon each atom's local chemical environment and are influenced by both through-bond and through-space interactions with other atoms and functional groups. The in silico prediction of NMR chemical shifts using quantum mechanical (QM) calculations is now commonplace in aiding organic structural assignment since spectra can be computed for several candidate structures and then compared with experimental values to find the best possible match. However, the computational demands of calculating multiple structural- and stereo-isomers, each of which may typically exist as an ensemble of rapidly-interconverting conformations, are expensive. Additionally, the QM predictions themselves may lack sufficient accuracy to identify a correct structure. In this work, we address both of these shortcomings by developing a rapid machine learning (ML) protocol to predict H-1 and C-13 chemical shifts through an efficient graph neural network (GNN) using 3D structures as input. Transfer learning with experimental data is used to improve the final prediction accuracy of a model trained using QM calculations. When tested on the CHESHIRE dataset, the proposed model predicts observed C-13 chemical shifts with comparable accuracy to the best-performing DFT functionals (1.5 ppm) in around 1/6000 of the CPU time. An automated prediction webserver and graphical interface are accessible online at http://nova.chem.colostate.edu/cascade/. We further demonstrate the model in three applications: first, we use the model to decide the correct organic structure from candidates through experimental spectra, including complex stereoisomers; second, we automatically detect and revise incorrect chemical shift assignments in a popular NMR database, the NMRShiftDB; and third, we use NMR chemical shifts as descriptors for determination of the sites of electrophilic aromatic substitution.",Not About Sufficiency
Assessing the sustainability impacts of concentrated solar power deployment in Europe in the context of global value chains,"In the context of the European Green Deal and the Recovery Plan for Europe, CSP can play its role, by providing dispatchable and flexible energy when other renewable technologies cannot. The aim of this paper is to identify the potential socioeconomic, social and environmental impacts associated to the future deployment of CSP projects in Spain, taking into account the global value chain. Based on an extended multiregional input-output model developed by the authors, this paper identifies the country and sector-origin of nine sustainability in-dicators for the two dominant CSP technologies (parabolic trough and central receiver). The research considers the deployment of a 200 MW CSP power plant in Spain to compare the sustainability impacts of these two technologies under three different scenarios regarding the country-origin of the main components. The results show that central receivers have more positive economic impacts, both in terms of value added and employment creation, and lower negative environmental and social impacts than the parabolic trough alternative. The eco-nomic and environmental impacts of the CSP deployed in Spain depend on the origin of components, with the highest negative environmental impacts occurring when the components come from China and the lowest when they come from Germany. The same occurs for the social impacts and supply risks, which are lower when Germany supplies the main components. The scenario in which Spain supplies all the components performs better than the Chinese supply scenario in terms of social risks, whereas no major differences among them were found on supply risks.",Not About Sufficiency
Precision livestock farming applied to grazingland monitoring and management-A review,"To meet the expected demand for food while protecting animal welfare, environmental sustainability, and profitability, animal production efficiency must improve. Improvements in grazinglands management techniques can impact livestock production efficiency. The current stage of artificial intelligence development, mainly machine learning techniques, remote sensing (RS), and precision agriculture technologies, automatizes data collection and raises the monitoring capacity to support on-farm decision-making. This literature review presents current developments in precision livestock farming (PLF) applied to grazinglands monitoring and management, demonstrates some knowledge gaps, and discusses potential solutions of grazinglands management issues. Although the implementation of precision technologies in grazing systems is advancing rapidly, challenges, such as lack of reliable reference data and low variability of datasets used to calibrate models, are examples of constraints to be addressed in future studies. More effort in terms of relationship strengthening between farmers and researchers, benefits elucidation, cooperation among professionals with different expertise, and software or app development must be directed to make the knowledge accessible and largely implemented in field conditions.",Not About Sufficiency
Address as a Citizenshipp Component: the case of postal reordering in the city of Caceres- MT,"In this study, we present an analysis of the postal reordering carried out by the post office in Caceres - MT, and its consequences. The idea of the research was born from the following question: What is the understanding of the residents of the Jardim Uniao neighborhood about the postal reordering developed in the city of Caceres - MT, in the year 2014, as well as its legal aspects and the social impacts on the community? In view of this, as a general objective, we sought to analyze the postal reodering in Caceres - MT, with a view to understanding the legal aspects an social impacts on the city's population, with a socio-spatial cut in the Jardim Uniao neighborhood. The city went through a process of redefinition of the street naming and house numbering, it was an articulated action between the Post Office, the City Hall and the Council, as it is a necessity for the postal reorganization of the city. We identified a considerable portion of residents in the community who do not receive mail at their homes. The results indicated, in our understanding, that the postal reordering will contribute to the implementation of the official address and consequently guarantee the delivery of correspondence in the respective residences.",Not About Sufficiency
An extensible and unifying approach to retrospective clinical data modeling: the BrainTeaser Ontology,"Automatic disease progression prediction models require large amounts of training data, which are seldom available, especially when it comes to rare diseases. A possible solution is to integrate data from different medical centres. Nevertheless, various centres often follow diverse data collection procedures and assign different semantics to collected data. Ontologies, used as schemas for interoperable knowledge bases, represent a state-of-the-art solution to homologate the semantics and foster data integration from various sources. This work presents the BrainTeaser Ontology (BTO), an ontology that models the clinical data associated with two brain-related rare diseases (ALS and MS) in a comprehensive and modular manner. BTO assists in organizing and standardizing the data collected during patient follow-up. It was created by harmonizing schemas currently used by multiple medical centers into a common ontology, following a bottom-up approach. As a result, BTO effectively addresses the practical data collection needs of various real-world situations and promotes data portability and interoperability. BTO captures various clinical occurrences, such as disease onset, symptoms, diagnostic and therapeutic procedures, and relapses, using an event-based approach. Developed in collaboration with medical partners and domain experts, BTO offers a holistic view of ALS and MS for supporting the representation of retrospective and prospective data. Furthermore, BTO adheres to Open Science and FAIR (Findable, Accessible, Interoperable, and Reusable) principles, making it a reliable framework for developing predictive tools to aid in medical decision-making and patient care. Although BTO is designed for ALS and MS, its modular structure makes it easily extendable to other brain-related diseases, showcasing its potential for broader applicability.Database URL https://zenodo.org/records/7886998.",Not About Sufficiency
Prediction of Suicidal Behaviors in the Middle-aged Population: Machine Learning Analyses of UK Biobank,"Background: Suicidal behaviors, including suicide deaths and attempts, are major public health concerns. However, previous suicide models required a huge amount of input features, resulting in limited applicability in clinical practice.Objective: We aimed to construct applicable models (ie, with limited features) for short-and long-term suicidal behavior prediction. We further validated these models among individuals with different genetic risks of suicide.Methods: Based on the prospective cohort of UK Biobank, we included 223 (0.06%) eligible cases of suicide attempts or deaths, according to hospital inpatient or death register data within 1 year from baseline and randomly selected 4460 (1.18%) controls (1:20) without such records. We similarly identified 833 (0.22%) cases of suicidal behaviors 1 to 6 years from baseline and 16,660 (4.42%) corresponding controls. Based on 143 input features, mainly including sociodemographic, environmental, and psychosocial factors; medical history; and polygenic risk scores (PRS) for suicidality, we applied a bagged balanced light gradient-boosting machine (LightGBM) with stratified 10-fold cross-validation and grid-search to construct the full prediction models for suicide attempts or deaths within 1 year or between 1 and 6 years. The Shapley Additive Explanations (SHAP) approach was used to quantify the importance of input features, and the top 20 features with the highest SHAP values were selected to train the applicable models. The external validity of the established models was assessed among 50,310 individuals who participated in UK Biobank repeated assessments both overall and by the level of PRS for suicidality.Results: Individuals with suicidal behaviors were on average 56 years old, with equal sex distribution. The application of these full models in the external validation data set demonstrated good model performance, with the area under the receiver operating characteristic (AUROC) curves of 0.919 and 0.892 within 1 year and between 1 and 6 years, respectively. Importantly, the applicable models with the top 20 most important features showed comparable external-validated performance (AUROC curves of 0.901 and 0.885) as the full models, based on which we found that individuals in the top quintile of predicted risk accounted for 91.7% (n=11) and 80.7% (n=25) of all suicidality cases within 1 year and during 1 to 6 years, respectively. We further obtained comparable prediction accuracy when applying these models to subpopulations with different genetic susceptibilities to suicidality. For example, for the 1-year risk prediction, the AUROC curves were 0.907 and 0.885 for the high (>2nd tertile of PRS) and low (<1st) genetic susceptibilities groups, respectively. Conclusions: We established applicable machine learning-based models for predicting both the short-and long-term risk of suicidality with high accuracy across populations of varying genetic risk for suicide, highlighting a cost-effective method of identifying individuals with a high risk of suicidality.",Not About Sufficiency
Construction of a minute ventilation model to address inter-individual inhaled dose variability within identical exposure scenarios using wearable devices,"Inhaled dose is crucial for accurately assessing exposure to air pollution, determined by pollutant concentration and minute ventilation (VE). However, the VE predictive models and its application to assess the health effects of air pollution are still lacking. In this study, we developed VE predictive models using machine learning techniques, utilizing data obtained from eighty participants who underwent a laboratory cardiopulmonary exercise test (CPET). VE predictive models were developed using generalized additive model (GAM), random forest model (RF) and extreme gradient boosting (XGBoost) and analyzed for explanation of input variables. The Random Forest model, cross-validated, exhibited outstanding performance with an R-2 of 0.986 and a MAE of 1.816 L/min. The median difference between the measured VE and the predicted VE was 0.18 L/min, and the median difference between the black carbon (BC) inhaled dose based on predicted VE and measured VE was 0.02 ng. Employing explainable machine learning, the results showed that metabolic equivalent (METs), heart rate, and body weight are the three top important variables, emphasizing the significance of incorporating METs variables when constructing VE models. Through multiple linear regression models and an adjusted stratified analysis model, the significant adverse association between BC concentration and inhaled dose on diastolic blood pressure (DBP) was only observed in female. The disparity in the effect of BC inhaled dose compared to BC concentration on DBP reached up to 115%. This study is the first to explore the ability of different machine learning algorithms to construct VE prediction models and directly apply the models to assess health effects of an example pollutant. This study contributes to the accurate assessment of air pollution exposure leveraging wearable devices, an approach useful for environmental epidemiology studies.",Not About Sufficiency
An interlaboratory comparison of mid-infrared spectra acquisition: Instruments and procedures matter,"Diffuse reflectance spectroscopy has been extensively employed to deliver timely and cost-effective predictions of a number of soil properties. However, although several soil spectral laboratories have been established worldwide, the distinct characteristics of instruments and operations still hamper further integration and interoperability across mid-infrared (MIR) soil spectral libraries. In this study, we conducted a large-scale ring trial experiment to understand the lab-to-lab variability of multiple MIR instruments. By developing a systematic evaluation of different mathematical treatments with modeling algorithms, including regular preprocessing and spectral standardization, we quantified and evaluated instruments' dissimilarity and how this impacts internal and shared model performance. We found that all instruments delivered good predictions when calibrated internally using the same instruments' characteristics and standard operating procedures by solely relying on regular spectral preprocessing that accounts for light scattering and multiplicative/additive effects, e.g., using standard normal variate (SNV). When performing model transfer from a large public library (the USDA NSSCKSSL MIR library) to secondary instruments, good performance was also achieved by regular preprocessing (e. g., SNV) if both instruments shared the same manufacturer. However, significant differences between the KSSL MIR library and contrasting ring trial instruments responses were evident and confirmed by a semi-unsupervised spectral clustering. For heavily contrasting setups, spectral standardization was necessary before transferring prediction models. Non-linear model types like Cubist and memory-based learning delivered more precise estimates because they seemed to be less sensitive to spectral variations than global partial least square regression. In summary, the results from this study can assist new laboratories in building spectroscopy capacity utilizing existing MIR spectral libraries and support the recent global efforts to make soil spectroscopy universally accessible with centralized or shared operating procedures.",Not About Sufficiency
"Developing the ontological foundations of a terminological system for end-stage diseases, organ failure, dialysis and transplantation","The Etablissement francais des Greffes (EfG) is a state agency dealing with Public Health issues related to organ, tissue and cell transplantation in France. The evaluation of organ retrieval and transplantation activities, one of its missions, is supported by a national information system (EfG-IS). The EfG-IS is moving towards a new n-tier architecture comprising a terminology server for end-stage diseases, organ failure, dialysis and transplantation (EfG-TS). Following a preliminary audit of the existing coding system and in order to facilitate data recording, to improve the quality of information, to assume compatibility with terminological existing standards and to allow semantic interoperability with other local, national or international registries, a specific work has been conducted on the thesauri to integrate within the EfG-TS. In this paper focusing on the server's content rather than the container, we report first the functional and cognitive requirements that resulted from the preliminary audit. We then describe the methodological approach used to build the terminological server on ""sound ontological foundations"". We performed the semantic analysis of existing medical terms to set up disease description frame-like structures. These diseases description frames consist of a limited set of nosological discriminating slots such as etiology, semiology, pathology, evolution and associated diseases. Each relevant medical term is thus associated to a concept defined and inserted within a hierarchy according to disease description frame resulting from the semantic analysis. Last, because this terminological server is shared by various transplant and dialysis centers to record patient data at different time point, contextualization of terms appeared as one of the functional requirements. We will also point out various contexts for medical terms and how they have been taken into account. (C) 2003 Elsevier Ireland Ltd. All rights reserved.",Not About Sufficiency
Identifying key soil characteristics for Francisella tularensis classification with optimized Machine learning models,"Francisella tularensis (Ft) poses a significant threat to both animal and human populations, given its potential as a bioweapon. Current research on the classification of this pathogen and its relationship with soil physical-chemical characteristics often relies on traditional statistical methods. In this study, we leverage advanced machine learning models to enhance the prediction of epidemiological models for soil-based microbes. Our model employs a two-stage feature ranking process to identify crucial soil attributes and hyperparameter optimization for accurate pathogen classification using a unique soil attribute dataset. Optimization involves various classification algorithms, including Support Vector Machines (SVM), Ensemble Models (EM), and Neural Networks (NN), utilizing Bayesian and Random search techniques. Results indicate the significance of soil features such as clay, nitrogen, soluble salts, silt, organic matter, and zinc , while identifying the least significant ones as potassium, calcium, copper, sodium, iron, and phosphorus. Bayesian optimization yields the best results, achieving an accuracy of 86.5% for SVM, 81.8% for EM, and 83.8% for NN. Notably, SVM emerges as the top-performing classifier, with an accuracy of 86.5% for both Bayesian and Random Search optimizations. The insights gained from employing machine learning techniques enhance our understanding of the environmental factors influencing Ft's persistence in soil. This, in turn, reduces the risk of false classifications, contributing to better pandemic control and mitigating socio-economic impacts on communities.",Not About Sufficiency
Sequence-Defined Dendrons Dictate Supramolecular Cogwheel Assembly of Dendronized Perylene Bisimides,"A dendronized perylene bisimide (PBI) that self-organizes into hexagonal arrays of supramolecular double helices with identical single-crystal-like order that disregards chirality was recently reported. A cogwheel model of self-assembly that explains this process was proposed. Accessing the highly ordered cogwheel phase required very slow heating and cooling or extended periods of annealing. Analogous PBIs with linear alkyl chains did not exhibit the cogwheel assembly. Here a library of sequence-defined dendrons containing all possible compositions of linear and racemic alkyl chains was employed to construct self-assembling PBIs. Thermal and structural analysis of their assemblies by differential scanning calorimetry (DSC) and fiber X-ray diffraction (XRD) revealed that the incorporation of n-alkyl chains accelerates the formation of the high order cogwheel phase, rendering the previously invisible phase accessible under standard heating and cooling rates. Small changes to the primary structure, as constitutional isomerism, result in significant changes to macroscopic properties such as melting of the periodic array. This study demonstrated how changes to the sequence-defined primary structure, including the relocation of methyl groups between two constitutional isomers, dictate tertiary and quaternary structure in hierarchical assemblies. This led to the discovery of a sequence that self-organizes the cogwheel assembly much faster than even the corresponding homochiral compounds and demonstrated that defined-sequence, which has long been recognized as a determinant for the complex structure of biomacromolecules including proteins and nucleic acids, plays the same role also in supramolecular synthetic systems.",Not About Sufficiency
Evaluation of Green Deal compliance performance with a hybrid comparative multi-attribute decision model,"The environmental and economic transformation initiated by the European Green Deal is influencing not only EU member states, but also other countries engaged in trade with the EU. This transformation mandates a shift towards sustainable practices in sectors such as agriculture, industry, transportation, energy, and finance. Compliance with the Green Deal is increasingly crucial for countries, particularly those with significant commercial ties to the EU. However, differing levels of development lead to varying degrees of implementation. While studies on the Green Deal compliance of EU member states exist, an evaluation of T & uuml;rkiye-a candidate EU member with substantial trade relations-would address a significant gap in the literature. This study introduces a novel hybrid comparative multi-attribute decision model that combines subjective (Best Worst Method and its derivatives) and objective (Entropy) weighting to determine the relative importance of 14 sub-criteria under three main criteria: ""Reducing"" the impacts of climate change, ""Protecting"" the planet and health, and ""Enabling"" a green and just transition. Using the Ratio Product Model (RPM), T & uuml;rkiye's Green Deal compliance from 2017 to 2021 was evaluated. Sensitivity analysis was conducted to validate the robustness of the findings, and comparisons were made using various multi-attribute decision models, including SAW, TOPSIS, and VIKOR, as well as benchmarking against EU countries. The criteria weights, derived from a combination of subjective and objective methods, highlighted the dominant importance of the ""Reducing"" criterion across all methods. The ""Enabling"" and ""Protecting"" criteria showed slight variations in importance depending on the method. The RPM results indicated that 2021 was T & uuml;rkiye's best-performing year in terms of Green Deal compliance, while 2017 showed the weakest performance. Despite some fluctuations between 2018 and 2019, T & uuml;rkiye demonstrated significant progress overall. The study not only advances the methodology for evaluating Green Deal compliance but also provides actionable insights for policymakers and industry stakeholders, enabling more informed decision-making. By highlighting T & uuml;rkiye's trajectory, the findings offer strategic guidance for non-EU countries aiming to align with Green Deal objectives, fostering sustainable development and strengthening economic ties with the EU.",Not About Sufficiency
"Inventory management performance for family planning, maternal and child health medicines in public health facilities of West Wollega zone, Ethiopia","Background Inventory management is the heart of the supply system in improving availability of medicines, reducing the cost, and improving patient care quality. However, in the government facilities' supply system, inventory management is poor. So, the purpose of this research is to assess inventory management performance for family planning, maternal and child health medicines in public health facilities of West Wollega zone, Oromia region, Ethiopia. Method Facility-based descriptive cross-sectional quantitative study was conducted using checklist, structured and semi-structured questionnaire, and triangulated with qualitative method. Quantitative data were coded and analyzed using SPSS Version 20 and Microsoft excel spreadsheet. Qualitative data were analyzed manually, using thematic analysis technique. Different indicators were used to measure variables. Results Among 23 health facilities assessed, availability of family planning/maternal and child health medicines ranged from 0 to 100%. Average availability of medicines was 14 (61.30%) with mean stock-out duration of 70.71 days. Bin cards were available for 559 (78.40%) of medicines, and 374 (52.45%) bin cards were accurate. Report submission rate was 116 (84.06%), with 47 (40.52%) report and resupply forms reported on time, 73 (62.93%) of them were complete and 69 (59.48%) were accurate. Supplier-related problem, lack of human resource, administrative problem, and lack of computer infrastructure were inventory management challenges identified. Conclusion Inventory management performance for Family planning/maternal and child health medicines was poor as indicated by low availability, high stock-out duration, and poor LMIS performance. Efforts should be undertaken by concerned bodies to improve it.",Not About Sufficiency
Exploring the views of planners and public health practitioners on integrating health evidence into spatial planning in England: a mixed-methods study,"Background This study explored barriers and facilitators to integrating health evidence into spatial planning at local authority levels and examined the awareness and use of the Public Health England 'Spatial Planning for Health' resource. Methods A sequential exploratory mixed-methods design utilized in-depth semi-structured interviews followed by an online survey of public health, planning and other bunt environment professionals in England. Results Views from 19 individuals and 162 survey responses revealed high awareness and use of the Spatial Planning for Health resource, although public health professionals reported greater awareness and use than other professionals. Key barriers to evidence implementation included differences in interpretation and the use of 'evidence' between public health and planning professionals, lack of practical evidence to apply locally and lack of resource and staff capacity in local authorities. Key facilitators included integrating health into the design of local plans, articulating wider benefits to multiple stakeholders and simplifying presenting evidence (regarding language and accessibility). Conclusion The Spatial Planning for Health resource is a useful resource at local authority level. Further work is needed to maximize its use by bunt environment professionals. Public health teams need support, capacity and skills to ensure that local health and well-being priorities are integrated into local planning documents and decisions.",Not About Sufficiency
Large-Scale Outlier Detection for Low-Cost PMx2081;x2080; Sensors,"Evaluating the air quality of classrooms is important as children spend a large amount of time at school. Massey University (NZ) led the development of a low-cost and affordable Indoor Air Quality (IAQ) platform called SKOMOBO that was deployed on a large scale across the classrooms of primary schools in New Zealand. When the data from SKOMBO units were collected, it was important to detect any unexpected high air pollution events. To address this concern, we propose a study of outlier detection for PM10 dataset from SKOMOBO units using MSD-Kmeans. MSD-Kmeans combines the statistical method of Mean and Standard Deviation (MSD) with the machine learning clustering algorithm K-means where the former eliminates as many noisy data to minimize the inference on clustering while the latter is able to achieve better local optimal clustering. We compare the performance of MSD-Kmeans with other similar outlier detection algorithms. Our experimental results illustrate that MSD-Kmeans outperforms the majority of performance indicators (e.g., TPR, FPR, Accuracy, F-measures) compared to other similar methods. We conclude that it is feasible to use MSD-Kmeans as an effective outlier detection tool on large scale datasets.",Not About Sufficiency
Urban Recreational Fisheries in the Australian Coastal Zone: The Sustainability Challenge,"Recreational fishing is an important wildlife harvesting activity in urban coastal areas, and recreational harvest in these areas can frequently exceed the commercial harvest. Recreational fishing is a key way that many members of the public experience the environment. The activity enhances social capital, promotes respect for nature, provides health benefits and can provide economic benefits to coastal communities. It is also an important driver of the science on aquatic animals and habitats, and an important tangible reason for many members of the public to conserve and protect aquatic resources. Overall, there has been little specific consideration of urban recreational fisheries management in Australia, despite the paramount importance of urban areas as a focus of recreational fishing activity. This paper identifies that in order to maximize individual and societal benefits from recreational fishing, there needs to be a refocussing of management with the aim of being more holistic. Historically, fisheries management in Australia has focused on maximum sustainable yield (MSY) or maximum economic yield (MEY) which is relevant for the commercial fishing sector, but neither of these is directly relevant to recreational fisheries. This paper identifies that Urban Fisheries Management Plans are required that recognize the specific issues associated with urban recreational fisheries. These plans need to coordinate within and between levels of government and have clear management objectives relevant to urban recreational fisheries. Enhanced opportunities for meaningful citizen science can be incorporated at multiple levels within these plans and this can engender public support for environmental stewardship, as well as fill a very important gap in the knowledge base necessary for managing the activity. As urban recreational fisheries are often occurring in highly modified or degraded habitats, a central element of these plans needs to be habitat restoration and this can have broader benefits for aquatic health. Other management tools include habitat creation (e.g., artificial reefs), optimization of coastal infrastructure as fisheries habitat, and stock enhancement. Overall, Urban Fisheries Management Plans represent a necessary evolution of fisheries management to better address the specific challenges of urban recreational fisheries management, and to best ensure that benefits are optimised.",Not About Sufficiency
Mental Health Predictive Analysis Using Machine-Learning Techniques,"Mental health problems are being very frequent for the employees at any workplace due to decreasing physical work and social interactions resulting more strain on mind leading to various mental health issues like anxiety, depression, irritability, frustration, and loss of zeal. Delay in detection of mental health issues can lead to severe health problems. In this paper, we implement the classification models like Logistic Regression (LR), Decision Tree (DT), Random Forest (RF) and K-Nearest Neighbor's (KNN). For this study, the dataset is taken from the Kaggle Repository. On comparing the performance of these models using Accuracy, Precision, Area under the Curve (AUC), we find that Decision Tree is the best-suited model with Kaggle dataset yielding the accuracy of 82%.",Not About Sufficiency
Learning factory FleXtory: Interactive loops between real and virtual factory through digital twin,"The digitalization increase in industrial processes is perceived as an opportunity to grow up the competitiveness of companies. Data is more and more accessible, potentially allowing making better decisions at all levels of the company. Job profiles are then changing and requiring new skills, more focused on new technologies and information systems. The most effective way to acquire necessary skills is a ""learning by doing"" way in industrial projects and processes. The learning factory FleXtory was designed and produced in this objective and at the crossroad of the academic and the industrial environment. It allows running combinations of theoretical and applied tools in the context of industry 4.0. This is based on interactive loops between the real and the virtual world passing through the digital twin of the learning factory. The pedagogical modules developed within this learning factory address the evolution of professional competencies and skills in according to the transition to industry 4.0. This transformation supposes the development of the ability for professionals to work within a digital environment. In this paper we propose an architecture model of FleXtory favoring the return on experience/information loop within the digital transformation of the learning factory. The originality of our work is to consider this architecture from the point of view of the pedagogical specifications of the proposed learning models and the future perspective of their use. Copyright (C) 2022 The Authors.",Not About Sufficiency
Bioaerosols associated with animal production operations,"Air emissions from animal housing and manure management operations include a complex mixture of biological. microbial, and inorganic particulates along with odorous volatile compounds. This report highlights the state of current issues, technical knowledge, and remaining challenges to be addressed in evaluating the impacts of airborne microorganisms, dusts, and odorants on animals and workers at animal production facilities and nearby communities. Reports documenting bioaerosol measurements illustrate some of the technical issues related to sample collection, analysis, as well as dispersion and transport to off-farm locations. Approaches to analysis, mitigation and modeling transport are discussed in the context of the risk reduction and management of airborne spread of bioaerosols from animal operations. The need for standardization and validation of bioaerosol collection and analytical techniques for indoor as well as outdoor animal agriculture settings is critical to evaluation of health effects from modern animal production systems that are increasingly situated near communities. Published by Elsevier Ltd",Not About Sufficiency
Predicting breast cancer risk using personal health data and machine learning models,"Among women, breast cancer is a leading cause of death. Breast cancer risk predictions can inform screening and preventative actions. Previous works found that adding inputs to the widely-used Gail model improved its ability to predict breast cancer risk. However, these models used simple statistical architectures and the additional inputs were derived from costly and / or invasive procedures. By contrast, we developed machine learning models that used highly accessible personal health data to predict five-year breast cancer risk. We created machine learning models using only the Gail model inputs and models using both Gail model inputs and additional personal health data relevant to breast cancer risk. For both sets of inputs, six machine learning models were trained and evaluated on the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial data set. The area under the receiver operating characteristic curve metric quantified each models performance. Since this data set has a small percentage of positive breast cancer cases, we also reported sensitivity, specificity, and precision. We used Delong tests (p < 0.05) to compare the testing data set performance of each machine learning model to that of the Breast Cancer Risk Prediction Tool (BCRAT), an implementation of the Gail model. None of the machine learning models with only BCRAT inputs were significantly stronger than the BCRAT. However, the logistic regression, linear discriminant analysis, and neural network models with the broader set of inputs were all significantly stronger than the BCRAT. These results suggest that relative to the BCRAT, additional easy-to-obtain personal health inputs can improve five-year breast cancer risk prediction. Our models could be used as non-invasive and cost-effective risk stratification tools to increase early breast cancer detection and prevention, motivating both immediate actions like screening and long-term preventative measures such as hormone replacement therapy and chemoprevention.",Not About Sufficiency
Learning with Previously Unseen Features,"We study the problem of improving a machine learning model by identifying and using features that are not in the training set. This is applicable to machine learning systems deployed in an open environment. For example, a prediction model built on a set of sensors may be improved when it has access to new and relevant sensors at test time. To effectively use new features, we propose a novel approach that learns a model over both the original and new features, with the goal of making the joint distribution of features and predicted labels similar to that in the training set. Our approach can naturally leverage labels associated with these new features when they are accessible. We present an efficient optimization algorithm for learning the model parameters and empirically evaluate the approach on several regression and classification tasks. Experimental results show that our approach can achieve on average 11.2% improvement over baselines.",Not About Sufficiency
Prognostication of half-cell potential for slabs cathodically protected with AZ91D using explainable and interpretable machine learning,"Purpose - This study aims to investigate the use of 20 commonly applied regression methods to predict concrete corrosion. These models are assessed for accuracy and interpretability using SHapley Additive Explanations (SHAP) and Local Interpretable Model-agnostic Explanations (LIME) analysis to provide structural health monitoring prognostic tools. Design/methodology/approach - This study evaluated model performance using standard measures including root mean square error (RMSE), mean square error (MSE), R-squared (R-2) and mean absolute error (MAE). Interpretability was evaluated using SHAP and LIME. The X and Y distances, concrete age, relative humidity and temperature were input parameters, whereas half-cell potential (HCP) values were considered output. The experimental data set consisted of observations taken for 270 days. Findings - Gaussian process regression (GPR) models with rational quadratic, square exponential and matern 5/2 kernels outperformed others, with RMSE values around 16.35, MSE of roughly 267.50 and R-2 values near 0.964. Bagged and boosted ensemble models performed well, with RMSE around 17.20 and R-2 values over 0.95. Linear approaches, such as efficient linear least squares and linear SVM, resulted in much higher RMSE values (approximately 40.17 and 40.02) and lower R-2 values (approximately 0.79), indicating decreased prediction accuracy. Practical implications - The findings highlight the effectiveness of GPR models in forecasting corrosion in concrete buildings. The use of both SHAP and LIME for model interpretability improves the transparency of predictive maintenance models, making them more reliable for practical applications. Social implications - Safe infrastructure is crucial to public health. Predicting corrosion and other structural problems improves the safety of buildings, bridges and other community-dependent structures. Public safety, infrastructure durability and transportation and utility interruptions are improved by reducing catastrophic breakdowns. Originality/value - This study reduces the gap between model accuracy and interpretability in predicting concrete corrosion by proposing a data-driven method for structural health monitoring. The combination of GPR models and ensemble approaches provides a solid foundation for future research and practical applications in predictive maintenance. This comprehensive approach underscores the potential of data-driven methods for predictive maintenance in concrete structures, with implications for broader applications in various industries.",Not About Sufficiency
Impact of traffic congestion on asthma-related hospital visits in major Texas cities,"Asthma is one of the most prevalent chronic conditions in the United States and is particularly sensitive to environmental changes in urban areas. While it is known that traffic congestion contributes to increased vehicle emissions and poorer air quality, its direct association with asthma incidence has not been thoroughly explored. This study aimed to address this void by analyzing 148 city-level observations from 2016 to 2020 in Texas, using data from the Texas A&M Transportation Institute and Definitive Healthcare. We investigated the association between traffic congestion, measured by the travel time index, and annual city-level asthma hospital discharges, while adjusting for refinery productivity, minority groups, and education levels through multivariate regression. Our findings revealed a significant positive correlation between the travel time index and asthma visits, indicating that higher traffic congestion is associated with increased hospital visits for asthma. This finding remains consistent across different models, regardless of whether control variables are included. For the control variables, we found that higher refinery productivity was linked to elevated risks of asthma-related hospitalizations, aligning with previous research findings. Although correlations with Black or African American and Hispanic or Latino populations, as well as those with less than a high school education, were not statistically significant, a positive trend was observed. These results emphasize the impact of traffic congestion on asthma prevalence and the necessity for targeted public health interventions and urban planning strategies.",Not About Sufficiency
MEASURING STUDENT ENGAGEMENT LEVEL USING FACIAL INFORMATION,"In this paper, we propose a novel framework that measures the engagement level of students either in a class environment or in an e-learning environment. The proposed framework captures the user's video and tracks their faces' through the video's frames. Different features are extracted from the user's face e.g., facial fiducial points, head pose, eye gaze, learned features, etc. These features are then used to detect the Facial Action Coding System (FACS), which decomposes facial expressions in terms of the fundamental actions of individual muscles or groups of muscles (i.e., action units). The decoded action units (AU's) are then used to measures the student's willingness to participate in the learning process (i.e., behavioral engagement) and his/her emotional attitude towards learning (i.e., emotional engagement). This framework will allow the lecturer to receive a real-time feedback from facial features, gaze, and other body kinesics. The framework is robust and can be utilized in numerous applications including but not limited to the monitoring the progress of students with various degrees of learning disabilities, and the analysis of nerve palsy and its effects on facial expression and social interactions.",Not About Sufficiency
Drug Mechanism: A bioinformatic update,"A drug Mechanism of Action (MoA) is a complex biological phenomenon that describes how a bioactive compound produces a pharmacological effect. The complete knowledge of MoA is fundamental to fully understanding the drug activity. Over the years, many experimental methods have been developed and a huge quantity of data has been produced. Nowadays, considering the increasing omics data availability and the improvement of the accessible computational resources, the study of a drug MoA is conducted by integrating experimental and bioinformatics approaches. The development of new in silico solutions for this type of analysis is continuously ongoing; herein, an updating review on such bioinformatic methods is presented. The methodologies cited are based on multi-omics data integration in biochemical networks and Machine Learning (ML). The multiple types of usable input data and the advantages and disadvantages of each method have been analyzed, with a focus on their applications. Three specific research areas (i.e. cancer drug development, antibiotics discovery, and drug repurposing) have been chosen for their importance in the drug discovery fields in which the study of drug MoA, through novel bioinformatics approaches, is particularly productive.",Not About Sufficiency
The long-term welfare effects of colonial institutions: Evidence from Central India,"In Central India, the Narmada River separates two regions that have been ruled by different types of government only during the colonial period, and for reasons independent of their initial economic development. I implement a spatial RDD on village population in the Nineteenth Century as well as in 1901, and I run the same model on proxies of welfare in 2015. My results highlight that divergence has realised where the river overlapped with the colonial border, but not in a neighbouring area where the same Narmada River separates two shores with the same type of former colonial institution. I discuss the following transmission mechanism. The treated group was directly administered by the British with more modern state tools -such as the enforcement of property rights and a transparent taxation system -that made it easier to develop private investment. The most prominent example is the mixed-capital enterprise in charge of the construction of the first trans-continental railway. Infrastructure endowment seems to be crucial for the long-term transmission of the colonial institutional characteristics to the outcomes measured in 2015, which show better average welfare outcomes, as well as higher wealth inequality in the treated group. My work provides an explanation of how the improvement in the quality of the institutions of an embryonic state may sustain the growth of the local markets it deems relevant.",Not About Sufficiency
Semi-supervised Seizure Prediction with Generative Adversarial Networks,"Many outstanding studies have reported promising results in seizure prediction that is considered one of the most challenging predictive data analysis. This is mainly because electroencephalogram (EEG) bio-signal intensity is very small, in 1V range, and there are significant sensing difficulties given physiological and non-physiological artifacts. In this article, we propose an approach that can make use of not only labeled EEG signals but also the unlabeled ones which are more accessible. We also suggest the use of data fusion to further improve the seizure prediction accuracy. Data fusion in our vision includes EEG signals, cardiogram signals, body temperature and time. We use the short-time Fourier transform on 28-s EEG windows as a pre-processing step. A generative adversarial network (GAN) is trained in an unsupervised manner where information of seizure onset is disregarded. The trained Discriminator of the GAN is then used as feature extractor. Features generated by the feature extractor are classified by two fully-connected layers (can be replaced by any classifier) for the labeled EEG signals. This semi-supervised seizure prediction method achieves area under the operating characteristic curve (AUC) of 77.68% and 75.47% for the CHBMIT scalp EEG dataset and the Freiburg Hospital intracranial EEG dataset, respectively. Unsupervised training without the need of labeling is important because not only it can be performed in real-time during EEG signal recording, but also it does not require feature engineering effort for each patient.",Not About Sufficiency
Contrasting Roles of Transcription Factors Spineless and EcR in the Highly Dynamic Chromatin Landscape of Butterfly Wing Metamorphosis,"Development requires highly coordinated changes in chromatin accessibility in order for proper gene regulation to occur. Here, we identify factors associated with major, discrete changes in chromatin accessibility during butterfly wing metamorphosis. By combining mRNA sequencing (mRNA-seq), assay for transposase-accessible chromatin using sequencing (ATAC-seq), and machine learning analysis of motifs, we show that distinct sets of transcription factors are predictive of chromatin opening at different developmental stages. Our data suggest an important role for nuclear hormone receptors early in metamorphosis, whereas PAS-domain transcription factors are strongly associated with later chromatin opening. Chromatin immunoprecipitation sequencing (ChIP-seq) validation of select candidate factors showed spineless binding to be a major predictor of opening chromatin. Surprisingly, binding of ecdysone receptor (EcR), a candidate accessibility factor in Drosophila, was not predictive of opening but instead marked persistent sites. This work characterizes the chromatin dynamics of insect wing metamorphosis, identifies candidate chromatin remodeling factors in insects, and presents a genome assembly of the model butterfly Junonia coenia.",Not About Sufficiency
STRATEGIC ALLIANCE FORMATION MOTIVES IN FORMAL STANDARDIZATION - EMPIRICAL EVIDENCE FROM GERMANY,"In this paper, we explore firms' strategic motives towards standardization in formal standards development organizations. First, we present general motives for strategic alliance formation and relate them to specific motives to join standardization alliances. Then, we identify a parsimonious set of motives by means of a factor analysis. The results show that the motives can be clustered according to the following groups: company interests, technological solutions, knowledge seeking, regulation, and market access. In a second step, we test hypotheses on the relationship between the importance of strategic motives and firm level variables, i.e. R&D intensity, innovation activities, and firm size. The results reveal that innovative firms have a particular strong interest in ensuring industry friendly regulations via standardization. Moreover, the results reveal that especially small firms use standardization alliances to access the knowledge of other stakeholders.",Not About Sufficiency
Bidirectional convolutional LSTM for the prediction of nitrogen dioxide in the city of Madrid,"Nitrogen dioxide is one of the pollutants with the most significant health effects. Advanced information on its concentration in the air can help to monitor and control further consequences more effectively, while also making it easier to apply preventive and mitigating measures. Machine learning technologies with available methods and capabilities, combined with the geospatial dimension, can perform predictive analyses with higher accuracy and, as a result, can serve as a supportive tool for productive management. One of the most advanced machine learning algorithms, Bidirectional convolutional LSTM, is being used in ongoing work to predict the concentration of nitrogen dioxide. The model has been validated to perform more accurate spatiotemporal analysis based on the integration of temporal and geospatial factors. The analysis was carried out according to two scenarios developed on the basis of selected features using data from the city of Madrid for the periods January-June 2019 and January-June 2020. Evaluation of the model's performance was conducted using the Root Mean Square Error and the Mean Absolute Error which emphasises the superiority of the proposed model over the reference models. In addition, the significance of a feature selection technique providing improved accuracy was underlined. In terms of execution time, due to the complexity of the Bidirectional convolutional LSTM architecture, convergence and generalisation of the data took longer, resulting in the superiority of the reference models.",Not About Sufficiency
Limits and possibilities for the development of public health research in the legal system,"OBJECTIVE: To characterize databases of the courts of justice of Brazil as a potential tool for research in Collective Health, in its interface with the legal sciences. METHODS: Cross-sectional study of quantitative and descriptive nature, focusing on analysis of strategic management and judicial systems. RESULTS: Databases used by the Common Justice in the Federation Units to systematize judicial processes were identified and analyzed. A total of 123 databases were found in the courts of justice per state, with emphasis on the South and Northeast regions, in contrast to the North region, which has a smaller number of systems. This large number of judicial systems limits access to legal operators, and hinders the collection of evidence by health researchers and, consequently, impacts the strategic management of the Executive Branch. There were limitations from design to transparent and democratic data extraction by the users themselves, as well as restricted integration between bases. CONCLUSIONS: Although advances have been made in recent years by the courts of justice to unify these databases, the multiplicity of information systems used in the Common State Justice complicates the management of knowledge, limits the development of research, even when carried out by lawyers or researchers in the legal area, as well as generates slow data extraction for public management. It is recognized the need for additional efforts for standardization, as well as for improvement of these databases, expanding access, transparency and integration with a view to a transdisciplinary look between the field of Law and Collective Health.",Not About Sufficiency
Exploring Publicly Accessible Optical Coherence Tomography Datasets: A Comprehensive Overview,"Artificial intelligence has transformed medical diagnostic capabilities, particularly through medical image analysis. AI algorithms perform well in detecting abnormalities with a strong performance, enabling computer-aided diagnosis by analyzing the extensive amounts of patient data. The data serve as a foundation upon which algorithms learn and make predictions. Thus, the importance of data cannot be underestimated, and clinically corresponding datasets are required. Many researchers face a lack of medical data due to limited access, privacy concerns, or the absence of available annotations. One of the most widely used diagnostic tools in ophthalmology is Optical Coherence Tomography (OCT). Addressing the data availability issue is crucial for enhancing AI applications in the field of OCT diagnostics. This review aims to provide a comprehensive analysis of all publicly accessible retinal OCT datasets. Our main objective is to compile a list of OCT datasets and their properties, which can serve as an accessible reference, facilitating data curation for medical image analysis tasks. For this review, we searched through the Zenodo repository, Mendeley Data repository, MEDLINE database, and Google Dataset search engine. We systematically evaluated all the identified datasets and found 23 open-access datasets containing OCT images, which significantly vary in terms of size, scope, and ground-truth labels. Our findings indicate the need for improvement in data-sharing practices and standardized documentation. Enhancing the availability and quality of OCT datasets will support the development of AI algorithms and ultimately improve diagnostic capabilities in ophthalmology. By providing a comprehensive list of accessible OCT datasets, this review aims to facilitate better utilization and development of AI in medical image analysis.",Not About Sufficiency
Intelligent compilation of patent summaries using machine learning and natural language processing techniques,"Patents are a type of intellectual property with ownership and monopolistic rights that are publicly accessible published documents, often with illustrations, registered by governments and international organizations. The registration allows people familiar with the domain to understand how to re-create the new and useful invention but restricts the manufacturing unless the owner licenses or enters into a legal agreement to sell ownership of the patent. Patents reward the costly research and development efforts of inventors while spreading new knowledge and accelerating innovation. This research uses artificial intelligence natural language processing, deep learning techniques and machine learning algorithms to extract the essential knowledge of patent documents within a given domain as a means to evaluate their worth and technical advantage. Manual patent abstraction is a time consuming, labor intensive, and subjective process which becomes cost and outcome ineffective as the size of the patent knowledge domain increases. This research develops an intelligent patent summarization methodology using artificial intelligence machine learning approaches to allow patent domains of extremely large sizes to be effectively and objectively summarized, especially for cases where the cost and time requirements of manual summarization is infeasible. The system learns to automatically summarize patent documents with natural language texts for any given technical domain. The machine learning solution identifies technical key terminologies (words, phrases, and sentences) in the context of the semantic relationships among training patents and corresponding summaries as the core of the summarization system. To ensure the high performance of the proposed methodology, ROUGE metrics are used to evaluate precision, recall, accuracy, and consistency of knowledge generated by the summarization system. The Smart machinery technologies domain, under the sub-domains of control intelligence, sensor intelligence and intelligent decision-making provide the case studies for the patent summarization system training. The cases use 1708 training pairs of patents and summaries while testing uses 30 randomly selected patents. The case implementation and verification have shown the summary reports achieve 90% and 84% average precision and recall ratios respectively.",Not About Sufficiency
"""Vaccines for pregnant women ...?! Absurd"" - Mapping maternal vaccination discourse and stance on social media over six months","Objective: To understand the predominant topics of discussion, stance and associated language used on social media platforms relating to maternal vaccines in 15 countries over a six-month period. Background: In 2019, the World Health Organisation prioritised vaccine hesitancy as a top ten global health threat and recognized the role of viral misinformation on social media as propagating vaccine hesitancy. Maternal vaccination offers the potential to improve maternal and child health, and to reduce the risk of severe morbidity and mortality in pregnancy. Understanding the topics of discussion, stance and language used around maternal vaccines on social media can inform public health bodies on how to combat vaccine misinformation and vaccine hesitancy. Methods: Social media data was extracted (Twitter, forums, blogs and comments) for six months from 15 countries (Australia, Brazil, Canada, France, Germany, India, Italy, Korea, Mexico, Panama, South Africa, Spain, United Kingdom and United States). We used stance, discourse and topic analysis to provide insight into the most frequent and weighted keywords, hashtags and themes of conversation within and across countries. Results: We exported a total of 19,192 social media posts in 16 languages obtained between 1st November 2018 and 30th April 2019. After screening all posts, 16,000 were included in analyses, while excluding retweets, 2,722 were annotated for sentiment. Main topics of discussion were the safety of the maternal influenza and pertussis vaccines. Discouraging posts were most common in Italy (44.9%), and the USA (30.8%). Conclusion: The content and stance of maternal vaccination posts from November 2018 to April 2019 differed across countries, however specific topics of discussion were not limited to geographical location. These discussions included the promotion of vaccination, involvement of pregnant women in vaccine research, and the trust and transparency of institutions. Future research should examine the relationship between stance (promotional, neutral, ambiguous, discouraging) online and maternal vaccination uptake in the respective regions. Crown Copyright (C) 2020 Published by Elsevier Ltd. All rights reserved.",Not About Sufficiency
Strategies for Early Keratoconus Diagnosis: A Narrative Review of Evaluating Affordable and Effective Detection Techniques,"Keratoconus is a progressive corneal disorder that can lead to irreversible visual impairment if not detected early. Despite its high prevalence, early diagnosis is often delayed, especially in low-to-middle-income countries due to limited awareness and restricted access to advanced diagnostic tools such as corneal topography, tomography, optical coherence tomography, and corneal biomechanical assessments. These technologies are essential for identifying early-stage keratoconus, yet their high cost limits accessibility in resource-limited settings. While cost and portability are important for accessibility, the sensitivity and specificity of diagnostic tools must be considered as primary metrics to ensure accurate and effective detection of early keratoconus. This review examines both traditional and advanced diagnostic techniques, including the use of machine learning and artificial intelligence, to enhance early diagnosis. Artificial intelligence-based approaches show significant potential for transforming keratoconus diagnosis by improving the accuracy and sensitivity of early diagnosis, especially when combined with imaging devices. Notable innovations include tools such as SmartKC, a smartphone-based machine-learning application, mobile corneal topography through the null-screen test, and the Smartphone-based Keratograph, providing affordable and portable solutions. Additionally, contrast sensitivity testing demonstrates potential for keratoconus detection, although a precise platform for routine clinical use has yet to be established. The review emphasizes the need for increased awareness among clinicians, particularly in underserved regions, and advocates for the development of accessible, low-cost diagnostic tools. Further research is needed to validate the effectiveness of these emerging technologies in detecting early keratoconus.",Not About Sufficiency
MULTI-FIDELITY GLOBAL-LOCAL OPTIMIZATION OF A TRANSONIC COMPRESSOR ROTOR,"In aerodynamic design, accurate and robust surrogate models are important to accelerate computationally expensive CFD-based optimization. Machine learning techniques can also enable affordable exploration of high-dimensional design spaces with targeted selection of sparse high-fidelity data. In this paper, a multi fidelity global-local approach is presented and applied to the surrogate-based design optimization of a highly-loaded transonic compressor rotor. The key idea is to train multi fidelity surrogates with fewer high-fidelity RANS predictions and more rapid and inexpensive lower-fidelity RANS evaluations. The framework also introduces a global-local search algorithm that can spin-off multiple local optimization threads over narrow and targeted design spaces, concurrently to a constantly adapting global optimization thread. The approach is demonstrated with an optimization of the transonic NASA rotor 37, yielding significant increase in performance within a dozen of optimization iterations.",Not About Sufficiency
The international safety regime of radioactive materials transport,"Every day thousands of shipments of radioactive materials are transported on international and national routes. These consignments, which are carried by road, rail,sea, air and inland waterway, can range from smoke detectors and cobalt sources for medical uses to reprocessed fuel for use in electricity generation. The transport of radioactive materials worldwide is governed by stringent regulatory regime, which includes standards, codes and regulations that have been continuously revised and updated over the past four decades. The safety measures have been developed to protect the general public, transport workers, emergency response teams and the environment against the risks posed by the cargoes. These risks include the radioactivity itself and other chemical risks that the cargoes may pose, such as toxicity or corrosivity. In addition to the safety regulations, the regulatory regime addresses other, related issues such as physical protection and liability. It was recognized that these standards should provide a uniform, global regime to ensure that all parties apply the same provisions. Since 1961, the UN (United Nations)has published and periodically reviewed and updated the regulations for the safe transport of radioactive material. These regulations are used today by more than 60 countries as the basic for their national regulations. In addition, the main international modal organizations responsible for the safe transport of dangerous goods by road,rail, sea,air and inland waterways have incorporated the relevant parts of the UN regulations into their own instruments. This paper will discuss and outline the principal regulations that apply to the transport of radioactive materials such as the UN regulations for the safe transport of radioactive materials, The UN regime governing the international transport of dangerous goods, the principal modal regulations governing the transport of dangerous goods and achievement of a more harmonized regime. and the international organizations responsible for their development and implementation.",Not About Sufficiency
OPIOID CONSUMER HEALTH INFORMATION LITERACIES IN ALABAMA'S PUBLIC LIBRARIES: AN EXPLORATORY WEBSITE CONTENT ANALYSIS,"The acceleration of Opioid deaths over the last decade has made it a serious national public health crisis. Alabama has not been immune to this epidemic, with dramatically increased age-adjusted drug overdose death rates. These increases have occurred in a state with limited resources for Opioid health prevention, treatment, and recovery services. This chapter introduces the term ""o-CHIL"" in order to better understand the multi-factorial layers of intertwining health injustices (in the plural) experienced in Alabama's communities and their embedded public libraries. It highlights the complexities in Opioid consumer health information literacies, the culturally situated dimensions of the Opioid crisis in Alabama, and the uniquely relevant consumer health literacies in its public libraries. Findings are based on an empirical assessment of representative information support services identified in February 2020 on the websites of the 230 public libraries listed as members of the Alabama Public Library Service. The exploratory study applies website content analysis to identify seven examples of information offerings and to class offerings into three categories: (1) information sources (collections, resources); (2) information policy and planning (assigned Opioid-related role, strategic representation); and (3) connections (internal, external, news and events). The discussion potentially provides new directions, approaches, and opportunities to build collaborations of sharing within Alabama's network of public libraries and beyond for them to better serve their local and regional communities impacted by the Opioid crisis.",Not About Sufficiency
Transforming e-participation: VR-dialogue - building and evaluating an AI-supported framework for next-gen VR-enabled e-participation research,"Purpose The purpose of this study is to explore whether immersive virtual reality (VR) can complement e-participation and help alleviate some major obstacles that hinder effective communication and collaboration. Immersive virtual reality (VR) can complement e-participation and help alleviate some major obstacles hindering effective communication and collaboration. VR technologies boost discussion participants' sense of presence and immersion; however, studying emerging VR technologies for their applicability to e-participation is challenging because of the lack of affordable and accessible infrastructures. In this paper, the authors present a novel framework for analyzing serious social VR engagements in the context of e-participation. Design/methodology/approach The authors propose a novel approach for artificial intelligence (AI)-supported, data-driven analysis of group engagements in immersive VR environments as an enabler for next-gen e-participation research. The authors propose a machine-learning-based VR interactions log analytics infrastructure to identify behavioral patterns. This paper includes features engineering to classify VR collaboration scenarios in four simulated e-participation engagements and a quantitative evaluation of the proposed approach performance. Findings The authors link theoretical dimensions of e-participation online interactions with specific user-behavioral patterns in VR engagements. The AI-powered immersive VR analytics infrastructure demonstrated good performance in automatically classifying behavioral scenarios in simulated e-participation engagements and the authors showed novel insights into the importance of specific features to perform this classification. The authors argue that our framework can be extended with more features and can cover additional patterns to enable future e-participation immersive VR research. Research limitations/implications This research emphasizes technical means of supporting future e-participation research with a focus on immersive VR technologies as an enabler. This is the very first use-case for using this AI and data-driven infrastructure for real-time analytics in e-participation, and the authors plan to conduct more comprehensive studies using the same infrastructure. Practical implications The authors' platform is ready to be used by researchers around the world. The authors have already received interest from researchers in the USA (Harvard University) and Israel and run collaborative online sessions. Social implications The authors enable easy cloud access and simultaneous research session hosting 24/7 anywhere in the world at a very limited cost to e-participation researchers. Originality/value To the best of the authors' knowledge, this is the very first attempt at building a dedicated AI-driven VR analytics infrastructure to study online e-participation engagements.",Not About Sufficiency
Operationalising energy sufficiency for low-carbon built environments in urbanising India,"India's urbanisation is considered the largest national urban transformation of the 21st century, with its trajectory having a decisive impact on carbon emissions globally. This study defines and operationalises the concept of energy sufficiency for the growth of Indian cities which is expected to be largely driven by low- and middle-income housing. It combines theoretical framing with quantitative assessment of test models to present an operational framework of energy sufficiency that can be implemented through urban planning regulations. Based on the global budgetary limit to restrict global warming to 1.5 degrees C, a precautionary target of 1.0 tCO2 per capita per year is estimated as an energy sufficiency allowance for upcoming residential built environments. By optimising the average dwelling size, limiting the land-use intensity to low-rise (four storeys) and compact urban forms, and improving operational energy performance with adaptive thermal comfort, the carbon emissions from residential buildings can be sufficiently optimised to 0.84 tCO2 per capita per year. The integration of rooftop renewable energy can further reduce it to 0.56 tCO2. The co-benefits of the optimised costs of construction and operation make the case for economic feasibility and wider affordability.",Not About Sufficiency
Digital twin for healthy indoor environment: A vision for the post-pandemic era,"Indoor environment has significant impacts on human health as people spend 90% of their time indoors. The COVID-19 pandemic and the increased public health awareness have further elevated the urgency for cultivating and maintaining a healthy indoor environment. The advancement in emerging digital twin technologies including building information modeling (BIM), Internet of Things (IoT), data analytics, and smart control have led to new opportunities for building design and operation. Despite the numerous studies on developing methods for creating digital twins and enabling new functionalities and services in smart building management, very few have focused on the health of indoor environment. There is a critical need for understanding and envisaging how digital twin paradigms can be geared towards healthy indoor environment. Therefore, this study reviews the techniques for developing digital twins and discusses how the techniques can be customized to contribute to public health. Specifically, the current applications of BIM, IoT sensing, data analytics, and smart building control technologies for building digital twins are reviewed, and the knowledge gaps and limitations are discussed to guide future research for improving environmental and occupant health. Moreover, this paper elaborates a vision for future research on integrated digital twins for a healthy indoor environment with special considerations of the above four emerging techniques and issues. This review contributes to the body of knowledge by advocating for the consideration of health in digital twin modeling and smart building services and presenting the research roadmap for digital twin-enabled healthy indoor environment.",Not About Sufficiency
GAN-MD: A myocarditis detection using multi-channel convolutional neural networks and generative adversarial network-based data augmentation,"Myocarditis is a significant public health concern because of its potential to cause heart failure and sudden death. The standard invasive diagnostic method, endomyocardial biopsy, is typically reserved for cases with severe complications, limiting its widespread use. Conversely, non-invasive cardiac magnetic resonance (CMR) imaging presents a promising alternative for detecting and monitoring myocarditis, because of its high signal contrast that reveals myocardial involvement. To assist medical professionals via artificial intelligence, the authors introduce generative adversarial networks - multi discriminator (GAN-MD), a deep learning model that uses binary classification to diagnose myocarditis from CMR images. Their approach employs a series of convolutional neural networks (CNNs) that extract and combine feature vectors for accurate diagnosis. The authors suggest a novel technique for improving the classification precision of CNNs. Using generative adversarial networks (GANs) to create synthetic images for data augmentation, the authors address challenges such as mode collapse and unstable training. Incorporating a reconstruction loss into the GAN loss function requires the generator to produce images reflecting the discriminator features, thus enhancing the generated images' quality to more accurately replicate authentic data patterns. Moreover, combining this loss function with other regularisation methods, such as gradient penalty, has proven to further improve the performance of diverse GAN models. A significant challenge in myocarditis diagnosis is the imbalance of classification, where one class dominates over the other. To mitigate this, the authors introduce a focal loss-based training method that effectively trains the model on the minority class samples. The GAN-MD approach, evaluated on the Z-Alizadeh Sani myocarditis dataset, achieves superior results (F-measure 86.2%; geometric mean 91.0%) compared with other deep learning models and traditional machine learning methods.",Not About Sufficiency
Development of the Music Therapy in Transition to Long-Term Care Model,"Transition to long-term care can be a challenging period for older adults, with high risk for negative outcomes, including depression, anxiety, and fear. However, music therapy has the potential to enhance related protective factors because it emphasizes individual strengths by leveraging culture-specific resources, facilitates relationships and a sense of belonging through joint music-making, and provides opportunities to process and make sense of one's experiences in the ""new normal"" through sharing of music-related emotions. The purpose of this study was to explore the perspectives of older adult long-term care residents, their care team staff, and their music therapists to develop a conceptual framework for the role of music therapy in older adults' transition and adjustment to long-term care. A grounded theory approach was used to conceptualize this process. Interviews with 17 participants were transcribed and analyzed using open, axial, and selective coding. The resulting theoretical model describes a progression of qualities and benefits of music therapy that leads to residents ""feeling their best self."" Related categories include: Music therapy is accessible and engaging; Music therapy is personal and meaningful; Music therapy acts as a bridge to other resources; Music therapy facilitates transformation; and Music therapy facilitates community integration. This initial theoretical model provides a foundation for clinical assessment and interventions. Future research is needed for continued testing and refining this theory.",Not About Sufficiency
"Accurate, interpretable predictions of materials properties within transformer language models","Property prediction accuracy has long been a key parameter of machine learning in materials informatics. Accordingly, advanced models showing state-of-the-art performance turn into highly parameterized black boxes missing interpretability. Here, we present an elegant way to make their reasoning transparent. Human-readable text-based descriptions automatically generated within a suite of open-source tools are proposed as materials representation. Transformer language models pretrained on 2 million peer-reviewed articles take as input well-known terms such as chemical composition, crystal symmetry, and site geometry. Our approach outperforms crystal graph networks by classifying four out of five analyzed properties if one considers all available reference data. Moreover, fine-tuned text-based models show high accuracy in the ultra-small data limit. Explanations of their internal machinery are produced using local interpretability techniques and are faithful and consistent with domain expert rationales. This language-centric framework makes accurate property predictions accessible to people without artificial-intelligence expertise.",Not About Sufficiency
Predicting Team Well-Being through Face Video Analysis with AI,"Well-being is one of the pillars of positive psychology, which is known to have positive effects not only on the personal and professional lives of individuals but also on teams and organizations. Understanding and promoting individual well-being is essential for staff health and long-term success, but current tools for assessing subjective well-being rely on time-consuming surveys and questionnaires, which limit the possibility of providing the real-time feedback needed to raise awareness and change individual behavior. This paper proposes a framework for understanding the process of non-verbal communication in teamwork, using video data to identify significant predictors of individual well-being in teamwork. It relies on video acquisition technologies and state-of-the-art artificial intelligence tools to extract individual, relative, and environmental characteristics from panoramic video. Statistical analysis is applied to each time series, leading to the generation of a dataset of 125 features, which are then linked to PERMA (Positive Emotion, Engagement, Relationships, Meaning, and Accomplishments) surveys developed in the context of positive psychology. Each pillar of the PERMA model is evaluated as a regression or classification problem using machine learning algorithms. Our approach was applied to a case study, where 80 students collaborated in 20 teams for a week on a team task in a face-to-face setting. This enabled us to formulate several hypotheses identifying factors influencing individual well-being in teamwork. These promising results point to interesting avenues for research, for instance fusing different media for the analysis of individual well-being in teamwork.",Not About Sufficiency
"Optimizing the environmental design and management of public green spaces: Analyzing urban infrastructure and long-term user experience with a focus on streetlight density in the city of Las Vegas, NV","In Las Vegas and many other desert cities, the unique climatic conditions, marked by high daytime temperatures, naturally encourage residents to seek outdoor recreational activities during the cooler evening hours. However, the approach to streetlight management has been less than optimal, leading to inadequate illumination in public parks after dark. This lack of proper lighting compromises not only the safety but also the enjoyment opportunity of these spaces during the night, a time when they could offer a much-needed respite during summer heat. Recent scholarship has highlighted the deterrence of park usage due to poor design of the street lighting, pointing to a broader issue in urban planning that requires attention to adapt infrastructures to local climates for the benefit of public health and well-being. This study seeks to contribute to the existing scholarship on park lighting by utilizing diverse data sources and creating longitudinal measures to examine how population behaviors in urban parks vary over time in different locations. It seeks to explore the impact of park users' demographics, particularly variations across race and income levels, and the density of street lighting on the nighttime usage of public green spaces by using the time fixed effect method. It aims to understand how demographic diversity among park users and the physical environment, specifically street lighting density, influences patterns of nighttime activities in public parks. Using this analysis, we develop an improved predictive model for determining the density of street lighting in public green spaces by comparing multiple types of machine learning models. This model will consider the demographic diversity of users and the observed patterns of nighttime usage, with the goal of enhancing accessibility, safety, and utilization of these spaces during nighttime hours. The significance of this research contributes to the broader objective of creating resilient, healthy, and inclusive cities that cater to the well-being of their residents.",Not About Sufficiency
Machine learning driven instance segmentation providing new porosity insights into wire arc directed energy deposited Ti-22V-4Al,"Non-destructive x-ray methods such as micro-computed tomography (micro-CT) are useful for investigating porosity defects in additively manufactured products. Understanding the porosity knowledge accessible to microCT technologies relies on quantifying the spatial and morphological characteristics of porosity instances. So far, machine learning-based semantic segmentation techniques and threshold-based image analysis methods have been employed for this purpose. However, the reliability of these methods is compromised by their limitations in segmentation accuracy and delineating connected pores. This work proposed a porosity investigation strategy involving a deep learning-based instance segmentation process on micro-CT slices followed by a 3D reconstruction process to reliably examine the spatial and morphological data of 3D porosities, cased studied by a wire arc directed energy deposited Ti-22 V-4Al alloy. Systematic data analysis captured new insights of pore defects in terms of pore formation tendency, the relationship between pore size and pore sphericity, pore evolution, and the discovery of pore-free zones. The proposed workflows are transferrable for the porosity investigation of other additively manufactured components. The new insights may also guide future advancements in porosity elimination and manipulation techniques to fabricate mechanically robust complex structured components.",Not About Sufficiency
"Planning for a Warmer Future: Heat Risk Assessment and Mitigation in Lahti, Finland","With global climate change causing temperature increases, even cooler regions like Finland are facing increasing heat risks. The city of Lahti is expected to experience a higher-than-average temperature increase, making heat risk mitigation essential. This study aims to assess present and future heat risks in Lahti using exposure and social vulnerability indicators to identify heat risk hotspots and provide strategies for mitigation within the city's urban planning framework. The method utilizes a combination of Land Surface Temperature (LST) data (2014-2024), climate projections, and microclimate analysis to identify heat risk in the city. Geographic Information Systems (GIS) and ENVI-met modeling were employed to assess the relationship between land surface temperatures (LST), urban structure, and green infrastructure. Risk assessments were conducted using social and environmental vulnerability indicators, and future projections were based on a combined SSP2-4.5 scenario. The results show a significant increase in high-risk areas by 2040, rising from 9.79% to 23.65% of Lahti's core urban area. Although the current urban planning framework of the city (Masterplan 2035) is effective in terms of maintaining exposure levels, the continued increase in projected air temperatures, as modeled based on outputs of the EC-Earth3-veg GCM, remains a concern. Microclimate modeling confirmed that urban greenery significantly reduces heat stress and improves thermal comfort. To address future heat risks, Lahti must integrate more green infrastructure into its urban design and identify seasonal heat mitigation methodologies. Additionally, the findings emphasize the need for adaptive planning strategies to mitigate rising temperatures and ensure urban resilience.",Not About Sufficiency
Interlinking FinTech and eHealth: a qualitative study,"Introduction This study investigates the integration of financial technology (FinTech) and electronic health (eHealth) to explore the opportunities, challenges, and implications arising from their interlinkage in Saudi Arabia.Methods Utilizing qualitative semi-structured interviews with 26 participants-including physicians, patients, technical and administrative managers, and FinTech consultants-the research adopts an inductive approach to understand diverse perspectives.Results Key findings reveal significant benefits such as improved efficiency in administrative processes, enhanced access to healthcare services, increased financial inclusion, better decision-making, improved patient experience, and the promotion of innovation and sustainability. However, barriers including regulatory challenges, data privacy and security concerns, interoperability issues, the digital divide, resistance to change, and cost implications were also identified.Conclusion Overall, the integration of FinTech and eHealth holds substantial promise for advancing healthcare delivery in Saudi Arabia. Future implications include the expansion of telehealth services, an increase in startups, the integration of wearable health devices, blockchain-based systems, evolving regulatory frameworks, and heightened collaborations. Addressing the identified challenges is crucial for realizing the full potential of this integration.",Not About Sufficiency
"Can wild urban woodlands be integrated into urban green infrastructure? Insights from urbanites and new urbanites in Chongqing, China","Wild urban woodlands (WUWs) are a novel category of urban forests and have been established as vital sources of diverse ecosystem services for local residents. Despite their potential to mitigate green gentrification, WUWs are often neglected in urban planning. Assessing whether different groups of residents accept WUWs as a part of urban green spaces and determining their preferred WUW type are critical for cities lacking green space supply. Therefore, a collection of photographs of 15 WUW patches was employed in this study to explore the preferences and attitudes of urbanites (n = 200) and new urbanites (i.e., individuals who have undergone a transition from rural to urban residency due to urban expansion; n = 206) toward five prevalent WUW categories and examine their perspectives on the future transformation of these WUWs. Accordingly, the following study results were obtained. (1) New urbanites exhibited greater support for integrating WUWs into urban green infrastructure compared to urbanites, primarily due to the limited supply of green spaces in their residential surroundings. (2) The one-way ANOVA shows significant differences in WUW preference scores between new urbanites and urbanites. Urbanites strongly preferred WUWs situated within stream corridors and the least for WUWs in urban villages. Conversely, new urbanites preferred WUWs on agricultural lands and found WUWs on industrial lands the least appealing. (3) Most participants expressed a desire to witness the planned transformation of WUWs. New urbanites preferred additional spaces for leisure activities, whereas urbanites hoped to witness visual improvements in the WUWs. (4) Multifactor ANOVA shows rural background significantly affects preferences scores. Spearman correlation indicates terrain ruggedness and surrounding construction intensity relate to preference. These findings underscore the substantial potential of WUWs in the urban greening in China. They contribute to urban managers' understanding of the diverse needs of the two urban resident groups regarding WUWs, thereby fostering equity in green space planning.",Not About Sufficiency
Industrial air pollution and self-reported respiratory and irritant health effects on adjacent residents: a case study of Islamabad Industrial Estate (IEI),"Human health deterioration due to industrial air emissions is gaining global attention. These health impacts are coupled mainly with chronic and acute health issues that are persistent in residents in close proximity to the industrial estates. Similarly, the inhabitants of Islamabad Industrial Estate (IEI) are exposed to intense industrial pollutants that have caused respiratory and irritant health issues. This cross-sectional study has compared two groups on deploying 378 close-ended questionnaires at the household level, after adjusting the potential confounders. The groups were framed as per their distance from IEI as 'Band I' - residence <= 650 m and 'Band II' - residence >= 650-1300 m. The distances were calculated from the respondent's residence to the outer digitized boundary (perimeter of IEI), whereas the perimeter was digitized using Google Earth and imported into a Geographical Information System. The results of multiple logistic regression for odds ratios confirmed significant increase in chronic respiratory problems (chronic bronchitis OR 1.93 (1.05-3.55), phlegm OR 2.5 (1.4-4.5), dyspnoea OR 2.18 (0.3-1.81)) and acute irritation issues (eye irritation OR 2.59 (1.63-3.90), throat irritation OR 1.782 (0.876-2.725)) in case of Band I. The same diseases were found in Band II but in less severity; however, their existence remained life-threatening for the locality. The study calls for preventive measures by the local residents, health safety measures by the federal government and relocation of populace to safe areas/sectors.",Not About Sufficiency
A comprehensive review of standards and best practices for utility grid integration with electric vehicle charging stations,"The present work presents a comprehensive state-of-the-art bibliographical review of standards related to utility grid integration and best practices of the electric vehicle (EV) charging stations. The presence of a robust tuning method is essential for successful utility grid integration with the charging stations. The lack of system standardization may hamper the EV uptake as well as successful utility grid integration with the charging stations. The distributed energy resources (DER) and vehicle to grid (V2G) are going to play a vital role in the power system operation and control. The applicability of criterions within the utility grid integration with charging station area is important to the grid operators, charging service providers, manufacturers, fleet operators, and so forth to ensure safety, dependability, and interoperability. Hence, this work tries to deliver a comprehensive and systematic review of standards and best practices for utility grid interaction with charging stations. It will help the specialists of power as well as transport sectors to track down every one of the norms and best practices, which are accessible at one stage to compare different guidelines. This article is categorized under: Energy Infrastructure > Systems and Infrastructure Wind Power > Systems and Infrastructure Fossil Fuels > Economics and Policy Fossil Fuels > Systems and Infrastructure",Not About Sufficiency
COVID-19 vaccination performance of the US states: a hybrid model of DEA and ensemble machine learning methods,"Vaccination is seen as the most promising one among the efforts to stop COVID-19 and the U.S. government has given great importance to vaccination. However, which states have performed well in administering COVID-19 vaccines and which have not is an open significant question. Another important question is what makes a state more successful than others when evaluating vaccination performance. To answer both of these questions, we proposed a hybrid method that consists of Data Envelopment Analysis and Ensemble ML Methods. DEA was employed to find the vaccine efficiency of the states using the data aggregated from counties. ML techniques are then applied for the vaccine efficiency prediction and understanding the significance of the variables in the prediction. Our findings revealed that there are considerable differences between U.S. States' performance and only 16 of the states were efficient in terms of their vaccination performance. Furthermore, Light GBM, Random Forest and XGBoost models provided the best results among the five ensemble machine learning methods that were applied. Therefore, an information fusion-based sensitivity analysis method was used to combine the results of each ML technique and ascertain the relative significance of the factors in the prediction of the efficiency. As the findings for factors affecting vaccination performance, percentage of vaccine doses delivered and COVID-19 deaths were found to be the major influential factors on the prediction of the efficiency and percentage of fully vaccinated people, the number of healthcare employees and human development index followed these variables.",Not About Sufficiency
A data-driven air quality assessment method based on unsupervised machine learning and median statistical analysis: The case of China,"Because the equation for calculating the air quality index (AQI) only considers the most serious pollutants, it remains controversial, and a comprehensive AQI has been a focus of subsequent studies. The present research transformed 22,504,440 pollution characteristics per year into datasets of a Louvain community detection clustering analysis. Unsupervised machine learning was applied to classify 367 cities across China into seven categories. The seven representative cities were selected from seven categories, and the augmented Dickey-Fuller test was used to detect their stationarity. A situation-based composite AQI calculation method was proposed. To verify the method, we clarified the difference between it and the AQI. We also compared its relative error with the total AQI. The relative error values in the seven representative cities were 8.9%, 12.16%, 12.91%, 8.75%, 13.42%, 11.41%, and 11.27%. These values were smaller than the total AQI. The median values of IAQICO, IAQINO2 , IAQISO2 , IAQIPM2.5, and IAQIPM10 remained relatively stable. Only the changes in IAQIO3 were dramatic. High Thour values are generally encountered in more polluted cities. When there is at most one chief pollutant (CP), then Thour = 1. When there are several CPs, Thour > 1. The findings of this research provide the public with an intuitive understanding of air pollution and guidance on effective assessments of outdoor air quality.",Not About Sufficiency
Balancing Environmental and Human Needs: Geographic Information System-Based Analytical Hierarchy Process Land Suitability Planning for Emerging Urban Areas in Bni Bouayach Amid Urban Transformation,"Urbanization in Bni Bouayach, Morocco, threatens vital irrigated areas and agricultural land, raising concerns about environmental sustainability. This study employs a GIS-based Analytical Hierarchy Process (GIS-AHP) framework to assess land suitability for sustainable development. It addresses knowledge gaps in urban planning as follows: (i) Evaluating land suitability for sustainable development: this analysis identifies areas appropriate for urban expansion while minimizing environmental impact. (ii) Balancing environmental and human needs: the framework integrates ten criteria encompassing accessibility, economic, social, geomorphological, and environmental factors. This comprehensive approach results in a Land Suitability Map with five categories: prohibited/unfit, extremely unsuitable, moderately unsuitable, adequately suitable, and highly suitable. Notably, 39.5% of the area falls within the adequately suitable or highly suitable categories, primarily consisting of accessible bare lands and pastures. These findings provide valuable insights for policymakers to guide Bni Bouayach towards sustainable urban development, ensuring balanced growth that respects both environmental preservation and resident needs.",Not About Sufficiency
Towards a decolonial I in AI: mapping the pervasive effects of artificial intelligence on the art ecosystem,"This paper delves into the intricate relationship between Artificial Intelligence (AI) and the art ecosystem, emphasizing the need for a decolonizing approach in the face of AI's growing influence. It argues that the development of AI is not just a technological leap but also a significant cultural and societal moment, akin to the advent of moving images that Walter Benjamin famously analyzed. The paper examines how AI, particularly in its current oligarchical and corporate-driven form, perpetuates and magnifies the existing social inequalities, thereby necessitating a critical and radical rethinking of its role in society and the arts. At the heart of the discussion is the concept of AI as a broad term encompassing various forms of machine intelligence, from natural language processing to computer vision. The paper criticizes the dominant anthropocentric view of intelligence and creativity, proposing a more inclusive approach that considers the diverse forms of intelligence present in other species and potentially in AI itself. It underscores the role of AI in shaping the art ecosystem, not just in the creative process but also in gatekeeping and decision-making. The paper proposes a framework for decolonizing AI in the art ecosystem, focusing on four key tasks: recognizing access as a form of power, understanding and addressing biases inherent in AI, assessing the impact of AI on marginalized communities, and challenging dominant narratives and epistemologies to create space for alternative voices and perspectives. It emphasizes the need for artists and the art community to engage actively with AI, shaping its development towards more equitable and just outcomes. In conclusion, the paper calls for a radical reimagination of AI's role in society and the arts, advocating for a future where AI is not just about technological advancement but also about fostering a more inclusive, equitable, and creatively diverse world. It invites artists, thinkers, and innovators to join in this journey of reimagining and reshaping the future of AI and the art ecosystem.",Not About Sufficiency
AI-PSM: Where are we now?,"Since its public debut in late 2022, ChatGPT has sparked growing interest in AI (Artificial Intelligence) within the Process Safety Management (PSM) community, serving as a tool for data capture and industry-wide knowledge utilization. While machine learning (ML) had previously been explored in process safety, the emergence of large language models with chat-style interfaces has made AI more accessible to non-experts. Over the past year, I have managed the ""AI-PSM"" LinkedIn group, facilitating discussions among PSM practitioners on the AI applications in PSM. These discussions have been analyzed using Prof. Thomas Malone's ""4 Roles of AI"" framework. This paper explores prevailing sentiments among AI-PSM group members using Malone's model, addressing challenges such as mathematical problem-solving, errors, and benchmarking.",Not About Sufficiency
PIPEiD: Pipeline Infrastructure Database,"Numerous water sector management practitioners have stated an urgent need to have a unified platform for the nation's water pipeline infrastructure data and information that is universally accessible and useful. Such a platform, PIPEiD (Pipeline Infrastructure Database), is envisioned to provide access to the data sources, tools, and models that enable the analysis, simulation, visualization, and evaluation of the behavior of pipeline infrastructure. PIPEiD will assist the users in more effective management of these assets for sustainability and resiliency. Virginia Tech, WERF, Washington Suburban Sanitary Commission, Denver Water, and American Water jointly sponsored three workshops to sharpen the PIPEiD (Pipeline Infrastructure Database) vision and mission, both of which are focused on enhancing the practice of drinking water, wastewater, and stormwater pipeline asset management. With invited researchers from academia, utilities, regulators, organizations, and industry, the workshop identified opportunities and knowledge gaps relative to critical areas of sustainable and resilient pipeline infrastructure systems and other pipe associated assets. The goal of the workshop was to develop a prioritization that can guide fundamental and applied research at institutions and entities funding research in water pipeline infrastructure. A key question discussed at the workshop was ""how to develop data standards, model specifications, and decision support tools for advanced pipeline asset management to allow for higher reliability and improve performance, cost-effectiveness, risk management, efficiency, sustainability, security, and resiliency."" This paper presents the workshop outcome focused on the design and development of a national database platform to allow a practitioner to address all three major water pipeline infrastructure management levels: strategic, tactical, and operational, and for water utilities of all sizes.",Not About Sufficiency
Dual-Level Sensor Selection with Adaptive Sensor Recovery to Extend WSNs' Lifetime,"Wireless sensor networks (WSNs) have garnered much attention in the last decades. Nowadays, the network contains sensors that have been expanded into a more extensive network than the internet. Cost is one of the issues of WSNs, and this cost may be in the form of bandwidth, computational cost, deployment cost, or sensors' battery (sensor life). This paper proposes a dual-level sensor selection (DLSS) model used to reduce the number of sensors forming WSNs. The sensor reduction process is performed at two consecutive levels. First, a combination of the Fisher score method and ANOVA test at the filter level weighs all the network sensors and produces only a reduced set of sensors. Additionally, the grey wolf optimizer algorithm produces the optimum sensor subset, while an adaptive sensor recovery solution is proposed to extend the network lifetime even longer using sensors failure management. The proposed model performance is evaluated using four different datasets. In comparison with to other similar methods, the results indicated that the proposed model achieved a more efficient subset of sensors preserving a high accuracy rate.",Not About Sufficiency
Prediction of hepatitis E using machine learning models,"Background Accurate and reliable predictions of infectious disease can be valuable to public health organizations that plan interventions to decrease or prevent disease transmission. A great variety of models have been developed for this task. However, for different data series, the performance of these models varies. Hepatitis E, as an acute liver disease, has been a major public health problem. Which model is more appropriate for predicting the incidence of hepatitis E? In this paper, three different methods are used and the performance of the three methods is compared. Methods Autoregressive integrated moving average(ARIMA), support vector machine(SVM) and long short-term memory(LSTM) recurrent neural network were adopted and compared. ARIMA was implemented by python with the help of statsmodels. SVM was accomplished by matlab with libSVM library. LSTM was designed by ourselves with Keras, a deep learning library. To tackle the problem of overfitting caused by limited training samples, we adopted dropout and regularization strategies in our LSTM model. Experimental data were obtained from the monthly incidence and cases number of hepatitis E from January 2005 to December 2017 in Shandong province, China. We selected data from July 2015 to December 2017 to validate the models, and the rest was taken as training set. Three metrics were applied to compare the performance of models, including root mean square error(RMSE), mean absolute percentage error(MAPE) and mean absolute error(MAE). Results By analyzing data, we tookARIMA(1, 1, 1),ARIMA(3, 1, 2) as monthly incidence prediction model and cases number prediction model, respectively. Cross-validation and grid search were used to optimize parameters of SVM. Penalty coefficientCand kernel function parametergwere set 8, 0.125 for incidence prediction, and 22, 0.01 for cases number prediction. LSTM has 4 nodes. Dropout and L2 regularization parameters were set 0.15, 0.001, respectively. By the metrics of RMSE, we obtained 0.022, 0.0204, 0.01 for incidence prediction, using ARIMA, SVM and LSTM. And we obtained 22.25, 20.0368, 11.75 for cases number prediction, using three models. For MAPE metrics, the results were 23.5%, 21.7%, 15.08%, and 23.6%, 21.44%, 13.6%, for incidence prediction and cases number prediction, respectively. For MAE metrics, the results were 0.018, 0.0167, 0.011 and 18.003, 16.5815, 9.984, for incidence prediction and cases number prediction, respectively. Conclusions Comparing ARIMA, SVM and LSTM, we found that nonlinear models(SVM, LSTM) outperform linear models(ARIMA). LSTM obtained the best performance in all three metrics of RSME, MAPE, MAE. Hence, LSTM is the most suitable for predicting hepatitis E monthly incidence and cases number.",Not About Sufficiency
Integrating machine learning and artificial intelligence in life-course epidemiology: pathways to innovative public health solutions,"The integration of machine learning (ML) and artificial intelligence (AI) techniques in life-course epidemiology offers remarkable opportunities to advance our understanding of the complex interplay between biological, social, and environmental factors that shape health trajectories across the lifespan. This perspective summarizes the current applications, discusses future potential and challenges, and provides recommendations for harnessing ML and AI technologies to develop innovative public health solutions. ML and AI have been increasingly applied in epidemiological studies, demonstrating their ability to handle large, complex datasets, identify intricate patterns and associations, integrate multiple and multimodal data types, improve predictive accuracy, and enhance causal inference methods. In life-course epidemiology, these techniques can help identify sensitive periods and critical windows for intervention, model complex interactions between risk factors, predict individual and population-level disease risk trajectories, and strengthen causal inference in observational studies. By leveraging the five principles of life-course research proposed by Elder and Shanahan-lifespan development, agency, time and place, timing, and linked lives-we discuss a framework for applying ML and AI to uncover novel insights and inform targeted interventions. However, the successful integration of these technologies faces challenges related to data quality, model interpretability, bias, privacy, and equity. To fully realize the potential of ML and AI in life-course epidemiology, fostering interdisciplinary collaborations, developing standardized guidelines, advocating for their integration in public health decision-making, prioritizing fairness, and investing in training and capacity building are essential. By responsibly harnessing the power of ML and AI, we can take significant steps towards creating healthier and more equitable futures across the life course.",Not About Sufficiency
An urban climate assessment and management tool for combined heat and air quality judgements at neighbourhood scales,"Meteorology and air quality are key aspects for city life and urban metabolism. Both aspects build upon urban and natural processes, involve stocks and flows of heat and pollution, with in the end consequences for stocks and flows concerning other urban entities and processes such as human outdoor activities, leisure, transport modes, as well as for urban design and planning. During hot summer days cities experience an urban heat island effect mainly at the start of the evening and at night. As a result inhabitants might be subject to a reduced human thermal comfort. Hot summer days often also coincide with relatively poor air quality conditions. Both short-term effects are known to increase the mortality rates, and it is challenging to distinguish their impacts on health effects. Moreover, climate change scenarios indicate an enhancement of future heat wave frequency and intensity and a further deterioration of human thermal comfort. These issues raise the need and the urgency for city adaptation, but an integrated method for the assessment is still missing. Effective city adaptation is hampered by the complexity of the urban climate (induced by both meteorology and urban morphological characteristics) and its potential health risks, combined with the complex spatial interaction of stakeholders and economical functions. To warn the general public, and to effectively perform urban planning interventions, straightforward environmental indicators are required. indicators have been developed for the individual aspects before, but indices that combine both air quality and urban heat are rather scarce (Rainham and Smoyer-Tomic, 2003). This study develops a novel metric that combines the impact of thermal comfort and air quality by accounting for the relative health risk for both aspects. A straightforward quantification method for this metric has been developed to provide an objective, rational assessment. The application of this new Urban Climate assessment tool is then applied to a case study for a heat wave in the Northwestern Europe, and applied for a sample intervention in the city center of Ghent (Belgium). (C) 2016 The Author(s). Published by Elsevier B.V.",Not About Sufficiency
Understanding Outcomes After Major Surgery,"The global volume of surgery is increasing. Adverse outcomes after surgery have resource implications and long-term impact on quality of life and consequently represent a significant and underappreciated public health issue. Standardization of outcome reporting is essential for evidence synthesis, risk stratification, perioperative care planning, and to inform shared decision-making. The association between short- and long-term outcomes, which persists when corrected for base-line risk, has significant implications for patients and providers and warrants further investigation. Candidate mechanisms include sustained inflammation and reduced physician activity, which may, in the future, be mitigated by targeted interventions.",Not About Sufficiency
Enhancing Typhoid Fever Diagnosis Based on Clinical Data Using a Lightweight Machine Learning Metamodel,"Background: Typhoid fever remains a significant public health challenge, especially in developing countries where diagnostic resources are limited. Accurate and timely diagnosis is crucial for effective treatment and disease containment. Traditional diagnostic methods, while effective, can be time-consuming and resource-intensive. This study aims to develop a lightweight machine learning-based diagnostic tool for the early and efficient detection of typhoid fever using clinical data. Methods: A custom dataset comprising 14 clinical and demographic parameters-including age, gender, headache, muscle pain, nausea, diarrhea, cough, fever range (degrees F), hemoglobin (g/dL), platelet count, urine culture bacteria, calcium (mg/dL), and potassium (mg/dL)-was analyzed. A machine learning metamodel, integrating Support Vector Machine (SVM), Gaussian Naive Bayes (GNB), and Decision Tree classifiers with a Light Gradient Boosting Machine (LGBM), was trained and evaluated using k-fold cross-validation. Performance was assessed using precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC). Results: The proposed metamodel demonstrated superior diagnostic performance, achieving a precision of 99%, recall of 100%, and an AUC of 1.00. It outperformed traditional diagnostic methods and other standalone machine learning algorithms, offering high accuracy and generalizability. Conclusions: The lightweight machine learning metamodel provides a cost-effective, non-invasive, and rapid diagnostic alternative for typhoid fever, particularly suited for resource-limited settings. Its reliance on accessible clinical parameters ensures practical applicability and scalability, potentially improving patient outcomes and aiding in disease control. Future work will focus on broader validation and integration into clinical workflows to further enhance its utility.",Not About Sufficiency
A conceptual meta-level digital twin architecture for energy communities in Romania and other ex-communist countries,"In contrast to the prevalent ecological motivations seen in European Energy Communities (ECs), in Romania, the driving forces behind EC initiatives are somewhat different. Approximately 60% of these initiatives are primarily focused on addressing energy poverty. The remaining 40% are primarily driven by a desire for energy autonomy. This article explores the intricate landscape of EC projects, focusing on their role in aligning with climate change necessities. We delve into the current state of the energy industry, identifying critical needs, gaps, and challenges that hinder their full potential. Furthermore, we propose potential research directions to bridge these gaps, emphasizing the development of a Meta-level digital twin (DT) architecture. It aims to enhance decision-making processes by simulating energy systems and their real-time responses to various scenarios and regulatory changes. Then, we focus on cost-effectiveness of installing PV systems in Romania and estimate the current technical potential for households (12.9 GW) and prosumers' PV installations in 2030 and 2050. To forecast the adoption of PV from 2025 to 2030 and 2050, the proposed model relies on several assumptions, such as annual decreases in CAPEX by 1%, in OPEX by 0.15%, increment in electricity prices by 0.1% per year, degradation rate of 0.1% per year for PV systems. The following projections are obtained for 2030 (3948 MW) and for 2050 (5265 MW), estimating that the growth rate from 2030 to 2050 will be 33%.",Not About Sufficiency
Can ChatGPT answer patient questions regarding reverse shoulder arthroplasty?,"Introduction: In recent years, artificial intelligence (AI) has seen substantial progress in its utilization, with Chat Generated Pre-Trained Transformer (ChatGPT) is emerging as a popular language model. The purpose of this study was to test the accuracy and reliability of ChatGPT's responses to frequently asked questions (FAQ) pertaining to reverse shoulder arthroplasty (RSA). Methods: The ten most common FAQs were queried from institution patient education websites. These ten questions were then input into the chatbot during a single session without additional contextual information. The responses were then critically analyzed by two orthopedic surgeons for clarity, accuracy, and the quality of evidence-based information using The Journal of the American Medical Association (JAMA) Benchmark criteria and the DISCERN score. The readability of the responses was analyzed using the Flesch-Kincaid Grade Level. Results: In response to the ten questions, the average DISCERN score was 44 (range 38-51). Seven responses were classified as fair and three were poor. The JAMA Benchmark criteria score was 0 for all responses. Furthermore, the average Flesch-Kincaid Grade Level was 14.35, which correlates to a college graduate reading level. Conclusion: Overall, ChatGPT was able to provide fair responses to common patient questions. However, the responses were all written at a college graduate reading level and lacked reliable citations. The readability greatly limits its utility. Thus, adequate patient education should be done by orthopedic surgeons. This study underscores the need for patient education resources that are reliable, accessible, and comprehensible. Level of evidence: IV.",Not About Sufficiency
Spatiotemporal Variations of Global Human-Perceived Heatwave Risks and their Driving Factors Based on Machine Learning,"With ongoing global warming, heatwave-related disasters are on the rise, exerting a multifaceted impact on both the natural ecosystem and human society. While temperature has been extensively studied in the effects of extreme heat on human health, humidity has often been ignored. It is crucial to consider the combined influence of temperature and humidity when assessing heatwave risks and safeguarding human well-being. This study, leveraging remote sensing products and reanalysis data, presented the first analysis of the spatiotemporal variations in global human-perceived heatwaves on a seasonal scale from 2000 to 2020, and further employed the Random Forest (RF) regression model to quantitatively assess the explanatory power of seven driving factors. The study found that since the 21st century (1) changes in Heat Index (HI) have varied significantly worldwide, with the majority of regions witnessing an increase, particularly at higher latitudes. The largest HI-increasing area was observed in the second quarter (S2), while the overall HI increase peaked in the third quarter (S3); (2) except for the decreasing area of none-risk regions, the regions under all other risk levels expanded (the proportion of high-risk areas in the world increased from 2.97% to 3.69% in S2, and from 0.04% to 0.35% in the fourth quarter (S4)); (3) aspect demonstrated the greatest explanatory power for the global heatwave distribution (0.69-0.76), followed by land-use coverage (LUCC, 0.48-0.55) and precipitation (0.16-0.43), while the explanatory power of slope and nighttime light (NTL) was rather low; (4) over the years, the explanatory power of each factor for heatwave distribution underwent a minor decrease without significant trend, but exhibited seasonal periodicity. Climatic factors and LUCC were most impactful in the first quarter (S1), while DEM and other human factors dominated in S2; and (5) interaction factors showed no significant trends over the years, but the explanatory power of DEM and slope increased notably when interacting with climate factor, aspect, and LUCC, respectively. The interactions between the aspect and LUCC with precipitation yielded the highest explanatory power (above 0.85) across all interactions. To effectively tackle heatwave risks, it is suggested to concentrate on high-latitude regions, reinforce land use and urban planning with eco-friendly strategies, scrutinize the interplay between precipitation and heatwaves, capitalize on topographic data for devising well-informed prevention measures, and tailor response strategies to accommodate seasonal fluctuations. This study offers valuable insights for enhancing climate change adaptation, disaster prevention, and mitigation strategies, ultimately contributing to the alleviation of extreme heatwaves and risk reduction.",Not About Sufficiency
"Enhancing Data Integration, Interoperability, and Reuse to Address Complex and Emerging Environmental Health Problems","Environmental health sciences (EHS) span many diverse disciplines. Within the EHS community, the National Institute of Environmental Health Sciences Superfund Research Program(SRP) funds multidisciplinary research aimed to address pressing and complex issues on how people are exposed to hazardous substances and their related health consequences with the goal of identifying strategies to reduce exposures and protect human health. While disentangling the interrelationships that contribute to environmental exposures and their effects on human health over the course of life remains difficult, advances in data science and data sharing offer a path forward to explore data across disciplines to reveal new insights. Multidisciplinary SRP-funded teams are well-positioned to examine how to best integrate EHS data across diverse research domains to address multifaceted environmental health problems. As such, SRP supported collaborative research projects designed to foster and enhance the interoperability and reuse of diverse and complex data streams. This perspective synthesizes those experiences as a landscape view of the challenges identified while working to increase the FAIR-ness (Findable, Accessible, Interoperable, and Reusable) of EHS data and opportunities to address them.",Not About Sufficiency
"Innovation and environmental, social, and governance factors influencing sustainable business models-Meta-analysis","The article summarizes the state of knowledge in the field of factors affecting sustainable business models of enterprises, with particular emphasis on non -financial factors, ESG (environmental, social, governance) and innovation. Research results published in over 72 articles were analyzed. The article uses meta-analysis, the least absolute shrinkage and selection operator method and logistic regression in order to analyze the results of the research in an international context (Asia, America, Africa, Europe), and finally focuses on an in-depth analysis of the experiences of European countries. We found that innovations affect sustainable business models in an unambiguously positive way generally for every country. In addition, there is moderately strong evidence that cultivating social capital affects sustainable business models in a positive way. The limitation and challenge of this study was to include environmental, social and governance factors in the analysis, in particular their standardization and categorization. The applied research approach and methodology allowed these difficulties to be overcome.& nbsp; (c) 2021 Elsevier Ltd. All rights reserved.",Not About Sufficiency
Accounting for Impacts: Exploring Sustainability Reporting in Bulgarian Companies under the CSRD,"Sustainability reporting is a key tool for managing uncertainty because it provides transparency, accountability and reliable information on companies' environmental, social and governance (ESG) performance. By disclosing relevant data and insights, sustainability reports enable stakeholders to make informed decisions, understand companies' environmental and social impacts, and assess their long-term sustainability strategies. This paper examines the evolving landscape of corporate sustainability reporting in Bulgaria, taking into account the broader context of corporate impact assessment and impact accounting. The study begins with an introduction to the growing importance of sustainability reporting in today's business environment, emphasising the need for transparency, accountability and compliance. A review of the current literature on the subject reveals a notable trend towards increased disclosure of environmental initiatives and impacts, alongside persistent challenges such as inadequate quality and quantity of reporting, expertise gaps and inconsistencies in guidance. To address these issues, the research uses a mixed-methods approach to explore the nature of sustainability reporting, regulatory frameworks and accountability practices in Bulgarian companies. Through data analysis and interpretation of findings, the study reveals a positive trend towards greater disclosure of sustainability initiatives and impacts, albeit with significant variations in reporting quality and methodology. As Bulgarian companies prepare for the implementation of the Corporate Sustainable Reporting Directive (CSRD), the study highlights the essential need to align with evolving sustainability reporting standards to promote transparency and accountability. By addressing these challenges, companies can better meet stakeholder expectations, contribute to sustainable development goals, and build reputation and trust in the corporate sector. Finally, this paper contributes to the field of sustainability reporting by providing valuable insights into the current state of affairs in Bulgarian companies.",Not About Sufficiency
NASCENT: A Non-Invasive Solution for Detecting Utilization of Servers in Bare-Metal Cloud,"Physical servers are available as-a-service in bare-metal public and private cloud platforms, and their demand has been proliferating because of the high levels of privacy and security guarantees they provide to the tenants. This raises the need for efficient management of bare-metal clouds to keep operational costs low such as by reducing energy consumption. For efficiently managing the cloud infrastructure, bare-metal cloud operators need to monitor the utilization of servers. However, the privacy and security concerns prohibit the installation of third-party monitoring agents on the servers; thus, finding the server-utilization becomes a challenge. In this work, we present NASCENT, a scalable machine-learning (ML) based non-invasive solution for finding the utilization of servers without compromising the privacy and security of bare-metal cloud tenants. Our key idea is to infer utilization from various sensor readings accessible via a server's baseboard management controller (BMC) hardware. We evaluate the proposed solution with three regression based supervised ML algorithms in a Bare-metal-as-a-service (BMaaS) cloud. Our experimental evaluation shows that one of the ML algorithms employed in NASCENT infers the utilization with a root-mean-square error (RMSE) between 2.9 to 9.3 for different workloads. Also, the proposed solution uses minimal memory resources (19 KB) and can even run on BMC hardware which has very limited memory. We also propose a BMaaS cloud architecture that seamlessly integrates automated training and deployment of the ML algorithm in our solution into the life-cycle of bare-metal servers. NASCENT's codebase can be found at https://github.com/iithcandle/dhi-ojas",Not About Sufficiency
Artificial intelligence-enabled non-invasive ubiquitous anemia screening: The HEMO-AI pilot study on pediatric population,"Objective Determine whether data collected from a smartphone camera can be used to detect anemia in a pediatric population.Methods HEMO-AI (Hemoglobin Easy Measurement by Optical Artificial Intelligence), a clinical study carried out from December 2020 to February 2023, recruited patients from the Pediatric Emergency Department, Pediatric Inpatient Department and Pediatric Hematology Unit of the Haemek Medical Center, Afula, Israel. A population-based sample of 823 patients aged 6 months to 18 years who had undergone a venous blood draw for a complete blood count since being admitted to the hospital were enrolled. Patients with total leukonychia, nailbed darkening or discoloration due to medication, nail clubbing, clinically indicated jaundice, subungual hematoma, nailbed lacerations, avulsion injuries, or nail polish applied on fingernails were not eligible for study recruitment. Video and images of the patients' hand placed in a collection chamber were collected using a smartphone camera.Results About 823 samples, 531 from a 12.2 megapixel camera and 256 from a 12.2 megapixel camera, were collected. About 26 samples were excluded by the study coordinator for irregularities. About 97% of fingernails and 68% of skin samples were successfully identified by a post-trained machine learning model. Separate models built to detect anemia using images taken from the Pixel 3 had an average precision of 0.64 and an average recall of 0.4, whereas models built using the Pixel 6 had an average precision of 0.8 and an average recall of 0.84. Further supplementation of training data with synthetic data boosted the precision of the latter to 0.84 and the average recall to 0.87.Conclusions This study lays the groundwork for the future evolution of non-invasive, pain-free, and accessible anemia screening tools tailored specifically for pediatric patients. It identifies important sample collection parameters and design, provides critical algorithms for the pre-processing of fingernail data, and reports an initial capability to detect anemia with 87% sensitivity and 84% specificity.Trial Registration Prospectively registered on www.clinicaltrials.gov (Identifier: NCT04573244) on 15 September 2020, prior to subject recruitment.",Not About Sufficiency
Promoting patient safety and enabling evidence-based practice through informatics,"Objectives: The purposes of this article are to highlight the role of informatics in promoting patient safety and enabling evidence-based practice (EBP), 2 significant aspects for assuring healthcare quality; to delineate some challenges for the future; and to provide key recommendations for education, practice, policy, and research. Methods: First, we describe the components of an informatics infrastructure for patient safety and evidence-based practice. Second, we address the role of informatics in 4 areas: 1) information access; 2) automated surveillance for real-time error detection and prevention; 3) communication among members of the healthcare team; and 4) standardization of practice patterns. Last, we delineate some future challenges for nursing and for informatics and provide key recommendations for education, practice, policy, and research. Results: The components of an informatics infrastructure are available and applications that bring together these components to promote patient safety and enable EBP have demonstrated positive or promising results. Conclusions: Challenges must be addressed so that an informatics infrastructure and related applications that promote patient safety and enable EBP can be realized.",Not About Sufficiency
Parental involvement in residential care and perceptions of their offspring's life satisfaction in residential facilities for adults with intellectual disability,"Background This study examined parental involvement in relocation and post-placement care of offspring in residential facilities for adults with intellectual disability, as well as the characteristics of residents, parents, and residential institutions and the effect of those variables on parental perceptions of their offspring's life satisfaction. Method Seventy-one adults who had moved from their family home to a community-based residence for people with intellectual disability completed the Parental Involvement Questionnaire and the parents' form of the Lifestyle Satisfaction Scale. Results Parents reported a high level of pre-placement involvement: selecting the current residence, visiting the prospective residence, and attending admissions committee meetings. High post-placement involvement was expressed by frequent visits to the residence and participation in social activities. Parents perceived themselves as being more involved in relocation than in post-placement care. Smaller facility size, attendance at admissions committee meetings, and full partnership in residential care were related to higher levels of perceived life satisfaction. Conclusions Professional intervention during and after the process of relocation to a community residence can be an effective way of improving parental perceptions of their offspring's life satisfaction. From the parents' perspective, a smaller residence may ensure a better quality of life for their son or daughter.",Not About Sufficiency
ncRDense: A novel computational approach for classification of non-coding RNA family by deep learning,"With the rapidly growing importance of biological research, non-coding RNAs (ncRNA) attract more attention in biology and bioinformatics. They play vital roles in biological processes such as transcription and translation. Classification of ncRNAs is essential to our understanding of disease mechanisms and treatment design. Many approaches to ncRNA classification have been developed, several of which use machine learning and deep learning. In this paper, we construct a novel deep learning-based architecture, ncRDense, to effectively classify and distinguish ncRNA families. In a comparative study, our model produces comparable results with existing state-of-the-art methods. Finally, we built a freely accessible web server for the ncRDense tool, which is available at http://nsclbio.jbnu.ac.kr/tools/ncRDense/.",Not About Sufficiency
Predicting cardiovascular risk from national administrative databases using a combined survival analysis and deep learning approach,"Background Machine learning-based risk prediction models may outperform traditional statistical models in large datasets with many variables, by identifying both novel predictors and the complex interactions between them. This study compared deep learning extensions of survival analysis models with Cox proportional hazards models for predicting cardiovascular disease (CVD) risk in national health administrative datasets. Methods Using individual person linkage of administrative datasets, we constructed a cohort of all New Zealanders aged 30-74 who interacted with public health services during 2012. After excluding people with prior CVD, we developed sex-specific deep learning and Cox proportional hazards models to estimate the risk of CVD events within 5 years. Models were compared based on the proportion of explained variance, model calibration and discrimination, and hazard ratios for predictor variables. Results First CVD events occurred in 61 927 of 2 164 872 people. Within the reference group, the largest hazard ratios estimated by the deep learning models were for tobacco use in women (2.04, 95% CI: 1.99, 2.10) and chronic obstructive pulmonary disease with acute lower respiratory infection in men (1.56, 95% CI: 1.50, 1.62). Other identified predictors (e.g. hypertension, chest pain, diabetes) aligned with current knowledge about CVD risk factors. Deep learning outperformed Cox proportional hazards models on the basis of proportion of explained variance (R-2: 0.468 vs 0.425 in women and 0.383 vs 0.348 in men), calibration and discrimination (all P <0.0001). Conclusions Deep learning extensions of survival analysis models can be applied to large health administrative datasets to derive interpretable CVD risk prediction equations that are more accurate than traditional Cox proportional hazards models.",Not About Sufficiency
An Efficient Deep Learning Approach for Malaria Parasite Detection in Microscopic Images,"Background: Malaria is a life-threatening disease spread by infected mosquitoes, affecting both humans and animals. Its symptoms range from mild to severe, including fever, muscle discomfort, coma, and kidney failure. Accurate diagnosis is crucial but challenging, relying on expert technicians to examine blood smears under a microscope. Conventional methods are inefficient, while machine learning approaches struggle with complex tasks and require extensive feature engineering. Deep learning, however, excels in complex tasks and automatic feature extraction. Objective: This paper presents EDRI, which is a novel hybrid deep learning model that integrates multiple architectures for malaria detection from red blood cell images. The EDRI model is designed to capture diverse features and leverage multi-scale analysis. Methods: The proposed EDRI model is trained and evaluated on the NIH Malaria dataset comprising 27,558 labeled microscopic red blood cell images. Results: Experiments demonstrate its effectiveness, achieving an accuracy of 97.68% in detecting malaria, making it a valuable tool for clinicians and public health professionals. Conclusions: The results demonstrate the effectiveness of proposed model's ability to detect malaria parasite from red blood cell images, offering a robust tool for rapid and reliable malaria diagnosis.",Not About Sufficiency
A Population Spatialization Model at the Building Scale Using Random Forest,"Population spatialization reveals the distribution and quantity of the population in geographic space with gridded population maps. Fine-scale population spatialization is essential for urbanization and disaster prevention. Previous approaches have used remotely sensed imagery to disaggregate census data, but this approach has limitations. For example, large-scale population censuses cannot be conducted in underdeveloped countries or regions, and remote sensing data lack semantic information indicating the different human activities occurring in a precise geographic location. Geospatial big data and machine learning provide new fine-scale population distribution mapping methods. In this paper, 30 features are extracted using easily accessible multisource geographic data. Then, a building-scale population estimation model is trained by a random forest (RF) regression algorithm. The results show that 91% of the buildings in Lin'an District have absolute error values of less than six compared with the actual population data. In a comparison with a multiple linear (ML) regression model, the mean absolute errors of the RF and ML models are 2.52 and 3.21, respectively, the root mean squared errors are 8.2 and 9.8, and the R-2 values are 0.44 and 0.18. The RF model performs better at building-scale population estimation using easily accessible multisource geographic data. Future work will improve the model accuracy in densely populated areas.",Not About Sufficiency
Inequalities in the social determinants of health of Aboriginal and Torres Strait Islander People: a cross-sectional population-based study in the Australian state of Victoria,"Introduction: Aboriginal Australians are a culturally, linguistically and experientially diverse population, for whom national statistics may mask important geographic differences in their health and the determinants of their health. We sought to identify the determinants of health of Aboriginal adults who lived in the state of Victoria, compared with their non-Aboriginal counterparts. Methods: We obtained data from the 2008 Victorian Population Health Survey: a cross-sectional computer--assisted telephone interview survey of 34,168 randomly selected adults. The data included measures of the social determinants of health (socioeconomic status (SES), psychosocial risk factors, and social capital), lifestyle risk factors, health care service use, and health outcomes. We calculated prevalence ratios (PR) using a generalised linear model with a log link function and binomial distribution; adjusted for age and sex. Results: Aboriginal Victorians had El higher prevalence of self rated fair or poor health, cancer, depression and anxiety, and asthma; most notably depression and anxiety (PR = 1.7, 95% Cl; 1.4-2.2). Determinants that were statistically significantly different between Aboriginal and non-Aboriginal Victorians included: a higher prevalence of psychosocial risk factors (psychological distress, food insecurity and financial stress); lower SES (not being employed and low income); lower social capital (neighbourhood tenure of less than one year, inability to get help from family, didn't feel valued by society, didn't agree most people could be trusted, not a member of a community group); and a higher prevalence of lifestyle risk factors (smoking, obesity and inadequate fruit intake). A higher proportion of Aboriginal Victorians sought help for a mental health related problem and had had a blood pressure check in the previous two years. Conclusions: We identified inequalities in health between Aboriginal and non-Aboriginal Victorians, most notably in the prevalence of depression and anxiety, and the social determinants of health (psychosocial risk factors, SES, and social capital). This has implications for evidence-based policy development and may inform the development of public health interventions.",Not About Sufficiency
Implementing crowdsourcing initiatives in land consolidation procedures in Poland,"Rural areas in Poland are inhabited by 39.8 % of the country's population (GUS (Statistics Poland), 2018a). These areas face various challenges related to the development of efficient and competitive agriculture and forest management, while also strengthening their structures as viable living spaces for rural citizens that enable independent life and varied economic activity. Another important challenge arising in this context is preservation of the cultural landscape and natural environment of these areas. Measures to improve rural development instruments have been pursued in Poland for more than a decade. Rural Development Programme for 2014-2020, setting a framework for the development of rural areas in Poland, highlights the need to activate their residents, and to use endogenic potentials to foster local development. The process of spatial planning within rural areas in Poland requires measures promoting creativity among rural residents and enabling their involvement in joint projects. One of the objectives of current rural development projects is to incorporate citizen perspectives within the process of local development. This research aims to identify potential measures that local governments can implement to develop local identity and the sense of belonging. The emergence of Web 2.0 and the release of public application programming interfaces (APIs) for online mapping tools and sites that enable uploading georeferenced content, along with the introduction of mobile location tagging devices, brought a wide range of new possibilities, challenges, and perspectives for rural development. Crowdsourcing a type of participative online activity is one of them. It allows effective targeting of relevant social groups, benefitting from their knowledge of the area, their opinions and ideas, and then involving them in the implementation of the planning works, thus enabling exchange of reflections and views among authorities, experts, and the public (""crowd""). Crowdsourcing entails a combination of top-down, traditional, hierarchical process, and a bottom-up, open process engaging an online community. The analysis of trends observed in European policies (such as Collective Awareness Platforms for Sustainability and Social Innovation [CAPs] projects) has revealed that the mobilization of public engagement in land use planning is both desirable and important. The article presents possibilities to introduce the crowdsourcing concept into rural development programmes in Poland, particularly in planning works related to the implementation of land consolidation. A dedicated application LC-CApp (Land Consolidation-Crowdsourcing Application) was created in the GIS environment specifically for this purpose.",Not About Sufficiency
Learning fair representations for accuracy parity,"With the application of artificial intelligence in various fields of society, algorithm fairness has become a concern in social decision-making. Researchers have proposed various fair algorithms to achieve statistical parity and equal opportunity of outputs. However, the accuracy parity of algorithms has not been fully discussed. Existing fair algorithms either ignore the accuracy parity or sacrifice the joint accuracy of the model while achieving accuracy parity. A novel fairness algorithm referred to as balanced fair representation for accuracy parity (BFA) is proposed to reduce the bias on the accuracy among sensitive attribute groups while maintaining their joint accuracy. BFA uses representation learning methods to learn similar representations from different sensitive attribute groups. The learned representations are used as training data to learn a fair classification model. The classification error of the model is constrained to be independent of the sensitive attributes. To obtain similar representations, BFA calculates the characteristic scores of different sensitive attribute groups, respectively. BFA takes the difference between scores of different sensitive attribute groups from the same label as the constraint objective of learning the representation. To achieve accuracy parity, BFA calculates the cross entropy loss of neural networks from the learned similar representations and reduces the correlation between the loss and sensitive information. Empirical results show that the proposed representations provide a better trade-off between accuracy parity and joint accuracy than state-of-the-art works.",Not About Sufficiency
MedChem Game: Gamification of Drug Design,"The growing importance of computer methods in drug discovery encourages greater accessibility of cheminformatics tools. The methods that enable automated analysis of molecules, e.g., molecular docking or machine learning models, remain out of reach for nonexpert computer users. To address the shortage of easily accessible tools that can be used to practice drug design, we created MedChem Game, an Android application that uses gamification and artificial intelligence to help users learn about medicinal chemistry and design new small-molecule drugs. Our application includes a simplified molecule drawing tool used to propose new drug candidates that can be docked to one of the four target proteins currently implemented in the game. Additionally, we have implemented a 3D ligand-protein viewer so that players can inspect molecular docking results. All the designed compounds are stored in a database with the sequence of operations the user performs. The key component of our system is a continuously trained deep generative model that utilizes user-developed compounds to improve the quality of generated molecules. Learning directly from the expertise and creativity of players, including medicinal chemists, could help advance computer-aided drug design in the future. Moreover, MedChem Game could be a useful supplement to courses on small-molecule drug design.",Not About Sufficiency
Green gentrification & the luxury effect: uniting isolated ideas towards just cities for people & nature,"The growth of cities creates challenges to biodiversity and social justice. Researchers addressing these challenges often do so in silos; their efforts persistently separate justice for non-humans and humans. We analyze this separation through two concepts-luxury effect and green gentrification-that each explores urban greenspace and justice, but from different angles. The luxury effect implicitly targets justice for non-humans by exploring biodiversity in cities and finds that wealth explains the distribution of biodiversity. Green gentrification explicitly targets justice for humans by examining how new greenspaces affect people and finds that such greenspaces often displace vulnerable people. Using scientometric analyses, we show the disjunction between these concepts. We draw on socio-ecological justice concepts to suggest that disregarding relationships and conflicts among humans and non-humans in concepts does not eliminate them in practice, but stalls progress toward just cities. We urge scholars to simultaneously focus on justice for humans and non-humans.",Not About Sufficiency
Adaptable Reduced-Complexity Approach Based on State Vector Machine for Identification of Criminal Activists on Social Media,"Security agencies face an emerging challenge of identifying and counter the malicious contents spread on the social media by the terrorists. However, text classification techniques are limited by visualization, pre-processing, features extraction, and larger features space. Additionally, change in criminal content require the learning models to identify altered malicious textual contents which poses extra challenge. This study proposes simplified yet adaptable framework that uses a novel features extraction algorithm for extracting features from the textual part of social media contents. The feature extraction considers selective features from only 8 dimensions and follows a six step process. The extracted features are suitably used to train the state vector machine for the classification of the malicious content. The performance of the proposed method is evaluated against other popular feature selection/extraction algorithms like term frequency-inverse document frequency, Gini Index (GI), Chi square statistics, and PCA. Additionally, machine learning classifiers like decision tree, random forest, and Naive Bayes are also used for classification. Results suggest that the proposed approach consumes less energy on text visualization, pre-processing, and dimensionality reduction. It also reduces the time-space complexity of the features extraction process and is capable to steer according to the changing strategies of the active criminal groups. In addition, it can effectively analyze the propaganda material published by the extremists. It automatically identifies the radical text on social media platforms allowing understanding of the behaviors, characteristics and subsequent blockage of such content.",Not About Sufficiency
"European Green Deal, Energy Transition and Greenflation Paradox under Austrian Economics Analysis","Greenflation or inflation for green energy transition in Europe becomes a structural problem of new scarcity and poverty, under Austrian Economics analysis. The current European public agenda on the Green Deal and its fiscal and monetary policies are closer to coercive central planning, against the markets, economic calculus, and Mises' theorem. In this paper, attention is paid to the green financial bubble and the European greenflation paradox: in order to achieve greater future social welfare, due to a looming climate risk, present wellbeing and wealth is being reduced, causing a real and ongoing risk of social impoverishment (to promote the SGD 13 on climate action, it is violated by SGD 1-3 on poverty and hunger and 7-12 on affordable energy, economic growth, sustainable communities, and production). According to the European Union data, the relations are explained between green transition and public policies (emissions, tax, debt, credit boom, etc.), GDP variations (real-nominal), and the increase of inflation and poverty. As many emissions are reduced, there is a decrease of GDP (once deflated) and GDP per capita, evidencing social deflation, which in turn means more widespread poverty and a reduction of the middle-class. Also, there is a risk of a green-bubble, as in the Great Recession of 2008 (but this time supported by the European Union) and possible stagflation (close to the 1970s). To analyze this problem generated by mainstream economics (econometric and normative interventionism), this research offers theoretical and methodological frameworks of mainline economics (positive explanations based on principles and empirical illustrations for complex social phenomena), especially the Austrian Economics and the New-Institutional Schools (Law and Economics, Public Choice, and Comparative Constitutional Economics).",Not About Sufficiency
Risk Factor Analysis and Multiple Predictive Machine Learning Models for Mortality in COVID-19: A Multicenter and Multi-Ethnic Cohort Study,"Background: The COVID-19 pandemic presents a significant challenge to the global health care system. Implementing timely, accurate, and cost-effective screening approaches is crucial in preventing infections and saving lives by guiding disease management. Objectives: The study aimed to use machine learning algorithms to analyze clinical features from routine clinical data to identify risk factors and predict the mortality of COVID-19. Methods: The data used in this research were originally collected for the study titled ""Neurologic Syndromes Predict Higher In-Hospital Mortality in COVID-19."" A total of 4711 patients with confirmed COVID-19 were enrolled consecutively from four hospitals. Three machine learning models, including random forest (RF), partial least squares discriminant analysis (PLS-DA), and support vector machine (SVM), were used to find risk factors and predict COVID-19 mortality. Results: The predictive models were developed based on three machine learning algorithms. The RF model was trained with 20 variables and had a receiver operating characteristic (ROC) value of 0.859 (95% confidence interval [CI] 0.804-0.920). The PLS-DA model was trained with 20 variables and had a ROC value of 0.775 (95% CI 0.694-0.833). The SVM model was trained with 10 variables and had a ROC value of 0.828 (95% CI 0.785-0.865). The nine variables that were present in all three models were age, procalcitonin, ferritin, C-reactive protein, troponin, blood urea nitrogen, mean arterial pressure, aspartate transaminase, and alanine transaminase. Conclusion: This study developed and validated three machine learning prediction models for COVID-19 mortality based on accessible clinical features. The RF model showed the best performance among the three models. The nine variables identified in the models may warrant further investigation as potential prognostic indicators of severe COVID-19. (c) 2023 Elsevier Inc. All rights reserved.",Not About Sufficiency
Formation of the organizational-managerial model of renovation of urban territories,"The pace of modern urban development dictates special requirements for the structure of zoning and the designation of the territories of megacities. Formed requirements for the objects of residential and recreational areas, urban infrastructure and communications facilities. A special role in the issue of improving the comfort of the urban environment, is the process of renovation of production areas. One of the main principles of urban planning is the location of production in the outskirts of cities and settlements. However, with the development of urban areas, once located at the disposal of production, are surrounded by residential and administrative-office blocks. This ""neighborhood"" not only causes discomfort to residents, but also creates an excessive environmental burden on the environment. In addition, the territory of the city, and especially large megacities, have a much higher cadastral value of land, and, therefore, create an additional tax burden on production, as added value to the output. All these elements make the products uncompetitive, especially in comparison with similar products produced outside the metropolitan area. Thus, the process of output of production beyond the city limits is actually cyclical and uninterrupted. Territories that remain after the withdrawal of production facilities are subject to comprehensive analysis, taking into account the social needs of the city, and further renovation.",Not About Sufficiency
Mapping heat-related risks in Swiss cities under different urban tree scenarios,"About three quarter of Swiss residents live in urban areas, and this proportion is expected to grow in future decades. An increasing number of people will therefore be exposed to urban heat, which can have adverse effects on human wellbeing, productivity and physical health. We explore the possibility to detect high-risk areas in five Swiss cities with the development of an urban heatbased risk-mapping approach. The included cities are Basel, Bern, Geneva, Lausanne and Zurich. The analysis is based on a combination of biophysical, including Landsat 8 derived Land Surface Temperature (LST), and socioeconomic data. Additionally, we assess the impact of urban trees on urban heat within the districts of these cities, helping to estimate how risk levels would change under two scenarios: one with increased tree cover (MaxTree) and another with no (NoTree) urban trees. The assessment on the impact of urban trees on heat showed that the areas with urban trees generally experience cooler temperatures compared to those without, both at the city and district levels. This underscores the positive role of urban trees in mitigating the urban heat effect. The risk mapping approach revealed a distinct spatial pattern for each city and high risk areas were identified. Generally, the high-risk areas in the analyzed cities cover the city centers and areas with high vulnerability. The 'NoTree' scenario showed higher risks compared to the baseline situation, illustrating that urban trees currently mitigate heat related risks in Swiss cities. The 'MaxTree' scenario results in lower risks, especially in the cities of Lausanne and Bern. The presented risk mapping approach, including the two idealized scenarios, can be used by policy- and decision-makers (e.g. city planners) can be a tool to determine where urban planning actions are the most urgent and where trees could be most beneficial in terms of adaptation to heat. The approach is easily adaptable and transferable to other cities, since it relies on a clear and simple methodological framework, openly available LST data, and basic socioeconomic variables at district scale that are available for many cities.",Not About Sufficiency
CONCENTRATION OF POVERTY IN THE LANDSCAPES OF BOOMTOWN HAMBURG - THE CREATION OF A NEW URBAN UNDERCLASS,"After a brief account of the state of the art of the German discussion on poverty, this paper criticises the lack of a theory integrating the complexity of poverty as a social configuration.  Moreover, the reduction on economic aspects of poverty in empirical studies is criticised.  Another bias is the lack of attention paid to spatial aspects of poverty:  space as a dimension of social inequalities and therefore part of poverty in cities itself, and-particularly in bigger cities-as places where poverty is concentrated.  A fourth lack is touched on in discussing urban poverty as dependent on processes of urban restructuring (economic restructuring and political regulation).  Hamburg as an example of a very heterogeneous development of social polarisation is used to describe poverty trends both in its magnitude and in its composition (social assistance, unemployment, low income and housing).  To analyse the role of 'Enterprise Hamburg' the concentration of poverty is seen as a model of urban development, namely local policies and urban planning.  Taking social polarisation as given, the conclusion is a plea for the protection of those spatial concentrations of poverty where economic capital is missed but where a broad range of cultural and social capital is concentrated.  These neighbourhoods can serve as an institution of integration and solidarity for urban society.",Not About Sufficiency
Metabarcoding of marine zooplankton in South Africa,"Metabarcoding is an emerging method in which DNA barcoding is combined with next-generation sequencing to determine the biodiversity of taxonomically complex samples. We assessed the current state of DNA barcode reference databases for marine zooplankton in South Africa and undertook a metabarcoding analysis to determine the species composition of samples collected with plankton tow nets. Analysis of DNA sequences mined from the literature and in online barcode reference databases revealed incomplete records for all taxa examined. Barcode records were dominated by meroplanktonic species with commercially important life-history phases (fishes and decapod crustaceans) and by species occurring in easily accessible nearshore habitats. Holoplanktonic species were underrepresented, despite making up the bulk of zooplankton biodiversity, including most potential indicator species. Metabarcoding analysis of plankton samples could identify 45% of amplicon sequence variants to species level based on BOLD databases (123 species) and similar numbers using GenBank and the MIDORI COI classifier. Morphological analysis of samples could not achieve comparable resolution at species level, but with some exceptions it recovered similar classes of organisms to those found by metabarcoding. The need for integrative molecular/morphological studies to increase and validate barcode reference databases of key zooplankton taxa is recognised. Metabarcoding of marine zooplankton in South Africa has now been successfully undertaken and the methodology is expected to facilitate high-resolution monitoring of zooplankton biodiversity in pelagic ecosystems and accelerate the discovery of new species.",Not About Sufficiency
Differences in ischemic heart disease between males and females using predictive artificial intelligence models,"Background: Cardiovascular health and preventative strategies are influenced by the sex of the individuals. To forecast cardiac events or detect ischemic heart disease (IHD) early, machine-learning algorithms can analyze complex patient data patterns. Early detection allows for lifestyle changes, medication management, or invasive treatments to slow disease progression and improve outcomes. Aim: To compare and predict the differences in the primary sources of IHD burden between males and females in various age groups, geographical regions, death versus alive, and comorbidity levels. Methods: A predictive and retrospective design was implemented in this study. Electronic health records were extracted, which were equally distributed among males and females with IHD. The dataset consisted of patients who were admitted between 2015 and 2022. Two of the eight models generated by Modeler software were implemented in this study: the Bayesian network model, which achieved the highest area under curve score (0.600), and the Chi-squared automatic interaction detection (CHAID) model, which achieved the highest overall accuracy score (57.199%). Results: The study sample included 17,878 men and women, 58% of whom had no comorbidities and 1.7% who died. Age, the Charlson comorbidity index score, and geographical location all predicted IHD, but age was more influential. Bayesian network analysis showed that IHD odds were highest in males 40-59 and females 60-79, with the highest mortality risk in females 80-100. North and south Jordan had higher IHD rates and middle-aged males from north and middle governorates had higher IHD rates according to CHAID. Conclusion: By using artificial intelligence, clinicians can improve patient outcomes, treatment quality, and save lives in the fight against cardiovascular illnesses. To predict IHD early, machine-learning algorithms can analyze complex patient data patterns to improve outcomes.",Not About Sufficiency
"Sustainable Urban Regeneration of Blighted Neighborhoods: The Case of Al Ghanim Neighborhood, Doha, Qatar","At a time when urban expansion and regeneration are being prioritized, many cities are undergoing significant widespread urban decay. Planning issues, such as the regeneration of historical areas and the redevelopment of blighted neighborhoods, have prompted a complex agenda to be put in place for urban planning practice. The most significant district is the city's core. It is a crucial indicator of the city's success because it contains the Central Business District (CBD) and housing. However, due to poor governmental attention, many city cores are experiencing new development, subsequently overwhelming the old neighborhoods. Consequently, old areas are witnessing urban disfigurement and fast deterioration in their physical and visual characteristics. This research utilizes urban regeneration to address the city's core challenges to help achieve sustainable development. To test the sustainability framework, the researchers used Qatar's local case study of the Old Ghanim neighborhood, one of Doha's oldest districts. As a result of the original population's relocation to suburban developments, the district has deteriorated, lacking street activity and increasing indigent public space. The researchers examined the literature on urban regeneration, conducted site visits and observations, reviewed and described case study limitations, and identified factors that contribute to the creation of a sustainable neighborhood, based on urban qualities such as integrated networks and walkable streets, open and green public spaces, and the regeneration of the old urban fabric. As a result, they intended to address a theoretical and practical gap in current local knowledge, and they additionally intended to provide a helpful tool for urban regeneration specialists. The researchers proposed a conceptual framework for rejuvenating neglected neighborhoods and ideas for ensuring urban cohesion, which is critical for improving the lives of individuals in these areas.",Not About Sufficiency
Impact of Misclassification Rates on Compression Efficiency of Red Blood Cell Images of Malaria Infection Using Deep Learning,"Malaria is a severe public health problem worldwide, with some developing countries being most affected. Reliable remote diagnosis of malaria infection will benefit from efficient compression of high-resolution microscopic images. This paper addresses a lossless compression of malaria-infected red blood cell images using deep learning. Specifically, we investigate a practical approach where images are first classified before being compressed using stacked autoencoders. We provide probabilistic analysis on the impact of misclassification rates on compression performance in terms of the information-theoretic measure of entropy. We then use malaria infection image datasets to evaluate the relations between misclassification rates and actually obtainable compressed bit rates using Golomb-Rice codes. Simulation results show that the joint pattern classification/compression method provides more efficient compression than several mainstream lossless compression techniques, such as JPEG2000, JPEG-LS, CALIC, and WebP, by exploiting common features extracted by deep learning on large datasets. This study provides new insight into the interplay between classification accuracy and compression bitrates. The proposed compression method can find useful telemedicine applications where efficient storage and rapid transfer of large image datasets is desirable.",Not About Sufficiency
Predicting ophthalmic clinic non-attendance using machine learning: Development and validation of models using nationwide data,"BackgroundOphthalmic clinic non-attendance in New Zealand is associated with poorer health outcomes, marked inequities and costs NZD$30 million per annum. Initiatives to improve attendance typically involve expensive and ineffective brute-force strategies. The aim was to develop machine learning models to accurately predict ophthalmic clinic non-attendance.MethodsThis multicentre, retrospective observational study developed and validated predictive models of clinic non-attendance. Attendance data for 3.1 million appointments from all New Zealand government-funded ophthalmology clinics from 2009 to 2018 were aggregated for analysis. Repeated ten-fold cross validation was used to train and optimise XGBoost and logistic regression models on several demographic and clinic-related variables. Models developed using the entire training set were compared with those restricted to regional subsets of the data.ResultsIn the testing data set from 2019, there were 407 574 appointments (median [range] age, 66 [0-105] years; 210 365 [51.6%] female) with a non-attendance rate of 5.7% (n = 23 309 missed appointments), XGBoost models trained on each region's data achieved the highest mean AUROC of 0.764 (SD 0.058) and mean AUPRC of 0.157 (SD 0.072). XGBoost performed better than logistic regression (mean AUROC = 0.756, p = 0.002). Training individual XGBoost models for each region led to better performance than training a single model on the complete nationwide dataset (mean AUROC = 0.754, p = 0.04).ConclusionMachine learning algorithms can predict ophthalmic clinic non-attendance with relatively basic demographic and clinic data. These findings suggest further research examining implementation of such algorithms in scheduling systems or public health interventions may be useful.",Not About Sufficiency
Development of Fuel/Engine Systems-The Way Forward to Sustainable Transport,"The global demand for transport energy is large, growing, and primarily met by petroleum-derived liquid fuels powering internal combustion engines (ICEs). Moreover, the demand for jet fuel and diesel is projected to grow faster than the demand for gasoline in the future, and is likely to result in low-octane gasoline components becoming more readily available. Significant initiatives with varying motivations are taking place to develop the battery electric vehicle (BEV) and the fuel cell as alternatives to ICE vehicles, and to establish fuels such as biofuels and natural gas as alternatives to conventional liquid fuels. However, each of these alternatives starts from a very low base and faces significant barriers to fast and unrestrained growth; thus, transport- and particularly commercial transport-will continue to be largely powered by ICEs running on petroleum-based liquid fuels for decades to come. Hence, the sustainability of transport in terms of affordability, energy security, and impact on greenhouse gas (GHG) emissions and air quality can only be ensured by improving ICEs. Indeed, ICEs will continue to improve while using current market fuels, through improvements in combustion, control, and after-treatment systems, assisted by partial electrification in the form of hybridization. However, there is even more scope for improvement through the development of fuel/engine systems that can additionally leverage benefits in fuels manufacture and use components that may be readily available. Gasoline compression ignition (GCI), which uses low-octane gasoline in a compression ignition engine, is one such example. GCI would enable diesel-like efficiencies while making it easier to control nitrogen oxides (NOx) and particulates at a lower cost compared with modern diesel engines. Octane on demand (OOD) also helps to ensure optimum use of available fuel anti-knock quality, and thus improves the overall efficiency of the system. (C) 2019 THE AUTHOR. Published by Elsevier LTD on behalf of Chinese Academy of Engineering and Higher Education Press Limited Company. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",Not About Sufficiency
"Prediction of a Multi-Gene Assay (Oncotype DX and Mammaprint) Recurrence Risk Group Using Machine Learning in Estrogen Receptor-Positive, HER2-Negative Breast Cancer-The BRAIN Study","Simple Summary: Multi-gene assays (MGAs), such as Oncotype DX and Mammaprint, are used to provide predictive and prognostic values in treatment of ER+HER2- breast cancer. However, their accessibility is restricted due to their high cost in some countries. For this reason, many studies have been conducted to develop the tests that can replace the multi-gene assays, but practicality is still insufficient. The aim of our study is to develop a highly accessible machine learning-based model for predicting the result of MGA. Our accurate and affordable machine learning-based predictive model may serve as a cost-effective alternative to the expensive multi-gene assays. This study aimed to develop a machine learning-based prediction model for predicting multi-gene assay (MGA) risk categories. Patients with estrogen receptor-positive (ER+)/HER2- breast cancer who had undergone Oncotype DX (ODX) or MammaPrint (MMP) were used to develop the prediction model. The development cohort consisted of a total of 2565 patients including 2039 patients tested with ODX and 526 patients tested with MMP. The MMP risk prediction model utilized a single XGBoost model, and the ODX risk prediction model utilized combined LightGBM, CatBoost, and XGBoost models through soft voting. Additionally, the ensemble (MMP + ODX) model combining MMP and ODX utilized CatBoost and XGBoost through soft voting. Ten random samples, corresponding to 10% of the modeling dataset, were extracted, and cross-validation was performed to evaluate the accuracy on each validation set. The accuracy of our predictive models was 84.8% for MMP, 87.9% for ODX, and 86.8% for the ensemble model. In the ensemble cohort, the sensitivity, specificity, and precision for predicting the low-risk category were 0.91, 0.66, and 0.92, respectively. The prediction accuracy exceeded 90% in several subgroups, with the highest prediction accuracy of 95.7% in the subgroup that met Ki-67 <20 and HG 1 similar to 2 and premenopausal status. Our machine learning-based predictive model has the potential to complement existing MGAs in ER+/HER2- breast cancer.",Not About Sufficiency
Improved Prediction Model of Protein Lysine Crotonylation Sites Using Bidirectional Recurrent Neural Networks,"Histone lysine crotonylation (Kcr) is a post-translational modification of histone proteins that is involved in the regulation of gene transcription, acute and chronic kidney injury, spermatogenesis, depression, cancer, and so forth. The identification of Kcr sites in proteins is important for characterizing and regulating primary biological mechanisms. The use of computational approaches such as machine learning and deep learning algorithms have emerged in recent years as the traditional wet-lab experiments are time-consuming and costly. We propose as part of this study a deep learning model based on a recurrent neural network (RNN) termed as Sohoko-Kcr for the prediction of Kcr sites. Through the embedded encoding of the peptide sequences, we investigate the efficiency of RNN-based models such as long short-term memory (LSTM), bidirectional LSTM (BiLSTM), and bidirectional gated recurrent unit (BiGRU) networks using cross-validation and independent tests. We also established the comparison between Sohoko-Kcr and other published tools to verify the efficiency of our model based on 3-fold, 5-fold, and 10-fold cross-validations using independent set tests. The results then show that the BiGRU model has consistently displayed outstanding performance and computational efficiency. Based on the proposed model, a webserver called Sohoko-Kcr was deployed for free use and is accessible at https://sohoko-research-9uu23.ondigitalocean.app.",Not About Sufficiency
HW/SW Development of Cloud-RAN in 3D Networks: Computational and Energy Resources for Splitting Options,"The continuous increase in demanding for availability and ultra-reliability of low-latency and broadband wireless connections is instigating further research in the standardization of next-generation mobile systems. 6G networks, among other benefits, should offer global ubiquitous mobility thanks to the utilization of the Space segment as an intelligent yet autonomous ecosystem. In this framework, multi-layered networks will take charge of providing connectivity by implementing Cloud-Radio Access Network (C-RAN) functionalities on heterogeneous nodes distributed over aerial and orbital segments. Unmanned Aerial Vehicles (UAVs), High-Altitude Platforms (HAPs), and small satellites compose the Space ecosystem encompassing the 3D networks. Recently, a lot of interest has been raised about splitting operations to distribute baseband processing functionalities among such nodes to balance the computational load and reduce the power consumption. This work focuses on the hardware development of C-RAN physical (PHY) layer operations to derive their computational and energy demand. More in detail, the 5G Downlink Shared Channel (DLSCH) and the Physical Downlink Shared Channel (PDSCH) are first simulated in MATLAB environment to evaluate the variation of computational load depending on the selected splitting options and number of antennas available at transmitter (TX) and receiver (RX) side. Then, the PHY-layer processing chain is software-implemented and the various splitting options are tested on low-cost processors, such as Raspberry Pi (RP) 3B+ and 4B. By overclocking the RPs, we compute the execution time and we derive the instruction count (IC) per program for each considered splitting option so to achieve the mega instructions per second (MIPS) for the expected processing time. Finally, by comparing the performance achieved by the employed RPs with that of Nvidia Jetson Nano (JN) processor used as benchmark, we shall discuss about size, weight, power and cost (SWaP-C) metrics related to the UAV payload design.",Not About Sufficiency
Evaluation of an AI-Based Voice Biomarker Tool to Detect Signals Consistent With Moderate to Severe Depression,"PURPOSE Mental health screening is recommended by the US Preventive Services Task Force for all patients in areas where treatment options are available. Still, it is estimated that only 4% of primary care patients are screened for depression. The goal of this study was to evaluate the efficacy of machine learning technology (Kintsugi Voice, v1, Kintsugi Mindful Wellness, Inc) to detect and analyze voice biomarkers consistent with moderate to severe depression, potentially allowing for greater compliance with this critical primary care public health need. METHODS We performed a cross-sectional study from February 1, 2021 to July 31, 2022 to examine >= 25 seconds of free-form speech content from English-speaking samples captured from 14,898 unique adults in the United States and Canada. Participants were recruited via social media, provided informed consent, and their voice biomarker results were compared with a self-reported Patient Health Questionnaire-9 (PHQ-9) at a cut-off score of 10 (moderate to severe depression). RESULTS From as few as 25 seconds of free-form speech, machine learning technology was able to detect vocal characteristics consistent with an increased PHQ-9 >= 10, with a sensitivity of 71.3 (95% CI, 69.0-73.5) and a specificity of 73.5 (95% CI, 71.5-75.5). CONCLUSIONS Machine learning has potential utility in helping clinicians screen patients for moderate to severe depression. Further research is needed to measure the effectiveness of machine learning vocal detection and analysis technology in clinical deployment.",Not About Sufficiency
Capacity Planning for Social Infrastructure of Smart Lifetime Neighbourhoods: Social value Approach,"Municipalities are responsible for organizing and financing long-term-care services. These expenditures will triple in the next 40 years (to 9.5% of the GDP of Norway). This study seeks to investigate the exposure to different risks presented by the built environments of current neighbourhoods and the valuation of the benefits attained through the development of lifetime neighbourhoods. Ambient assisted living technologies embedded in lifetime neighbourhoods can significantly decrease the risk of falls and other accidents. Digital social networks and other support to the community can facilitate inclusion and mitigate loneliness. The spatial planning, development, and management of lifetime neighbourhoods with embedded ambient intelligence as a risk mitigation strategy in fast ageing cities are of specific interest. We wish to evaluate how the development of lifetime neighbourhoods mitigates the risk of accidents and social exclusion, thereby creating value for the community The creation of value by the mitigation of the risk of falls, diseases, and social exclusion will be measured with the actuarial present value generated using the multiple decrement model approach, which is a novelty. The actuarial present value will provide scientific evidence of the benefits of the development and management of various lifetime neighbourhoods and housing arrangements. Copyright (C) 2022 The Authors.",Not About Sufficiency
"Can OpenEHR, ISO 13606, and HL7 FHIR Work Together? An Agnostic Approach for the Selection and Application of Electronic Health Record Standards to the Next-Generation Health Data Spaces","In order to maximize the value of electronic health records (EHRs) for both health care and secondary use, it is necessary for the data to be interoperable and reusable without loss of the original meaning and context, in accordance with the findable, accessible, interoperable, and reusable (FAIR) principles. To achieve this, it is essential for health data platforms to incorporate standards that facilitate addressing needs such as formal modeling of clinical knowledge (health domain concepts) as well as the harmonized persistence, query, and exchange of data across different information systems and organizations. However, the selection of these specifications has not been consistent across the different health data initiatives, often applying standards to address needs for which they were not originally designed. This issue is essential in the current scenario of implementing the European Health Data Space, which advocates harmonization, interoperability, and reuse of data without regulating the specific standards to be applied for this purpose. Therefore, this viewpoint aims to establish a coherent, agnostic, and homogeneous framework for the use of the most impactful EHR standards in the new-generation health data spaces: OpenEHR, International Organization for Standardization (ISO) 13606, and Health Level 7 (HL7) Fast Healthcare Interoperability Resources (FHIR). Thus, a panel of EHR standards experts has discussed several critical points to reach a consensus that will serve decision-making teams in health data platform projects who may not be experts in these EHR standards. It was concluded that these specifications possess different capabilities related to modeling, flexibility, and implementation resources. Because of this, in the design of future data platforms, these standards must be applied based on the specific needs they were designed for, being likewise fully compatible with their combined functional and technical implementation.",Not About Sufficiency
Promoting active mobility in old age through urban design,"In an ageing society, maintaining independent mobility into old age is an important objective. Mental and physical wellbeing depends not only on individual health status, but also to a large extent on the spatial conditions. Local politics and municipal administrations can influence this, especially in urban planning. This discussion article brings together perspectives from public health and urban planning on urban development and mobility against the background of health equity. The results of the AFOOT (Securing urban mobility of an ageing population) cross-sectional study on socio-spatial conditions in small- and medium-sized towns in northwestern Germany and walking and cycling by older people show the importance of residential environmental factors such as proximity to everyday destinations, walking and cycling infrastructure, and street connectivity. Preferences for the design of an age-friendly living environment and the quality of public spaces exist in terms of urban design quality, quality of stay, and safety in public spaces. In order to improve spatial conditions, the situation needs to be recorded using defined indicators and monitoring, and the perspectives of older people need to be integrated. Strategies and measures to promote active mobility in old age are aimed at the multifunctional design of public spaces, the prioritization of active mobility on everyday trips, and ensuring the accessibility of everyday destinations through urban development. Cross-sectoral cooperation between urban planning, transport planning, and public health is essential to promote the active mobility and health of older people.",Not About Sufficiency
"Study on sustainable urbanization literature based on Web of Science, scopus, and China national knowledge infrastructure: A scientometric analysis in CiteSpace","As the world enters a new round of large-scale urbanization, the coordination of a balance between social, economic, and environmental systems to promote sustainable urban development has become a global focus. Scholars have explored sustainable urbanization, collected relevant literature, and analyzed the influence of research in different countries, finding that China's influence is second only to that of the US. Accordingly, this study uses the literature in China National Knowledge Infrastructure (CNKI), the world's largest Chinese database. It selects 3640 studies on sustainable urbanization from Web of Science and Scopus (W&S), and CNKI. Using the bibliometric method in Citespace, it systematically analyzes the research status, development course, and potential trends of research on sustainable urbanization. The results show that: (1) The number of studies on sustainable urbanization in CNKI is decreasing by year, while it has increased gradually in W&S. China currently has the most literature in the field of sustainable urbanization. The US, China, and Germany have the most influential studies. (2) W&S and CNKI each have their separate and complementary emphases. (3) The two datasets are more similar in terms of the nature of their research institutions, but cooperation between the institutions is closer in W&S than in CNKI. There are no aggregation or scaling effects in the Chinese research institutions. The Chinese Academy of Sciences and Hong Kong Polytechnic University are leaders in this field. (4) The top five topics in the studies from W&S are urban development models and public administration, urban planning issues, protection of urban ecosystems, urban land-use changes, and pollution and resource consumption during urbanization. The top five topics in the CNKI studies are strategy of sustainable development, urban development, urbanization and population urbanization, land financing and urban economics, and urban development planning. The construction of urban ecosystems with reasonable structure, efficient processes, and complete functions is an important breakthrough in the application of sustainable urbanization. Reasonably controlling the pace of urban expansion, efficiently coordinating urban and rural development, formulating evidence-based urban development strategies, and ensuring the living standard of urban settlements are the key problems in sustainable urbanization, and will become important directions for related future research. (C) 2020 Elsevier Ltd. All rights reserved.",Not About Sufficiency
Miniature Mobile Robot Detection Using an Ultralow-Resolution Time-of-Flight Sensor,"Miniature mobile robots in multirobotic systems require reliable environmental perception for successful navigation, especially when operating in a real-world environment. One of the sensors that have recently become accessible in microrobotics due to their size and cost-effectiveness is a multizone time-of-flight (ToF) sensor. In this research, object classification using a convolutional neural network (CNN) based on an ultralow-resolution ToF sensor is implemented on a miniature mobile robot to distinguish the robot from other objects. The main contribution of this work is an accurate classification system implemented on low-resolution, low-processing power, and low-power consumption hardware. The developed system consists of a VL53L5CX ToF sensor with an 8 x 8 depth image and a low-power RP2040 microcontroller. The classification system is based on a customized CNN architecture to determine the presence of a miniature mobile robot within the observed terrain, primarily characterized by sand and rocks. The developed system trained on a custom dataset can detect a mobile robot with an accuracy of 91.8% when deployed on a microcontroller. The model implementation requires 7 kB of RAM, has an inference time of 34 ms, and an energy consumption during inference of 3.685 mJ.",Not About Sufficiency
Machine learning-assisted dual-atom sites design with interpretable descriptors unifying electrocatalytic reactions,"Low-cost, efficient catalyst high-throughput screening is crucial for future renewable energy technology. Interpretable machine learning is a powerful method for accelerating catalyst design by extracting physical meaning but faces huge challenges. This paper describes an interpretable descriptor model to unify activity and selectivity prediction for multiple electrocatalytic reactions (i.e., O2/CO2/N2 reduction and O2 evolution reactions), utilizing only easily accessible intrinsic properties. This descriptor, named ARSC, successfully decouples the atomic property (A), reactant (R), synergistic (S), and coordination effects (C) on the d-band shape of dual-atom sites, which is built upon our developed physically meaningful feature engineering and feature selection/sparsification (PFESS) method. Driven by this descriptor, we can rapidly locate optimal catalysts for various products instead of over 50,000 density functional theory calculations. The model's universality has been validated by abundant reported works and subsequent experiments, where Co-Co/Ir-Qv3 are identified as optimal bifunctional oxygen reduction and evolution electrocatalysts. This work opens the avenue for intelligent catalyst design in high-dimensional systems linked with physical insights.",Not About Sufficiency
An Intelligent System for Diabetes Prediction,"With the emerging increase of diabetes, that recently affects around 346 million people, of which more than one-third go undetected in early stage, a strong need for supporting the medical decision-making process is generated. A number of researches have focused either in using one of the algorithms or in the comparisons of the performances of algorithms on a given, usually predefined and static datasets that are accessible through the Internet. This paper focuses on the joint implementation of the support vector machine (SVM) and Naive Bayes statistical modeling, in the dataset acquired from the medical examinations of 402 patients, in order to improve the computer-supported diagnosis reliability. The dataset contains some attributes that have not been previously used in computer-based evaluations. The results show that the joint implementation of two algorithms improves significantly the overall reliability of the system outcome, which is crucial in the computer-supported diabetes diagnostic process.",Not About Sufficiency
Software agents for ambient intelligence based manufacturing,"There are several references to projects linked to the AmI concept in technical publications. These are mostly centered on applications linked to everyday situations concerning individuals: at home, in the street, in the car, in public places and relating to leisure. The AMLAB research group at TEKNIKER is analysing the technological, human and social needs and implications for AmI in the manufacturing field. For this aim we have set up a laboratory to spread the model of AmI to the area of manufacturing. This paper presents that Lab, the agent based architecture we have implemented as well as the application of machine learning techniques to identify the activities a worker is doing.",Not About Sufficiency
Prediction of carbon dioxide production from green waste composting and identification of critical factors using machine learning algorithms,"Controlling carbon dioxide produced from green waste composting is a vital issue in response to carbon neutralization. However, there are few computational methods for accurately predicting carbon dioxide pro-duction from green waste composting. Based on the data collected, this study developed novel machine learning methods to predict carbon dioxide production from green waste composting and made a comparison among six methods. After eliminating the extreme outliers from the dataset, the Random Forest algorithm achieved the highest prediction accuracy of 88% in the classification task and showed the top performance in the regression task (root mean square error = 23.3). As the most critical factor, total organic carbon, with the Gini index ac-counting for about 59%, can provide guidance for reducing carbon emissions from green waste composting. These results show that there is great potential for using machine learning algorithms to predict carbon dioxide output from green waste composting.",Not About Sufficiency
Thermal Comfort Maps to estimate the impact of urban greening on the outdoor human comfort,"Outdoor human comfort is an important factor in the evaluation of the liveability of a city as well as for promoting people's health and well-being. In hot arid climates in particular, urban planning and design can considerably impact the day-to-day thermal comfort of the pedestrians, for better or for worse. Strategies to reduce thermal discomfort include shading structures, water bodies, and the promotion of natural ventilation - and most significantly, green areas. Trees have a major impact on the pedestrians in the built environment as they not only provide shading but also improve the microclimate in urban areas, thereby reducing the time during which discomfort is felt. The objective of this paper is to present a new methodology for dynamically quantifying the impact of different plants in urban areas on outdoor human comfort, through 3D urban energy modelling. The proposed methodology makes use of an urban energy modelling tool, providing a comprehensive view of the city energy fluxes, with a focus on the impact of trees on the human thermal comfort. Outdoor human comfort is assessed using the Index of Thermal Stress for the campus of the Swiss International Scientific School of Dubai (UAE), where ""Thermal Comfort Maps"" are designed to quantify the pedestrian thermal sensation and its variation in time and space. Additionally, the energy fluxes impinging on the urban pedestrians are quantified, dynamically, in hourly time step, providing an important instrument to understand their thermal stress, and the environmental factors affecting it. Based on the simulations, thanks to native greening, a significant improvement in outdoor comfort conditions was achieved in the campus, reducing the ""warm/hot"" and ""very hot"" thermal sensations from 1,291 h (on average over the entire campus) to less than 300 h by planting Ghaf and Acacia trees.",Not About Sufficiency
Local warning integrated with global feature based on dynamic spectra for FAIMS data analysis in detection of clinical wound infection,"Infections have long been a thorny problem that severely threatened public health and resulted in tremendous economic losses worldwide. Current detection methods for wound infection do not fully meet the requirements of preventing and treating this disease. Therefore, people are looking for better alternatives, wherein FAIMS (Field Asymmetric Ion Mobility Spectrometry) technology, by virtue of its high sensitivity, rapid response and noninvasive operation, is a promising candidate. This paper aims to investigate the possibility of FAIMS technology in detecting wound infections quickly and accurately. For this purpose, we gathered an odor dataset of clinical wound samples with the employment of a FAIMS instrument, the Lonestar (Owlstone, UK) analyzer. To enhance delectability, we proposed a novel algorithm framework, i.e., Local Warning integrated with Global Feature (LWGF), which is verified on distinguishment between twenty patients with single or mixed infection of Escherichia coil (E. coli) and six wounded patients without infection. Experimental results showed that the LWGF successfully identified the patients with the best average AUC of 0.98, and the best recognition rate of 96.15%, which are much higher than the conventional methods.",Not About Sufficiency
Digital Twins in Manufacturing: A Three-layer Heat-map Analysis,"This paper explores the landscape of Digital Twins (DTs) in smart and sustainable manufacturing through a systematic literature review. In particular, the paper aims to identify the widespread trends and intersections of Digital Twin (DT) applications, maturity levels, and sustainability dimensions, by means of a three-layer heat-map chart. Observations indicate a concentration of DT applications in the production side of manufacturing, with notable impacts on economic and environmental sustainability. However, major deficiencies in addressing social sustainability persist in overall applications. Further insights suggest that DT research requires ongoing advancements in the establishment of granular maturity levels, in response to the need for a clearer categorization and definition of DTs. Copyright (c) 2024 The Authors. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)",Not About Sufficiency
Data Mining and Electronic Devices applied to Quality of Life Related to Health Data,"The development of new technologies, information systems, decision support systems and clinical parameters prediction algorithms using machine learning and data mining opens a new perspective in many area of health. In this context, relevance presents the concept of Quality of Life (QOL) in health and the possibility of developing Support Systems Clinical Decision (SADC) that use it. Through individual expectation of physical well-being, psychological, mental, emotional and spiritual patients, discussed variables and measures the quality of research area of life, we intend to make a study of data to establish correlations with laboratory, pharmaceutical data, socio-economic, among others, obtaining knowledge in terms of behavioral patterns of chronic patients, achieving a number of reliable data and easily accessible, capable of enhancing the decision-making process by the specialized medical teams, seeking to improve treatments and consequently the related quality of life with the Health chronically ill. This paper studied and compared related studies that develop systems for decision support and prediction in the clinical area, with emphasis on studies in the area of quality of life.",Not About Sufficiency
Arabic sentiment analysis about online learning to mitigate covid-19,"The Covid-19 pandemic is forcing organizations to innovate and change their strategies for a new reality. This study collects online learning related tweets in Arabic language to perform a comprehensive emotion mining and sentiment analysis (SA) during the pandemic. The present study exploits Natural Language Processing (NLP) and Machine Learning (ML) algorithms to extract subjective information, determine polarity and detect the feeling. We begin with pulling out the tweets using Twitter APIs and then preparing for intensive preprocessing. Second, the National Research Council Canada (NRC) Word-Emotion Lexicon was examined to calculate the presence of the eight emotions at their emotional weight. Third, Information Gain (IG) is used as a filtering technique. Fourth, the latent reasons behind the negative sentiments were recognized and analyzed. Finally, different classification algorithms including Naive Bayes (NB), Multinomial Naive Bayes (MNB), K Nearest Neighbor (KNN), Logistic Regression (LR), and Support Vector Machine (SVM) were examined. The experiments reveal that the proposed model performs well in analyzing the perception of people about coronavirus with a maximum accuracy of about 89.6% using SVM classifier. From a practical perspective, the method could be generalized to other topical domains, such as public health monitoring and crisis management. It would help public health officials identify the progression and peaks of concerns for a disease in space and time, which enables the implementation of appropriate preventive actions to mitigate these diseases.",Not About Sufficiency
Introduction to Machine Learning with Robots and Playful Learning,"Inspired by explanations of machine learning concepts in children's books, we developed an approach to introduce supervised, unsupervised, and reinforcement learning using a block-based programming language in combination with the benefits of educational robotics. Instead of using blocks as high-end APIs to access AI cloud services or to reproduce the machine learning algorithms, we use them as a means to put the student ""in the algorithm's shoes."" We adapt the training of neural networks, Q-learning, and k-means algorithms to a design and format suitable for children and equip the students with hands-on tools for playful experimentation. The children learn about direct supervision by modifying the weights in the neural networks and immediately observing the effects on the simulated robot. Following the ideas of constructionism, they experience how the algorithms and underlying machine learning concepts work in practice. We conducted and evaluated this approach with students in primary, middle, and high school. All the age groups perceived the topics to be very easy to moderately hard to grasp. Younger students experienced direct supervision as challenging, whereas they found Q-learning and k-means algorithms much more accessible. Most high-school students could cope with all the topics without particular difficulties.",Not About Sufficiency
Examining the Concept of Liveability in Urban Neighbourhoods in Iskandar Malaysia,"Liveability is a critical concept in urban planning and geography. It is well-used in planning policy with different geographical contexts; however, there are limitations in understanding this concept in the Malaysian context. This research examines the relationship of liveability between people and places in their daily lives in urban neighbourhoods through residents' perceptions. Quantitative data collected in 5 urban neighbourhoods in Iskandar Malaysia suggests that liveability must correspond to residents' requirements for good quality facilities and services, good neighbourhood conditions and positive community engagement.",Not About Sufficiency
DelibAnalysis: Understanding the quality of online political discourse with machine learning,"This article proposes an automated methodology for the analysis of online political discourse. Drawing from the discourse quality index (DQI) by Steenbergen et al., it applies a machine learning-based quantitative approach to measuring the discourse quality of political discussions online. The DelibAnalysis framework aims to provide an accessible, replicable methodology for the measurement of discourse quality that is both platform and language agnostic. The framework uses a simplified version of the DQI to train a classifier, which can then be used to predict the discourse quality of any non-coded comment in a given political discussion online. The objective of this research is to provide a systematic framework for the automated discourse quality analysis of large datasets and, in applying this framework, to yield insight into the structure and features of political discussions online.",Not About Sufficiency
Towards more Accessible Precision Medicine: Building a more Transferable Machine Learning Model to Support Prognostic Decisions for Micro- and Macrovascular Complications of Type 2 Diabetes Mellitus,"Although machine learning models are increasingly being developed for clinical decision support for patients with type 2 diabetes, the adoption of these models into clinical practice remains limited. Currently, machine learning (ML) models are being constructed on local healthcare systems and are validated internally with no expectation that they would validate externally and thus, are rarely transferrable to a different healthcare system. In this work, we aim to demonstrate that (1) even a complex ML model built on a national cohort can be transferred to two local healthcare systems, (2) while a model constructed on a local healthcare system's cohort is difficult to transfer; (3) we examine the impact of training cohort size on the transferability; and (4) we discuss criteria for external validity. We built a model using our previously published Multi-Task Learning-based methodology on a national cohort extracted from OptumLabs (R) Data Warehouse and transferred the model to two local healthcare systems (i.e., University of Minnesota Medical Center and Mayo Clinic) for external evaluation. The model remained valid when applied to the local patient populations and performed as well as locally constructed models (concordance: .73-.92), demonstrating transferability. The performance of the locally constructed models reduced substantially when applied to each other's healthcare system (concordance: .62-.90). We believe that our modeling approach, in which a model is learned from a national cohort and is externally validated, produces a transferable model, allowing patients at smaller healthcare systems to benefit from precision medicine.",Not About Sufficiency
Comparing the effectiveness of a brief intervention to reduce unhealthy alcohol use among adult primary care patients with and without depression: A machine learning approach with augmented inverse probability weighting,"Background: The combination of unhealthy alcohol use and depression is associated with adverse outcomes including higher rates of alcohol use disorder and poorer depression course. Therefore, addressing alcohol use among individuals with depression may have a substantial public health impact. We compared the effectiveness of a brief intervention (BI) for unhealthy alcohol use among patients with and without depression. Method: This observational study included 312,056 adult primary care patients at Kaiser Permanente Northern California who screened positive for unhealthy drinking between 2014 and 2017. Approximately half (48%) received a BI for alcohol use and 9% had depression. We examined 12-month changes in heavy drinking days in the previous three months, drinking days per week, drinks per drinking day, and drinks per week. Machine learning was used to estimate BI propensity, follow-up participation, and alcohol outcomes for an augmented inverse probability weighting (AIPW) estimator of the average treatment (BI) effect. This approach does not depend on the strong parametric assumptions of traditional logistic regression, making it more robust to model misspecification. Results: BI had a significant effect on each alcohol use outcome in the non-depressed subgroup (-0.41 to-0.05, all ps < .003), but not in the depressed subgroup (-0.33 to-0.01, all ps > .28). However, differences between subgroups were nonsignificant (0.00 to 0.11, all ps > .44). Conclusion: On average, BI is an effective approach to reducing unhealthy drinking, but more research is necessary to understand its impact on patients with depression. AIPW with machine learning provides a robust method for comparing intervention effectiveness across subgroups.",Not About Sufficiency
Predicting atmospheric particle formation days by Bayesian classification of the time series features,"Atmospheric new-particle formation (NPF) is an important source of climatically relevant atmospheric aerosol particles. NPF can be directly observed by monitoring the time-evolution of ambient aerosol particle size distributions. From the measured distribution data, it is relatively straightforward to determine whether NPF took place or not on a given day. Due to the noisiness of the real-world ambient data, currently the most reliable way to classify measurement days into NPF event/non-event days is a manual visualization method. However, manual labor, with long multi-year time series, is extremely time-consuming and human subjectivity poses challenges for comparing the results of different data sets. These complications call for an automated classification process. This article presents a Bayesian neural network (BNN) classifier to classify event/non-event days of NPF using a manually generated database at the SMEAR II station in Hyytiala, Finland. For the classification, a set of informative features are extracted exploiting the properties of multi-modal log normal distribution fitted to the aerosol particle concentration database and the properties of the time series representation of the data at different scales. The proposed method has a classification accuracy of 84.2 % for determining event/non-event days. In particular, the BNN method successfully predicts all event days when the growth and formation rate can be determined with a good confidence level (often labeled as class Ia days). Most misclassified days (with an accuracy of 75 %) are the event days of class II, where the determination of growth and formation rate are much more uncertain. Nevertheless, the results reported in this article using the new machine learning-based approach points towards the potential of these methods and suggest further exploration in this direction.",Not About Sufficiency
Examining Smart Neighborhood Platforms: A Qualitative Exploration of Features and Applications,"Smart neighborhood platforms have emerged as innovative solutions to address the challenges of urbanization and as smart living approaches to create sustainable, interconnected communities. This study aims to provide a qualitative exploration of multiple smart neighborhood platform initiatives in the German speaking region. The research examines the diverse characteristics, features and capabilities offered by these platforms, as well as their potential application in various aspects of smart community living. Qualitative research methods are employed to identify key parameters. Additionally, the study investigates the practical application of these platforms, ranging from optimizing resource consumption and improving quality of life to enhancing social interactions and fostering social cohesion and sustainable behaviors. The findings contribute to a deeper understanding of the design and applications of smart neighborhood platforms, providing valuable insights for further research as well as urban planners, policymakers, and technology developers seeking to create smarter and more livable communities.",Not About Sufficiency
Personalized Recommendation Classification Model of Students' Social Well-being Based on Personality Trait Determinants Using Machine Learning Algorithms,"The global trend of student social well-being has steadily declined in recent years. As a result, the need for a personalized recommendation classification model that can accurately assess and identify the individual student's social well-being has become increasingly important. This article will discuss the development of an adaptive personalized recommendation classification model for students' social well-being based on personality trait determinants. Social well-being is a field that analyses society, individual behavioural patterns, behavioural networks, and cultural elements of daily life. Social well-being develops critical thinking by understanding the social frameworks that affect humans by exposing the social basis of daily actions. For instance, when students are pleased, their academic achievement, behaviour, social integration, and happiness improve. This study classifies the effects of the Big 5 Personality Traits (Extraversion, Openness, Agreeableness, Emotional Stability, and Conscientiousness) on students' Industry 4.0 Social Well-being levels by analyzing their demographic and personality traits. A dataset was gathered through a survey distributed to students in a selected institution. The classifier's accuracy was assessed using the WEKA tool on a data set of 286 occurrences and 19 traits, and a confusion matrix was constructed. After analyzing the results of all algorithms, it was determined that the IBk and Randomizable Filtered Classifier algorithms give the best accuracy on social well-being readiness, with a comparable percentage value of 91.26%. The agreeableness personality trait, which represents a person's level of pleasantness, politeness, and helpfulness, had the greatest influence on the social well-being of the students. They have a positive outlook on human behaviour and get along well with others. Since social well-being contributes to a person's increased quality of life and happiness, improving students' current quality of life would lead to the development of a social parameter that can assess the growth of a country and the increased happiness of families and communities. Personality traits models have become an increasingly important tool for understanding and predicting human behavior. By analyzing different personality trait models, we can gain insights into how accurately and reliably they can predict individual behavior. This is especially useful in fields such as psychology, marketing, and recruitment, where understanding the nuances of individual personalities can be critical to success. In this study, how different personality trait models compare in terms of accuracy and reliability is explored using different machine learning algorithms using the WEKA tool. Personality trait models are increasingly being used to measure social well-being. This model is based on the idea that individuals' personalities are composed of a set of underlying traits which can be measured and compared. By understanding these traits, we can better understand the students' social well-being and how the environment around them may impact it.",Not About Sufficiency
VacSol-ML(ESKAPE): Machine learning empowering vaccine antigen prediction for ESKAPE pathogens,"The ESKAPE family, comprising Enterococcus faecium, Staphylococcus aureus, Klebsiella pneumoniae, Acinetobacter baumannii, Pseudomonas aeruginosa, and Enterobacter spp., poses a significant global threat due to their heightened virulence and extensive antibiotic resistance. These pathogens contribute largely to the prevalence of nosocomial or hospital-acquired infections, resulting in high morbidity and mortality rates. To tackle this healthcare problem urgent measures are needed, including development of innovative vaccines and therapeutic strategies. Designing vaccines involves a complex and resource-intensive process of identifying protective antigens and potential vaccine candidates (PVCs) from pathogens. Reverse vaccinology (RV), an approach based on genomics, made this process more efficient by leveraging bioinformatics tools to identify potential vaccine candidates. In recent years, artificial intelligence and machine learning (ML) techniques has shown promise in enhancing the accuracy and efficiency of reverse vaccinology. This study introduces a supervised ML classification framework, to predict potential vaccine candidates specifically against ESKAPE pathogens. The model's training utilized biological and physicochemical properties from a dataset containing protective antigens and non-protective proteins of ESKAPE pathogens. Conventional autoencoders based strategy was employed for feature encoding and selection. During the training process, seven machine learning algorithms were trained and subjected to Stratified 5-fold Cross Validation. Random Forest and Logistic Regression exhibited best performance in various metrics including accuracy, precision, recall, WF1 score, and Area under the curve. An ensemble model was developed, to take collective strengths of both the algorithms. To assess efficacy of our final ensemble model, a high-quality benchmark dataset was employed. VacSol-ML(ESKAPE) demonstrated outstanding discrimination between protective vaccine candidates (PVCs) and non-protective antigens. VacSol-ML(ESKAPE), proves to be an invaluable tool in expediting vaccine development for these pathogens. Accessible to the public through both a web server and standalone version, it encourages collaborative research. The web-based and standalone tools are available at http://vacsolml.mgbio.tech/.",Not About Sufficiency
Development of a Prediction Model for Healthy Life Years Without Activity Limitation: National Cross-sectional Study,"Background: In some countries, including Japan-the leading country in terms of longevity, life expectancy has been increasing; meanwhile, healthy life years have not kept pace, necessitating an effective health policy to narrow the gap.Objective: The aim of this study is to develop a prediction model for healthy life years without activity limitations and deploy the model in a health policy to prolong healthy life years.Methods: The Comprehensive Survey of Living Conditions, a cross-sectional national survey of Japan, was conducted by the Japanese Ministry of Health, Labour and Welfare in 2013, 2016, and 2019. The data from 1,537,773 responders were used for modelling using machine learning. All participants were randomly split into training (n=1,383,995, 90%,) and test (n=153,778, 10%) subsets. Extreme gradient boosting classifier was implemented. Activity limitations were set as the target. Age, sex, and 40 types of diseases or injuries were included as features. Healthy life years without activity limitations were calculated by incorporating the predicted prevalence rate of activity limitations in a life table. For the wide utility of the model in individuals, we developed an application tool for the model.Results: In the groups without (n=1,329,901) and with (n=207,872) activity limitations, the median age was 47 (IQR 30-64) and 69 (IQR 54-80) years, respectively (P<.001); female sex comprised 51.3% (n=681,794) in the group without activity limitations and 56.9% (n=118,339) in the group with activity limitations (P<.001). A total of 42 features were included in the feature set. Age had the highest impact on model accuracy, followed by depression or other mental diseases; back pain; bone fracture; other neurological disorders, pain, or paralysis; stroke, cerebral hemorrhage, or infarction; arthritis; Parkinson disease; dementia; and other injuries or burns. The model exhibited high performance with an area under the receiver operating characteristic curve of 0.846 (95% CI 0.842-0.849) with exact calibration for the average probability and fraction of positives. The prediction results were consistent with the observed values of healthy life years for both sexes in each year (range of difference between predictive and observed values: -0.89 to 0.16 in male and 0.61 to 1.23 in female respondents). We applied the prediction model to a regional health policy to prolong healthy life years by adjusting the representative predictors to a target prevalence rate. Additionally, we presented the health condition without activity limitations index, followed by the application development for individual health promotion.Conclusions: The prediction model will enable national or regional governments to establish an effective health promotion policy for risk prevention at the population and individual levels to prolong healthy life years. Further investigation is needed to validate the model's adaptability to various ethnicities and, in particular, to countries where the population exhibits a short life span.",Not About Sufficiency
Predictive Models for Suicide Attempts in Major Depressive Disorder and the Contribution of EPHX2: A Pilot Integrative Machine Learning Study,"Suicide is a major public health problem caused by a complex interaction of various factors. Major depressive disorder (MDD) is the most prevalent psychiatric disorder associated with suicide; therefore, it is essential to prioritize suicide prediction and prevention within this population. Integrated information from different dimensions, including personality, cognitive function, and social and genetic factors, is necessary to improve the performance of predictive models. Besides, recent studies have indicated the critical roles for EPHX2/P2X2 in the pathophysiology of MDD. Our previous studies found an association of EPHX2 and P2X2 with suicide in MDD. This study is aimed at (1) establishing predictive models with integrated information to distinguish MDD from healthy volunteers, (2) estimating the suicide risk of MDD, and (3) determining the contribution of EPHX2/P2X2. This cross-sectional study was conducted on 472 prospectively collected participants. The machine learning (ML) technique using Extreme Gradient Boosting (XGBoost) classifier was employed to evaluate the performance and relative importance of the extracted characteristics in recognising patients with MDD and depressed suicide attempters (DSA). In independent validation set, the model with clinical and cognitive information could recognise MDD with an area under the receiver operating characteristic curve (AUC) of 0.938 (95% confidence interval (CI), 0.898-0.977), and genetic information did not improve classification performance. The model with clinical, cognitive, and genetic information resulted in a significantly higher AUC of 0.801 (95% CI, 0.719-0.884) for identifying DSA than the model with only clinical information, in which the three single nucleotide polymorphisms of EPHX2 showed important roles. This study successfully established step-by-step predictive ML models to estimate the risk of suicide attempts in MDD. We found that EPHX2 can help improve the performance of suicidal predictive models. This trial is registered with NCT05575713.",Not About Sufficiency
Big Data Analytics for Rapid Assessment of Pipeline Condition,"Inline inspection (ILI) has served the midstream oil and gas industry extremely well for decades. Using traditional techniques such as defect assessment and corrosion growth assessment, ILI data can be used to inform corrective actions and generate robust integrity management plans for future pipeline operation. Given the age of the industry, a wealth of ILI data has now been accumulated, and at the same time the digital technologies used to manage data have become increasingly sophisticated and accessible. We can now collect, store, structure, and extract value from ""big data"" with relative ease. In this article the value of a large database of ILI data and asset metadata for pipelines from all across the world will be demonstrated. As an example, it is shown how such a database can be used to classify corroded pipelines based on their condition, without completing a detailed assessment. Simple condition metrics can then be related back to the properties of the asset for example, its age or coating type. These simple descriptive analytics techniques serve as a prelude to more rigorous classification techniques using machine learning.",Not About Sufficiency
An artificial intelligence-based non-intrusive load monitoring of energy consumption in an electrical energy system using a modified K-Nearest Neighbour algorithm,"Energy profligacy and appliance degradation are the apex reasons accounting for the continuous rise in power wastage and high energy bills. The decline in energy conservation and management in residences has been largely attributed to the financial implications of using intrusive methods. This work aimed to resolve the challenges of intrusive load monitoring by introducing artificial intelligence and machine learning to optimise load monitoring. To solve this challenge, a non-intrusive approach was proposed where modalities for load prediction and classification were achieved with a Bagging regressor and a modified multiclass K-Nearest Neighbour algorithms. This developed supervised learning models produced a 0.9624 R2 score and 78.24% accuracy for prediction and classification, respectively, when trained and tested on a Dutch Residential Energy Dataset. This work seeks to provide a cost-effective approach to the optimisation of energy using steady state active power features. Essentially, the adoption of this non-intrusive technique for load monitoring would effectively aid customers on the distribution network save cost on energy bills, facilitate the detection of faulty appliances, provide recommendations for smart homes and buildings with the required information for efficient decision making and planning of energy needs. In the long term, easing the pressure on power generation to meet demand would translate to reduction in carbon emissions based on a wide-scale implementation of this proposed system. Hence, these are important parameters in realising the development of smart sustainable cities and sustainable energy systems in this current industrial revolution. Non-intrusive load monitoring, prediction and forecasting.image",Not About Sufficiency
"The Long COVID experience from a patient's perspective: a clustering analysis of 27,216 Reddit posts","Objective: This work aims to study the profiles of Long COVID from the perspective of the patients spontaneously sharing their experiences and symptoms on Reddit.Methods: We collected 27,216 posts shared between July 2020 and July 2022 on Long COVID-related Reddit forums. Natural language processing, clustering techniques and a Long COVID symptoms lexicon were used to extract the different symptoms and categories of symptoms and to study the co-occurrences and correlation between them.Results: More than 78% of the posts mentioned at least one Long COVID symptom. Fatigue (29.4%), pain (22%), clouded consciousness (19.1%), anxiety (17.7%) and headaches (15.6%) were the most prevalent symptoms. They also highly co-occurred with a variety of other symptoms (e.g., fever, sinonasal congestion). Different categories of symptoms were found: general (45.5%), neurological/ocular (42.9%), mental health/psychological/behavioral (35.2%), body pain/mobility (35.1%) and cardiorespiratory (31.2%). Posts focusing on other concerns of the community such as vaccine, recovery and relapse and, symptom triggers were detected.Conclusions: We demonstrated the benefits of leveraging large volumes of data from Reddit to characterize the heterogeneity of Long COVID profiles. General symptoms, particularly fatigue, have been reported to be the most prevalent and frequently co-occurred with other symptoms. Other concerns, such as vaccination and relapse following recovery, were also addressed by the Long COVID community.",Not About Sufficiency
On the Challenges and Opportunities in Visualization for Machine Learning and Knowledge Extraction: A Research Agenda,"We describe a selection of challenges at the intersection of machine learning and data visualization and outline a subjective research agenda based on professional and personal experience. The unprecedented increase in the amount, variety and the value of data has been significantly transforming the way that scientific research is carried out and businesses operate. Within data science, which has emerged as a practice to enable this data-intensive innovation by gathering together and advancing the knowledge from fields such as statistics, machine learning, knowledge extraction, data management, and visualization, visualization plays a unique and maybe the ultimate role as an approach to facilitate the human and computer cooperation, and to particularly enable the analysis of diverse and heterogeneous data using complex computational methods where algorithmic results are challenging to interpret and operationalize. Whilst algorithm development is surely at the center of the whole pipeline in disciplines such as Machine Learning and Knowledge Discovery, it is visualization which ultimately makes the results accessible to the end user. Visualization thus can be seen as a mapping from arbitrarily high-dimensional abstract spaces to the lower dimensions and plays a central and critical role in interacting with machine learning algorithms, and particularly in interactive machine learning (iML) with including the human-in-the-loop. The central goal of the CD-MAKE VIS workshop is to spark discussions at this intersection of visualization, machine learning and knowledge discovery and bring together experts from these disciplines. This paper discusses a perspective on the challenges and opportunities in this integration of these discipline and presents a number of directions and strategies for further research.",Not About Sufficiency
Versatility Evaluation of Landslide Risk with Window Sizes and Sampling Techniques Based on Deep Learning,"Across the globe, landslides cause significant loss of life, injuries, and widespread damage to homes and infrastructure. Therefore, assessing and analyzing landslide hazards is crucial to human, environmental, cultural, economic, and social sustainability. This study utilizes ArcGIS 10.8 and Python 3.9 to create landslide databases for Niigata Prefecture (NIG), Iwate and Miyagi Prefectures (IWT-MYG), and Hokkaido (HKD), drawing on data obtained from the National Research Institute for Earth Science and Disaster Resilience, Japan. A distinguishing feature of this study is the application of a Convolutional Neural Network (CNN), which significantly outperforms traditional machine learning models in image-based pattern recognition by extracting contextual information from surrounding areas, a distinct advantage in image and pattern recognition tasks. Unlike conventional methods that often require manual feature selection and engineering, CNNs automate feature extraction, enabling a more nuanced understanding of complex patterns. By experimenting with CNN input window sizes ranging from 3 x 3 to 27 x 27 pixels and employing diverse sampling techniques, we demonstrate that larger windows enhance the model's predictive accuracy by capturing a wider range of environmental interactions critical for effective landslide modeling. CNN models with 19 x 19 pixel windows typically yield the best overall performance, with CNN-19 achieving an AUC of 0.950, 0.982 and 0.969 for NIG, HKD, and IWT-MYG, respectively. Furthermore, we improve prediction reliability using oversampling and a random window-moving method. For instance, in the NIG region, the AUC of the oversampling CNN-19 is 0.983, while the downsampling AUC is 0.950). These techniques, less commonly applied in traditional machine learning approaches to landslide detection, help address the issue of data imbalance often seen in landslide datasets, where instances of landslides are far outnumbered by non-landslide occurrences. While challenges remain in enhancing the model's generalization, this research makes significant progress in developing more robust and adaptable tools for landslide prediction, which are vital for ensuring environmental and societal resilience.",Not About Sufficiency
Machine learning for metallurgy II. A neural-network potential for magnesium,"Interatomic potentials are essential for studying fundamental mechanisms of deformation and failure in metals and alloys because the relevant defects (dislocations, cracks, etc.) are far above the scales accessible to first-principles studies. Existing potentials for non-fcc metals and nearly all alloys are, however, not sufficiently quantitative for many crucial phenomena. Here machine learning in the Behler-Parrinello neural-network framework is used to create a broadly applicable potential for pure hcp magnesium (Mg). Lightweight Mg and its alloys are technologically important while presenting a diverse range of slip systems and crystal surfaces relevant to both plasticity and fracture that present a significant challenge for any potential. The machine learning potential is trained on first-principles density-functional theory (DFT) computable metallurgically relevant properties and is then shown to well predict metallurgically crucial dislocation and crack structures and competing phenomena. Extensive comparisons to an existing very good modified embedded atom method potential are made. These results demonstrate that a single machine learning potential can represent the wide scope of phenomena required for metallurgical studies. The DFT database is openly available for use in any other machine learning method. The method is naturally extendable to alloys, which are necessary for engineering applications but where ductility and fracture are controlled by complex atomic-scale mechanisms that are not well predicted by existing potentials.",Not About Sufficiency
Multidimensional Data Integration and Analysis for Youth Health Care During the Covid-19 Pandemic,"The COVID-19 pandemic has been making big impact on mental and physical health of youth. Recent research shows that the COVID-19 pandemic has exacerbated existing mental health problems due to the unique combination of public health crises and social isolation. The objective of this study is to integrate and analyze various health data sources to improve health care for youth during the COVID-19 pandemic. The focus of the research is to merge self-assessment data from individuals, data obtained from wearable devices, and health data based on Traditional Chinese Medicine (TCM), utilizing machine learning techniques to gain a comprehensive perspective of youth health. The experiment results showed that the correlation between the TCM-based Health Score (TCMHS) in the TCM dimension and the Wearable Device Stress-based Health Score (WDSHS) in wearable devices was stronger than the correlation between the Self-assessed Subjective Health Score (SSHS) and the WDSHS. On the other hand, activity calorie consumption was the most important feature to both the SSHS and WDSHS while resting heart rate affected the TCMHS most.",Not About Sufficiency
Low-Carbon Quick Wins: Integrating Short-Term Sustainable Transport Options in Climate Policy in Low-Income Countries,"In low income countries (LICs) in Africa and Asia per capita transport greenhouse gas emissions are relatively low but are expected to grow. Therefore, a substantial reduction in projected increases is required to bring emissions in line with long-term global climate objectives. Literature on how LICs are integrating climate change mitigation and sustainable transport strategies is limited. Key drivers of transport policy include improving accessibility, congestion, air quality, energy security, with reducing greenhouse gas emissions being of lower priority. This paper assesses the current status, feasibility and potential of selected low-carbon transport measures with high sustainable development benefits that can be implemented in the short to medium term, so- called 'quick wins'. It examines to what extent ten such quick wins are integrated in climate change strategies in nine low- and middle-income countries in Africa and South Asia. The research method comprises expert interviews, an online questionnaire survey of experts and policymakers in the focus countries, and a review of literature and government plans. Results indicate that sustainable urban transport policies and measures are considered high priority, with vehicle-related measures such as fuel quality and fuel economy standards and electric two- and three-wheelers being of key relevance. In existing national climate change strategies, these quick wins are integrated to a certain extent; however, with better coordination between transport and energy and environment agencies such strategies can be improved. A general conclusion of this paper is that for LICs, quick wins can connect a top-down' climate perspective with a 'bottom-up' transport sector perspective. A knowledge gap exists as to the mitigation potential and sustainable development benefits of these quick wins in the local context of LICs.",Not About Sufficiency
"Assessment of groundwater arsenic contamination using machine learning in Varanasi, Uttar Pradesh, India","This paper presents a machine learning approach for classification of arsenic (As) levels as safe and unsafe in groundwater samples collected from the Indo-Gangetic region. As water is essential for sustaining life, heavy metals like arsenic pose a public health concern. In this study, various tree-based machine learning models namely Random Forest, Optimized Forest, CS Forest, SPAARC, and REP Tree algorithms have been applied to classify water samples. As per the guidelines of the World Health Organization (WHO), the arsenic concentration in water should not exceed 10 mu g/L. The groundwater quality parameter was ranked using a classifier attribute evaluator for training and testing the models. Parameters obtained from the confusion matrix, such as accuracy, precision, recall, and FPR, were used to analyze the performance of models. Among all models, Optimized Forest outperforms other classifier as it has a high accuracy of 80.64%, a precision of 80.70%, recall of 97.87%, and a low FPR of 73.33%. The Optimized Forest model can be used to test new water samples for classification of arsenic in groundwater samples.",Not About Sufficiency
"ISTAS 2013: Invited Plenary Session ""Healthcare & Public Health: Perspectives on Wearable Computing, Augmented Reality and the Veillances""","In the past decade during IEEE sponsored professional meetings(2),(3) the theme of ""Global Health Transformation through true Interoperability"" was brought to the forefront in the inaugural keynotes. Some technologies that started with the monitoring of hemodynamic variables of astronauts by NASA in the 60s were further developed by the Department of Defense for the purposes of treating their injured in the battlefield via Telemedicine. By August 511, 1997 President Clinton signed the first piece of legislation that was allowing the concepts of homecare to be tried to measure cost and medical effectiveness. With the development of Internet, the World Wide Web (WWW), social media, intelligent agents, mobile technology, sensors, and pieces of clothing containing them a new generation of devices have been created offering new possibilities for improvements particularly in areas such assist of living (for those suffering from chronic conditions), and homecare in general. The use of wearable computing in general and the use of augmented reality in the developed world in particular offer some unique opportunities to improve outcomes. In the 21(st) Century and as the Health Care and Public Health infrastructure intersect deeper into the many Information Technology (IT) subfields, abundant and formidable changes can occur that will allow society to shift current systems into some where wellness and disease prevention will be the focus. Many changes can affect positively medical and cost effective outcomes as well as the elimination of medical errors and patient safety for example. In these arenas, with the convergence of science, technology and with Information Technology acting as a catalyst for change, health care systems around the world are slowly shifting from -hospital based"" ones into distributed systems that include: hospitals, clinics, homecare systems with treatment and management of chronic diseases for the elderly via Internet, etc. In order to achieve such visions, multiple efforts have been tried for creating electronic health record as well as the information highway for their use. In the US the health system is very scattered and most hospital systems do not contain for example mental health, dental health and or vaccine registry information. On one hand through major medical research the emergence of clinical and health data repositories or ""Intelligent Data Warehouses"" that not only include traditional clinical data, but also advanced imaging, molecular medicine, tissue micro-array analysis and other bioinformatics information is available. These increasingly multi-modality data warehouses are constantly updated, continuously expanded and populated with millions of records. Although these repositories of electronic information can be leveraged not only to improve point of care clinical decision-making for individual patients, they can also support population health chronic and infectious disease analytics (i.e., epidemiology and surveillance), cost efficient multi-center (e.g., and multi-country) clinical trials, and comprehensive post-market pharmaco-vigilance. On the other hand the integration of healthcare and public health is a major concern as well. Globalization (i.e., the interdependencies that each country has with many others) for example has raised the sense of awareness through ""the information highway"". In 2004 the total production of flu vaccines coming to the US from the UK's Chiron had to be thrown away (approximately in the range of 42 to 44 million vaccines). During and since 2007 the US public has learnt through successive media stories related to: the death of pets due to food-import contamination, children's toys imports containing lead paint, food contamination, drug contamination, drug ingredients contaminated, etc. During 2008 we heard about: People getting very sick from fish containing the ciguatera toxin and Tab / drinking water containing about 36 different medications, e.g., antibiotics, antidepressants, etc. As the northern hemisphere prepared for the second wave of the 2009 H1N1 Flu Pandemic (which was expected to start around October 2009) all nations could have benefited by having epidemiology and surveillance data from all southern hemisphere nations available for the production of more ""accurate"" vaccines. In 2011 the European Union had to cut back in their consumption of vegetables and fruit because of an e-coli outbreak. Simultaneously the food from Asia in sonic cases was contaminated with radiation from the nuclear disaster caused by the combination of earthquake / tsunami at the Fukushima site. In South America the eruption of the volcano Puyehue in Chile closed all the airports in Uruguay, Argentina, Paraguay and the south of Brazil. All of these events resulted in major conflicts regarding world demand for food supplies. Still the perfect opportunity to transform our health care systems to a strategy of disease prevention and wellness is in the horizon. Using information technology as an enabler, we can encompass a wide range of opportunities that can start at the cellular, molecular and genetics levels and go as far as population health. Initial immunization studies show the level of antibody titers against viral diseases depends on the circadian time of inoculation. The concepts of chronobiology and chrono-therapeutics can be used to generate disease prevention strategies based on these circadian-rhythm dependencies. Just imagine how the public could also be better protected not only against environmental threats, water contamination, food borne diseases through the use of remote sensing data and a worldwide food enterprise architecture, but through alerts that could flow into a person through Wearable Computing, Augmented Reality and the Veillances. Data, Information, Knowledge and Wisdom could ""flow"" into an individual alerting him/her that they need to immediately visit their doctor, or stop consuming certain products. Some examples of our current problem environment could change outcomes by using these tools: 1. Getting the right information at the right time ---the steroid injection that they got for pain from laboratory x (in Massachusetts) is contaminated ( Just in the USA, between September 2012 and March 2013 at least 44 people have died and over 700 are contaminated from fungal meningitis according to the CDC) and their life may be at risk; 2. Preventing potential water and or food poisoning --- on February 4(th,), 2013, a report regarding the drinking water in many places within the State of California containing large amounts of Arsenic. Since we eat year round vegetables and fruits, livestock, poultry, etc. from that State, it may require the public to be cautious. As discovery from new research expands our knowledge about our body, its genome, and the cause-effect of new drugs, it also provides an opportunity to bring not only all these types of information to the forefront of the patient regardless where she /he may be at, but enable the transition from a system that has focused on disease to one that will focus on wellness through prevention and hopefully improve the quality of our lives.",Not About Sufficiency
Collision Detection and Avoidance using Optical Flow for Multicopter UAVs,"This paper presents the collision avoidance system for multicopter UAVs for autonomous navigation around obstacles using a simple camera as a main sensor and optical flow as the primary machine vision technique for obstacle detection. This is a very affordable and easily accessible combination to be used on UAVs of any type. Optical flow is a method that is used to detect the motion of pixels between pairs of images. If the clusters of like-colored pixels of an object move in a similar direction, the object is in a relative motion. An S1000 Octocopter was used as a test platform for the project. The vehicle is equipped with a Pixhawk 2 flight controller for autonomous navigation. Intel NUC processor was used as an onboard flight computer to process the video input from the sensor (camera) for collision detection, and to run the collision avoidance algorithms. The developed algorithm was tested in multiple flight tests in a flight test site with both natural and man-made obstacles present. Prior to flight tests, the algorithm was tested in simulation. Simulation and flight test results are shown.",Not About Sufficiency
A Technological Framework to Support Asthma Patient Adherence Using Pictograms,"Background: Low comprehension and adherence to medical treatment among the elderly directly and negatively affect their health. Many elderly patients forget medical instructions immediately after their appointments, misunderstand them, or fail to recall them altogether. Some identified causes include the short time slots allocated for appointments in the public health system in Chile, the complex terminology used by healthcare professionals, and the stress experienced by patients during appointments. One approach to improving patients' adherence to medical treatment is to combine written and oral instructions with graphical elements such as pictograms. However, several challenges arise due to the ambiguity of natural language and the need for pictograms to accurately represent various medication combinations, doses, and frequencies. Objective: This study introduces SIMAP (System for Integrating Medical Instructions with Pictograms), a technological framework aimed at enhancing adherence among asthma patients through the delivery of pictograms via a computational system. SIMAP utilizes a collaborative and user-centered methodology, involving health professionals and patients in the construction and validation of its components. Methods: The technological framework presented in this study is composed of three parts. The first two are medical indications and pictograms related to the treatment of the disease. Both components were developed through a comprehensive and iterative methodology that incorporates both qualitative and quantitative approaches. This methodology includes the utilization of focus groups, interviews, paper and online surveys, as well as expert validation, ensuring a robust and thorough development. The core of SIMAP is the technological component that leveraged artificial intelligence methods for natural language processing to analyze, tokenize, and associate words and their context to a set of one or more pictograms, addressing issues such as the ambiguity in the text, the cultural factor that involves many ways of expressing the same indication, and typographical errors in the indications. Results: Firstly, we successfully validated 18 clinical indications along with their respective pictograms. Some of the pictograms were redesigned based on the validation results. However, in the final validation, the comprehension percentages of the pictograms exceeded 70%. Furthermore, we developed a software called SIMAP, which translates medical indications into previously validated pictograms. Our proposed software, SIMAP, achieves a correct mapping rate of 96.69%. Conclusions: SIMAP demonstrates great potential as a technological component for supplementing medical instructions with pictograms when tested in a laboratory setting. The use of artificial intelligence for natural language processing can successfully map medical instructions, both structured and unstructured, into pictograms. This integration of textual instructions and pictograms holds promise for enhancing the comprehension and adherence of elderly patients to their medical indications, thereby improving their long-term health.",Not About Sufficiency
Climate justice and home-buyout programs: renters as a forgotten population in managed retreat actions,"For most Americans, the value of their home represents the largest portion of their total wealth; accordingly, homeowners even in very poor areas can obtain some benefit from a home-buyout program as a means to move away from risk and begin again. Renters, however, are an overlooked population during implementation of post-disaster retreat programs that predominantly focus on homeownership. Racism is a substantial factor in homeownership disparities between black and white Americans that can be traced to the post World War II GI Bill-a law that delivered to returning veterans federally-backed home mortgage loans, loans that were largely denied to returning black soldiers. These inequities have not been overcome, leaving minority renters as some of the most vulnerable populations after a disaster. Indeed, some renters may be substantially worse off after a buy-out program is implemented in an area. Renters represent an atypical ""trapped"" population when it comes to relocation programs because they may be economically forced to move to even more climate vulnerable housing. This paper will explore post-implementation impact on renters of home buy-out and similar retreat programs. We will examine the factors that contribute to this cycle of failed re-location efforts for this sub-group such as the lack of retreat policies aimed at assisting low-income renters, lack or limitations of home or rental insurance, the absence of ""duty to warn"" obligations from landlords to inform renters of repeated flooding risks at the property, and market failure to encompass climate risks in rental pricing.",Not About Sufficiency
Reframing Resilience: Equitable Access to Essential Services,"We urgently need to put the concept of resilience into practice if we are to prepare our communities for climate change and exacerbated natural hazards. Yet, despite the extensive discussion surrounding community resilience, operationalizing the concept remains challenging. The dominant approaches for assessing resilience focus on either evaluating community characteristics or infrastructure functionality. While both remain useful, they have several limitations to their ability to provide actionable insight. More importantly, the current conceptualizations do not consider essential services or how access is impaired by hazards. We argue that people need access to services such as food, education, health care, and cultural amenities, in addition to water, power, sanitation, and communications, to get back some semblance of normal life. Providing equitable access to these types of services and quickly restoring that access following a disruption are paramount to community resilience. We propose a new conceptualization of community resilience that is based on access to essential services. This reframing of resilience facilitates a new measure of resilience that is spatially explicit and operational. Using two illustrative examples from the impacts of Hurricanes Florence and Michael, we demonstrate how decisionmakers and planners can use this framework to visualize the effect of a hazard and quantify resilience-enhancing interventions. This ""equitable access to essentials"" approach to community resilience integrates with spatial planning, and will enable communities not only to ""bounce back"" from a disruption, but to ""bound forward"" and improve the resilience and quality of life for all residents.",Not About Sufficiency
Interoperable System for Automated Extraction and Identification of Machine Control Data in Brownfield Production,"In recent years, Industrial Internet of Things (IIoT) respectively Industry 4.0 have become increasingly established and thus a widespread technology among companies. By means of data acquisition, processes can not only be made more sustainable and resource-efficient but through emerging technologies like fault detection or predictive maintenance, a higher Overall Equipment Effectiveness (OEE) can be achieved. As a result, the availability of the machine tools remains high and process chains will not be disturbed. However, many companies still operate in brownfield production sites with legacy machines and therefore with limited opportunities for machine data access and use in an economically viable manner. As a result, many advantages that would increase the OEE of machine tools cannot be used. To present a solution for retrofitting brownfield machines, a low-effort system was developed that extracts machine control signals from different machine data sources and automatically identifies them using a multi-stage algorithm. This algorithm consists of analytical rule bases built using expert knowledge and a machine learning model for classification. The best performing models were selected as machine learning models, which are a Residual Network, a Fully Convolutional Network, a Long-Short-Term Memory, and a Random Forest. With different datasets from two machine tools, the overall model was tested and was able to correctly identify the signals present with an accuracy of 86.92 % to 98.93 % on average. Using this approach, the identified signals are assigned to an information model linking them to the respective machine tool axes. As a result, brownfield machines are also made accessible for modern technologies such as predictive maintenance or a reduction in rejects as a result of error detection. (c) 2023 The Authors. Published by ELSEVIER Ltd. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)",Not About Sufficiency
"Creative reflections on embodied filmmaking: in, through and between the senses and spaces of the medicalized body","This artist's reflection further explores the imagined and sensorial encounters of the body in medicine through a creative practice-led feminist, embodied, and intersubjective approach to filmmaking. It uses ""corporeal parenthesis"" as form and content to destabilize conventional readings associated with medicalization. The sensations we feel on the inside of the body shape our imagined anatomical understanding. This extension of proprioception is a sensorial way of seeing and experiencing the inside of one's own body. The visceral body, which entwines our real and imagined bodily inside, is positioned as a site for sense-making. This emphasizes knowledge as imaginative, intuitive, and lived to resist the standardization experienced within medicine. However, the passive horizontal body in the clinical encounter and the presumed intimacy of the therapeutic space, assumes the body to be accessible and consenting. This alters the ways in which we experience senses associated with pleasure and non-pleasure. By focusing on the artist's moving image work O (Symptom), creative practice is used to examine in, through, and between the spatial and sensorial boundaries of the medicalized body.",Not About Sufficiency
Changing and Evolution of Influenza Virus: Is It a Trivial Flu?,"Background: Influenza viruses are etiological agents which cause contagious respiratory, seasonal epidemics and, for influenza A subtypes, pandemics. The clinical picture of influenza has undergone continuous change over the years, due to intrinsic viral evolution as well as ""reassortment"" of its genomic segments. The history of influenza highlights its ability to adapt and to rapidly evolve, without specific circumstances. This reflects the complexity of this pathology and poses the fundamental question about its assumption as a ""common illness"" and its impact on public health. Summary: The global influenza epidemics and pandemics claimed millions of deaths, leaving an indelible mark on public health and showing the need for a better comprehension of the influenza virus. The clear understanding of genetic variations during the influenza seasonal epidemics is a crucial point for developing effective strategies for prevention, treatment, and vaccine design. The recent advance in next-generation sequencing approaches, model systems to virus culture, and bioinformatics pipeline played a key role in the rapid characterization of circulating influenza strains. In particular, the increase in computational power allowed the performance of complex tasks in healthcare settings through machine learning algorithms, which analyze different variables, such as medical and laboratory outputs, to optimize medical research and improve public health systems. The early detection of emerging and reemerging pathogens is a matter of importance to prevent future pandemics. Key Messages: The perception of influenza as a ""trivial flu"" or a more serious public health concern is a subject of ongoing debate, reflecting the multifaceted nature of this infectious disease. The variability in the severity of influenza sheds light on the unpredictability of the viral characteristics, coupled with the challenges in accurately predicting circulating strains. This adds complexity to the public health burden of influenza and highlights the need for targeted interventions.",Not About Sufficiency
Improvement of the 24 hr forecast of surface UV radiation using an ensemble approach,"A methodology is proposed to improve the 24 hr forecast of the ultraviolet (UV) index and the duration of exposure to obtain the minimal erythemal dose (MED). A forecast ensemble consisting of 10 members (differing in initial and boundary conditions) is examined to search for the best performed ensemble member. Routine UV measurements are used for the forecast validation. These are carried out at Belsk (20.8 degrees E, 51.8 degrees N) and in Raciborz (18.2 degrees E, 50.1 degrees N) representing a rural and an urban site in Poland, respectively. Each ensemble member is built using the clear-sky simulations by a radiative transfer model. The clear-sky irradiance is attenuated using the cloud modification factor (CMF) depending on the cloud cover by low- and mid-level clouds. The 24 hr forecast of cloudiness is obtained by the Weather Research and Forecasting (WRF) model. Every day, for each ensemble member, the optimal CMF values are built by the offline bootstrapping of the original CMF matrix. The performance of all ensemble members is evaluated for the day preceding the forecast. The best one is subsequently used for the next-day forecast. This procedure provides a more accurate forecast than that based on a single member of the ensemble. For both sites, the root mean square percentage error for the duration of the MED exposure changes from about 30% to about 15%, and mean absolute percentage error from about 20-25% to about 10%.",Not About Sufficiency
THE GOLDEN-EGG AS A NATURAL-RESOURCE - TOWARD A NORMATIVE THEORY OF GROWTH MANAGEMENT,"Growth management is the branch of urban planning concerned with the timing and sequencing of land development and the policies designed to mitigate the more negative impacts of growth. These policies are often justified on the ground that rapid or poorly planned development causes ''quality of place'' to deteriorate, reducing both current welfare and the prospects for future growth (ie., ''killing the goose that laid the golden egg''), In this article, I evaluate the normative assumptions underlying this popular argument. I conclude that the ''golden egg'' argument makes an implicit analogy to resource economics and raises legitimate issues of sustainability and dynamic efficiency. Although an understanding of resource economics can provide more normative guidance than one typically finds in the urban planning literature, using these concepts to make policy is no easy task. Thus the golden egg argument remains subject to cynical manipulation by both pro- and antigrowth forces.",Not About Sufficiency
"Social interaction in public space: Spatial edges, moveable furniture, and visual landmarks","Research on the relationship between space and social interaction has focused on indoor spaces, such as museums and offices. However, empirical evidence on the connection between the intensity of social interaction and outdoor public spaces is still lacking. Applying machine learning algorithms to a 9-hour time-lapse video of an urban park, we decipher the effects of two spatial features, edges, and landmarks, on visitors' activities. We identified dynamic visitor groups in the videos through a graph-based method and mapped the clustering pattern of interaction activities over time and space. In parallel, we used a computer vision algorithm to delineate fixed objects (notably the harbourfront, landside park boundary, a carousel, four benches, three pavilions, and four heart-shaped seating) and dynamic edges (formed by moveable furniture as park visitors repositioned them) onsite. We found that dynamic edges formed by moveable furniture and the fixed edge of a visual landmark consistently attracted more social interaction and group activities. In designing public spaces that encourage group activities, urban planners and designers can leverage the combination of fixed objects and flexible furniture to maximise the choices for visitors and curate a more engaging public open space.",Not About Sufficiency
Automatic Classification of Antimalarial Herbal Drugs Exposed to Ultraviolet Radiation from Unexposed Ones Using Laser-Induced Autofluorescence with Chemometric Techniques,"Exposure of antimalarial herbal drugs (AMHDs) to ultraviolet radiation (UVR) affects the potency and integrity of the AMHDs. Instant classification of the AMHDs exposed to UVR (UVR-AMHDs) from unexposed ones (Non-UVR-AMHDs) would be beneficial for public health safety, especially in warm regions. For the first time, this work combined laser-induced autofluorescence (LIAF) with chemometric techniques to classify UVR-AMHDs from Non-UVR-AMHDs. LIAF spectra data were recorded from 200 ml of each of the UVR-AMHDs and Non-UVR-AMHDs. To extract useful data from the spectra fingerprint, principal components (PCs) analysis was used. The performance of five chemometric algorithms: random forest (RF), neural network (NN), support vector machine (SVM), linear discriminant analysis (LDA), and k-nearest neighbour (KNN), were compared after optimization by validation. The chemometric algorithms showed that KNN, SVM, NN, and RF were superior with a classification accuracy of 100% for UVR-AMHDs while LDA had a classification accuracy of 98.8% after standardization of the spectra data and was used as an input variable for the model. Meanwhile, a classification accuracy of 100% was obtained for KNN, LDA, SVM, and NN when the raw spectra data was used as input except for RF for which a classification accuracy of 99.9% was obtained. Classification accuracy above 99.74 +/- 0.26% at 3 PCs in both the training and testing sets were obtained from the chemometric models. The results showed that the LIAF, combined with the chemometric techniques, can be used to classify UVR-AMHDs from Non-UVR-AMHDs for consumer confidence in malaria-prone regions. The technique offers a non-destructive, rapid, and viable tool for identifying UVR-AMHDs in resource-poor countries.",Not About Sufficiency
Applications of artificial intelligence in the field of air pollution: A bibliometric analysis,"BackgroundArtificial intelligence (AI) has become widely used in a variety of fields, including disease prediction, environmental monitoring, and pollutant prediction. In recent years, there has also been an increase in the volume of research into the application of AI to air pollution. This study aims to explore the latest trends in the application of AI in the field of air pollution. MethodsAll literature on the application of AI to air pollution was searched from the Web of Science database. CiteSpace 5.8.R1 was used to analyze countries/regions, institutions, authors, keywords and references cited, and to reveal hot spots and frontiers of AI in atmospheric pollution. ResultsBeginning in 1994, publications on AI in air pollution have increased in number, with a surge in research since 2017. The leading country and institution were China (N = 524) and the Chinese Academy of Sciences (N = 58), followed by the United States (N = 455) and Tsinghua University (N = 33), respectively. In addition, the United States (0.24) and the England (0.27) showed a high degree of centrality. Most of the identified articles were published in journals related to environmental science; the most cited journal was Atmospheric Environment, which reached nearly 1,000 citations. There were few collaborations among authors, institutions and countries. The hot topics were machine learning, air pollution and deep learning. The majority of the researchers concentrated on air pollutant concentration prediction, particularly the combined use of AI and environmental science methods, low-cost air quality sensors, indoor air quality, and thermal comfort. ConclusionResearches in the field of AI and air pollution are expanding rapidly in recent years. The majority of scholars are from China and the United States, and the Chinese Academy of Sciences is the dominant research institution. The United States and the England contribute greatly to the development of the cooperation network. Cooperation among research institutions appears to be suboptimal, and strengthening cooperation could greatly benefit this field of research. The prediction of air pollutant concentrations, particularly PM2.5, low-cost air quality sensors, and thermal comfort are the current research hotspot.",Not About Sufficiency
Embedded Machine Learning for Mango Classification Using Image Processing and Support Vector Machine,"Agriculture is an important economic aspect of many Asian countries, including Vietnam. One major obstacle, which prevents Vietnam from exporting some fruit species to western countries such as mangoes, is the irregular quality of the harvested fruits. Various research aimed to solve this problem by proposing different methods of estimating the weights of the fruits to determine anomalies. However, most methods are only tested for their maximum accuracy under favorable conditions with expensive equipments, and are therefore often not suitable for the poor farming regions of the developing countries. In this paper, with the main focus being mangoes, we would like to study a simpler approach using a combination of images processing techniques and support vector machine (SVM) for determining the approximate weight range of the fruits. The target device, on which the approach will be tested, is a Raspberry Pi 3B, which is fairly affordable even in developing countries. This will examine the feasibility of the weight classification for agriculture using only cheap hardware.",Not About Sufficiency
Consequence-based framework for buried infrastructure systems: A Bayesian belief network model,"The failure of municipal buried infrastructures (potable water supply, wastewater systems, and stormwater systems) may cause crucial consequences to the environment, society, health, and economy. The buried infrastructure management has transformed from reactive to the preventive action plan. In this study, a Bayesian belief network (BBN) based buried infrastructure consequence model is developed to assess the consequence index and to prioritize the buried infrastructures for maintenance/ rehabilitation/ replacement. The causal relationships between different parameters are constructed based on published literature and expert knowledge. The proposed model can provide information at pipe level by estimating the health & safety impact, environmental impact, social impact, and economical & organizational impact due to failure. The proposed model is also capable of highlighting the most sensitive and vulnerable pipes within the network. The applicability of the proposed model is demonstrated on the wastewater collection network of the City of Vernon, BC. Results indicate that proposed BBN-based consequence model can explicitly quantify uncertainties and handle the non-linear and sophisticated relationships between several factors.",Not About Sufficiency
Troops are Business Schools: Military Service and Entrepreneurial Behaviors in China,"Although research has examined the benefits of military service and the impact of military executives on business operations, the relationship between military service and entrepreneurial behavior remains poorly understood. Using the instrumental variable approach and a nationally representative male sample from the China Labor-forces Dynamic Survey, we discovered that military service significantly increased the probability of entrepreneurship in China, even after a series of robustness checks. Mechanism tests indicated that this positive effect could be explained by human and political capital accumulation, military-related social capital formation, and risk appetite traits. Our supplemental analyses demonstrated that exogenous shocks from the special military-in-business policy strengthened the positive entrepreneurial effect, whereas the higher education expansion policy and China's accession to the World Trade Organization weakened this effect. Additionally, military entrepreneurs had better business performance and more resilient, persistent, and confident traits. We found no evidence that Chinese veterans were forced to become entrepreneurs. This study enriches research on the styles and traits of military entrepreneurs and managers and provides important insights for assessing and improving veteran welfare policies in China and other developing countries.",Not About Sufficiency
Integrating Digital Tools in Engineering Education: Social Impact of Technological Integration,"The advancement of educational technology holds significant promise in accelerating, enriching, and cultivating students' skill sets, fostering motivation, and fostering active engagement in learning processes. This integration serves to bridge the gap between theoretical knowledge and practical application, thereby fostering the cultivation of a skilled and adaptable workforce poised for future economic demands. Moreover, this technological integration has a profound social impact, as it promotes inclusivity, access to education, and collaboration among students and educators. This research endeavors to conduct a comprehensive analysis and delineation of emerging trends within educational technology, subsequently offering insightful recommendations concerning efficacious tools geared towards augmenting learning outcomes through technological integration. Paramount among these trends is the incorporation of artificial intelligence, machine learning, augmented reality, virtual reality platforms, gamification strategies, STEAM pedagogical approaches, as well as the utilization of social media platforms within educational frameworks. Additionally, this discourse underscores the importance of integrating blockchain technology and enhancing cybersecurity measures within the educational domain. It is imperative for educators to remain abreast of evolving trends and technological tools within the educational landscape to facilitate the provision of optimal learning environments, thus equipping students with the requisite tools and opportunities for knowledge acquisition and practical application. Copyright (C) 2024 The Authors.",Not About Sufficiency
Academic and Demographic Cluster Analysis of Engineering Student Success,"Contribution: This article uses student semester grade point average (GPA) as a measure of student success to take into account the temporal effects in student success. The findings highlight the student performance based on their demographic status and use of university resources such as financial aid. College campuses should not only increase current resources but also raise awareness of current resources and make them more accessible (e.g., easier to apply or automatic applications). This is especially important for some demographics such as Hispanic first-generation students. Background: Higher education institutions are facing retention and graduation problems. One way to improve this is by understanding why students are not academically successful. Research Questions: In this study, demographic information and past academic records were analyzed to understand patterns of student success. Methodology: A cluster analysis was conducted to understand groups of students based on academic performance and demographic information. Examples of these factors are enrollment status, financial status, first-generation status, housing status, and transfer status. For the purpose of getting more accurate results, the students were separated into two different groups according to their admission status: 1) freshman and 2) transfer. Findings: The results indicate Hispanic, first-generation, low-income students are not likely to apply for financial aid although they are eligible. They have lower GPA and take fewer units per semester than other students. This can cause delayed graduation and accumulating more debt.",Not About Sufficiency
ESR Essentials: ten steps to cardiac MR-practice recommendations by ESCR,"Cardiovascular MR imaging has become an indispensable noninvasive tool in diagnosing and monitoring a broad range of cardiovascular diseases. Key to its clinical success and efficiency are appropriate clinical indication triage, technical expertise, patient safety, standardized preparation and execution, quality assurance, efficient post-processing, structured reporting, and communication and clinical integration of findings. Technological advancements are driving faster, more accessible, and cost-effective approaches. This ESR Essentials article presents a ten-step guide for implementing a cardiovascular MR program, covering indication assessments, optimized imaging, post-processing, and detailed reporting. Future goals include streamlined protocols, improved tissue characterization, and automation for greater standardization and efficiency.",Not About Sufficiency
"Organizational change, psychosocial work environment, and non-disability early retirement: a prospective study among senior public employees","Objective This study examines the impact of organizational change and psychosocial work environment on non-disability early retirement among senior public service employees. Methods In January and February 2011, Danish senior public service employees aged 58-64 years (N=3254) from the Capital Region of Denmark responded to a survey assessing psychosocial work environment (ie, social capital, organizational justice, and quality of management). Work-unit organizational changes (ie, change of management, merging, demerging, and relocation) were recorded from January 2009 to March 2011. Weekly data on non-disability early retirement transfer were obtained from the DREAM register database, which holds weekly information about all public benefit payments in Denmark. Hazard ratios (HR) for early retirement following employees' 60th birthday were estimated with Cox regression adjusted for age, gender, and socioeconomic status. Results Exposure to change of management [HR 1.37, 95% confidence interval (95% CI) 1.13-1.66], mergers (HR 1.23, 95% CI 1.02-1.48), and relocation of work unit (HR 1.24, 95% CI 1.01-1.54) increased rate of nondisability early retirement, while demerging of work unit did not (HR 1.03, 95% CI 0.79-1.33). Work units with lower levels of social capital (HR 1.22, 95% CI 1.05-1.41), organizational justice, (HR 1.18, 95% CI 1.04-1.32), and quality of management (HR 1.14, 95% CI 1.02-1.25) increased rate of early retirement. Conclusion Organizational change and poor psychosocial work environment contribute to non-disability early retirement among senior public service employees, measured at work-unit level.",Not About Sufficiency
The help-seeking process and predictors of mental health care use among individuals with depressive symptoms: a machine learning approach,"Purpose The goal of the study was to identify the most important influences on professional healthcare use of people with depressive symptoms. We incorporated findings from research areas of health behaviors, stigma, and motivation to predict the help-seeking process variables from a wide range of personal factors and attitudes.Methods A sample of 1,368 adults with untreated depressive symptoms participated in an online survey with three-and six-month follow-ups. We conducted multiple linear regressions for (a) help-seeking attitudes, and (b) help-seeking intentions, and logistic regression for (c) help-seeking behavior with machine learning methods.Results While self-stigma and treatment experience are important influences on help-seeking attitudes, complaint perception is relevant for intention. The best predictor for healthcare use remains the intention. Along the help-seeking process, we detected a shift of relevant factors from broader perceptions of mental illness and help-seeking to concrete suffering, i.e., subjective symptom perception.Conclusion The results suggest a spectrum of influencing factors ranging from personal, self-determined factors to socially normalized factors. We discuss social influences on professional help-seeking and the use of combined public health programs and tailored help-seeking interventions.Clinical trial registration German Clinical Trials Register (https://drks.de/search/en): Identifier DRKS00023557.",Not About Sufficiency
'No-one visits me anymore': Low Emission Zones and social exclusion via sustainable transport policy,"For many years, the literature has pointed to the difficulties with the development of transport policy measures which meet both social and environmental policy objectives. Low Emission Zones (LEZ) offer an interesting example of measures that aim to decrease traffic-related air pollution, but which might have significant social effects by reducing the mobility of vulnerable, car-dependent groups. The Antwerp LEZ (Belgium) is used as a case. The assumptions and views in policy documents were compared with the experiences of some affected persons. The research challenges the assumption that only households with a non-compliant vehicle living in the LEZ are impacted by the measure since the LEZ may have a social impact well beyond the delimited zone. Some people with their residence in the LEZ expressed the feeling that they put a burden on friends and relatives from outside the zone who want to visit them. Furthermore, the LEZ affects low-income car owners with an older, damage-prone vehicle that is allowed to enter the zone, by making replacement vehicles less affordable. In general, the case reveals how the views and experiences of those most likely affected by the policy measure are not fully taken into account.",Not About Sufficiency
Multifactorial Model Based on DWI-Radiomics to Determine HPV Status in Oropharyngeal Squamous Cell Carcinoma,"Featured Application Imaging-based predictors of HPV status could be used to non-invasively detect HPV positivity in cases in which biopsy is not feasible because of a hardly accessible tumor location, or when the HPV test results are conflicting. Radiomics signatures for HPV status would also help in OPSCC for an easier diagnosis in unknown primary head and neck tumors, contributing to re-ducing costs and invasiveness of the current gold-standard modality for disease diagnosis and staging. Lastly, it would be of interest to explore the role of image-based signatures for HPV status classification as prognostic biomarkers, in order to better identify distinct risk classes of OPSCC patients. Background: Oropharyngeal squamous cell carcinoma (OPSCC) associated with human papillomavirus (HPV) has higher rates of locoregional control and a better prognosis than HPV-negative OPSCC. These differences are due to some unique biological characteristics that are also visible through advanced imaging modalities. We investigated the ability of a multifactorial model based on both clinical factors and diffusion-weighted imaging (DWI) to determine the HPV status in OPSCC. Methods: The apparent diffusion coefficient (ADC) and the perfusion-free tissue diffusion coefficient D were derived from DWI, both in the primary tumor (PT) and lymph node (LN). First- and second-order radiomic features were extracted from ADC and D maps. Different families of machine learning (ML) algorithms were trained on our dataset using five-fold cross-validation. Results: A cohort of 144 patients was evaluated retrospectively, which was divided into a training set (n = 95) and a validation set (n = 49). The 50th percentile of D-PT, the inverse difference moment of ADCLN, smoke habits, and tumor subsite (tonsil versus base of the tongue) were the most relevant predictors. Conclusions: DWI-based radiomics, together with patient-related parameters, allowed us to obtain good diagnostic accuracies in differentiating HPV-positive from HPV-negative patients. A substantial decrease in predictive power was observed in the validation cohort, underscoring the need for further analyses on a larger sample size.",Not About Sufficiency
Solid waste management and Aedes aegypti infestation interconnections: A regression tree application,"Public health is at the core of all environmental and anthropic impacts. Urban and territorial planners should include public health concerns in their plans. Basic sanitation infrastructure is essential to maintaining public health and social and economic development. This infrastructure deficiency causes diseases, death and economic losses in developing countries. Framing interconnections among health, sanitation, urbanization and circular economy will assist sustainable development goal achievements. This study aims to identify the relationships between solid waste management indicators in Brazil and the Aedes aegypti mosquito infestation index. Regression trees were employed for modelling due to the complexity and characteristics of the data. The analyses were performed separately from data collected from 3501 municipalities and 42 indicators from the country's five regions. Results show that expenses and personnel indicators were the most critical indicators (in the mid-western, southeastern and southern regions), operational (northeastern (NE) region) and management (northern region). The mean absolute errors ranged from 0.803 (southern region) to 2.507 (NE region). Regional analyses indicate that the municipalities with better SWM results display lower infestation rates in buildings and residences. This research is innovative as it analyses infestation rates rather than dengue prevalence, using a machine learning method, in a multidisciplinary research field that needs further study.",Not About Sufficiency
Analytical Comparison between the Information Gain and Gini Index using Historical Geographical Data,"The historical geographical data of Kashmir province is spread across two disparate files having attributes of Maximum Temperature, Minimum Temperature, Humidity measured at 12 A.M., Humidity measured at 3 P.M., rainfall besides auxiliary parameters like date, year etc. The parameters Maximum Temperature, Minimum Temperature, Humidity measured at 12 A.M., Humidity measured at 3 P.M. are continuous in nature and here, in this study, we applied Information Gain and Gini Index on these attributes to convert continuous data into discrete values, their after we compare and evaluate the generated results. Of the four attributes, two have same results for Information Gain and Gini Index; one attribute has overlapping results while as only one attribute has conflicting results for Information Gain and Gini Index. Subsequently, continuous valued attributes are converted into discrete values using Gini index. Irrelevant attributes are not considered and auxiliary attributes are labeled accordingly. Consequently, the data set is ready for the application of machine learning (decision tree) algorithms.",Not About Sufficiency
Monitoring Green Sea Turtles in the San Gabriel River of Southern California,"Simple Summary The East Pacific population of green sea turtles (Chelonia mydas) has undergone substantial growth in recent years, and as such, green sea turtle sightings are becoming more common along the U.S. West Coast. The northernmost resident population of green sea turtles in the eastern Pacific Ocean lives near the mouth of the San Gabriel River in Long Beach, California, USA. Utilizing nine years (2013-2021) of citizen science data from the Aquarium of the Pacific's Southern California Sea Turtle Monitoring Project, we established a year-round presence of this population and determined that the areas along a 2.4-km (1.5 mile) stretch of the lower San Gabriel River with the most green sea turtle activity are near the Los Cerritos Wetlands and a power plant warm water effluent area, which are located approximately 1.3 and 2.9 km (0.8 and 1.8 miles), respectively, upriver from the mouth and entrance to Alamitos Bay. We hypothesize that turtles are attracted to these areas of the river for forage opportunity and thermal refuge. As green sea turtle presence in Southern California continues to increase, we recommend expanded monitoring programs to help understand essential habitat needs for this threatened population. Effective conservation of endangered species relies on the characterization of habitat use and tracking of long-term population trends, which can be especially challenging for marine species that migrate long distances and utilize a diversity of habitats throughout their lives. Since 2012, citizen science volunteers at the Aquarium of the Pacific in Long Beach, California, have been monitoring an urban population of East Pacific green sea turtles (Chelonia mydas) that resides near the mouth of the San Gabriel River (SGR) in Southern California, USA, in order to gain insights about how the population uses this area. Here, we collate and analyze nine years of citizen science data, including observed sightings collected across 10 observation stations. Our results confirm that green sea turtles are frequently present around warm water effluent from power plants, similar to research results reported for other locations in the eastern Pacific Ocean. Importantly, observational data also show notable green sea turtle activity around the outfalls for a small wetland habitat bordering the SGR, highlighting the importance of wetland ecosystems as a key habitat and foraging area for this threatened population. Finally, our results showcase the benefits of using citizen science to monitor sea turtle populations in easily accessible nearshore habitats.",Not About Sufficiency
Harmonization-Information Trade-Offs for SharingIndividual Participant Data in Biomedicine,"Biomedical practice is evidence-based. Peer-reviewed papers are the primary medium to present evidence and data-supported results to drive clinical practice. However, it could be argued that scientific literature does not contain data, but rather narratives about and summaries of data. Meta-analyses of published literature may produce biased conclusions due to the lack of transparency in data collection, publication bias, and inaccessibility to the data underlying a publication ('dark data'). Co-analysis of pooled data at the level of individual research participants can offer higher levels of evidence, but this requires that researchers share raw individual participant data (IPD). FAIR (findable, accessible, interoperable, and reusable) data governance principles aim to guide data lifecycle management by providing a framework for actionable data sharing. Here we discuss the implications of FAIR for data harmonization, an essential step for pooling data for IPD analysis. We describe the harmonization-information trade-off, which states that the level of granularity in harmonizing data determines the amount of information lost. Finally, we discuss a framework for managing the trade-off and the levels of harmonization. In the coming era of funder mandates for data sharing, research communities that effectively manage data harmonization will be empowered to harness big data and advanced analytics such as machine learning and artificial intelligence tools, leading to stunning new discoveries that augment our understanding of diseases and their treatments. By elevating scientific data to the status of a first-class citizen of the scientific enterprise, there is strong potential for biomedicine to transition from a narrative publication product orientation to a modern data-driven enterprise where data itself is viewed as a primary work product of biomedical research.",Not About Sufficiency
An Ad Hoc Network Infrastructure: Communication and Information Sharing for Emergency Response,"During an emergency response, access to a reliable communication infrastructure is required to exchange accurate information in a timely manner. Various communication technologies have been deployed for emergency response; however communication between different first response organizations has always been a problem. This is due to either broken networks or lack of knowledge regarding the channel frequency in use for the same device. According to recent investigations, text messaging was shown to be more reliable than voice to exchange short messages carrying critical information. Additionally, posting and updating the information on an electronic webpage accessible to all is also very useful. In addition, we would also suggest that team leaders physically stand together, thus improving network resource utilization plus ensuring receipt of updates and information from peers in the event the higher ranked person in the hierarchy is not reachable. In this paper, we present supporting arguments for the choice of a wireless mesh network as a candidate to provide communication infrastructure for emergency response. We also present a comprehensive set of technical, social and organizational challenges which we experienced first hand during several deployments, learned about in interviews with emergency responders and by examination of the after-incident reports. Many of these challenges become even more of a concern and have a greater impact on international disasters concerning multiple countries when traditionally different technologies are used often in conjunction with different languages. We also present the results of network performance analysis which identifies sources of bottleneck and overhead in communication. A distributed control hierarchical authority is necessary to prevent bottleneck and the need to cancel an already scheduled path due to resource unavailability or security breach.",Not About Sufficiency
On the forces of driver distraction: Explainable predictions for the visual demand of in-vehicle touchscreen interactions,"With modern infotainment systems, drivers are increasingly tempted to engage in secondary tasks while driving. Since distracted driving is already one of the main causes of fatal accidents, in-vehicle touchscreens must be as little distracting as possible. To ensure that these systems are safe to use, they undergo elaborate and expensive empirical testing, requiring fully functional prototypes. Thus, early-stage methods informing designers about the implication their design may have on driver distraction are of great value. This paper presents a machine learning method that, based on anticipated usage scenarios, predicts the visual demand of in-vehicle touchscreen interactions and provides local and global explanations of the factors influencing drivers' visual attention allocation. The approach is based on large-scale natural driving data continuously collected from production line vehicles and employs the SHapley Additive exPlanation (SHAP) method to provide explanations leveraging informed design decisions. Our approach is more accurate than related work and identifies interactions during which long glances occur with 68% accuracy and predicts the total glance duration with a mean error of 2.4 s. Our explanations replicate the results of various recent studies and provide fast and easily accessible insights into the effect of UI elements, driving automation, and vehicle speed on driver distraction. The system can not only help designers to evaluate current designs but also help them to better anticipate and understand the implications their design decisions might have on future designs.",Not About Sufficiency
Detecting rumors in social media: A survey,"With recent development of technology, especially mobile devices has made the social networks accessible 24/7. Information spreading has become faster than ever, regardless of the credibility of this information. This brings unparalleled challenges in ensuring the reliability of the information. Misinformation spreading has a strong relation especially in the context of breaking news, where the information released gradual, often starting as unverified information. Automatically identifying rumors from online social media especially micro-blogging websites is an important research. Recent research in detecting rumors automatically on social networks have addressed many languages. In this article, we provide an overview of the research into rumors detection in social media which we divided into three groups: supervised based approaches, unsupervised based approaches, and hybrid approaches based on the type of the machine learning used in each approach. (C) 2018 The Authors. Published by Elsevier B.V.",Not About Sufficiency
Relational interdependencies and the intra-EU mobility of African European Citizens,"How can we better understand the puzzle of low-skilled migrants who have acquired citizenship in a European Union country, often with generous social security provision, choosing to relocate to the United Kingdom? Drawing on Elias's figurational theory as a lens, we explore how relational interdependencies foster the mobility of low-skilled African European Citizens from European Union states to the United Kingdom. We found that African European Citizens rely on 'piblings networks', loose affiliations of putative relatives, to compensate for deficits in their situated social capital, facilitating relocation. The temporary stability afforded by impermanent bonds and transient associations, in constant flux in migrant communities, does not preclude integration but paradoxically promotes it by enabling an ease of connection and disconnection. Our study elucidates how these relational networks offer African European Citizens opportunities to achieve labour market integration, exercise self-efficacy, and realize desired futures; anchoring individuals in existing communities even when they are perpetually transforming.",Not About Sufficiency
Exploring the social and spatial potential of an intermodal approach to transport planning,"Gilbert and Perl (2007) established how different epochs in human history came with specific transport revolutions. Our research suggests that intermodal approaches could constitute an invisible transport planning revolution.Based on a review defining social sustainability as it applies to urban passenger transportation, we consider the potential of an intermodal focus to better integrate non-motorized (walking-cycling) and public transportation. We start from the premise that rather than being a primary mono-mode such as buses or trains, sustainable transport is best understood as an ecology of modes with specific strengths and complementarities that can be mobilized through planning.Using data from Metropolitan Santiago (Chile), we explore this potential, from a conceptual, mainly social, and spatial perspective. We find that paying more attention to different formats of cycling, public transport integration could significantly improve low-cost alternatives for individual and feeder trips. Moreover, adjusting land use to non-work trip purposes could yield substantial benefits. This approach also offers the possibility of developing relatively simple tools helpful for improving the deliberative aspects of participatory planning, thereby increasing buy-in as well as improving health, efficiency, safety, and other benefits of sustainable transport.",Not About Sufficiency
Nucleic acid based point-of-care diagnostic technology for infectious disease detection using machine learning empowered smartphone-interfaced quantitative colorimetry,"We report a nucleic acid-based point of care testing technology for infectious disease detection at resource limited settings by integrating a low-cost portable device with machine learning-empowered quantitative colorimetric analytics that can be interfaced via a smartphone application. We substantiate our proposition by demonstrating the efficacy of this technology in detecting COVID-19 infection from human swab samples, using the RT-LAMP protocol. Comparison with gold standard results from real-time PCR evidences high sensitivity and specificity, ensuring simplicity, portability, and user-friendliness of the technology at the same time. Colorimetric analytics of the reaction output without necessitating the opening of the reaction microchambers enables execution of the complete test workflow without any laboratory control that may otherwise be required stringently for safeguarding against carryover contamination. Seamless sample-to-answer workflow and machine learning-based readout further assures minimal human intervention for the test readout, thus eliminating inevitable inaccuracies stemming from erroneous execution of the test as well as subjectivity in interpreting the outcome. Our results further indicate the possibilities of upgrading the technology to predict the pathogenic load on the infected patients akin to the cyclic threshold value of the real-time PCR, when calibrated with reference to a wide range of 'training' data for the machine learner, thereby putting forward the same as viable alternative to the resource-intensive PCR tests that cannot be made readily accessible at underserved community settings.",Not About Sufficiency
"Beyond Accessibility: A Multidimensional Evaluation of Urban Park Equity in Yangzhou, China","Evaluating park equity can help guide the advancement of sustainable and equitable space policies. Previous studies have mainly considered accessibility when evaluating park equity while ignoring the selectivity and convenience of entering parks and residents' recognition of parks. Measuring equity based mainly on spatial thinking has resulted in the social aspects of parks receiving insufficient attention. In this study, we therefore integrated the spatial and social equity of parks and developed a multidimensional framework to evaluate park equity in four dimensions: accessibility (Ai), diversity (Di), convenience (Ci), and satisfaction (Si). Empirical analysis from Yangzhou, China showed that: (1) in Yangzhou's built-up districts, 23.43% of the communities received high- or relatively high-level park access but 17.72% received little or no park access. (2) The Gini coefficient indicated that all three dimensions showed a mismatch with population distribution, except for satisfaction (Si), which showed a relatively reasonable match. (3) Park access was generally better in communities with better locations, environments, and facilities. High-income groups enjoyed significantly better park access than low- and middle-income groups. These findings could help urban planners and policymakers develop effective policies to reduce inequality in park access.",Not About Sufficiency
CardioVision: A fully automated deep learning package for medical image segmentation and reconstruction generating digital twins for patients with aortic stenosis,"Aortic stenosis (AS) is the most prevalent heart valve disease in western countries that poses a significant public health challenge due to the lack of a medical treatment to prevent valve calcification. Given the aging population demographic, the prevalence of AS is projected to rise, resulting in a progressively significant healthcare and economic burden. While surgical aortic valve replacement (SAVR) has been the gold standard approach, the less invasive transcatheter aortic valve replacement (TAVR) is poised to become the dominant method for high-and medium-risk interventions. Computational simulations using patient-specific models, have opened new research avenues for optimizing emerging devices and predicting clinical outcomes. The traditional techniques of generating digital replicas of patients' aortic root, native valve, and calcification are time-consuming and laborintensive processes requiring specialized tools and expertise in anatomy. Alternatively, deep learning models, such as the U-Net architecture, have emerged as reliable and fully automated methods for medical image segmentation. Two-dimensional U-Nets have been shown to produce comparable or more accurate results than trained clinicians' manual segmentation while significantly reducing computational costs. In this study, we have developed a fully automatic AI tool capable of reconstructing the digital twin geometry and analyzing the calcification distribution on the aortic valve. The developed automatic segmentation package enables the modeling of patient-specific anatomies, which can then be used to simulate virtual interventional procedures, optimize emerging prosthetic devices, and predict clinical outcomes.",Not About Sufficiency
"CAR SHARING SYSTEMS AS A SUSTAINABLE TRANSPORT POLICY: A CASE STUDY FROM LISBON, PORTUGAL","Purpose - The dominance of road transport, both on passenger and freight movements, has reached alarming levels in what concerns their negative environmental impacts as well as societal and economic costs. To reverse this trend, a technology-driven approach and a behavioral change attitude need to be pursued. Promising results have been reported in Europe in the reduction of vehicle ownership, due to the introduction of an alternative transport mode known as car sharing. This work evaluates the contribution of car sharing to sustainable transport, based both in a technological shift and a potential behavioral change. Methodology/approach - The state of the art on car sharing and policies presents the effects of these systems and how they have been promoted. As those effects can vary according to the geographical area, the users profile, and service characteristics, a worldwide analysis on car sharing systems covering more than 400 cities was performed. Average service indicators were quantified and characterization variables were accounted to those cities' urban areas. Considering those normalized values, the authors performed an analysis of the car sharing system in Lisbon (Portugal). An initial assessment was made to estimate its current energy and environmental impacts. This outcome was then compared with the environmental and economic effects of using alternative vehicle technologies in car sharing. The results obtained enable a discussion of the more important variables for the success of the system and, consequently, to choose what policy instruments can help car sharing to succeed. Findings - The results of the existing car sharing schemes reveal the positive contribution of car sharing to fill a ""mobility gap"" in sustainable transport. It works as a complement to other sustainable transport options and it impacts positively both society and car-sharers in terms of mobility costs, environmental, and energy implications. These results are more significant if a technology shift to electric mobility is promoted. Within the case study in Lisbon, the adoption of electric mobility would allow decreases up to 47% and 65% in energy consumption and CO2 emissions, respectively. Moreover, the present value economic analysis revealed that, these systems will only be economically viable after approximately 7 years. A sensitivity analysis to the economic model was performed showing that the variables having higher influence were cost-related variables (reducing the break-even timeframe from 36% to 57%), such as vehicle purchase cost, insurance, maintenance and tax costs, and fuel cost. Social implications - Car sharing systems generally present social benefits to society as it leads to the reduction of car ownership, with all the positive effects that has on a lower demand for parking space, less congestion, reduced local pollutants and emissions. If the technology used by car sharing vehicles shifts from conventional to another type of technology, the effects both for society and car sharers are even more appealing from a social point of view. In the particular case study approached in the chapter, given the small scale of the car sharing network and low usage patterns, the local results have a low social impact at the city scale. A larger promotion of the system either with a more aggressive marketing campaign targeting specific population niches (e.g., environmentally conscious people), larger vehicle and parking availability, or better integration with the city's public transport system could foster the deployment of the system, similarly to other cities. Originality/value - Overall, the results obtained from this research work quantify the contribution of car sharing to sustainable transport and highlights the positive effects of promoting a technological shift. These facts reinforce the need for public policies to support the integration of car sharing within the city's solutions to promote a more sustainable mobility. The successful deployment of car sharing systems can be influenced by policies targeting features such as allocation of parking, the fees and complementarity with public transport, signage and markings, and marketing of social and environmental benefits.",Not About Sufficiency
A combined ML and DFT strategy for the prediction of dye candidates for indoor DSSCs,"The excellent ability of dye-sensitized solar cells (DSSCs) to capture ambient light and convert it into electric current makes them attractive power sources for indoor applications, including powering Internet of Things (IoT) devices. In this context, substantial research efforts have been devoted to the discovery of novel organic dyes able to harvest energy from a wide range of indoor light sources at different intensities. However, such activities are often based on trial-and-error procedures which are frequently expensive and time-consuming. Here, Machine Learning (ML) techniques and Density Functional Theory (DFT) methods have been combined in a two-stage approach, with the aim to accelerate the design of new, synthetically accessible organic dyes for indoor DSSC applications. By predicting the power conversion efficiency (PCE) under different indoor light sources and intensities, potentially high-performance organic dyes have been identified.",Not About Sufficiency
Characterizing the Online Discourse in Twitter: Users' Reaction to Misinformation around COVID-19 in Twitter,"During a pandemic, social media is a low-cost, accessible, and broad-reaching channel to disseminate information, however social media platforms can be hotbeds for misinformation. An analysis of misinformation around COVID-19 based on social media offers insights into what users perceive as misinformation, as well as their reactions. In order to identify the topics, sentiments, and user accounts in Twitter contributing to the spread of misinformation surrounding COVID-19, we analyze 12,000 tweets posted between February and May of 2020. We employ topic identification, network analysis, and sentiment analysis to study users' behaviors around misinformation. We identify six topics and train a set using several machine learning and neural network models to automatically classify tweets. The experimental results indicate the predictions of our models for six categories related to COVID-19 misinformation, achieving the highest accuracy of 95%. The network analysis identifies clusters of accounts and indicates how misinformation is spread. In addition, our analysis shows that sentiment scores are strongly influenced by government measures and public speeches from government officials, and the primary drivers of discourse are news agencies, public figures, health organizations, and lay citizens. In general, our proposed approaches provide a better understanding of the posts and user accounts who lead the discussion about misinformation around COVID-19.",Not About Sufficiency
Social Media Emerging as a Third Eye !! Decoding Users' Sentiment on Government Policy: A Case Study of GST,"Micro blogging sites and other social networking platforms have heroine the primary means of communication and knowledge sharing with progressing technological trends. People across the globe express their views on products & services, predict share price and present feedback on the policies of the regimes. Everything that is shared on social networks may not be authentic or denote the truth. However, it definitely forms a basis to investigate and comprehend the public sentiments. Public sentiments are capable of affecting the economic landscape via foreign investments and stock markets among having other financial and social impacts. In this paper, we have analyzed public sentiments on the Goods and Services tax, popularly known as GST in India. GST subsumes eight central and nine state taxes thereby integrating the absolute indirect tax framework in the country which paves the way for varied opinions & reactions imperative to analyze a collective sentiment. We used a hybrid approach to do the sentiment analysis which uses a combination of lexicon-based method and supervised machine learning approach to determine public sentiments. We accumulated 163,373 tweets over a span of three weeks from July 4th to 25th, 2017 after GST was implemented in India w.e.f. July 1st, 2017. A spatio temporal analysis was performed on the collected tweets. In this research, we annotated 22,000 unique tweets with the help of a lexicon based method and thenceforth applied supervised machine learning techniques with a set of six distinct algorithms to train and predict the polarity on the complete data set. K-fold cross validation technique, for K in range of 3-10, was used to assess the model for an independent data set. Subsequently, it was found that accuracy, precision, recall and Fl score of all the models provided the best results when K approached 10. Resultantly, we observed that SVM and Logistic Regression could predict the polarity of new incoming tweets with an accuracy of 77.6% and 79.31% respectively.",Not About Sufficiency
Optimized grass hopper algorithm for diagnosis of Parkinson's disease,"The Modified Grasshopper Optimization Algorithm which identifies Parkinson disease symptoms at an early (premature) stage was proposed. Parkinson disease, type of movement ailment, could be life-threatening if not treated at premature stage. Therefore, diagnosis of Parkinson disease became essential in early stages so that all the symptoms could be controlled by giving required medication to the patient. Hence ensuring the patient longevity. As part of this research work, a novel model Modified Grasshopper Optimization Algorithm was introduced which was based on the traditional Grasshopper Optimization Algorithm and search strategy for feature selection. Grasshopper Optimization Algorithm was relatively a novel heuristic optimization swarm intelligence algorithm which was stimulated by grasshoppers searching for food. This population-based method has capability to provide solution for real-life problems in undefined search space. It mimics grasshopper swarm's behaviour and their social interaction. Popular algorithms like Random Forest, Decision Tree and k-Nearest Neighbour classifier were used in judgement on shortlisted aka selected features. Different datasets of handwriting (meander and spiral), speech and voice were used for evaluating the presented model. The proposed algorithm was effective in Parkinson disease identification having accuracy (computed) of 95.37%, 99.47% detection rate and 15.78% false alarm rate. This helps larger cause of patient in receiving treatment in pre-mature stage. The presented bio-inspired algorithm was adequately steady and has ability to identify the optimal feature set. Finally results obtained from the assessment of introduced Modified Grasshopper Optimization Algorithm on these data sets were evaluated and contrasted with respect to outcome of Modified Grey Wolf Optimizer and Optimized Cuttlefish Algorithm. The experiment's outcome revealed that the presented Modified Grasshopper Optimization Algorithm assists in reducing the selected features count and improving the accuracy.",Not About Sufficiency
Design Fire Methodology for Vehicle Spaces Onboard Ships,"The purpose of this paper is to contribute towards establishing procedures of assessment of the fire safety of Ro-Ro ship cargo spaces, by employing suitable design fires. Data collected from fire experiments involving vehicles are utilized in order to deduce representative heat release rates for common types of passenger cars and heavy goods vehicles (HGVs). An existing mathematical scheme for generating parametric heat release rate curves is extended for handling vehicle fire scenarios. The methodology is demonstrated through a case study. In particular, the Fire Dynamics Simulator (FDS) code is used for investigating fire propagation on the vehicle deck of a Ro-Ro ship, initiated from a single HGV. On this deck are assumed to be transported a combination of cars and HGVs. The evolution of the fire is assessed assuming different ventilation conditions. The circumstances for spreading to adjacent vehicles are examined. Human survivability and the time that the deck remains accessible are evaluated by considering the International Maritime Organization (IMO) life safety performance criteria.",Not About Sufficiency
"Developing a Comprehensive Smart City Rating System: Case of Riyadh, Saudi Arabia","Urbanization is one of the biggest challenges in the world today. Sixty-eight percent of the world's population will reside in cities and urban areas by 2050. Transitioning to a smart city is the key to overcoming the urbanization challenges. This study aimed to develop a smart city rating system that determines the overall smartness score of Riyadh City. A mixed method approach was used: qualitative research involved interviews, whereas quantitative analysis utilized the fuzzy analytic hierarchy process (AHP) and multiple-criteria decision analysis (MCDA) to determine city smartness scores using main pillars and subfactors. The findings showed that Riyadh's smart city development is influenced by several factors. The smart economy carries a weight of 17.8% and a smartness score of 74.15%, while smart people holds a weight of 8.1% and a score of 59.79%. Culture diversity, education, and the Human Development Index emerge as the most dominant subfactors. Smart living carries a weight of 8.2% and a smartness score of 51.86%, with health and security being crucial aspects of human life. Smart governance carries a weight of 17.8% and a smartness score of 81.451%, with smart business services, transparency, open communication, and collaboration being the most important factors. Smart urban services hold a weight of 17.9% and a score of 43%, with transport, logistics, urban planning, infrastructure, smart parking, digital monitoring, and safety being the most dominant subfactors. Smart construction carries a weight of 3.9% and a smartness score of 43.83%, with sustainable houses and smart buildings being the most dominant subfactors. Smart mobility holds a weight of 18% and a smartness score of 52.34%, with congestion, accidents, transport efficiency, and a sustainable transportation system being the most dominant subfactors. Understanding the strengths and weaknesses of Riyadh smart city from the current research will help developers improve and maintain areas aligned with Saudi Vision 2030 while also providing a foundation for future research on similar cities or varying factors.",Not About Sufficiency
Biophysical principles predict fitness of SARS-CoV-2 variants,"SARS-CoV-2 employs its spike protein's receptor binding domain (RBD) to enter host cells. The RBD is constantly subjected to immune responses, while requiring efficient binding to host cell receptors for successful infection. However, our understanding of how RBD's biophysical properties contribute to SARS-CoV-2's epidemiological fitness remains largely incomplete. Through a comprehensive approach, comprising large-scale sequence analysis of SARS-CoV-2 variants and the identification of a fitness function based on binding thermodynamics, we unravel the relationship between the biophysical properties of RBD variants and their contribution to viral fitness. We developed a biophysical model that uses statistical mechanics to map the molecular phenotype space, characterized by dissociation constants of RBD to ACE2, LY-CoV016, LY-CoV555, REGN10987, and S309, onto an epistatic fitness landscape. We validate our findings through experimentally measured and machine learning (ML) estimated binding affinities, coupled with infectivity data derived from population -level sequencing. Our analysis reveals that this model effectively predicts the fitness of novel RBD variants and can account for the epistatic interactions among mutations, including explaining the later reversal of Q493R. Our study sheds light on the impact of specific mutations on viral fitness and delivers a tool for predicting the future epidemiological trajectory of previously unseen or emerging low -frequency variants. These insights offer not only greater understanding of viral evolution but also potentially aid in guiding public health decisions in the battle against COVID-19 and future pandemics.",Not About Sufficiency
In Vivo and In Vitro Studies of Cigarette Smoke Effects on Innate Responses to Influenza Virus: A Matter of Models?,"Cigarette smoke (CS) is a significant public health problem and a leading risk factor for the development of chronic obstructive pulmonary disease (COPD) in the developed world. Respiratory viral infections, such as the influenza A virus (IAV), are associated with acute exacerbations of COPD and are more severe in cigarette smokers. To fight against viral infection, the host has developed an innate immune system, which has complicated mechanisms regulating the expression and activation of cytokines and chemokines to maximize the innate and adaptive antiviral response, as well as limiting the immunopathology that leads to exaggerated lung damage. In the case of IAV, responders include airway and alveolar epithelia, lung macrophages and dendritic cells. To achieve a successful infection, IAV must overcome these defenses. In this review, we summarize the detrimental role of CS in influenza infections. This includes both immunosuppressive and proinflammatory effects on innate immune responses during IAV infection. Some of the results, with respect to CS effects in mouse models, appear to have discordant results, which could be at least partially addressed by standardization of animal viral infection models to evaluate the effect of CS exposure in this context.",Not About Sufficiency
The Outcome of the 2022 Landslide4Sense Competition: Advanced Landslide Detection From Multisource Satellite Imagery,"The scientific outcomes of the 2022 Landslide4Sense (L4S) competition organized by the Institute of Advanced Research in Artificial Intelligence are presented here. The objective of the competition is to automatically detect landslides based on large-scale multiple sources of satellite imagery collected globally. The 2022 L4S aims to foster interdisciplinary research on recent developments in deep learning (DL) models for the semantic segmentation task using satellite imagery. Over the past few years, DL-based models have achieved performance that meets expectations on image interpretation due to the development of convolutional neural networks. The main objective of this article is to present the details and the best-performing algorithms featured in this competition. The winning solutions are elaborated with state-of-the-art models, such as the Swin Transformer, SegFormer, and U-Net. Advanced machine learning techniques and strategies, such as hard example mining, self-training, and mix-up data augmentation, are also considered. Moreover, we describe the L4S benchmark dataset in order to facilitate further comparisons and report the results of the accuracy assessment online. The data are accessible on Future Development Leaderboard for future evaluation at https://www.iarai.ac.at/landslide4sense/challenge/, and researchers are invited to submit more prediction results, evaluate the accuracy of their methods, compare them with those of other users, and, ideally, improve the landslide detection results reported in this article.",Not About Sufficiency
Shared e-scooter Usage Trends in a Swedish City: A Spatial Analysis,"Amidst rapid urbanization and evolving transport needs, electric scooters (e-scooters) have been reshaping short-distance urban trips. This study offers a systematic framework for spatial analysis of shared micro-mobility in Gothenburg, Sweden-using Geographically Weighted Regression (GWR) and Multiscale-GWR (MGWR) models. The research aims to decipher the city's shared e-scooter demand for various factors such as transit proximity, land use patterns, road infrastructure, demographics, and weather conditions. Investigation comparatively deciphers GWR and MGWR models, which outperform global regression models in terms of fitness, and interpretability for the spatial heterogeneity in shared e-scooter demand. However, MGWR's complexity sometimes leads to overfitting, with its results lacking clear interpretation. The study identifies significant spatial variations in shared e-scooter demand, linked with specific urban characteristics, providing a deeper understanding of how different factors contribute to shared e-scooter usage across various city zones. These findings are crucial for shared e-scooter ventures, urban planners and policymakers, offering a nuanced framework for integrating e-scooters into urban transport systems. The research underscores the effectiveness of spatial econometric approaches in urban mobility management, highlighting the importance of efficient spatial models for shared e-scooter demand analysis in urban contexts.",Not About Sufficiency
Barrington Lecture 2023/24 Economic Geography and the Irish Border: A Market Access Approach,"This paper examines the economic impact of Ireland's partition, assessing market access losses using detailed geospatial data and multimodal transport network analysis. The study reveals that partition significantly reduced market access on both sides of the border, contributing to population decline. Districts closest to the border were the most affected, with estimated population figures being approximately 10 per cent lower than they would have been without the border. This negative impact has persisted, remaining evident despite the reduction of many physical border barriers. A counterfactual analysis suggests that absent the border, the current populations of the Republic of Ireland and Northern Ireland would have been 3 per cent and 5 per cent higher, respectively. These findings illustrate the persistent role of political borders in shaping regional economic activity.",Not About Sufficiency
"Territorial development in Bavaria between spatial justice and austere federalism: A historical-materialist policy analysis of Bavarian regional development politics and policies, 2008-2018","This paper examines the territorial development reforms in the federal state of Bavaria, Germany between 2008 and 2018 in light of the rise of austerity policies, introducing the concept of 'austere federalism' as a new state spatial process in the aftermath of the global financial crisis. Methodologically, the paper draws on the historical-materialist policy analysis and identifies three processes - municipalisation, competitisation, and responsibilisation - as key elements of a new hegemonic project. Our findings suggest that the years following the crisis saw a paradigm shift in spatial planning taking place, characterised by a carrot-and-stick policy of planning deregulation and austerity discipline. Rather than diminishing disparities, however, this shift runs the risk of exacerbating spatial inequality.",Not About Sufficiency
BIOREALM--An ontology of comparative biogeography: New insights into the semantics of biodiversity conservation,"Aim: We aimed to apply ontological techniques to address semantic ambiguities in protected area and conservation informatics. By doing so, we aimed to create a coherent, machine--actionable semantic representation of the biogeographic areas (which often overlap protected areas) to support more efficient and standardized informatics, supporting research and decision--making. We present BIOREALM, the first informatic ontology for comparative biogeography. Location: Global. Taxon: Any taxon can be integrated in BIOREALM. Methods: We convert a cladogram of biogeographic areas--generated by a process known as bioregionalization--into a series of ontological classes. Areas of endemism are treated as formal objects related by hierarchical relationships and constrained by a condition of monophyly. We use semantic web approaches to extend the Environment Ontology (ENVO) with classes for (often semantically confounded) biogeographic entities, including biogeographic areas, areas of endemism and endemic areas. We applied this approach to a bioregionalization of Australia as a case study. In all, 20 subregions which are part of the Austral Bioregionalisation Atlas have been selected for the study and integrated in BIOREALM. Results: We have created an ontology--formatted in the Web Ontology Language and adhering to the practices of the Open Biomedical and Biological Ontology Foundry-which provides a rigorous, extensible and machine--actionable framework that can improve biogeographic analyses and interoperability between systems. One main class and 20 individuals per class were implemented. Main Conclusions: BIOREALM encodes a model--theoretic view of endemism using semantic web approaches, offering new avenues to express and analyse biogeographic units. This approach offers a means to identify monophyletic biogeographic areas for conservation, based on specific combinations of monophyletic endemic taxa. Such an ontology provides knowledge representation solutions which supports interoperability along the FAIR (Findable, Accessible, Interoperable, Reusable) principles, thus fostering more consistent ecological informatics.",Not About Sufficiency
A feminist perspective on urban politics and social space in the neo-liberal city. Theoretical outlooks and social practices in the Italian context.,"Gender dimension, from a feminist perspective, in urban policies is a subject that urban planners, urban sociologists, politicians, and activists have often grappled with over time. However, the need to achieve an overall feminist take (Mol) or a gender urban advocacy approach (Kern, 2020) has frequently clashed, in urban activism or in the analysis of policy areas and segments open to gender issues, with a cultural climate resistant to gender inequalities, especially in the context of the neoliberal city. The latter is focused on maximizing its ability to extract value from cities and citizens, and naturally tends not to pay the necessary attention to the spatialization of inequalities (both gender and intersectional) that occur in the urban context. On the other hand, the increasingly widespread adoption of the gender mainstreaming paradigm seems to permeate various institutional levels, descending from the supranational level of the EU, where it was formulated, down to a more formal than substantive incorporation in urban contexts. This paper aims to analyse, first and foremost, from a theoretical perspective with the necessary reference to the gender claim of urban space and the introduction of the gender mainstreaming paradigm in public policies, and secondly with an empirical approach dedicated to Italian metropolitan cities, the ways in which the right to the city (Lefebvre) is interpreted in terms of gender. The goal of the work is to highlight a formalistic and non-substantive adherence to the gender mainstreaming paradigm in the context of Italian cities, where, in the light of references to gender equality in the right to the city, predominantly symbolic and commemorative references to the role of women are made, proceeding only in limited cases towards planning urban spaces and services tailored to women, supporting women's participation in city life, and fully integrating gender issues into the programming of social, educational, and city-sized welfare services.",Not About Sufficiency
Assessing governance implications of city digital twin technology: A maturity model approach,"Digital twin technology has great potential to transform urban planning. However, the governance aspects of city-scale digital twins (CDTs)- a virtual representation of urban environments -are understudied. This study bridges this knowledge gap by adopting a framework that scrutinizes the maturity stages of technology. We introduce the CITYSTEPS Maturity Model, a pioneering maturity framework tailored for CDTs, to assess all development stages of CDTs, including those utilizing artificial intelligence, and analyze the technology's role in urban governance. We highlight the promise of CDTs in enhancing public participation in urban planning and addressing key smart city concerns, such as accountability and transparency. However, significant challenges remain, including public participation, public trust in privacy protection, and technical impediments like inadequate data integration, systems integration, and interoperability. There's also the pressing issue of social inclusion: the potential exclusion of marginalized groups, including those often overlooked in data collection, like the hidden homeless and informal sector workers. We propose CDTs should be designed with a humancentric approach, transparent and unbiased data collection and algorithm development, and be led by an adaptive regulatory framework. The CITYSTEPS Maturity Model lays out a framework to assess CDTs' present state, forecast their future, and understand their governance implications, promoting more inclusive technology adoption.",Not About Sufficiency
Distance Difficult to Overcome? Analysis of Norwegian and Polish Position on the Path to Sustainable Development,"Sustainable development is undoubtedly the key challenge of the contemporary world. On the way to this development Poland if far behind the other countries, especially Norway, which seems to be an unquestionable leader in this respect in Europe. There is a distance between Norway and Poland in all ten dimensions (themes) described in the EU Sustainable Development Strategy (socio-economic development, sustainable consumption and production, social inclusion, demographic changes, public health, climate change and energy, sustainable transport, natural resources, global partnership and good governance). Unfortunately it is not likely that the distance could be significantly reduced in the coming years. Poland must accept that fact and simultaneously learn from the leader and take advantage of its experience, but can also benefit from financial support, which is possible to obtain from the EEA and Norway Grants the funds provided to particular programs aimed at reducing economic and social disparities and strengthening bilateral relations between Norway and Poland.",Not About Sufficiency
"Coping strategies and trajectories of life satisfaction among households in a voluntary planned program of relocation from a flood-risk area (vol 45, pg 861, 2020)",,Not About Sufficiency
Studies of the effectiveness of transport sector interventions in low- and middle-income countries: An evidence and gap map,"Background There are great disparities in the quantity and quality of infrastructure. European countries such as Denmark, Germany, Switzerland, and the UK have close to 200 km of road per 100 km(2), and the Netherlands over 300 km per 100 km(2). By contrast, Kenya and Indonesia have <30, Laos and Morocco <20, Tanzania and Bolivia <10, and Mauritania only 1 km per 100 km(2). As these figures show, there is a significant backlog of transport infrastructure investment in both rural and urban areas, especially in sub-Saharan Africa. This situation is often exacerbated by weak governance and an inadequate regulatory framework with poor enforcement which lead to high costs and defective construction. The wellbeing of many poor people is constrained by lack of transport, which is called ""transport poverty"". Lucas et al. suggest that up to 90% of the world's population are transport poor when defined as meeting at least one of the following criteria: (1) lack of available suitable transport, (2) lack of transport to necessary destinations, (3) cost of necessary transport puts household below the income poverty line, (4) excessive travel time, or (5) unsafe or unhealthy travel conditions. Objectives The aim of this evidence and gap map (EGM) is to identify, map, and describe existing evidence from studies reporting the quantitative effects of transport sector interventions related to all means of transport (roads, rail, trams and monorail, ports, shipping, and inland waterways, and air transport). Methods The intervention framework of this EGM reframes Berg et al's three categories (infrastructure, prices, and regulations) broadly as infrastructure, incentives, and institutions as subcategories for each intervention category which are each mode of transport (road, rail trams and monorail, ports, shipping, and inlands waterways, and air transport). This EGM identifies the area where intervention studies have been conducted as well as the current gaps in the evidence base. This EGM includes ongoing and completed impact evaluations and systematic reviews (SRs) of the effectiveness of transport sector interventions. This is a map of effectiveness studies (impact evaluations). The impact evaluations include experimental designs, nonexperimental designs, and regression designs. We have not included the before versus after studies and qualitative studies in this map. The search strategies included both academic and grey literature search on organisational websites, bibliographic searches and hand search of journals. An EGM is a table or matrix which provides a visual presentation of the evidence in a particular sector or a subsector. The map is presented as a matrix in which rows are intervention categories (e.g., roads) and subcategories (e.g., infrastructure) and the column outcome domains (e.g., environment) and subcategories as (e.g., air quality). Each cell contains studies of the corresponding intervention for the relevant outcome, with links to the available studies. Included studies were coded according to the intervention and outcomes assessed and additional filters as region, population, and study design. Critical appraisal of included SR was done using A Measurement Tool to Assess Systematic Reviews (AMSTAR -2) rating scale. Selection Criteria The search included both academic and grey literature available online. We included impact evaluations and SRs that assessed the effectiveness of transport sector interventions in low- and middle-income countries. Results This EGM on the transport sector includes 466 studies from low- and middle-income countries, of which 34 are SRs and 432 impact evaluations. There are many studies of the effects of roads intervention in all three subcategories-infrastructure, incentives, and institutions, with the most studies in the infrastructure subcategories. There are no or fewer studies on the interventions category ports, shipping, and waterways and for civil aviation (Air Transport). In the outcomes, the evidence is most concentrated on transport infrastructure, services, and use, with the greatest concentration of evidence on transport time and cost (193 studies) and transport modality (160 studies). There is also a concentration of evidence on economic development and health and education outcomes. There are 139 studies on economic development, 90 studies on household income and poverty, and 101 studies on health outcomes. The major gaps in evidence are from all sectors except roads in the intervention. And there is a lack of evidence on outcome categories such as cultural heritage and cultural diversity and very little evidence on displacement (three studies), noise pollution (four studies), and transport equity (2). There is a moderate amount of evidence on infrastructure quantity (32 studies), location, land use and prices (49 studies), market access (29 studies), access to education facilities (23 studies), air quality (50 studies), and cost analysis including ex post CBA (21 studies). The evidence is mostly from East Asia and the Pacific Region (223 studies (40%), then the evidence is from the sub-Saharan Africa (108 studies), South Asia (96 studies), Latin America & Caribbean (79 studies). The least evidence is from Middle East & North Africa (30 studies) and Europe & Central Asia (20 studies). The most used study design is other regression design in all regions, with largest number from East Asia and Pacific (274). There is total 33 completed SRs identified and one ongoing, around 85% of the SR are rated low confidence, and 12% rated as medium confidence. Only one review was rated as high confidence. This EGM contains the available evidence in English. Conclusion This map shows the available evidence and gaps on the effectiveness of transport sector intervention in low- and middle-income countries. The evidence is highly concentrated on the outcome of transport infrastructure (especially roads), service, and use (351 studies). It is also concentrated in a specific region-East Asia and Pacific (223 studies)-and more urban populations (261 studies). Sectors with great development potential, such as waterways, are under-examined reflecting also under-investment. The available evidence can guide the policymakers, and government-related to transport sector intervention and its effects on many outcomes across sectors. There is a need to conduct experimental studies and quality SRs in this area. Environment, gender equity, culture, and education in low- and middle-income countries are under-researched areas in the transport sector.",Not About Sufficiency
Modular Construction System (MCS) in Malaysia: Mass Customization Through Combinatorial,"Combinatorial is another sub-topic and sub ordinate of Golden Mean theory. This paper is part of on-going research of modular construction system (MCS) in Malaysia. Objectives of the research were to propose an appropriate scheme and concept for sustainable design for modular home based on research findings.The researcher conducted case study (Personnel Expert Interviews) to collect data of MCS in Malaysia. Proposed combinatorial design for spatial planning of home design has been carried out by the researcher at the end of analysis as design experimental yet need validation from computer science experts. Combinatorial in modular architecture is the theory of conceptual design for future building design process of MCS. The researcher discuss in details and to clarify the proportion combinatorial for MCS in aspect of building forms and arrangements through proposed combinatorial formula. In Malaysia, design technique should relate to local culture, weather, and available materials sources as mentioned by the experts. Combinatorial design of modular architecture is an alternative design for compact affordable home. This combinatorial geometry (3D module) repeated, duplicate and can be oriented in a few ways to forming combinatorial models of building design. Pattern A and B are considered to be the best in economic aspects and practicality in term of shape of green. Proposed concept possibly can improve user decision making (digital design process) e.g. building players to plan their building lifecycle projects in term of economical design, green shape, reduced construction time and cost. Thus, it may be able to promote user friendly design, fast track construction, and quality based product, less wastage and varied of design selection can be chosen of the user.",Not About Sufficiency
Feature replacement methods enable reliable home video analysis for machine learning detection of autism,"Autism Spectrum Disorder is a neuropsychiatric condition affecting 53 million children worldwide and for which early diagnosis is critical to the outcome of behavior therapies. Machine learning applied to features manually extracted from readily accessible videos (e.g., from smartphones) has the potential to scale this diagnostic process. However, nearly unavoidable variability in video quality can lead to missing features that degrade algorithm performance. To manage this uncertainty, we evaluated the impact of missing values and feature imputation methods on two previously published autism detection classifiers, trained on standard-of-care instrument scoresheets and tested on ratings of 140 children videos from YouTube. We compare the baseline method of listwise deletion to classic univariate and multivariate techniques. We also introduce a feature replacement method that, based on a score, selects a feature from an expanded dataset to fill-in the missing value. The replacement feature selected can be identical for all records (general) or automatically adjusted to the record considered (dynamic). Our results show that general and dynamic feature replacement methods achieve a higher performance than classic univariate and multivariate methods, supporting the hypothesis that algorithmic management can maintain the fidelity of video-based diagnostics in the face of missing values and variable video quality.",Not About Sufficiency
Response of organic aerosol to Delhi's pollution control measures over the period 2011-2018,"Some of the world's highest air pollution episodes occur in Delhi, India and studies have shown particulate matter (PM) is the leading air pollutant to cause adverse health effects on Delhi's population. It is therefore vital to chart sources of PM over long time periods to effectively identify trends, particularly as multiple air quality mitigation measures have been implemented in Delhi over the past 10 years but remain unevaluated. An automated offline aerosol mass spectrometry (AMS) method has been developed which has enabled high-throughput analysis of PM filters. This novel offline-AMS method uses an organic solvent mix of acetone and water to deliver high extraction recoveries of organic aerosol (OA) (95.4 +/- 8.3%). Positive matrix factorisation (PMF) source apportionment was performed on the OA fraction extracted from PM10 filter samples collected in Delhi in 2011, 2015 and 2018 to provide snapshots of the responses of OA to changes in sources in Delhi. The nine factors of OA resolved by PMF group into four primary source categories: traffic, cooking, coal-combustion and burning-related (solid fuel or open burning). Burning-related OA made the largest contribution during the winter and post-monsoon, when total OA concentrations were at their highest. Annual mean burning-related OA concentrations declined by 47% between 2015 and 2018, likely associated with the 2015 ban on open waste burning and controls and incentives to reduce crop-residue burning. Compositional analysis of OA factors shows municipal waste burning tracers still present in 2018, indicating further scope to reduce burning-related OA. The closure of the two coal power stations, along with initiatives to decrease coal use in industry, businesses, and residential homes, resulted in a significant decrease (87%) in coal-combustion OA. This corresponds to a 17% reduction in total OA, which shows the effectiveness of these measures in reducing PM10. Increases in traffic OA appear to have been offset by the introduction of the Bharat stage emissions standards for vehicles as the increases do not reflect the rapid increase in registered vehicles. However, daytime restrictions on heavy goods vehicles (HGVs) entering the city is linked to large increases in PM10 during the winter and post-monsoon, likely because the large influx of diesel-engine HGVs during the early mornings and evenings is timed with a particularly low planetary boundary layer height that enhances surface concentrations.",Not About Sufficiency
ENTREPRENEURSHIP EDUCATION IN EUROPE NEEDS TO CHANGE,"The European Union (EU) needs to increase its competitiveness in order to remain able to provide high living standards for its citizens and therefore competitiveness in high in the EU political agenda. It is commonly acknowledged that entrepreneurship is crucial for a country's global competitiveness. While the EU countries have consistently high competitiveness rankings in the Global Competitiveness Reports, it is argued that Europe lags the world in entrepreneurship. My analysis shows that not all kinds of entrepreneurship are equally important for the country's competitiveness in the EU. Only intrapreneurship is positively and strongly correlated with the country's competitiveness. This means that in order to increase its competitiveness, Europe needs more intrapreneurship. My analysis confirms that in the EU innovation and competitiveness are related phenomena, as innovation-driven economies are more competitive. Innovation requires entrepreneurial behaviour among employees working in companies i.e. intrapreneurship. But the levels of intrapreneurial activity are still low in the EU countries, which creates a challenge for European companies, especially for small and medium-sized enterprises (SMEs), regarding their competitiveness. Education is the means to increase intrapreneurship in Europe. The sense of initiative and entrepreneurship is included amongst the EU list of eight key competences for lifelong learning. Developing the needed competencies of future employees is the role of higher education institutions (HEIs). This is especially relevant for the European SMEs, that are the engine of the European economy, but have fairly limited resources and capacity for in-service training of their employees. There are two definitions of entrepreneurship education: narrow definition - students are supported to become entrepreneurs themselves, and broad definition - students are supported to act entrepreneurially and acquire entrepreneurial competencies, which they can use in working life, being employed by companies or other organisations. Entrepreneurship programmes in European HEIs typically focus on independent entrepreneurship, rather than developing an entrepreneurial mind-set. Students are prepared to become entrepreneurs, but not intrapreneurs. My analysis shows that entrepreneurship education based on the narrow definition is not correlated with the intrapreneurship levels nor the competitiveness of the countries in the EU. Entrepreneurship courses are also often elective courses in European HEIs and not available for non-business students. These are the major shortcomings of entrepreneurship education offered in Europe. Entrepreneurship education in Europe needs to change if Europe wants to increase its competitiveness. The way forward is to use the broad definition of entrepreneurship education in the design of the curricula of entrepreneurship courses, which should be compulsory for all students, irrespective of their field of study.",Not About Sufficiency
Comparing machine learning to a rule-based approach for predicting suicidal behavior among adolescents: Results from a longitudinal population-based survey,"Introduction: Suicidal thoughts and suicide attempts are one of the most prominent public health concerns in adolescents and therefore early detection is important to initiate preventive interventions and closer monitoring. Method: We examined whether the Machine Learning models Random Forest and Lasso Regression better predict future suicidal behavior than a simple decision rule that classifies every adolescent with history of suicide ideation at baseline as at risk (current practice). We used data from a general population of students in second and fourth year of secondary education in Amsterdam, the Netherlands. Results: Both the Random Forest and the Lasso Regression resulted in slightly better prediction. The AUC of the Random Forest (0.79) and Lasso regression (0.76) were both higher than the AUC of the decision rule (0.64). The Random Forest achieved slightly (but non-significantly) higher sensitivity than the decision rule (0.37 versus 0.34), with the same specificity (0.94). With Lasso Regression the sensitivity increased significantly (0.52), but at the expense of the specificity (0.85). Limitations: The loss of cases after merging the data, the use of self-reported data, confidential data collection and the use of only four questions to measure suicidal behavior. Conclusions: This is the first study applying Machine Learning techniques to predict future suicidal behavior on survey data collected in a general population of adolescents. Our study showed that integrating machine learning techniques in screening practice will result in a small improvement in the ability to predict suicide. The models need to be further optimized to improve accuracy.",Not About Sufficiency
Weather clustering for machine learning-based hourly building energy prediction models at design phase,"With global efforts aimed at reaching carbon neutrality by 2050, there is an increased emphasis on optimizing building energy management. Accurate hourly building energy predictions support crucial tasks such as predicting peak loads for equipment sizing, comparing energy systems, and optimization during the design phase. The main methods used to model building energy are physics-based and data-driven. The former method has been extensively studied, whereas the latter has not been thoroughly investigated. This paper investigates the advantages of using the machine learning (ML) model as a surrogate model in building engineering, specifically for predicting hourly building energy consumption during the design phase. Synthetic data is commonly used for training and testing ML models when real-life measured data is unavailable due to privacy concerns or preconstruction scenarios. However, the challenge arises from the vast dataset of synthetic data generated by combining long-term hourly meteorological data with building characteristics. Using an example building, 82 million data points were generated as a result of simulating 8,760 h when considering ten building performance parameters. To address this issue, a methodology utilizing weather clustering techniques is proposed in this work. This approach aims to reduce dataset size associated with day-by-day simulations by identifying representative weather patterns. Consequently, 7 million data points were generated by identifying ten weather patterns and selecting 30 days, with three days chosen from each cluster. The Extreme Gradient Boosting (XGBoost) algorithm is applied to develop the ML model using the condensed dataset. This model demonstrated commendable performance with testing results that are within the tolerances established by ASHRAE guideline 14. Although we used data from a residential building in Qatar, our application demonstrated that the approaches could be applied to other building types and climate zones. The developed ML model, utilizing easily accessible inputs, can predict hourly building energy consumption. It is user-friendly for non-experts, such as city developers and stakeholders, during the design and retrofit stage.",Not About Sufficiency
"Month-Wise Estimates of Tobacco Smoking During Pregnancy for the United States, 2002-2009","The timing of prenatal exposure to tobacco cigarette smoking can be crucial for the developing fetus. Pushing the field beyond prior pregnancy trimester-focused smoking estimates, we estimated month-specific prevalence proportions for tobacco cigarette smoking among pregnant and non-pregnant women of the United States, with consideration of tobacco dependence (TD) as well. In advance, we posited that pregnancy onset might prompt smoking cessation in early months, before the end of the 1st trimester, and that TD might account for sustained smoking in later months, especially months 8-9, when there are added reasons to quit. Estimates are from the 2002-2009 National Surveys on Drug Use and Health Restricted-Data Analysis System (R-DAS), with large nationally representative samples of US civilians, including 12-44 year old women (n similar to 70,000) stratified by pregnancy status and month of pregnancy, with multi-item assessment of TD as well as recently active smoking. Age was held constant via the Breslow-Day indirect standardization approach, a methodological detail of potential interest to other research teams conducting online R-DAS analyses. Among 12-44 year old women in Month 1 of pregnancy, as well as non-pregnant women, just over one in four was a recently active smoker (26-27 %), and approximately one-half of these smokers qualified as a TD case (52 %). Corresponding estimates for women in Month 3 were 17.6 % and two-thirds, respectively, lending some support for our advance hypotheses. Nonetheless, our a priori TD hypothesis about Months 8-9 seems to be contradicted: an increased concentration of TD among smokers surfaced early in pregnancy. Evidence of a possible ameliorative pregnancy effect on smoking prevalence as well as TD's effect on smoking persistence might be seen quite early in pregnancy. Substitution of a month-specific view for the traditional trimester view sheds new light on how pregnancy might shape smoking behavior before the end of trimester 1, with TD seeming to thwart a public health goal of 100 % cessation, early in pregnancy.",Not About Sufficiency
Long-term changes of marine subtidal benthic communities in North East Asia (Yellow and Japan seas) in a global change context: A review,"A review of the long-term changes and variations in benthic communities and the current status of the marine invasive species (MIS) in shallow waters of the Yellow Sea (Chinese sector) and the Sea of Japan (Russian and partly Korean sectors) is presented. This paper reflects on the progress and lessons learned, recommending actions for the future about the conservation of biodiversity. In the Bohai Sea, the benthic ecosystem has been degenerating due to anthropogenic activities such as overfishing and pollution since the 1950s. The dominant position of K-strategy species is gradually being lost and replaced by R-strategy species. In the Yellow Sea, the macrobenthic community is different from other areas due to the Yellow Sea Cold Water Mass. Many economic species have been destroyed, and the biotic structure has changed significantly due to overfishing and climate change. In the Russian sector of the Sea of Japan, the macrobenthic communities in the shallow-water soft bottom have generally been in a stable condition for the last decades, except for some heavily polluted or disturbed areas due to dredging operations. The abundance of select large invertebrate species has changed considerably due to commercial fishing and poaching. Variations in macro- and meiobenthic communities under aquaculture conditions have occurred on a local scale during the last five decades. MIS show obvious differences between China and Russia in the following aspects: introduction pathways of MIS, composition and number of non-native species, threats and impacts of MIS to native communities and ecosystems, and economic and public health impacts. Long-term monitoring programmes should be developed to reveal future biotic changes and to separate the effects of cyclic variations of benthic communities from the impacts of pollution and eutrophication. Standardization of sampling procedures is required to compare changes/alternations in benthos across various regions worldwide.",Not About Sufficiency
GAMING AS A DISEMBODIED EXPERIENCE OF THE CITY: FROM ASSASSIN'S CREED TO 'SMART LEARNER',"This paper explores the role of gaming as a learning tool in the design disciplines and suggests a methodology of work that bridges urban planning and virtually reconstructed environments. Building on the growing body of literature at the intersection of gaming, learning, and urban planning, the paper analyzes the simulation of the city of Rome in the Assassin's Creed video game and argues for the reliability of its morphology in relation to the real city. Along these lines, the paper confirms the didactic relevance to the transition from the real city to the digital one, allowing for the creation of a theoretical and empirical reflection on the method for use in an educational setting. The paper builds on the idea of how gaming offers a disembodied experience of the city, arguing for its didactic and social impact on a new generation of ""smart learners.""",Not About Sufficiency
Negotiating Authority in Global Biofuel Governance: Brazil and the EU in the WTO,"The global demand for biofuels (liquid or gas fuels deriving from biomass) has grown dramatically in recent years. European Union policies that promote biofuels as more sustainable sources of transport fuel are partly driving this development. In this article, we analyze how Brazil, as a key producer of biofuels, navigates an emerging global governance context for sustainable biofuels. We do so by examining how Brazil responds to EU biofuel sustainability imperatives, including by evoking World Trade Organization disciplines in questioning their transnational validity and reach. While domestically Brazil emphasizes the social and developmental objectives of its biofuel policies, globally it frames itself as a leading producer of sustainable biofuels. In so doing, it navigates intersecting spheres of authority in a manner that promotes its own biofuel policy agenda, partly by seeking to reframe ""sustainability"" debates internationally to reflect its developmental agenda.",Not About Sufficiency
Sharing the effort of the European Green Deal among countries,"In implementing the European Green Deal to align with the Paris Agreement, the EU has raised its climate ambition and in 2022 is negotiating the distribution of increased mitigation effort among Member States. Such partitioning of targets among subsidiary entities is becoming a major challenge for implementation of climate policies around the globe. We contrast the 2021 European Commission proposal - an allocation based on a singular country attribute - with transparent and reproducible methods based on three ethical principles. We go beyond traditional effort-sharing literature and explore allocations representing an aggregated least regret compromise between different EU country perspectives on a fair allocation. While the 2021 proposal represents a nuanced compromise for many countries, for others a further redistribution could be considered equitable. Whereas we apply our approach within the setting of the EU negotiations, the framework can easily be adapted to inform debates worldwide on sharing mitigation effort among subsidiary entities. An ethically-based method for allocating climate change mitigation effort among subsidiaries, applicable worldwide, is proposed. Applied to the EU Green Deal, this results in a wider range of targets than the Commission's proposal of 2021.",Not About Sufficiency
A LDA-Based Social Media Data Mining Framework for Plastic Circular Economy,"The mass production of plastic waste has caused an urgent worldwide public health crisis. Although government policies and industrial innovation are the driving forces to meet this challenge, trying to understand public attitudes may improve the efficiency of this process. Social media has become the main ways for the public to obtain information and express opinions and feelings. This motivated us to mine the perceptions and behavioral responses towards plastic usage using social media data. In this paper, we proposed a framework for data collection and analysis based on mainstream media in the UK to obtain public opinions on plastics. An unsupervised machine learning model based on Latent Dirichlet Allocation (LDA) has been employed to analyse and cluster the topics to deal with the lack of annotation of the data contents. An additional dictionary method was then proposed to evaluate the sentiment of the comments. The framework also provides tools to visualise the model and results to stimulate insightful understandings. We validated the framework's effectiveness by applying it to analyse three mainstream social media, where 6 first-level topic categories and 13 second-level topic categories from the comment texts related to plastics have been identified. The results show that public sentiment towards plastic products is generally stable. The spatiotemporal distribution of each topic's sentiment is highly correlated with the number of occurrences.",Not About Sufficiency
DESIGNING XML PIVOT MODELS FOR MASTER DATA INTEGRATION VIA UML PROFILE,"The federation of data sources and the definition of pivot models are strongly interrelated topics. This paper explores the difficulties of a mediation solution based on XML architecture and the concept of Master Data Management. In this solution, pivot models use the standard XML Schema allowing the definition of complex data structures. To date, the graphical modeling of XML Schema models is not standardized. The introduction of a models definition formalism is a mean to make modeling more accessible. UML is a modeling object language which is more and more used and recognized as a standard in the software engineering field, which makes it an ideal candidate for the modeling of XML Schema models. In this purpose we introduce features of the UML formalism, through a profile, to facilitate the collaborative definition and the exchange of these models, and to introduce the capacity to express semantic constraints in XML models. These constraints will be used to perform data factorisation and to optimise data operations.",Not About Sufficiency
Global rainfall erosivity database (GloREDa) and monthly R-factor data at 1 km spatial resolution,"Here, we present and release the Global Rainfall Erosivity Database (GloREDa), a multi-source platform containing rainfall erosivity values for almost 4000 stations globally. The database was compiled through a global collaboration between a network of researchers, meteorological services and environmental organisations from 65 countries. GloREDa is the first open access database of rainfall erosivity (R-factor) based on hourly and sub-hourly rainfall records at a global scale. This database is now stored and accessible for download in the long-term European Soil Data Centre (ESDAC) repository of the European Commission's Joint Research Centre. This will ensure the further development of the database with insertions of new records, maintenance of the data and provision of a helpdesk. In addition to the annual erosivity data, this release also includes the mean monthly erosivity data for 94% of the GloREDa stations. Based on these mean monthly R-factor values, we predict the global monthly erosivity datasets at 1 km resolution using the ensemble machine learning approach (ML) as implemented in the mlr package for R. The produced monthly raster data (GeoTIFF format) may be useful for soil erosion prediction modelling, sediment distribution analysis, climate change predictions, flood, and natural disaster assessments and can be valuable inputs for Land and Earth Systems modelling. (c) 2023 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/)",Not About Sufficiency
"Perceived Impact of the Parental Rights in Education Act (""Don't Say Gay"") on LGBTQ plus Parents in Florida","Prior studies indicate that anti-LGBTQ+ legislation has negative consequences for the well-being of LGBTQ+ people, their families, and their communities. In July of 2022, Florida's Parental Rights in Education Act, also called the ""Don't Say Gay"" bill, was signed into law. The law aimed to limit K-3 instruction and discussion related to sexuality and gender, encompassing LGBTQ+ identities. The present study surveyed 113 LGBTQ+ parents in Florida about their experiences and perceptions related to the Parental Rights in Education Act. Qualitative content analysis revealed five major themes and 14 subthemes, including: (a) living in Florida: pros and cons; (b) initial reactions to the law; (c) feelings over time; (d) coping with worries; and (e) beyond ""coping"": considering the future and possibility of relocation. Recommendations center on the need for counseling psychologists to use their privilege and training to advocate on behalf of LGBTQ+ parents, families and others impacted by this legislation.",Not About Sufficiency
Who benefits from AVs? Equity implications of automated vehicles policies in full-scale prototype cities,"While researchers have stressed the potential of automated vehicle (AV) technology in improving mobility and accessibility for a range of people, only a few attempts have been made to examine the impact of this new technology on different segments of the population in a realistic setting using high-fidelity simulation. To fill this gap, we analyze the equity implications of Automated Mobility-on-Demand (AMoD) in three full-scale prototype cities using SimMobility, a state-of-theart activity- and agent-based framework. The prototype cities were developed based on two autodependent typologies, representing cities largely in the US/Canada, and a dense transit-oriented typology. We perform equity analyses at the individual and income-group level, in order to reveal the winners and losers from the introduction of AVs under two scenarios: (1) AMoD Intro, in which a low-cost AMoD service competes with mass transit, and (2) AMoD Transit Integration, where AMoD complements mass transit, via access/egress connectivity service to rapid transit stations. We evaluate the following outcomes: induced demand by age and income groups, mode share by income levels, individual kilometers traveled by different modes and income levels, and the spatial distribution of change in fare and accessibility. Outcomes are considered as equityoriented if they reduce accessibility gaps, particularly among disadvantaged populations. Our results indicate that in large population-dense and transit-oriented cities, the most equity-oriented outcomes can be achieved, due to extensive mass transit usage, which depresses car usage and restricts induced demand for AMoD. Such cities provide greater opportunities for low-income groups. Specifically, the AMoD Transit Integration scenario results in the best outcomes and implies a new market share, as disadvantaged groups, such as children and low-income individuals, were able to travel more using the integrated AMoD-transit service. Nevertheless, in cardependent cities, where accessibility gaps are much larger, AMoD Intro scenario performs better compared to AMoD Transit Integration, as it serves the less accessible population and significantly improves their opportunities.",Not About Sufficiency
The role of peer effects and the built environment on individual travel behavior,"While urban planners and transportation geographers have long emphasized the importance of social influences on individual travel behavior, many challenges remain to bridge the gap between complex conceptual frameworks and operational behavioral models. Improving the ability of models to forecast activity-travel behavior can provide greater insights into urban planning issues. This paper proposes a new model framework by evaluating how individual travel behavior is influenced by inter- and intra-household interactions. The built environment, land-use mix, and social interactions influence household member choices among different transport modes. We propose a spatial multivariate Tobit specification that allows each individual to face a set of potential destinations and transport modes and takes into consideration the travel behavior of other household members and nearby neighbors. Using the Greater Cincinnati Household Travel Survey, we analyzed more than 37,000 trips made by 1968 individuals located in Hamilton County in Cincinnati, Ohio. Results reveal that social influences and the built environment have a strong impact on the willingness to walk and to cycle.",Not About Sufficiency
"Antenatal care is associated with adherence to iron supplementation among pregnant women in selected low-middle-income-countries of Asia, Africa, and Latin America & the Caribbean regions: Insights from Demographic and Health Surveys","Anaemia is a global public health problem affecting 800 million women and children globally. Anaemia is associated with perinatal mortality, child morbidity and mortality, mental development, immune competence, susceptibility to lead poisoning and performance at work. The objective of this article is to identify whether antenatal care-seeking was associated with the uptake of iron supplementation among pregnant women, adjusting for a range of covariates. This article used data from the cross-sectional recent Demographic and Health Surveys (DHS) of 12 countries in Asia, Africa and Latin America & the Caribbean regions. The individual-level data from 273,144 women of reproductive age (15-49 years) were analysed from multi-country DHS. Multiple Logistic regression analyses were conducted using Predictive Analytics Software for Windows (PASW), Release 18.0. Receiving at least four antenatal care visits was significantly associated with the consumption of 90 or more iron-containing supplements in 12 low and middle income countries across three regions after adjusting for different household and respondent characteristics, while mass media exposure was found to be a significant predictor in India and Indonesia. Antenatal care seems to be the most important predictor of adherence to iron intake in the selected countries across Africa, Asia, Latin America and Caribbean regions.",Not About Sufficiency
"Inferred networks, machine learning, and health data","This paper presents a network science approach to investigate a health information dataset, the Sexual Acquisition and Transmission of HIV Cooperative Agreement Program (SATHCAP), to uncover hidden relationships that can be used to suggest targeted health interventions. From the data, four key target variables are chosen: HIV status, injecting drug use, homelessness, and insurance status. These target variables are converted to a graph format using four separate graph inference techniques: graphical lasso, Meinshausen Buhlmann (MB), k-Nearest Neighbors (kNN), and correlation thresholding (CT). The graphs are then clustered using four clustering methods: Louvain, Leiden, and NBR-Clust with VAT and integrity. Promising clusters are chosen using internal evaluation measures and are visualized and analyzed to identify marker attributes and key relationships. The kNN and CT inference methods are shown to give useful results when combined with NBR-Clust clustering. Examples of cluster analysis indicate that the methodology produces results that will be relevant to the public health community.",Not About Sufficiency
The role of artificial intelligence in the future of urogynecology,"Artificial intelligence (AI) in medicine is a rapidly growing field aimed at using machine learning models to improve health outcomes and patient experiences. Many new platforms have become accessible and therefore it seems inevitable that we consider how to implement them in our day-to-day practice. Currently, the specialty of urogynecology faces new challenges as the population grows, life expectancy increases, and quality of life expectation is much improved. As AI has a lot of potential to promote the discipline of urogynecology, we aim to explore its abilities and possible use in the future. Challenges and risks are associated with using AI, and a responsible use of such resources is required.",Not About Sufficiency
Diffusion in nanoporous materials with special consideration of the measurement of determining parameters (IUPAC Technical Report),"The random motion (the diffusion) of guest molecules in nanoporous host materials is key to their manifold technological applications and, simultaneously, a ubiquitous phenomenon in nature quite in general. Based on a specification of the different conditions under which molecular diffusion in nanoporous materials may occur and of the thus resulting relevant parameters, a survey of the various ways of the measurement of the determining parameters is given. Starting with a condensed introduction to the respective measuring principles, the survey notably includes a summary of the various parameters accessible by each individual technique, jointly with an overview of their strengths and weaknesses as well as of the respective ranges of observation. The presentation is complemented by basic relations of diffusion theory and molecular modeling in nanoporous materials, illustrating their significance for enhancing the informative value of each measuring technique and the added value attainable by their combination. By providing guidelines for the measurement and reporting of diffusion properties of chemical compounds in nanopores, the document aims to contribute to the clarification and standardization of the presentation, nomenclature, and methodology associated with the documentation of diffusion phenomena in nanoporous materials serving for catalytic, mass separation, and other relevant purposes.",Not About Sufficiency
Lancet Commission on Hypertension Group position statement on the global improvement of accuracy standards for devices that measure blood pressure,"The Lancet Commission on Hypertension identified that a key action to address the worldwide burden of high blood pressure (BP) was to improve the quality of BP measurements by using BP devices that have been validated for accuracy. Currently, there are over 3 000 commercially available BP devices, but many do not have published data on accuracy testing according to established scientific standards. This problem is enabled through weak or absent regulations that allow clearance of devices for commercial use without formal validation. In addition, new BP technologies have emerged (e.g. cuffless sensors) for which there is no scientific consensus regarding BP measurement accuracy standards. Altogether, these issues contribute to the widespread availability of clinic and home BP devices with limited or uncertain accuracy, leading to inappropriate hypertension diagnosis, management and drug treatment on a global scale. The most significant problems relating to the accuracy of BP devices can be resolved by the regulatory requirement for mandatory independent validation of BP devices according to the universally-accepted International Organization for Standardization Standard. This is a primary recommendation for which there is an urgent international need. Other key recommendations are development of validation standards specifically for new BP technologies and online lists of accurate devices that are accessible to consumers and health professionals. Recommendations are aligned with WHO policies on medical devices and universal healthcare. Adherence to recommendations would increase the global availability of accurate BP devices and result in better diagnosis and treatment of hypertension, thus decreasing the worldwide burden from high BP.",Not About Sufficiency
Improved screening of fall risk using free-living based accelerometer data,"Falls are one of the most costly population health issues. Screening of older adults for fall risks can allow for earlier interventions and ultimately lead to better outcomes and reduced public health spending. This work proposes a solution to limitations in existing fall screening techniques by utilizing a hip-based accelerometer worn in free-living conditions. The work proposes techniques to extract fall risk features from periods of free-living ambulatory activity. Analysis of the proposed techniques is conducted and compared with existing screening methods using Functional Tests and Lab-based Gait Analysis. 1705 Older Adults from Umea (Sweden) were assessed. Data consisted of 1 Week of hip worn accelerometer data, gait measurements and performance metrics for 3 functional tests. Retrospective and Prospective fall data were also recorded based on the incidence of falls occurring 12 months before and after the study commencing respectively. Machine learning based ex-periments show accelerometer based measures perform best when predicting falls. Prospective falls had a sensitivity and specificity of 0.61 and 0.66 respectively while retrospective falls had a sensitivity and specificity of 0.61 and 0.68 respectively.",Not About Sufficiency
Skateboarding as Social and Environmental Praxis: Navigating a Sustainable Future,"Young people's political agency manifests in ways that are different from adults and that agency is typified by different spatial, mobile, and place-based dynamics. Varied expressions of political agency among young people exist, among them those responding to pressing global challenges that implicate social justice, climate change, and urbanization. Transport is one of the sectors of social and spatial life both deeply affected by and strongly influencing these particular challenges. Among some young people, several responses exist in relation to the perennial questions about how to be mobile: walk, cycle, blade, skate, practice parkour, catch public transport, drive cars, or use combinations of these modes. For some, other supports such as wheelchairs are used. This paper focuses on one of these: skateboarding. The claim that is examined here is whether, how, to what extent, and with what effects skateboarding can be (and be seen to be) both social and environmental praxis. Consideration is given to how to comprehend skateboarding as a means to mitigate and adapt to climate change, to be mobile, to claim a place in public spaces, and to strike a balance between consuming and producing wisely. What might skateboarding as social and environmental praxis reveal about youthful politics, agency, political rights, and protection, and what new insights might emerge from these reflections?",Not About Sufficiency
Toward a generalizable machine learning workflow for neurodegenerative disease staging with focus on neurofibrillary tangles,"Machine learning (ML) has increasingly been used to assist and expand current practices in neuropathology. However, generating large imaging datasets with quality labels is challenging in fields which demand high levels of expertise. Further complicating matters is the often seen disagreement between experts in neuropathology-related tasks, both at the case level and at a more granular level. Neurofibrillary tangles (NFTs) are a hallmark pathological feature of Alzheimer disease, and are associated with disease progression which warrants further investigation and granular quantification at a scale not currently accessible in routine human assessment. In this work, we first provide a baseline of annotator/rater agreement for the tasks of Braak NFT staging between experts and NFT detection using both experts and novices in neuropathology. We use a whole-slide-image (WSI) cohort of neuropathology cases from Emory University Hospital immunohistochemically stained for Tau. We develop a workflow for gathering annotations of the early stage formation of NFTs (Pre-NFTs) and mature intracellular (iNFTs) and show ML models can be trained to learn annotator nuances for the task of NFT detection in WSIs. We utilize a model-assisted-labeling approach and demonstrate ML models can be used to aid in labeling large datasets efficiently. We also show these models can be used to extract case-level features, which predict Braak NFT stages comparable to expert human raters, and do so at scale. This study provides a generalizable workflow for various pathology and related fields, and also provides a technique for accomplishing a high-level neuropathology task with limited human annotations.",Not About Sufficiency
P08-03 Integrating public health expertise to support green space planning by promoting active lifestyles in Slovenia,,Not About Sufficiency
Estimating global burden of COVID-19 with disability-adjusted life years and value of statistical life metrics,"Background: Global burden of COVID-19 has not been well studied, disability-adjusted life years (DALYs) and value of statistical life (VSL) metrics were therefore proposed to quantify its impacts on health and economic loss globally. Methods: The life expectancy, cases, and death numbers of COVID-19 until 30th April 2021 were retrieved from open data to derive the epidemiological profiles and DALYs (including years of life lost (YLL) and years loss due to disability (YLD)) by four periods. The VSL estimates were estimated by using hedonic wage method (HWM) and contingent valuation method (CVM). The estimate of willingness to pay using CVM was based on the meta-regression mixed model. Machine learning method was used for classification. Results: Globally, DALYs (in thousands) due to COVID-19 was tallied as 31,930 from Period I to IV. YLL dominated over YLD. The estimates of VSL were US$591 billion and US$5135 billion based on HWM and CVM, respectively. The estimate of VSL increased from US$579 billion in Period I to US$2160 billion in Period IV using CVM. The higher the human development index (HDI), the higher the value of DALYs and VSL. However, there exits the disparity even at the same level of HDI. Machine learning analysis categorized eight patterns of global burden of COVID-19 with a large variation from US$0.001 billion to US$691.4 billion. Conclusion: Global burden of COVID-19 pandemic resulted in substantial health and value of life loss particularly in developed economies. Classifications of such health and economic loss is informative to early preparation of adequate resource to reduce impacts. Copyright (C) 2021, Formosan Medical Association. Published by Elsevier Taiwan LLC.",Not About Sufficiency
Theta Autoregressive Neural Network: A Hybrid Time Series Model for Pandemic Forecasting,"Forecasting time series present a perpetual topic of research in statistical machine learning for the last five decades. Due to the unprecedented outbreak of the novel coronavirus (COVID-19), forecasting the COVID-19 pandemic became a key research interest for both epidemiologists and statisticians. These future predictions are useful for the effective allocation of health care resources, stockpiling, and help in strategic planning for clinicians, government authorities, and public-health policymakers. This paper develops an effective forecasting model that can generate real-time short-term (ten days) and long-term (fifty days) out-of-sample forecasts of COVID-19 outbreaks for eight profoundly affected countries, namely the United States of America, Brazil, India, Russia, South Africa, Mexico, Spain, and Iran. A novel hybrid approach based on the Theta method and Autoregressive neural network (ARNN) model, named Theta-ARNN (TARNN) model, is proposed. The proposed method outperforms previously available single and hybrid forecasting models for COVID-19 predictions in most data sets. The ergodicity and asymptotic stationarity of the TARNN model are also studied.",Not About Sufficiency
Zika Virus Prediction Using AI-Driven Technology and Hybrid Optimization Algorithm in Healthcare,"The Zika virus presents an extraordinary public health hazard after spreading from Brazil to the Americas. In the absence of credible forecasts of the outbreak's geographic scope and infection frequency, international public health agencies were unable to plan and allocate surveillance resources efficiently. An RNA test will be done on the subjects if they are found to be infected with Zika virus. By training the specified characteristics, the suggested Hybrid Optimization Algorithm such as multilayer perceptron with probabilistic optimization strategy gives forth a greater accuracy rate. The MATLAB program incorporates numerous machine learning algorithms and artificial intelligence methodologies. It reduces forecast time while retaining excellent accuracy. The projected classes are encrypted and sent to patients. The Advanced Encryption Standard (AES) and TRIPLE Data Encryption Standard (TEDS) are combined to make this possible (DES). The experimental outcomes improve the accuracy of patient results communication. Cryptosystem processing acquires minimal timing of 0.15 s with 91.25 percent accuracy.",Not About Sufficiency
Signature based ransomware detection based on optimizations approaches using RandomClassifier and CNN algorithms,"As Ransomware encrypts user files to prevent access to infected systems its harmful impacts must be quickly identified and remedied. It can be challenging to identify the metrics and parameters to check, especially when using unknown ransomware variants in tests. The proposed work uses machine learning techniques to create a general model that can be used to detect the variations of ransomware families while observing the characteristics of malware. However, early detection is impeded by a dearth of data during the initial phases of an attack, which results in low detection accuracy and a high proportion of false alarms. To overcome these restrictions, our research suggests a revolutionary technique, in machine learning techniques we have proposed RandomClassifier with SMOTE optimizer based on the results received from LazyPredictAutoML, and then deep learning algorithm ANN using Root Mean Square Propagation (Adam) has been implemented to get the hidden patterns which were not accessible in the machine learning approach. The further study focused on improving CNN's performance over RMSprop & Adam, which maintains per-parameter learning rates that are adjusted based on the average of most recent weight gradient magnitudes, using the Adam optimizer. The best option for internet and non-stationary issues is CNN with Adam (e.g. noisy). As gradients grow sparser toward the end of optimization, Adam somewhat surpasses RMSprop. Adam uses CNN and uses the average of the second moments of the gradients (the uncentered variance). The proposed model reported 5.14 ms of prediction time and 99.18% accuracy.",Not About Sufficiency
A Digital Twin Driven Human-Centric Ecosystem for Industry 5.0,"Industry 5.0 embodies the vision for the future of factories, emphasizing the importance of sustainable industrialization and the role of industry in society, through the key concept of placing the well-being of workers at the center of the production process. Building upon this vision, we propose a new paradigm to design human-centric industrial applications. To this end, we exploit Digital Twin (DT) technology to build a digital replica for each entity on the shop floor and support and augment interaction among workers and machines. While so far DTs in automation have been proposed for machine digitalization, the core element of the proposed approach is the Operator Digital Twin (ODT). In this scenario, biometrics allows to build a reliable model of those operator's characteristics that are relevant in working contexts. Biometric traits are measured and processed to detect physical, emotional, and mental conditions, which are used to define the operator's state. Perspectively, this allows to manage and monitor production and processes in an operator-in-the-loop manner, where not only is the operator aware of the state of the plant, but also any technological agent in the plant acts and reacts according to the operator's needs and conditions. In this paper, we define the modeling of the envisioned ecosystem, present the designed DT's blue-print architecture, discuss its implementation in relevant application scenarios, and report an example of implementation in a collaborative robotics scenario. Note to PractitionersuThis paper was motivated by the problem of designing human-cyber-physical systems, where production processes are managed by concurrently taking into account operators, machines and plant status. This answers the needs of the novel Industry 5.0 paradigm, which aims to enhance social sustainability of modern factories. To this end, we propose an architecture based on digital twins that allows to develop a digital layer, detached from the physical one, where the plant can be monitored and managed. This allows the creation of a digital ecosystem where machines, operators, and the interactions among them are represented, augmented, and managed. We discuss how the proposed architecture can be applied to three relevant scenarios: remote training and maintenance, line operation and line supervision. Moreover, the implementation in a collaborative robotics scenario is presented, to provide an example of the proposed architecture can be implemented in industrial scenarios.",Not About Sufficiency
Rapid Bacterial Detection and Identification of Bacterial Strains Using Machine Learning Methods Integrated With a Portable Multichannel Fluorometer,"Rapid and sensitive bioburden detection is of paramount importance in different applications including public health, and food and water safety. To overcome the traditional limitations of bacterial detection i.e., lengthy culture time, and complicated procedure, a low-cost, portable multichannel fluorometer coupled with machine learning (ML) has been implemented in this study. Five different strains of bacterial samples were tested along with the negative control for time-series fluorescence data collection and analysis. We applied different conventional unsupervised and supervised machine learning techniques with extracted features followed by preprocessing of the data. Initially, machine learning algorithms were applied for the qualitative detection of bacteria by binary classification followed by regression analysis to predict the level of contamination for E. coli. The multiclass classification was used to identify gram-positive, and gram-negative bacterial strains and differentiate all the bacterial strains tested. Our results show that around 97.9% accuracy can be achieved for bacterial contamination detection for as low as 1 CFU/mL while 92.1% accuracy can be achieved for differentiating the gram-positive and gram-negative strains. Additionally, with 1 minute of data, high accuracy is obtained for detecting bioburden, proving the multichannel fluorometer's rapid detection capability. The multichannel fluorometer integrated with ML analytics is capable of automating data analysis and determining accurate and rapid bacterial detection on-site with the prediction of bioburden levels and differentiating bacterial strains and the protocol can be applied to the biosensors with a similar data type.",Not About Sufficiency
Climate justice and territory,"The territorial impacts of climate change will affect millions. This will happen not only as a direct consequence of climate change, but also because of policies for mitigating it-for example, through the installation of large wind and solar farms, the conservation of land in its role as carbon sink, and the extraction of materials needed for renewable energy technologies. In this article, we offer an overview of the justice-related issues that these impacts create. The literature on climate justice and territory is vast and spans a range of disciplines, so we limit our discussion to a specific understanding of territory and a specific understanding of injustice that arises from its loss. We understand territory as a normative concept that describes a place under some agent's jurisdiction, where the agent is a politically organized collective and where the jurisdictional rights over that place secure a relevant degree of self-determination for that collective. Accordingly, we consider that the main injustice connected to the loss of territory due to climate change is the loss or undermining of the ability to exercise the collective right to self-determination, which requires some control over the place. This can happen if a territorial agent literally loses the ground where to stand as a direct effect of climate change, raising issues of justice in relocation; or if their place changes due to mitigation policies, affecting their use and understanding of territory, raising issues of justice in energy transition. In concluding, we point to topics for future research.This article is categorized under:Climate, Nature, and Ethics > Climate Change and Global JusticeClimate, Nature, and Ethics > Climate Change and Human RightsPolicy and Governance > Governing Climate Change in Communities, Cities, and Regions",Not About Sufficiency
Spatial capability and A qualitative study of happiness in Mexico City and San Jos�,"This qualitative study focuses on how individuals living in two that rank highest in the happiness index develop and sustain their happiness research employs quantitative methods to compare national recent studies advocate for a contextual and process-focused This research emphasises the spatial aspects of happiness and hance it, contributing to a deeper understanding of happiness in The cases are Mexico City, Mexico and San Jos & eacute;, Costa Rica, which high in happiness despite their low GDP. A total of 16 in-depth out online with participants living in each of the case study sites. that interviewees' happiness is shaped by context-specific factors curity, and housing. The COVID-19 pandemic, which restricted housing quality, highlighted barriers to spatial capability. The viduals developed happiness strategies and community participation satisfaction and happiness capability. Conformity-driven happiness phenomenon, essential to life satisfaction. The paper contributes standing of happiness in different environments and people's happiness.Highlights for public administration, management and planning: Influence of Environmental and Social Factors on Happiness: For tration and urban planning, this study highlights how basic life as creating and maintaining environments, enhance residents' piness. Role of Spatial Capability in Happiness: For public policy and concept of spatial capability emphasises the need to improve design and transportation to support citizens' overall happiness Impact of Social Support and Conformity on Well-being: For public and community planning, fostering strong social networks and health initiatives can be vital in promoting residents' happiness being. Implications for Public Policy and Urban Planning: The findings policies should aim to enhance people's capabilities by providing efficient mobility, and support for social groups. This approach happier, more sustainable communities.",Not About Sufficiency
Exploration on the level and influencing factors of rural teachers ' self-identification of social identity from the perspective of space-Based on the machine learning model of SHAP interpretation method,"Embodied cognition theory suggests that cognition is the cognition of the body, including the brain, and the body is embedded in the environment, intuition, body and environment are a unity. Therefore, the subjective experience of teachers' body in rural society provides the basic content for their teaching, and the more rural teachers identity with rural society, the more likely they are to carry out high-quality teaching of productive pedagogical construct based on rural society. However, the irregular flow of capital labels the social identity formed by being removed from the marginal space of production capacity and production factors with obvious weaknesses. An analysis based on the eXtreme Gradient Boosting (XGBoost) algorithm combined with the Shapley Additive exPlanations (SHAP) approach balances the requirements of discriminatory accuracy and indicator interpretability in actual situations. The study found that identity was at an intermediate level, except for emotional identity, which was highest among peri-urban rural teachers, followed by remote rural teachers, and lowest among teachers in rural centres, while overall identity, idea identity and action identity were from highest to lowest among peri-urban rural teachers, rural centre teachers and remote rural teachers. There are structural differences in the social identity of teachers by gender, age, education and status. By calculating SHAP values and using the machine learning models RF, DT and GBDT as robustness checks, it was found that overall and action identities were mainly shaped by economic capital and less influenced by social, cultural and symbolic capital; idea identity was jointly determined by cultural, social and economic capital; in contrast, emotional identity was less influenced by economic capital and mainly depended on social capital. The spatial justice layout of economic capital, the correction of cultural capital and the accumulation of social capital are the fundamental ways to enhance the level of self-identification of rural teachers' social identity.",Not About Sufficiency
An Analytical Framework for Assessing Equity of Access to Public Electric Vehicle Charging Stations: The Case of Shanghai,"With the unprecedented growth of electric vehicles usage, the equitable population-based provision of public charging services has become an important concern in high-density urban centers. To address sustainability concerns, this study explores an analytical framework for assessing the equity of access to public charging services. By comprehensively analyzing factors such as accessibility, the Gini coefficient, the correlation coefficient, and supply-demand matching, we investigated the unequal access to public charging stations within 24 types of sites in central Shanghai. The spatial distribution and accessibility were visualized to illustrate differences in service access. Subsequently, social equity was assessed by considering the population distribution and identifying areas of supply-demand imbalance. The results show that 81% of households share only 10% of public charging services, suggesting a generalized inequality within areas and facilities. Residents of large-scale, low-density, low-grade neighborhoods have difficulties accessing services. Nearly 66.96% of subdistricts have supply and demand conflicts. In addition, priority types of improvement were identified and directions for improvement were suggested, as well as recommendations for the integration of PCSs with exterior built places. We also found significant differences in accessibility and equity at both the district and subdistrict level. The findings of this study will help urban planners assess and locate unequal areas and provide insights and the basis for further expansion into the analysis methods adopted at different stages to achieve sustainable development.",Not About Sufficiency
Challenges to achieving greater and fairer stakeholder involvement in marine spatial planning as illustrated by the Lyme Bay scallop dredging closure,"The statutory closure of 60 square miles of Lyme Bay to towed fishing gear in 2008 marked the culmination of nearly 20 years of discussions between conservationists and fishermen. Prior to the 2008 decision, voluntary, bottom-up led agreements resulted in inadequate protection of the bay's biological resources and significant erosion of social capital. Lyme Bay provides an excellent case study of the challenges likely to be posed by wider stakeholder involvement imposed under the new Marine Act. This paper examines a broad range of perspectives in relation to the Lyme Bay consultation and subsequent closure, via semi-structured interviews with 25 representatives of different interest groups in Lyme Bay. All respondents acknowledged significant flaws in the process leading to the Lyme Bay decision and felt the Marine Act was well placed to tackle many of these criticisms. However, while the Marine Act should provide a framework for resolving conflicts, it will not prevent them. Success will depend on collaboration between different marine interests, and also on the government acknowledging that outcomes are unlikely to favour everyone in the short-term and that top-down interventions are inevitable. (C) 2011 Elsevier Ltd. All rights reserved.",Not About Sufficiency
"Reversing displacement: Navigating the spontaneity of spatial networks of craft, tradition and memory in post-war Old Mosul","The destruction of Mosul's Old Town has led to sudden and unmanaged displacements of different ethnic, cultural and professional communities who departed northern Iraq's medieval trade and cultural centre. While the reconstruction of historic monuments was prioritised for the post-ISIS recovery process, the disappearance of trade, culture, and communities had a more lasting impact on the erasure of memory, traditional practices and social interactions in the Historic Centre. Moving away from the conventions of planned and structured return in post-conflict cities, this paper investigates the growing and unstructured spontaneous processes of displacement, relocation, and rebuilding as an unmanaged process where the central government and the local authority had limited impact on the daily and active return of displaced communities and craftsmen. We argue that the active and interconnected networks of trade, craft communities and livelihoods in the Old City can be activated by individualistic efforts to trigger a spontaneous, yet effective and decentralised approach to post-conflict return in Iraq. This paper navigates local narratives, spaces of memory and spatial patterns of displacement and return, using the observations, spatial mapping, first-hand local narratives and flows of displacement.",Not About Sufficiency
Understanding power positions in a new digital landscape: perceptions of Syrian refugees and data experts on relocation algorithm,"This study explores the differences and similarities between the perceptions of data experts and refugees as data subjects, in the context of a refugee relocation algorithm. The study conducted in-depth interviews with data experts and Syrian refugees in Estonia and Turkey. The results indicate that both refugees and data experts acknowledge the algorithms' potential power for structuring the everyday life experiences of people. Whereas refugees mainly focused on cultural and social concerns, the data experts underlined the importance of refugees' agency and the potential drawbacks of algorithms in terms of transparency and accountability. While both groups of interviewees thought the relocation algorithm could be useful especially in economic terms, the study demonstrates that algorithms create complex power relations and place extra pressure on both refugees and data experts. The new digital landscapes produced by algorithms entail a 'triple agency' - an agency of experts developing and using these datafied solutions, an agency of data subjects being targets of those calculations, and an agency of algorithms. For solving the issue of 'false authority', where the modelling of spatial choice cannot grasp the socio-cultural reality, it is necessary to consider the socio-cultural context of the calculative devices. A paradigm shift in machine learning is necessary from learning machines as autonomous subjects to machines learning from social contexts and individuals' experiences. Rather than experimenting with algorithmic solutions to speed up decisions about human lives, migration policies and relevant datafied solutions should consider the diversity of human experiences expressed in individuals' everyday life.",Not About Sufficiency
COVID-19 studies involving machine learning methods: A bibliometric study,"Background:Machine learning (ML) and artificial intelligence (AI) techniques are gaining popularity as effective tools for coronavirus disease of 2019 (COVID-19) research. These strategies can be used in diagnosis, prognosis, therapy, and public health management. Bibliometric analysis quantifies the quality and impact of scholarly publications. ML in COVID-19 research is the focus of this bibliometric analysis.Methods:A comprehensive literature study found ML-based COVID-19 research. Web of Science (WoS) was used for the study. The searches included ""machine learning,"" ""artificial intelligence,"" and COVID-19. To find all relevant studies, 2 reviewers searched independently. The network visualization was analyzed using VOSviewer 1.6.19.Results:In the WoS Core, the average citation count was 13.6 +/- 41.3. The main research areas were computer science, engineering, and science and technology. According to document count, Tao Huang wrote 14 studies, Fadi Al-Turjman wrote 11, and Imran Ashraf wrote 11. The US, China, and India produced the most studies and citations. The most prolific research institutions were Harvard Medical School, Huazhong University of Science and Technology, and King Abdulaziz University. In contrast, Nankai University, Oxford, and Imperial College London were the most mentioned organizations, reflecting their significant research contributions. First, ""Covid-19"" appeared 1983 times, followed by ""machine learning"" and ""deep learning."" The US Department of Health and Human Services funded this topic most heavily. Huang Tao, Feng Kaiyan, and Ashraf Imran pioneered bibliographic coupling.Conclusion:This study provides useful insights for academics and clinicians studying COVID-19 using ML. Through bibliometric data analysis, scholars can learn about highly recognized and productive authors and countries, as well as the publications with the most citations and keywords. New data and methodologies from the pandemic are expected to advance ML and AI modeling. It is crucial to recognize that these studies will pioneer this subject.",Not About Sufficiency
Seeing Like a Neighbor: Rethinking Neighborhoods as Service-oriented Communities,"Neighborhoods remain primary units of measurement and empowerment among city planners, and governments ask neighborhood groups to take on more of the work of the ""state."" Missing from these discussions of neighborhood expectations is how to effectively foster social capital and sense of community among neighbors and what it means to be a neighbor today. Through interviews with residents in a Lawrence, KS, neighborhood, we asked what makes an ideal neighbor. Findings show residents regard, or would like to regard, neighborhood as a service-oriented community built on informal, mutual aid rather than social, political, or consumer services-based models dominant in scholarly literature. However, participants voiced concern about how people can know when others need help if they do not know each other. We find that proximity, chance encounters, and informal communication are key and offer suggestions for revised roles of planners, policy makers, and researchers in neighborhood empowerment.",Not About Sufficiency
"The Distributive E.ects of Risk Prediction in Environmental Compliance: Algorithmic Design, Environmental Justice, and Public Policy","Government agencies are embracing machine learning to support a variety of resource allocation decisions. The U.S. Environmental Protection Agency (EPA), for example, has engaged academic research labs to test the use of machine learning in support of an important national initiative to reduce Clean Water Act violations. We evaluate prototypical risk prediction models that can support compliance interventions and demonstrate how critical algorithmic design choices can generate or mitigate disparate impact in environmental enforcement. First, we show that the de.nition of which facilities to focus on through this national compliance initiative hinges on arbitrary di.erences in state-level permitting schemes, causing a shift in environmental protection away from areas with more minority populations. Second, the policy objective to reduce the noncompliance rate is encoded in a classi.cation model, which does not account for the extent of pollution beyond the permitted limit. We hence compare allocation schemes between regression and classi.cation, and show that the latter directs attention towards facilities in more rural and white areas. Overall, our study illustrates that as machine learning enters government, algorithmic design can both embed and elucidate sources of administrative policy discretion with discernable distributional consequences.",Not About Sufficiency
Dynamic set point model for driver alert state using digital image processing,"The driver fatigue and lose of attention while driving are the most important causes of traffic accidents. Each year more than one million of deaths occur due to these facts. Thus, this problem has been converted into a serious social issue with high impact not only in economic terms, but also in the public health sector all around the world. Several approaches based on computer vision systems have been proposed to deal with this severe situation, but none of them have fully considered the non-fatigue state as a primary knowledge to detect an unusual event of a person while driving. In fact, typical approaches to deal with the problem of fatigue detection, are based on the analysis of behavioral features extracted with digital image processing such as frequency of blinking, yawning, among others. However, the huge limitation is the short interval of time between each analysis, that generally is few frames per second. Furthermore, all available methods are focus in modeling the fatigue, instead of representing the set point alert state of the driver, which is the main core of the proposed strategy. Hence, in this paper a dynamic set point model for alert state while driving using digital image processing and machine learning techniques is presented. The approach uses an embedded system build with a Raspberry prototyping board and a USB HD camera. Raspbian operative system controls OPEN CV libraries written in Python to detect face parts with an algorithm running Harr descriptors. The features extracted were the position and orientation of the head throw several minutes. Then, a mixture of Gaussians model with its learning and updating stages is used to represent the behaviour of features. Also, a dataset was built considering professional and non-professional drivers under two main scenarios: real and simulated conditions. Experimental results show the viability of the method for posterior analysis of unusual events while driving like fatigue detection, cellphone call or chat detection, or any other distraction not related to the driving process.",Not About Sufficiency
Data-Driven and Machine-Learning Methods to Project Coronavirus Disease 2019 Pandemic Trend in Eastern Mediterranean,"Background: The coronavirus disease 2019 (COVID-19) pandemic has become a major public health crisis worldwide, and the Eastern Mediterranean is one of the most affected areas. Materials and Methods: We use a data-driven approach to assess the characteristics, situation, prevalence, and current intervention actions of the COVID-19 pandemic. We establish a spatial model of the spread of the COVID-19 pandemic to project the trend and time distribution of the total confirmed cases and growth rate of daily confirmed cases based on the current intervention actions. Results: The results show that the number of daily confirmed cases, number of active cases, or growth rate of daily confirmed cases of COVID-19 are exhibiting a significant downward trend in Qatar, Egypt, Pakistan, and Saudi Arabia under the current interventions, although the total number of confirmed cases and deaths is still increasing. However, it is predicted that the number of total confirmed cases and active cases in Iran and Iraq may continue to increase. Conclusion: The COVID-19 pandemic in Qatar, Egypt, Pakistan, and Saudi Arabia will be largely contained if interventions are maintained or tightened. The future is not optimistic, and the intervention response must be further strengthened in Iran and Iraq. The aim of this study is to contribute to the prevention and control of the COVID-19 pandemic.",Not About Sufficiency
Development of a Risk Model to Predict Social Determinants of Health Needs Using Electronic Health Record and Social Vulnerability Index Data With Machine Learning,,Not About Sufficiency
Fostering the Sustainable Development Goals with technologies underpinned by frugal innovation,"The race against time to attain the Sustainable Development Goals (SDGs) intensifies, with inequality, poor living conditions, and biodiversity threats still prevalent. Achieving the SDGs is constrained by limited resources that require frugal solutions that are affordable, easy-to-use, ecologically sustainable, and socially inclusive. The role of infrastructure companies developing technologies with a frugal orientation that focus on the 17 SDGs must be at the core of efforts, because sustainable infrastructure is critical to the SGDs. This paper examines how such company-efforts foster the SDGs through the lens of frugal innovation. An exploratory multiple case study methodology was used with a 3-step and 4-dimensional evaluation approach to determine what technologies are fostering the SDGs, and their frugal characteristics. The results show that alternative resource and energy innovations are at the core of how infrastructure companies are promoting sustainability and fostering the SDGs. Most cases cluster around ecological contributions (mainly climate and energy), with sustainability and cost-effectiveness being the common characteristics. Implications for theory and practice are discussed, as well implications for policy-making.",Not About Sufficiency
Macro-Scale Temporal Attenuation for Electoral Forecasting: A Retrospective Study on Recent Elections,"Forecasting election outcomes is a complex scientific challenge with notable societal implications. Existing approaches often combine statistical analysis, machine learning, and economic indicators. However, research in network science has emphasized the importance of temporal factors in the dissemination of opinions. This study presents a macro-scale temporal attenuation (TA) model, which integrates micro-scale opinion dynamics and temporal epidemic theories to enhance forecasting accuracy using pre-election poll data. The findings suggest that the timing of opinion polls significantly influences opinion fluctuations, particularly as election dates approach. Opinion ""pulse"" is modeled as a temporal function that increases with new poll inputs and declines during stable periods. Two practical variants of the TA model, ETA and PTA, were tested on datasets from ten elections held between 2020 and 2024 around the world. The results indicate that the TA model outperformed several statistical methods, ARIMA models, and best pollster predictions (BPPs) in six out of ten elections. The two TA implementations achieved an average forecasting error of 6.92-6.95 percentage points across all datasets, compared to 7.65 points for BPP and 14.42 points for other statistical methods, demonstrating a performance improvement of 10-83%. Additionally, the TA methods maintained robust performance even with limited poll availability. As global pre-election survey data become more accessible, the TA model is expected to serve as a valuable complement to advanced election-forecasting techniques.",Not About Sufficiency
Urban Sprawl and Routing: A Comparative Study on 156 European Cities,"To address the growing challenges urban sprawl poses, it is essential to understand its influence on urban transportation, a primary source of economic, social, and environmental impact. This study fills this gap by quantifying the consequences of sprawl on transportation efficiency, proposing an interdisciplinary methodology that integrates knowledge from operations research. Specifically, adopting a broad European perspective, we investigate how urban sprawl correlates with travel distances and optimal routes in 156 spatially heterogeneous cities across 28 European countries. We discover a significant correlation between five sprawl indicators (Land usage, Gini coefficient, Shannon entropy, Moran I index, and Bribiesca index) and both travel distances and routes by car and bicycle: transportation is inherently less efficient in cities with higher levels of sprawl. Among the considered indicators, Shannon entropy emerges as the best predictor of route efficiency. We offer insights into the geography of sprawl in Europe, finding that many Spanish cities stand out for their compactness and route efficiency, while hotspots of sprawl are present in many Western and Central European countries. Our results underline the underestimated importance of addressing urban sprawl to reduce transportation's economic, social, and environmental costs and encourage policymakers and urban planners to prioritize compact city development to foster sustainable urban growth.",Not About Sufficiency
Multicomponent (bio)markers for obesity risk prediction: a scoping review protocol,"Introduction Despite international efforts, the number of individuals struggling with obesity is still increasing. An important aspect of obesity prevention relates to identifying individuals at risk at early stage, allowing for timely risk stratification and initiation of countermeasures. However, obesity is complex and multifactorial by nature, and one isolated (bio)marker is unlikely to enable an optimal risk stratification and prognosis for the individual; rather, a combined set is required. Such a multicomponent interpretation would integrate biomarkers from various domains, such as classical markers (eg, anthropometrics, blood lipids), multiomics (eg, genetics, proteomics, metabolomics), lifestyle and behavioural attributes (eg, diet, physical activity, sleep patterns), psychological traits (mental health status such as depression) and additional host factors (eg, gut microbiota diversity), also by means of advanced interpretation tools such as machine learning. In this paper, we will present a protocol that will be employed for a scoping review that attempts to summarise and map the state-of-the-art in the area of multicomponent (bio)markers related to obesity, focusing on the usability and effectiveness of such biomarkers.Methods and analysis PubMed, Scopus, CINAHL and Embase databases will be searched using predefined key terms to identify peer-reviewed articles published in English until January 2024. Once downloaded into EndNote for deduplication, CADIMA will be employed to review and select abstracts and full-text articles in a two-step procedure, by two independent reviewers. Data extraction will then be carried out by several independent reviewers. Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews and Peer Review of Electronic Search Strategies guidelines will be followed. Combinations employing at least two biomarkers from different domains will be mapped and discussed.Ethics and dissemination Ethical approval is not required; data will rely on published articles. Findings will be published open access in an international peer-reviewed journal. This review will allow guiding future directions for research and public health strategies on obesity prevention, paving the way towards multicomponent interventions.",Not About Sufficiency
Large scale urban development projects and local governance: From democratic urban planning to besieged local governance,"This article analyses the interaction between social exclusion in the city, the implementation of largescale development projects and changes in urban governance. The first part of the article analyses the evolution in urban restructuring tendencies and its consequences for social exclusion and integration mechanisms. The relationships between urban restructuring and changes in urban public policy are reflected in the rise of the New Urban Policy' that has provided increasing freedom of action to urban developers and public-private ventures in which the market logic predominates. The remainder of the article focuses on specific features of urban policy and governance as they appear from the case-studies covered in this issue: the physical bias of urban policy, the challenge of mainstream policy by integrated approaches to neighbourhood development, the rise of 'exceptionality' procedures in urban planning, and the threat of New Urban Policy to the good working of local democracy.",Not About Sufficiency
ProtPlat: an efficient pre-training platform for protein classification based on FastText,"Background: For the past decades, benefitting from the rapid growth of protein sequence data in public databases, a lot of machine learning methods have been developed to predict physicochemical properties or functions of proteins using amino acid sequence features. However, the prediction performance often suffers from the lack of labeled data. In recent years, pre-training methods have been widely studied to address the small-sample issue in computer vision and natural language processing fields, while specific pre-training techniques for protein sequences are few. Results: In this paper, we propose a pre-training platform for representing protein sequences, called ProtPlat, which uses the Pfam database to train a three-layer neural network, and then uses specific training data from downstream tasks to fine-tune the model. ProtPlat can learn good representations for amino acids, and at the same time achieve efficient classification. We conduct experiments on three protein classification tasks, including the identification of type III secreted effectors, the prediction of subcellular localization, and the recognition of signal peptides. The experimental results show that the pre-training can enhance model performance effectively and ProtPlat is competitive to the state-of-the-art predictors, especially for small datasets. We implement the ProtPlat platform as a web service (https://compbio.sjtu.edu.cn/protplat) that is accessible to the public. Conclusions: To enhance the feature representation of protein amino acid sequences and improve the performance of sequence-based classification tasks, we develop ProtPlat, a general platform for the pre-training of protein sequences, which is featured by a large-scale supervised training based on Pfam database and an efficient learning model, FastText. The experimental results of three downstream classification tasks demonstrate the efficacy of ProtPlat.",Not About Sufficiency
COVID-19 and the flu: data simulations and computational modelling to guide public health strategies,"Background: Pandemics threaten lives and economies. This article addresses the global threat of the anticipated overlap of COVID-19 with seasonal-influenza. Objectives: Scientific evidence based on simulation methodology is presented to reveal the impact of a dual outbreak, with scenarios intended for propagation analysis. This article aims at researchers, clinicians of family medicine, general practice and policy-makers worldwide. The implications for the clinical practice of primary health care are discussed. Current research is an effort to explore new directions in epidemiology and health services delivery. Methods: Projections consisted of machine learning, dynamic modelling algorithms and whole simulations. Input data consisted of global indicators of infectious diseases. Four simulations were run for '20% versus 60% flu-vaccinated populations' and '10 versus 20 personal contacts'. Outputs consisted of numerical values and mathematical graphs. Outputs consisted of numbers for 'never infected', 'vaccinated', 'infected/recovered', 'symptomatic/asymptomatic' and 'deceased' individuals. Peaks, percentages, R-0, durations are reported. Results: The best-case scenario was one with a higher flu-vaccination rate and fewer contacts. The reverse generated the worst outcomes, likely to disrupt the provision of vital community services. Both measures were proven effective; however, results demonstrated that 'increasing flu-vaccination rates' is a more powerful strategy than 'limiting social contacts'. Conclusions: Results support two affordable preventive measures: (i) to globally increase influenza-vaccination rates, (ii) to limit the number of personal contacts during outbreaks. The authors endorse changing practices and research incentives towards multidisciplinary collaborations. The urgency of the situation is a call for international health policy to promote interdisciplinary modern technologies in public health engineering.",Not About Sufficiency
Untangling Housing Cost and Transportation Interactions: The Location Affordability Index ModelVersion 2 (LAIM2),"It is now accepted that to have an understanding of housing affordability one must consider not only housing costs, but also the transportation costs associated with that household location. To make this information readily accessible to the public, the United States government created an Internet resource, the Location Affordability Portal - Version 2 (www.locationaffordability.info), to provide housing and transportation costs for every neighborhood in all 50 states and the District of Columbia. Although the statistical model at the heart of this resource was designed for predictive accuracy, its design and parameter estimates can provide additional insights into the interaction of housing cost and transportation choices (and thus its cost). This study describes the development and explores the policy implications (and limitations) of this structural equations model, the Location Affordability Index Model - Version 2 (LAIM2).",Not About Sufficiency
Modeling of Cognitive Agents,"Agent-based Modeling (ABM), a novel computational modeling paradigm, is the modeling of phenomena as dynamical systems of interacting agents. Here, we apply this methodology for designing cognitive agents that are allowed to perform categorization process of input training items. The internal agent structure, as in presented previously brainstorming algorithm, and it is equipped with the set of basic machine learning, or clustering algorithms, which allow it for constructing prototypes of categories. Agent links prototypical categories with the subsets of training objects (so called prototypes for a category) during the simulation time. The equilibration process is described here using the mean-field theory, and fully connected cellular automata network of different categories. The individual outcomes of clustering, or machine learning algorithms are combined in order to determine the most effective partitioning of a given training data into the set of distinct categories. The dynamics of cellular automata network simulates the higher level of information integration acquired from repetitive learning trials. The final categorization of training objects is therefore consistent with equilibrium state of the complex system of linked and interacting machine learning methods, each representing different category. The proposed cognitive agent is the first autonomous cognitive system that is able to build the classification system for given perceptual information using ensemble of machine learning algorithms.",Not About Sufficiency
MLQD: A package for machine learning-based quantum dissipative dynamics,"Machine learning has emerged as a promising paradigm to study the quantum dissipative dynamics of open quantum systems. To facilitate the use of our recently published ML-based approaches for quantum dissipative dynamics, here we present an open-source Python package MLQD (https:// github .com /Arif -PhyChem /MLQD), which currently supports the three ML-based quantum dynamics approaches: (1) the recursive dynamics with kernel ridge regression (KRR) method, (2) the non-recursive artificial-intelligence-based quantum dynamics (AIQD) approach and (3) the blazingly fast one-shot trajectory learning (OSTL) approach, where both AIQD and OSTL use the convolutional neural networks (CNN). This paper describes the features of the MLQD package, the technical details, optimization of hyperparameters, visualization of results, and the demonstration of the MLQD's applicability for two widely studied systems, namely the spin-boson model and the Fenna-Matthews-Olson (FMO) complex. To make MLQD more user-friendly and accessible, we have made it available on the Python Package Index (PyPi) platform and it can be installed via pip install mlqd. In addition, it is also available on the XACS cloud computing platform (https://XACScloud .com) via the interface to the MLATOM package (http://MLatom .com). Program summary Program Title: MLQD CPC Library link to program files: https://doi .org /10 .17632 /yxp37csy5x .1 Developer's repository link: https://github .com /Arif -PhyChem /MLQD Code Ocean capsule: https://codeocean .com /capsule /5563143 /tree Licensing provisions: Apache Software License 2.0 Programming language: Python 3.0 Supplementary material: Jupyter Notebook-based tutorials External routines/libraries: Tensorflow, Scikit-learn, Hyperopt, Matplotlib, MLatom Nature of problem: Fast propagation of quantum dissipative dynamics with machine learning approaches. Solution method: We have developed MLQD as a comprehensive framework that streamlines and supports the implementation of our recently published machine learning-based approaches for efficient propagation of quantum dissipative dynamics. This framework encompasses: (1) the recursive dynamics with kernel ridge regression (KRR) method, as well as the non-recursive approaches utilizing convolutional neural networks (CNN), namely (2) artificial intelligence-based quantum dynamics (AIQD), and (3) oneshot trajectory learning (OSTL). Additional comments including restrictions and unusual features:",Not About Sufficiency
Systematic causality mapping of factors leading to accidental falls of older adults,"Introduction: According to WHO's statistical evidence, accidental falls are the second leading causes of death worldwide. This systematic literature review and meta-analysis aims to provide a holistic view of risk factors and unfold the missing or less addressed but crucial factors that lead to accidental falls of the older adults. It also intends to profile the risk factors at different levels, which helps exhibit the level of consistency relationship between various risk factors and falls.Study design: Systematic literature review.Methods: A systematic review on the risk factors leading to accidental falls of older adults by retrieving English journal papers published starting from 1980 was conducted on April 2018. A method of literature synthesis and causal mapping was adopted to aggregate those fall-leading factors into macro variables and a coherent causal tracing network was thereby built, which can reflect not only the causal relationship of various macro variables but also the ""consistency of agreement"" between macro variables and falls of the older adults.Results: A hypothesized causal relationship diagram of 19 aggregated macro variables and their 31 causal rela-tionship suggested by the observational evidences is demonstrated. The consistency relationship between macro variables and elderly accidental fall are summarized and demonstrated. Our analysis reveals that ""Time"", ""Sea-son"" and ""Weather"" are three less-studied factors in the literature. In our comprehensive analysis, our study also indicates neglected countries and senior populations such as Africa and Oceania, which requires more attention from the research community and global funding agencies. It is found that major quantitative tools focus on the traditional statistical analysis. Conclusion: With the accelerated aging and increase of longevity worldwide, national and regional policies, and public health programs to provide adequate care services for the older people are crucially needed in both indus-trialized and developing countries. Evidences identified in the research are valuable inputs for policy design and decision makers of different stakeholders and prevention design of risk factors for falls in the older adults. The categorization of research methods in different literature also suggests that more quantitative approaches including simulation, optimization in operational research, and maybe machine learning are needed to enrich the research paradigm. We suggest researchers to consider using our presented causal map and the way of building it and explore the possibility of extending this framework to uncover more research topics in health-related research.",Not About Sufficiency
Developing Machine Learning Algorithm Literacy with Novel Plugged and Unplugged Approaches,"Data science and machine learning should not only be research areas for scientists and researchers but should also be accessible and understandable to the general audience. Enabling students to understand the details behind the technology will support them in becoming aware consumers and encourage them to become active participants. In this paper, we present instructional materials developed for introducing students to two key machine learning algorithms: decision trees and k-nearest neighbors. The materials were tested in a middle school's afterschool artificial intelligence program with four participating students aged 12 to 13. A combination of hands-on activities, innovative technology, and intuitive examples facilitated student learning. With hand-drawn decision trees and penguin species classifications, students used the algorithms to solve problems and anticipate other possible applications. We present the technology used, curriculum materials developed, and classroom structure. Following the guidelines from AI4K12 and introducing foundational machine learning algorithms, we hope to foster student interest in STEM fields.",Not About Sufficiency
Is air transport sustainable in remote regions? A socio-environmental perspective,"Air transport is crucial for ensuring population mobility owing to time travel savings, especially in remote regions. Despite its benefits, air transport is still one of the most polluting industries worldwide. Under the European Green Deal, sustainable transport needs to be enhanced to promote sustainable economic development, revealing the need for better decision-making tools. Nonetheless, limited research presents valuation frameworks under uncertainty and socio-environmental impacts simultaneously. Using a real options analysis and utility theory, a valuation framework is developed to assess the economic and socio-environmental impacts of air transport under Public Service Obligations. The results for the Azores case, a nine-island archipelago in Portugal, show a positive impact on social welfare. This study contributes to the literature by developing a valuation framework suitable for assessing sustainable transport networks and by presenting a case study with real data.",Not About Sufficiency
Automated classification of citrus disease on fruits and leaves using convolutional neural network generated features from hyperspectral images and machine learning classifiers,". Citrus black spot (CBS) is a fungal disease caused by Phyllosticta citricarpa that poses a quarantine threat and can restrict market access to fruits. It manifests as lesions on the fruit surface and can result in premature fruit drops, leading to reduced yield. Another significant disease affecting citrus is canker, which is caused by the bacterium Xanthomonas citri subsp. citri (syn. X. axonopodis pv. citri); it causes economic losses for growers due to fruit drops and blemishes. Early detection and management of groves infected with CBS or canker through fruit and leaf inspection can greatly benefit the Florida citrus industry. However, manual inspection and classification of disease symptoms on fruits or leaves are labor-intensive and time-consuming processes. Therefore, there is a need to develop a computer vision system capable of autonomously classifying fruits and leaves, expediting disease management in the groves. This paper aims to demonstrate the effectiveness of convolutional neural network (CNN) generated features and machine learning (ML) classifiers for detecting CBS infected fruits and leaves with canker symptoms. A custom shallow CNN with radial basis function support vector machine (RBF SVM) achieved an overall accuracy of 92.1% for classifying fruits with CBS and four other conditions (greasy spot, melanose, wind scar, and marketable), and a custom Visual Geometry Group 16 (VGG16) with the RBF SVM classified leaves with canker and four other conditions (control, greasy spot, melanoses, and scab) at an overall accuracy of 93%. These preliminary findings demonstrate the potential of utilizing hyperspectral imaging (HSI) systems for automated classification of citrus fruit and leaf diseases using shallow and deep CNN-generated features, along with ML classifiers.",Not About Sufficiency
Visualization and analytics tools for infectious disease epidemiology: A systematic review,"Background: A myriad of new tools and algorithms have been developed to help public health professionals analyze and visualize the complex data used in infectious disease control. To better understand approaches to meet these users' information needs, we conducted a systematic literature review focused on the landscape of infectious disease visualization tools for public health professionals, with a special emphasis on geographic information systems (GIS), molecular epidemiology, and social network analysis. The objectives of this review are to: (1) identify public health user needs and preferences for infectious disease information visualization tools; (2) identify existing infectious disease information visualization tools and characterize their architecture and features; (3) identify commonalities among approaches applied to different data types; and (4) describe tool usability evaluation efforts and barriers to the adoption of such tools. Methods: We identified articles published in English from January 1, 1980 to June 30, 2013 from five bibliographic databases. Articles with a primary focus on infectious disease visualization tools, needs of public health users, or usability of information visualizations were included in the review. Results: A total of 88 articles met our inclusion criteria. Users were found to have diverse needs, preferences and uses for infectious disease visualization tools, and the existing tools are correspondingly diverse. The architecture of the tools was inconsistently described, and few tools in the review discussed the incorporation of usability studies or plans for dissemination. Many studies identified concerns regarding data sharing, confidentiality and quality. Existing tools offer a range of features and functions that allow users to explore, analyze, and visualize their data, but the tools are often for siloed applications. Commonly cited barriers to widespread adoption included lack of organizational support, access issues, and misconceptions about tool use. Discussion and conclusion: As the volume and complexity of infectious disease data increases, public health professionals must synthesize highly disparate data to facilitate communication with the public and inform decisions regarding measures to protect the public's health. Our review identified several themes: consideration of users' needs, preferences, and computer literacy; integration of tools into routine workflow; complications associated with understanding and use of visualizations; and the role of user trust and organizational support in the adoption of these tools. Interoperability also emerged as a prominent theme, highlighting challenges associated with the increasingly collaborative and interdisciplinary nature of infectious disease control and prevention. Future work should address methods for representing uncertainty and missing data to avoid misleading users as well as strategies to minimize cognitive overload. (C) 2014 The Authors. Published by Elsevier Inc.",Not About Sufficiency
UAV and Structure-From-Motion Photogrammetry Enhance River Restoration Monitoring: A Dam Removal Study,"Dam removal is a river restoration technique that has complex landscape-level ecological impacts. Unmanned aerial vehicles (UAVs) are emerging as tools that enable relatively affordable, repeatable, and objective ecological assessment approaches that provide a holistic perspective of restoration impacts and can inform future restoration efforts. In this work, we use a consumer-grade UAV, structure-from-motion (SfM) photogrammetry, and machine learning (ML) to evaluate geomorphic and vegetation changes pre-/post-dam removal, and discuss how the technology enhanced our monitoring of the restoration project. We compared UAV evaluation methods to conventional boots-on-ground methods throughout the Bellamy River Reservoir (Dover, NH, USA) pre-/post-dam removal. We used a UAV-based vegetation classification approach that used a support vector machine algorithm and a featureset composed of SfM-derived elevation and visible vegetation index values to map other, herbaceous, shrub, and tree cover throughout the reservoir (overall accuracies from 83% to 100%), mapping vegetation succession as well as colonization of exposed sediments that occurred post-dam removal. We used SfM-derived topography and the vegetation classifications to map erosion and deposition throughout the reservoir, despite its heavily vegetated condition, and estimate volume changes post-removal. Despite some limitations, such as influences of refraction and vegetation on the SfM topography models, UAV provided information on post-dam removal changes that would have gone unacknowledged by the conventional ecological assessment approaches, demonstrating how UAV technology can provide perspective in restoration evaluation even in less-than-ideal site conditions for SfM. For example, the UAV provided perspective of the magnitude and extent of channel shape changes throughout the reservoir while the boots-on-ground topographic transects were not as reliable for detecting change due to difficulties in navigating the terrain. In addition, UAV provided information on vegetation changes throughout the reservoir that would have been missed by conventional vegetation plots due to their limited spatial coverage. Lastly, the COVID-19 pandemic prevented us from meeting to collect post-dam removal vegetation plot data. UAV enabled data collection that we would have foregone if we relied solely on conventional methods, demonstrating the importance of flexible and adaptive methods for successful restoration monitoring such as those enabled via UAV.",Not About Sufficiency
Investigation and prediction of Energy consumption at St. Olays Hospital,"The objective of this study is to evaluate and predict the energy use in different buildings during COVID-19 pandemic period at St. Olays Hospital in Trondheim. Based on machine learning, operational data from St. Olays hospital combined with weather data will be used to predict energy use for the hospital. Analysis of the energy data showed that the case buildings at the hospital did not have any different energy use during the pandemic this year compared to the same period last year, except for the lab center. The energy consumption of electricity, heating and cooling is very similar both in 2019 and 2020 for all buildings, but in 2020 during the pandemic, the lab center had a reduction of 35% in electricity, compared to last year. An analysis of the energy needed for heating and cooling in the end of June to the end of November was also calculated for operating room 1 and was estimated to 256 kWh/m2 for operation room 1. The machine learning algorithms perform very well to predict the energy consumption of case buildings, Random Forest and AdaBoost proves as the best models, with less than 10% margin of error, some of the models have only 4% error. An analysis of the effect of humidification of ventilation air on energy consumption in operating room 1 was also carried out. The impact on energy consumption were high in winter and will at the coldest periods be able to double the energy consumption needed in the ventilation.",Not About Sufficiency
Lessons Learned in Creating Interoperable Fast Healthcare Interoperability Resources Profiles for Large-Scale Public Health Programs,"Objective This article describes lessons learned from the collaborative creation of logical models and standard Health Level Seven (HL7) Fast Healthcare Interoperability Resources (FHIR) profiles for family planning and reproductive health. The National Health Service delivery program will use the FHIR profiles to improve federal reporting, program monitoring, and quality improvement efforts. Materials and Methods Organizational frameworks, work processes, and artifact testing to create FHIR profiles are described. Results Logical models and FHIR profiles for the Family Planning Annual Report 2.0 dataset have been created and validated. Discussion Using clinical element models and FHIR to meet the needs of a real-world use case has been accomplished but has also demonstrated the need for additional tooling, terminology services, and application sandbox development. Conclusion FHIR profiles may reduce the administrative burden for the reporting of federally mandated program data.",Not About Sufficiency
ReLiance: a machine learning and literature-based prioritization of receptor-ligand pairings,"Motivation: The prediction of receptor-ligand pairings is an important area of research as intercellular communications are mediated by the successful interaction of these key proteins. As the exhaustive assaying of receptor-ligand pairs is impractical, a computational approach to predict pairings is necessary. We propose a workflow to carry out this interaction prediction task, using a text mining approach in conjunction with a state of the art prediction method, as well as a widely accessible and comprehensive dataset. Among several modern classifiers, random forests have been found to be the best at this prediction task. The training of this classifier was carried out using an experimentally validated dataset of Database of Ligand-Receptor Partners (DLRP) receptor-ligand pairs. New examples, co-cited with the training receptors and ligands, are then classified using the trained classifier. After applying our method, we find that we are able to successfully predict receptor-ligand pairs within the GPCR family with a balanced accuracy of 0.96. Upon further inspection, we find several supported interactions that were not present in the Database of Interacting Proteins (DIPdatabase). We have measured the balanced accuracy of our method resulting in high quality predictions stored in the available database ReLiance.",Not About Sufficiency
Reflected Light Spectrometry and AI-Based Data Analysis for Detection of Rapid Chicken Eggshell Change Caused by Mycoplasma Synoviae,"Featured Application The proposed method may be used in the production process for fast identification of eggs originating from poultry infected with MS. Mycoplasma synoviae (MS) is a pathogen that causes economic losses in the poultry industry. It can be transmitted, amongst others, via the respiratory tract and spread relatively quickly. As such, MS infections are mainly controlled by maintaining MS-free breeder flocks. Routine diagnosis for the detection of MS may be based on serological, culture, and molecular tests. Here, we propose an optical solution where AI-based analysis of spectral data obtained from the light reflected from the eggshells is used to determine whether they originate from healthy or Mycoplasma synoviae-infected hens. The wavelengths proposed for spectral MS detection are limited to those of VIS and NIR DPSS lasers, which are freely accessible on market. The results are satisfactory: for white eggshells, the F-score is over 95% for five different combinations of wavelengths (using eight or nine wavelengths); for brown eggshells, the F-score is above 85%, also for five different combinations of 6-9 wavelengths.",Not About Sufficiency
"Screening for frequent hospitalization risk among community-dwelling older adult between 2016 and 2023: machine learning-driven item selection, scoring system development, and prospective validation","Background: Screening for frequent hospitalizations in the community can help prevent super-utilizers from growing in the inpatient population. However, the determinants of frequent hospitalizations have not been systematically examined, their operational definitions have been inconsistent, and screening among community members lacks tools. Nor do we know if what determined frequent hospitalizations before COVID-19 continued to be the determinant of frequent hospitalizations at the height of the pandemic. Hence, the current study aims to identify determinants of frequent hospitalization and their screening items developed from the Comprehensive Geriatric Assessment (CGA), as our 273-item CGA is too lengthy to administer in full in community or primary care settings. The stability of the identified determinants will be examined in terms of the prospective validity of pre-COVID-selected items administered at the height of the pandemic. Methods: Comprehensive Geriatric Assessments (CGAs) were administered between 2016 and 2018 in the homes of 1,611 older adults aged 65+ years. Learning models were deployed to select CGA items to maximize the classification of different operational definitions of frequent hospitalizations, ranging from the most inclusive definition, wherein two or more hospitalizations over 2 years, to the most exclusive, wherein two or more hospitalizations must appear during year two, reflecting different care needs. In addition, the CGA items selected by the best-performing learning model were then developed into a random-forest-based scoring system for assessing frequent hospitalization risk, the validity of which was tested during 2018 and again prospectively between 2022 and 2023 in a sample of 329 older adults recruited from a district adjacent to where the CGAs were initially performed. Results: Seventeen items were selected from the CGA by our best-performing algorithm (DeepBoost), achieving 0.90 AUC in classifying operational definitions of frequent hospitalizations differing in temporal distributions and care needs. The number of medications prescribed and the need for assistance with emptying the bowel, housekeeping, transportation, and laundry were selected using the DeepBoost algorithm under the supervision of all operational definitions of frequent hospitalizations. On the other hand, reliance on walking aids, ability to balance on one's own, history of chronic obstructive pulmonary disease (COPD), and usage of social services were selected in the top 10 by all but the operational definitions that reflect the greatest care needs. The prospective validation of the original risk-scoring system using a sample recruited from a different district during the COVID-19 pandemic achieved an AUC of 0.82 in differentiating those rehospitalized twice or more over 2 years from those who were not. Conclusion: A small subset of CGA items representing one's independence in aspects of (instrumental) activities of daily living, mobility, history of COPD, and social service utilization are sufficient for community members at risk of frequent hospitalization. The determinants of frequent hospitalization represented by the subset of CGA items remain relevant over the course of COVID-19 pandemic and across sociogeography.",Not About Sufficiency
Empowering Healthcare: TinyML for Precise Lung Disease Classification,"Respiratory diseases such as asthma pose significant global health challenges, necessitating efficient and accessible diagnostic methods. The traditional stethoscope is widely used as a non-invasive and patient-friendly tool for diagnosing respiratory conditions through lung auscultation. However, it has limitations, such as a lack of recording functionality, dependence on the expertise and judgment of physicians, and the absence of noise-filtering capabilities. To overcome these limitations, digital stethoscopes have been developed to digitize and record lung sounds. Recently, there has been growing interest in the automated analysis of lung sounds using Deep Learning (DL). Nevertheless, the execution of large DL models in the cloud often leads to latency, dependency on internet connectivity, and potential privacy issues due to the transmission of sensitive health data. To address these challenges, we developed Tiny Machine Learning (TinyML) models for the real-time detection of respiratory conditions by using lung sound recordings, deployable on low-power, cost-effective devices like digital stethoscopes. We trained three machine learning models-a custom CNN, an Edge Impulse CNN, and a custom LSTM-on a publicly available lung sound dataset. Our data preprocessing included bandpass filtering and feature extraction through Mel-Frequency Cepstral Coefficients (MFCCs). We applied quantization techniques to ensure model efficiency. The custom CNN model achieved the highest performance, with 96% accuracy and 97% precision, recall, and F1-scores, while maintaining moderate resource usage. These findings highlight the potential of TinyML to provide accessible, reliable, and real-time diagnostic tools, particularly in remote and underserved areas, demonstrating the transformative impact of integrating advanced AI algorithms into portable medical devices. This advancement facilitates the prospect of automated respiratory health screening using lung sounds.",Not About Sufficiency
Comparison of Machine Learning Models for Hazardous Gas Dispersion Prediction in Field Cases,"Dispersion prediction plays a significant role in the management and emergency response to hazardous gas emissions and accidental leaks. Compared with conventional atmospheric dispersion models, machine leaning (ML) models have both high accuracy and efficiency in terms of prediction, especially in field cases. However, selection of model type and the inputs of the ML model are still essential problems. To address this issue, two ML models (i.e., the back propagation (BP) network and support vector regression (SVR) with different input selections (i.e., original monitoring parameters and integrated Gaussian parameters) are proposed in this paper. To compare the performances of presented ML models in field cases, these models are evaluated using the Prairie Grass and Indianapolis field data sets. The influence of the training set scale on the performances of ML models is analyzed as well. Results demonstrate that the integrated Gaussian parameters indeed improve the prediction accuracy in the Prairie Grass case. However, they do not make much difference in the Indianapolis case due to their inadaptability to the complex terrain conditions. In addition, it can be summarized that the SVR shows better generalization ability with relatively small training sets, but tends to under-fit the training data. In contrast, the BP network has a stronger fitting ability, but sometimes suffers from an over-fitting problem. As a result, the model and input selection presented in this paper will be of great help to environmental and public health protection in real applications.",Not About Sufficiency
Morphology of Urban Villages in China: A Case Study of Dayuan Village in Guangzhou,"Urban villages play an important role in providing affordable housing to urban migrants in Chinese cities. They are considered as supplementary to the dual rural-urban system in China. Of central importance to studying urban villages is how the morphology of these informal settlements a ffects urban life. It is essential for urban planners and designers to examine the morphology of urban villages. This paper, therefore, investigates the morphology of urban villages using the case study of Dayuan Village in Guangzhou, China. The morphology of this urban village is tested against four main elements of urban morphology: urban density, accessibility, functional mix, and urban interface. Our results revealed that the type of street within the urban village has considerable influence on accessibility, functional mix, and urban interface. Regarding urban density, our results show that buildings' height is not influenced by the centrality of buildings nor land value; however, it is likely that it is affected by planning agreements between the village committee and the local government. Land coverage does not comply with the planning regulation for residential districts. Regarding accessibility analysis, the number of entrances to streets is influenced by the type of street under analysis. The distribution of different types of functional mix is also affected by the type of street within the urban village. The buildings with a mix of `live/visit' are concentrated along the formal streets and primary inner streets. The mono-functional use of `live' and the bi-functional mix of `live/work' are mostly located in the secondary inner streets. Regarding urban interface, our results demonstrate that the formal streets have an interface with considerable porosity, and that this can contribute to the livelihood of the immediate area.",Not About Sufficiency
A life course perspective on the travel of Australian millennials,"Recent research suggests that the millennial generation may be inclined to more sustainable travel habits than previous generations, reflected in lower rates of driver licensing in many countries and greater use of sustainable modes in others. However, it is still unknown whether millennials will continue to use sustainable transport modes as they age, or whether their travel patterns will revert to the car dependence displayed by previous generations. This research addresses this overlooked area in the travel behaviour research through an in-depth, qualitative prospective exploration of the interactions between life course and mobility of millennials in three Australian cities (Sydney, Melbourne and Canberra). Drawing from life-course transition research, fifty-five in-depth interviews found that Australian millennial life courses could be categorised into three typologies: (a) traditional, (b) delayed-traditional and (c) non-traditional/uncertain. In addition, millennial mobility was categorised into four typologies: (a) choice multi modals, (b) captive multi-modals, (c) choice drivers and (d) captive drivers. Many millennials preferred living in inner urban areas, were multi-modal and somewhat 'mode agnostic,' open to using whatever travel mode best suited their needs; very few showed a strong preference for cars. However, the research does suggest that as millennials approach adult milestones such as having children, the difficulty in finding suitable housing near transit may push some of them into neighbourhoods where sustainable transport is no longer a practical option. Policy interventions that support a sustainable lifestyle are suggested and research directions are discussed. (C) 2017 Elsevier Ltd. All rights reserved.",Not About Sufficiency
Multi-Class Classification of Turkish Texts with Machine Learning Algorithms,"The problem of text classification is the process of supervised assignment of text documents to one or more predefined categories or classes according to the content of the processed texts with natural language processing methods. Text classification applications are actively used in various fields such as categorization of social interactions, web pages and news texts, optimization of search engines, extracting information, and automatically processing e-mails. In this context, it is aimed to classify Turkish texts with methods based on supervised machine learning. In this context, the classification success of supervised learning models on Turkish texts was analyzed with different parameters. These models have been tested for classification of news texts on five predefined classes (economy, politics, sport, health, and technology) and the system was trained with different number of training documents and the classification process was carried out. In this context, the classification performances of Multinomial Naive Bayes, Bernoulli Naive Bayes, Support Vector Machine, K-Nearest Neighbor, and Decision Trees algorithms on Turkish news texts are compared and interpreted in the light of the results obtained with different parameters. As a result of the study, the procedure with the best classification success was the Multinomial Naive Bayes algorithm with a classification success of about 90%. These results show that the Naive Bayes probability model can be used as an effective classifier method in classifying Turkish texts compared to other methods. In this context, it is envisaged that the proposed methodology could be applied to Turkish texts on different web platforms (social networks, forums, communication networks, etc.) for different purposes.",Not About Sufficiency
Serum proteomics reveals early biomarkers of Alzheimer's disease: The dual role of APOE-?4,"Alzheimer's disease (AD), the leading cause of dementia, significantly impacts global public health, with cases expected to exceed 150 million by 2050. Late-onset Alzheimer's disease (LOAD), predominantly influenced by the APOE-64 allele, exhibits complex pathogenesis involving amyloid-beta (A beta) plaques, neurofibrillary tangles (NFTs), neuroinflammation, and blood-brain barrier (BBB) disruption. Proteomics has emerged as a pivotal technology in uncovering molecular mechanisms and identifying biomarkers for early diagnosis and intervention in AD. This paper reviews the genetic and molecular roles of APOE-64 in the pathology of AD, including its effects on A beta aggregation, tau phosphorylation, neuroinflammation, and BBB integrity. Additionally, it highlights recent advances in serum proteomics, revealing APOE-64-dependent and independent protein signatures with potential as early biomarkers for AD. Despite technological progress, challenges such as population diversity, standardization, and distinguishing AD-specific biomarkers remain. Directions for future research emphasize multicenter longitudinal studies, multi-omics integration, and the clinical translation of proteomic findings to enable early detection of AD and personalized treatment strategies. Proteomics advances in AD research hold the promise of improving patient outcomes and reducing the global disease burden.",Not About Sufficiency
Deep Analysis of Risks and Recent Trends Towards Network Intrusion Detection System,"In the modern world, information security and communications concerns are growing due to increasing attacks and abnormalities. The presence of attacks and intrusion in the network may affect various fields such as social welfare, economic issues and data storage. Thus intrusion detection (ID) is a broad research area, and various methods have emerged over the years. Hence, detecting and classifying new attacks from several attacks are complicated tasks in the network. This review categorizes the security threats and challenges in the network by accessing present ID techniques. The major objective of this study is to review conventional tools and datasets for implementing network intrusion detection systems (NIDS) with open source malware scanning software. Furthermore, it examines and compares state-of-art NIDS approaches in regard to construction, deployment, detection, attack and validation parameters. This review deals with machine learning (ML) based and deep learning (DL) based NIDS techniques and then deliberates future research on unknown and known attacks.",Not About Sufficiency
"Identification of Poverty Areas by Remote Sensing and Machine Learning: A Case Study in Guizhou, Southwest China","As an objective social phenomenon, poverty has accompanied the vicissitudes of human society, which is a chronic dilemma hindering human civilization. Remote sensing data, such as nighttime lights imagery, provides abundant poverty-related information that can be related to poverty. However, it may be insufficient to rely merely on nighttime lights data, because poverty is a comprehensive problem, and poverty identification may be affected by topography, especially in some developing countries or regions where agriculture accounts for a large proportion. Therefore, some geographical features may be necessary for supplements. With the support of the random forest machine learning method, we extracted 23 spatial features base on remote sensing including nighttime lights data and geographical data, and carried out the poverty identification in Guizhou Province, China, since 2012. Compared with the identifications using support vector machines and the artificial neural network, random forest showed a better accuracy. The results supported that nighttime lights and geographical features are better than those only by nighttime lights features. From 2012 to 2019, the identified poor counties in Guizhou Province showed obvious dynamic spatiotemporal characteristics. The number of poor counties has decreased consistently and contiguous poverty-stricken areas have fragmented; the number of poor counties in the northeast and southwest regions decreased faster than other areas. The reduction in poverty probability exhibited a pattern of spreading from the central and northern regions to the periphery parts. The poverty reduction was relatively slow in areas with large slope and large topographic relief. When poor counties are adjacent to more non-poor counties, they can get rid of poverty easier. This study provides a method for feature selection and recognition of poor counties by remote sensing images and offers new insights into poverty identification and regional sustainable development for other developing countries and areas.",Not About Sufficiency
Tap Water Avoidance Is Associated with Lower Food Security in the United States: Evidence from NHANES 2005-2018,"Background Food insecurity has profound nutritional and public health consequences. Water insecurity may exacerbate food insecurity, yet little is known about the associ-ation between water and food insecurity in the United States or other high-income countries. Objective This study aimed to estimate how tap water avoidance, a proxy of water insecurity, covaries with food insecurity; examine how the probability of food insecurity changed by tap water avoidance between 2005 and 2018; and test how the association between tap water avoidance and food insecurity differed across income and housing statuses. Design This was a secondary analysis of the cross-sectional 2005-2018 National Health and Nutrition Examination Survey. Participants/setting Participants were 31,390 US adults 20 years and older. Main outcome measures The main outcome was food insecurity, using the US Food Security Survey Module. Statistical analyses Adjusted logistic regression models estimated how tap water avoidance was associated with the odds of food insecurity. Predicted probabilities of food insecurity over time and by income and housing status were plotted using mar-ginal standardization. Results Adults who avoided tap water had 21% higher odds (95% CI 1.09 to 1.34) of food insecurity compared with those who drank tap water. The probability of any food insecurity doubled between 2005-2006 and 2017-2018 and was consistently higher for tap water avoiders. Food insecurity decreased across both tap water drinkers and avoiders as income increased, but was higher among tap water avoiders at all income levels. Likewise, food insecurity was higher among renters than among homeowners but was higher among tap water avoiders in both housing groups. Conclusions Tap water avoidance is positively associated with food insecurity in the United States, and both insecurities have increased over time. Efforts to mitigate food insecurity should simultaneously address water insecurity issues, including tap water availability and quality, as these may be a modifiable contributors to food insecurity. J Acad Nutr Diet. 2023;123(1):29-40.",Not About Sufficiency
Hybrid-electric system truth test: Energy analysis of Toyota Prius IV in real urban drive conditions,"The experimental work, carried out through road tests in the city of Rome (Italy), assesses the energy performance of the Toyota Prius IV generation with Full Hybrid traction system under real conditions of use in the urban environment. The evaluation was carried out through a data collection campaign on a 37 km urban route, in both fast-flowing sections and traffic-congested areas of Rome. On this route, 60 driving tests were carried out by individual with different characteristics in terms of gender, age and hybrid technology driving experience. In particular, the analysis involved the operation as ZEV (Zero Emission Vehicle) vehicle, the hybrid system Power flows, fuel consumption and yields. The most relevant results relate to the high percentage (in time and space) of operation in ZEV mode: this mode is achieved both in ""electric"" operation, that is, with power output to the wheels by the electric motor only (with the ICE off), and in the inertial operation, i.e. without power output to the wheels (always with ICE off). Another important result concerns the high overall efficiency of the hybrid traction system, obtained thanks to the high average yield of the ICE, and especially through the very high deceleration energy recovery. The recovered energy provides a very significant contribution to the total energy needed by the wheels for the car movement purposes and, in particular, provides the vast majority of electricity required by the electric motor.",Not About Sufficiency
Machine learning powered tools for automated analysis of muscle sympathetic nerve activity recordings,"Automated analysis and quantification of physiological signals in clinical practice and medical research can reduce manual labor, increase efficiency, and provide more objective, reproducible results. To build a novel platform for the analysis of muscle sympathetic nerve activity (MSNA), we employed state-of-the-art data processing and machine learning applications. Data processing methods for integrated MSNA recordings were developed to evaluate signals regarding the overall quality of the signal, the validity of individual signal peaks regarding the potential to be MSNA bursts and the timing of their occurrence. An overall probability score was derived from this flexible platform to evaluate each individual signal peak automatically. Overall, three deep neural networks were designed and trained to validate individual signal peaks randomly sampled from recordings representing only electrical noise and valid microneurography recordings. A novel data processing method for the whole signal was developed to differentiate between periods of valid MSNA signal recordings and periods in which the signal was not available or lost due to involuntary movement of the recording electrode. A probabilistic model for timing of the signal bursts was implemented as part of the system. Machine Learning algorithms and data processing tools were implemented to replicate the complex decision-making process of manual MSNA analysis. Validation of manual MSNA analysis including intra- and inter-rater validity and a comparison with automated MSNA tools is required. The developed toolbox for automated MSNA analysis can be extended in a flexible way to include algorithms based on other datasets.",Not About Sufficiency
Mitigating Local Natural Disaster through Social Aware Preparedness Using Complexity Approach,"During and after natural disaster, such as, eruption of vulcano, many people have to abandon their living place to a temporary shelter. Usually, there could be several time for the occurrence of the eruption. This situation, for example, happened at Sinabung vulcano, located in Karo district of North Sumatera Province, Indonesia. The people in the disaster area have become indifferent. In terms of the society, the local natural disaster problem belong to a complex societal problem. This research is to find a way what should be done to these society to raise their social awareness that they had experienced serious natural disaster and they will be able to live normally and sustainable as before. Societal complexity approach is used to solve the problems. Social studies referred to in this activity are to analyze the social impacts arising from the implementation of the relocation itself. Scope of social impact assessments include are The social impact of the development program of relocation, including the impact of construction activities and long-term impact of construction activity, particularly related to the source and use of clean water, sewerage system, drainage and waste management (solid waste), Social impacts arising associated with occupant relocation sites and the availability of infrastructure (public facilities, include: worship facilities, health and education) in the local environment (pre-existing). Social analysis carried out on the findings of the field, the study related documents and observations of the condition of the existing social environment Siosar settlements.",Not About Sufficiency
CellProfiler Analyst 3.0: accessible data exploration and machine learning for image analysis,"Image-based experiments can yield many thousands of individual measurements describing each object of interest, such as cells in microscopy screens. CellProfiler Analyst is a free, open-source software package designed for the exploration of quantitative image-derived data and the training of machine learning classifiers with an intuitive user interface. We have now released CellProfiler Analyst 3.0, which in addition to enhanced performance adds support for neural network classifiers, identifying rare object subsets, and direct transfer of objects of interest from visualization tools into the Classifier tool for use as training data. This release also increases interoperability with the recently released CellProfiler 4, making it easier for users to detect and measure particular classes of objects in their analyses.",Not About Sufficiency
Identification and Classification of Urban Shrinkage in Northeast China,"The phenomenon of shrinking cities is a significant challenge faced by many cities today. To more accurately identify the leading factors driving urban shrinkage and develop rational recommendations, precise identification and classification of urban shrinkage has become an indispensable part of the process. This paper focuses on the typical population loss region of China's three northeastern provinces, using 497 identified physical cities as the basic research unit. Based on multi-source geographical big data and utilizing the geographically weighted regression (GWR) model, spatial modeling of population in the three provinces of northeast China was conducted, resulting in spatialized population data, followed by identification and classification of shrinking cities among the physical cities. Cities with a total population change rate of less than 0 are defined as shrinking cities. In cities where the total population change rate is greater than 0, cities with both a city shrinking area ratio and a decreased population ratio greater than 5% are defined as locally shrinking cities. Based on this, 90 (18.1%) shrinking cities and 118 (23.7%) locally shrinking cities were identified within the three provinces of northeast China. The phenomenon of urban shrinkage is distributed throughout various regions, mainly in smaller cities located near larger cities. According to the standards of the urban shrinkage classification model, the spatial pattern of population loss regions was divided into four types, identifying 13 (6.3%) global type, 111 (53.4%) concentrated type, 64 (30.7%) perforated type, and 20 (9.6%) edge type. Analysis of shrinking cities based on their classification revealed that the main reasons for urban shrinkage are the decline and dissolution of large industrial enterprises, abandonment and neglect of buildings, and unreasonable design planning in cities. Economic development and inward population flow can be promoted in shrinking cities by creating job opportunities, improving living standards, developing transportation, adjusting urban planning or concentrating urban population, as well as vigorously developing urban center areas. These measures can provide support for the revival and development of shrinking cities.",Not About Sufficiency
Quality improvement collaborative to optimize heart failure care in patients from a network of clinics in Argentina during the COVID-19 pandemic,"Heart failure (HF) is a major clinical and public health problem associated with significant mortality, morbidity, and health-care costs. Despite the existence of evidence-based guidelines for the optimal treatment of HF, the quality of care remains suboptimal. Our aim was to increase the use a care bundle in 50% of enrolled subjects during their hospitalization and discharge and to reduce their readmission for HF causes by 10%. We conducted an uncontrolled before-after study in eight hospitals in Argentina to evaluate the effect of a quality improvement intervention on the use of an HF care bundle in patients with HF New York Heart Association (NYHA) Class II-III. The HF bundle of care included medication, continuum of care, lifestyle habits, and predischarge examinations. Training and follow-up of multidisciplinary teams in each center were performed through learning sessions and plan-do-study-act improvement cycles. Data collectors reviewed bundle compliance in the health records of recruited patients after their hospital discharge and verified readmissions through phone calls to patients within 30-40 days after discharge. We recruited 200 patients (83 before and 127 during the intervention phase), and bundle compliance increased from 9.6% to 28.3% [odds ratio 3.71, 95% confidence interval (8.46; 1.63); P =.002]. Despite a slow improvement during the first months, bundle compliance gained momentum near the end of the intervention surpassing 80%. We observed a non-significant decreased readmission rate within 30 days of discharge due to HF in the postintervention period [8.4% vs. 5.5%, odds ratio 0.63, 95% CI (1.88; 0.21); P =.410]. Qualitative analysis showed that members of the intervention teams acknowledged the improvement of work organization and standardization of care, teamwork, shared mental model, and health record completeness as well as the utility of training fellows. Despite the challenges related to the pandemic, better care of patients with HF NYHA Class II-III was possible through simple interventions and collaborative work.",Not About Sufficiency
Enhanced SARS-CoV-2 case prediction using public health data and machine learning models,"Objectives The goal of this study is to propose and test a scalable framework for machine learning (ML) algorithms to predict near-term severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) cases by incorporating and evaluating the impact of real-time dynamic public health data.Materials and Methods Data used in this study include patient-level results, procurement, and location information of all SARS-CoV-2 tests reported in West Virginia as part of their mandatory reporting system from January 2021 to March 2022. We propose a method for incorporating and comparing widely available public health metrics inside of a ML framework, specifically a long-short-term memory network, to forecast SARS-CoV-2 cases across various feature sets.Results Our approach provides better prediction of localized case counts and indicates the impact of the dynamic elements of the pandemic on predictions, such as the influence of the mixture of viral variants in the population and variable testing and vaccination rates during various eras of the pandemic.Discussion Utilizing real-time public health metrics, including estimated Rt from multiple SARS-CoV-2 variants, vaccination rates, and testing information, provided a significant increase in the accuracy of the model during the Omicron and Delta period, thus providing more precise forecasting of daily case counts at the county level. This work provides insights on the influence of various features on predictive performance in rural and non-rural areas.Conclusion Our proposed framework incorporates available public health metrics with operational data on the impact of testing, vaccination, and current viral variant mixtures in the population to provide a foundation for combining dynamic public health metrics and ML models to deliver forecasting and insights in healthcare domains. It also shows the importance of developing and deploying ML frameworks in rural settings. This study aims to propose and test a scalable framework for machine learning (ML) algorithms to predict near-term severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) cases by county by incorporating and evaluating the impact of real-time dynamic public health data. Data used in this study include patient-level results, procurement, and location information of all SARS-CoV-2 tests reported in West Virginia as part of their mandatory reporting system from January 2021 to March 2022. We propose a method for incorporating and comparing widely available public health metrics inside of a ML framework, specifically a long-short-term memory network, to forecast SARS-CoV-2 cases across various feature sets. Our approach provides better prediction of localized case counts, recommendation of locations of outbreaks, and indicates the impact of the dynamic elements of the pandemic on predictions, such as the influence of the mixture of viral variants in the population and variable testing and vaccination rates during various eras of the pandemic. Incorporating available public health metrics with operational data on the impact of testing, vaccination, and current viral variant mixtures in the population provides a foundation for combining dynamic public health metrics and ML models to deliver improved forecasting and insights in healthcare domains. This approach provides a model for utilizing ML to forecast, deploy, and understand the impact of public health data during coronavirus disease and other pandemics.",Not About Sufficiency
Deciphering the Social Vulnerability of Landslides Using the Coefficient of Variation-Kullback-Leibler-TOPSIS at an Administrative Village Scale,"Yu'nan County is located in the Pacific Rim geological disaster-prone area. Frequent landslides are an important cause of population, property, and infrastructure losses, which directly threaten the sustainable development of the regional social economy. Based on field survey data, this paper employs the coefficient of variation method (CV) and an improved TOPSIS model (Kullback-Leibler-Technique for Order Preference by Similarity to an Ideal Solution) to assess the social vulnerability to landslide disasters in 182 administrative villages of Yu'nan County. Also, it conducts a ranking and comprehensive analysis of their social vulnerability levels. Finally, the accuracy of the evaluation results is validated by applying the losses incurred from landslide disasters per unit area within the same year. The results indicate significant spatial variability in social vulnerability across Yu'nan County, with 68 out of 182 administrative villages exhibiting moderate vulnerability levels or higher. This suggests a high risk of widespread damage from potential disasters. Among these, Xincheng village has the highest social vulnerability score, while Chongtai village has the lowest, with a 0.979 difference in their vulnerabilities. By comparing the actual losses incurred per unit area from landslides, it is found that the social vulnerability results predicted by the CV-KL-TOPSIS model are more consistent with the actual survey results. Furthermore, among the ten sub-factors, population density, building value, and road value contribute most significantly to the overall weight with 0.269, 0.152, and 0.105, respectively, suggesting that in mountainous areas where the population is relatively concentrated, high social vulnerability to landslide hazards is a reflection of population characteristics and local economic level. The evaluation framework and evaluation indicators proposed in this paper can systematically and accurately evaluate the social vulnerability of landslide-prone areas, which provide a reference for urban planning and management in landslide-prone areas.",Not About Sufficiency
In Silico Prediction of Endocrine Disrupting Chemicals Using Single-Label and Multilabel Models,"Endocrine disruption (ED) has become a serious public health issue and also poses a significant threat to the ecosystem. Due to complex mechanisms of ED, traditional in silico models focusing on only one mechanism are insufficient for detection of endocrine disrupting chemicals (EDCs), let alone offering an overview of possible action mechanisms for a known EDC. To remove these limitations, in this study both single-label and multilabel models were constructed across six ED targets, namely, AR (androgen receptor), ER (estrogen receptor alpha), TR (thyroid receptor), GR (glucocorticoid receptor), PPARg (peroxisome proliferator-activated receptor gamma), and aromatase. Two machine learning methods were used to build the single-label models, with multiple random under-sampling combining voting classification to overcome the challenge of data imbalance. Four methods were explored to construct the multilabel models that can predict the interaction of one EDC against multiple targets simultaneously. The single-label models of all the six targets have achieved reasonable performance with balanced accuracy (BA) values from 0.742 to 0.816. Each top single-label model was then joined to predict the multilabel test set with BA values from 0.586 to 0.711. The multilabel models could offer a significant boost over the single-label baselines with BA values for the multilabel test set from 0.659 to 0.832. Therefore, we concluded that single-label models could be employed for identification of potential EDCs, while multilabel ones are preferable for prediction of possible mechanisms of known EDCs.",Not About Sufficiency
"Prevalence of chronic obstructive pulmonary disease (COPD) and its associated factors among adults in Abeshge District, Ethiopia: a cross sectional study","Background Chronic obstructive pulmonary disease (COPD) is one of the major public health problems worldwide. Despite an increasing burden of COPD in the world, it is often a neglected disease in low income countries and COPD prevalence studies are rare in Sub-Saharan Africa. The objective of this study was to determine the prevalence of COPD and its associated factors among adults in Ethiopia. Methods A community based cross sectional study was conducted from February 5 to May 20, 2019 in Abeshge district, Southern Ethiopia. A total of 734 adults aged at least 30 years were selected using multistage cluster sampling technique and included in the study. All participants were interviewed about socio-demographic characteristics, respiratory symptoms, smoking status and clinical characteristics. Moreover, all participants underwent spirometry. We defined COPD as a post-bronchodilator FEV1/FVC of less than 70%. Data were entered into Epi-data manager 4.4 and analyzed using SPSS version 23. Descriptive statistics and binary logistic regression analysis were used and p-value < 0.05 was considered as significant. Results Of the 779 adults invited to participate, 734 adults (421 men and 313 women) were participated in this study. The mean (SD) age of the participants was 39.15 (+/- 9.36) years, within the age range of 30-75 years. The prevalence of COPD was 17.8% (95% confidence interval [CI], 15.1-20.6). Factors significantly associated with COPD were age above 50 years (adjusted odds ratio [AOR] = 1.91, 95% CI [1.10, 3.30]), being smoker (AOR = 4.54, 95% CI [2.69, 7.66]), Exposed to biomass smoke (AOR = 2.05, 95% CI [1.06, 3.95]) and poor ventilated kitchen (AOR = 4.12, 95% CI [2.67, 6.34]). Conclusion It is evident from this study that the prevalence of COPD in Ethiopia is high. Factors such as old age, cigarette smoking, exposure to biomass smoke and poor kitchen ventilation plays a role in the development of COPD.",Not About Sufficiency
A Long Short-Term Memory Based Framework for Early Detection of Mild Cognitive Impairment From EEG Signals,"Mild cognitive impairment (MCI) is an irreparable progressive neuro-degenerative disorder, which seems to be a precursor to Alzheimer's disease (AD) that may lead to dementia in elderly people. It is a major public health challenge for healthcare in the 21st century. Because there is no curative or therapy to halt or reverse the course of MCI, early identification is critical for successful treatment programs to enhance patients' quality of life. Currently, Electroencephalography (EEG) has been emerged as an efficient tool to investigate MCI. Traditional methods for finding MCI from EEG data use shallow machine learning-based architectures that cannot find important biomarkers in deep, hidden layers of the data and also have trouble dealing with a large amount of EEG data. To reduce this issue, this research will use EEG data to provide a deep learning-based framework using the Long Short-term Memory (LSTM) model for effective identification of MCI individuals from healthy volunteers (HV). The suggested framework consists of four phases: denoising, segmentation, downsampling, uncovering deep hidden features using the LSTM model, and identifying MCI patients with the sigmoid classifier. This study has designed 20 different LSTM models and investigated them to a publicly available MCI database to find out the best one. After performing 5-fold cross validation, the best model achieved 96.41% of accuracy, 96.55% of sensitivity and 95.95% of specificity. The proposed LSTM-based deep learning model provides a robust biomarker and guide technologists to create a new automatic diagnosis system for MCI detection.",Not About Sufficiency
Can cargo bikes compete with cars? Cargo bike sharing users rate cargo bikes superior on most motives - Especially if they reduced car ownership,"The transport sector and especially private cars pose environmental, economic, and social challenges. For this reason, cargo bikes and shared mobility are considered viable alternatives for road transport. In order to understand the potential and barriers of alternative transport modes, it is essential to analyze underlying motives. Moreover, comparing sustainable alternatives (such as public transport) to cars in terms of motives has been established as a research approach (Steg, 2003). Despite increasing interest in cargo bikes and cargo bike sharing, research on this topic is relatively rare. Particularly, there exists a lack of research addressing the impact of cargo bike sharing on car ownership. Against this background, this study quantifies the car ownership reduction effect of cargo bike sharing. In addition, it is investigated how cargo bikes differ from cars with regard to the underlying motives of users which also helps understanding potential barriers. To answer these research questions, this study is based on a large-scale survey with n = 2,590 cargo bike sharing users. The results imply that cargo bike sharing has a notable impact on car ownership. In general, cargo bikes are rated superior in regard to affective, symbolic, and environmental motives as well as on flexibility and price. However, discrepancies to cars do exist in terms of other instrumental aspects (traffic safety, travel speed, comfort, weatherindependence). Notably, users who reduced car ownership tend to rate cargo bikes superior compared to car -dependent users. The results imply that cargo bikes can play a marked role in reducing car dependency. Improving infrastructure and cargo bike technology as well as stimulating favorable social norms for cargo bikes have been identified as beneficial conditions that could help to leverage this potential.",Not About Sufficiency
Evidence as a Specific Knowledge to Inform Humanitarian Decision-Making in Migration Crisis Contexts: The Case of Syrian Refugees in Jordan,"To fulfill their missions, humanitarian organizations are engaged in the production and dissemination of a specific form of knowledge about refugees-namely, evidence. Yet relatively little scholarly work has been done on evidence as a basis for humanitarian decision-making in refugee crises. In this article, we analyze the evidence produced on Syrian refugees in Jordan between 2012 and 2017 by humanitarian actors, its appropriateness and relevance, to assess (a) potential side-effects for refugees and (b) functions for humanitarian organizations. Drawing on 234 documents on Syrian refugees in Jordan collected on major humanitarian platforms and interviews with experts from international organizations, this article shows that, due to the type of evidence produced, Syrian refugees living outside camps are invisibilized and their needs might be less considered in aid projects. The article also shows that some evidence practices are related to the outsourcing of data collection and analysis to specialized actors, and to quantification and standardization of evidence that plays a part in consolidating the legitimacy of humanitarian organizations. Our results highlight the importance of considering this specific form of knowledge when studying humanitarian interventions in international migration and refugee governance.",Not About Sufficiency
Applying Natural Language Processing to Textual Data From Clinical Data Warehouses: Systematic Review,"Background: In recent years, health data collected during the clinical care process have been often repurposed for secondary use through clinical data warehouses (CDWs), which interconnect disparate data from different sources. A large amount of information of high clinical value is stored in unstructured text format. Natural language processing (NLP), which implements algorithms that can operate on massive unstructured textual data, has the potential to structure the data and make clinical information more accessible.Objective: The aim of this review was to provide an overview of studies applying NLP to textual data from CDWs. It focuses on identifying the (1) NLP tasks applied to data from CDWs and (2) NLP methods used to tackle these tasks.Methods: This review was performed according to the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines. We searched for relevant articles in 3 bibliographic databases: PubMed, Google Scholar, and ACL Anthology. We reviewed the titles and abstracts and included articles according to the following inclusion criteria: (1) focus on NLP applied to textual data from CDWs, (2) articles published between 1995 and 2021, and (3) written in English.Results: We identified 1353 articles, of which 194 (14.34%) met the inclusion criteria. Among all identified NLP tasks in the included papers, information extraction from clinical text (112/194, 57.7%) and the identification of patients (51/194, 26.3%) were the most frequent tasks. To address the various tasks, symbolic methods were the most common NLP methods (124/232, 53.4%), showing that some tasks can be partially achieved with classical NLP techniques, such as regular expressions or pattern matching that exploit specialized lexica, such as drug lists and terminologies. Machine learning (70/232, 30.2%) and deep learning (38/232, 16.4%) have been increasingly used in recent years, including the most recent approaches based on transformers. NLP methods were mostly applied to English language data (153/194, 78.9%).Conclusions: CDWs are central to the secondary use of clinical texts for research purposes. Although the use of NLP on data from CDWs is growing, there remain challenges in this field, especially with regard to languages other than English. Clinical NLP is an effective strategy for accessing, extracting, and transforming data from CDWs. Information retrieved with NLP can assist in clinical research and have an impact on clinical practice.",Not About Sufficiency
"Development and Validation of Risk Scores for All-Cause Mortality for a Smartphone-Based ""General Health Score"" App: Prospective Cohort Study Using the UK Biobank","Background: Given the established links between an individual's behaviors and lifestyle factors and potentially adverse health outcomes, univariate or simple multivariate health metrics and scores have been developed to quantify general health at a given point in time and estimate risk of negative future outcomes. However, these health metrics may be challenging for widespread use and are unlikely to be successful at capturing the broader determinants of health in the general population. Hence, there is a need for a multidimensional yet widely employable and accessible way to obtain a comprehensive health metric. Objective: The objective of the study was to develop and validate a novel, easily interpretable, points-based health score (""C-Score"") derived from metrics measurable using smartphone components and iterations thereof that utilize statistical modeling and machine learning (ML) approaches. Methods: A literature review was conducted to identify relevant predictor variables for inclusion in the first iteration of a points-based model. This was followed by a prospective cohort study in a UK Biobank population for the purposes of validating the C-Score and developing and comparatively validating variations of the score using statistical and ML models to assess the balance between expediency and ease of interpretability and model complexity. Primary and secondary outcome measures were discrimination of a points-based score for all-cause mortality within 10 years (Harrell c-statistic) and discrimination and calibration of Cox proportional hazards models and ML models that incorporate C-Score values (or raw data inputs) and other predictors to predict the risk of all-cause mortality within 10 years. Results: The study cohort comprised 420,560 individuals. During a cohort follow-up of 4,526,452 person-years, there were 16,188 deaths from any cause (3.85%). The points-based model had good discrimination (c-statistic=0.66). There was a 31% relative reduction in risk of all-cause mortality per decile of increasing C-Score (hazard ratio of 0.69, 95% CI 0.663-0.675). A Cox model integrating age and C-Score had improved discrimination (8 percentage points; c-statistic=0.74) and good calibration. ML approaches did not offer improved discrimination over statistical modeling. Conclusions: The novel health metric (""C-Score"") has good predictive capabilities for all-cause mortality within 10 years. Embedding the C-Score within a smartphone app may represent a useful tool for democratized, individualized health risk prediction. A simple Cox model using C-Score and age balances parsimony and accuracy of risk predictions and could be used to produce absolute risk estimations for app users.",Not About Sufficiency
Predictive Control of a Human-in-the-Loop Network System Considering Operator Comfort Requirements,"We propose a model-predictive control (MPC)-based approach to solve a human-in-the-loop control problem for a network system lacking sensors and actuators to allow for a fully automatic operation. The humans in the loop are, therefore, essential; they travel between the network nodes to provide the remote controller with measurements and to actuate the system according to the controller's commands. Time instant optimization MPC is utilized to compute when the measurement and actuation actions are to take place to coordinate them with the network dynamics. The time instants also minimize the burden of human operators by tracking their energy levels and scheduling the necessary breaks. Fuel consumption related to the operators' travel is also minimized. The results in a digital twin of the Dez Main Canal illustrate that the new algorithm outperforms previous methods in terms of meeting operational objectives and taking care of human well-being, but at the cost of higher computational requirements.",Not About Sufficiency
Human intention and workspace recognition for collaborative assembly,"Human-Robot-Collaboration is being used more often nowadays in assembly processes that can not be fully automated in an optimal way. Skill-based task sharing showed to be a good alternative to manual labor mainly when tasks are either non ergonomic or require high precision. While collaborating with humans, robots are usually equipped with sensors to perceive the environment or the human when necessary. This paper presents a method for not only recognizing the product being assembled along with its components but also tracking the live motions of the human hands in the assembly area. This method can either be used as an input for the decision making module of the robot, or as a measure for quality insurance and human fault detection. Copyright (C) 2022 The Authors.",Not About Sufficiency
Founding Concepts and Foundational Work: Establishing the Framework for the Use of Acknowledgments as Indicators,"Building on the concepts of the reward system of science and social capital, Blaise Cronin brought forth the idea that rewards in science are threefold, forming a triangle built from authorship, citations, and acknowledgements. Of these, acknowledgments are the hardest to grasp and evaluate. After nearly 45 years of multidisciplinary research on acknowledgments and a corpus of over 80 scientific contributions, there is still no consensus on the value of acknowledgments in scholarly communication. This study aims to further acknowledgments research with a meta-synthesis of the literature, establishing the theoretical framework for the use of acknowledgments as bibliometric indicators. Based on in-progress content analyses, broad categories emerge revealing contextual information crucial to the understanding of acknowledgments. Applying our framework on data from the Web of Science, further phases of this study will provide large-scale findings based on a multidisciplinary sample. From there, it will be possible to envision recommendations for the standardization and use of acknowledgments as indicators. However, grounding the study of acknowledgments in their underlying theoretical considerations and conceptual foundations will ensure these recommendations respect the diverse traditions of the scientific field.",Not About Sufficiency
"A mixed methods study to inform fatal overdose prevention in San Diego, California: Perspectives from people who use drugs","Background: In the United States, community overdose education and naloxone distribution (OEND) programs have demonstrated efficacy in reducing opioid-related mortality. OEND programs have expanded across San Diego County, California, but differential naloxone accessibility among people who use drugs (PWUD) has not been assessed. We examined factors that shape individual naloxone accessibility in San Diego. Methods: We employed a convergent parallel mixed methods design using surveys (n = 194) and qualitative interviews (n = 20). Ordinal logistic regression examined factors associated with individual naloxone accessibility (i.e., the frequency with which participants could access naloxone within five minutes, categorized as never, sometimes, or always). Qualitative interviews explored participant perceptions of naloxone accessibility and whether and how they maintained naloxone. We organized multilevel findings into a modified social-ecological model. Results: In quantitative and qualitative samples, participants were majority male (72 % and 70 % respectively), non-White race/ethnicity (55 % and 75 %), with an average age around 42 years. In the quantitative sample, 24 % never had personally accessible naloxone, 52 % sometimes did, and 24 % always did. Factors independently associated with individual naloxone accessibility were female gender (Adjusted Odds Ratio [AdjOR]: 2.51, 95 % Confidence Interval [CI]: 1.31-4.85), monthly income <$500 (AdjOR: 0.42, 95 %CI:0.19, 0.90), witnessing an overdose (AdjOR: 3.51, 95 %CI:1.67-7.55), and knowing where to get free naloxone (AdjOR: 3.44, 95 %CI: 1.79-6.75). Qualitative data suggested that naloxone was generally easy to acquire in San Diego due to community harm reduction outreach and mutual aid among peers, albeit community barriers including distance to harm reduction providers and frequent relocation/displacement for those experiencing homelessness. Individual attitudes toward overdose risk, naloxone, and community responsibility contributed to varied individual naloxone accessibility. Conclusions: This study highlights multilevel factors influencing individual naloxone accessibility among people who use drugs in San Diego, emphasizing the importance of harm reduction outreach and peer-to-peer support. We identified opportunities for interventions that address both individual attitudes and community-level barriers to improve naloxone accessibility.",Not About Sufficiency
COVID-19 Vaccination and Public Health Countermeasures on Variants of Concern in Canada: Evidence From a Spatial Hierarchical Cluster Analysis,"Background: There is mounting evidence that the third wave of COVID-19 incidence is declining, yet variants of concern (VOCs) continue to present public health challenges in Canada. The emergence of VOCs has sparked debate on how to effectively control their impacts on the Canadian population. Objective: Provincial and territorial governments have implemented a wide range of policy measures to protect residents against community transmission of COVID-19, but research examining the specific impact of policy countermeasures on the VOCs in Canada is needed. Our study objective was to identify provinces with disproportionate prevalence of VOCs relative to COVID-19 mitigation efforts in provinces and territories in Canada. Methods: We analyzed publicly available provincial-and territorial-level data on the prevalence of VOCs in relation to mitigating factors, summarized in 3 measures: (1) strength of public health countermeasures (stringency index), (2) the extent to which people moved about outside their homes (mobility index), and (3) the proportion of the provincial or territorial population that was fully vaccinated (vaccine uptake). Using spatial agglomerative hierarchical cluster analysis (unsupervised machine learning), provinces and territories were grouped into clusters by stringency index, mobility index, and full vaccine uptake. The Kruskal-Wallis test was used to compare the prevalence of VOCs (Alpha, or B.1.1.7; Beta, or B.1.351; Gamma, or P.1; and Delta, or B.1.617.2 variants) across the clusters. Results: We identified 3 clusters of vaccine uptake and countermeasures. Cluster 1 consisted of the 3 Canadian territories and was characterized by a higher degree of vaccine deployment and fewer countermeasures. Cluster 2 (located in Central Canada and the Atlantic region) was typified by lower levels of vaccine deployment and moderate countermeasures. The third cluster, which consisted of provinces in the Pacific region, Central Canada, and the Prairies, exhibited moderate vaccine deployment but stronger countermeasures. The overall and variant-specific prevalences were significantly different across the clusters. Conclusions: This ???up to the point??? analysis found that implementation of COVID-19 public health measures, including the mass vaccination of populations, is key to controlling VOC prevalence rates in Canada. As of June 15, 2021, the third wave of COVID-19 in Canada is declining, and those provinces and territories that had implemented more comprehensive public health measures showed lower VOC prevalence. Public health authorities and governments need to continue to communicate the importance of sociobehavioural preventive measures, even as populations in Canada continue to receive their primary and booster doses of vaccines.",Not About Sufficiency
An analytical framework for assessing equitable access to public electric vehicle chargers,"Inequitable distribution of public electric vehicle (EV) charging facilities may result in disparities in charging accessibility, potentially impeding the uptake of EVs. In response, this paper proposes an analytical framework to assess the accessibility-based equity of public EV charging infrastructure systematically, using Hong Kong (HK) as the study area. The results demonstrated severe spatial inequity of charging facilities in central, north, and southwest HK. Meanwhile, the Gini index of all public EV charger types for the total population was 0.751, and the indices for low-income, low-education level, and government-funded housing subgroups were 0.791, 0.809, and 0.893, respectively. These indicated considerable horizontal and vertical inequity from a statistical perspective. Furthermore, age, education level, family structure, and housing type were identified as significant socio-demographic characteristics correlated to the accessibility-based equity of public EV charging infrastructure in HK. These findings are expected to be useful for future policymaking and infrastructure planning.",Not About Sufficiency
Role of consistent terminology in XPS reproducibility,"The use of inconsistent and incorrect terminology in scientific publications contributes to misunderstanding, confusion, and erroneous results in the scientific literature. This issue is of particular importance in x-ray photoelectron spectroscopy (XPS) that is in widespread use for many different purposes by scientists with diverse backgrounds. A set of surface analysis terminology, approved through consensus by international experts, has been developed by International Organization for Standardization Technical Committee TC201 on Surface Chemical Analysis. To encourage wide use, the terminology is accessible at several web sites at no cost. This short overview provides examples to highlight the importance of agreed terminology in eliminating confusion between similar terms. Examples are provided of terms that are commonly misused or confused in the literature. Other examples highlight terminology that provides a common basis for comparing instrument parameters and performance. As science advances, it is important to clarify terminology for describing evolving concepts and developments important to XPS.",Not About Sufficiency
Realistic galaxy images and improved robustness in machine learning tasks from generative modelling,"We examine the capability of generative models to produce realistic galaxy images. We show that mixing generated data with the original data improves the robustness in downstream machine learning tasks. We focus on three different data sets: analytical Sersic profiles, real galaxies from the COSMOS survey, and galaxy images produced with the SKIRT code, from the IllustrisTNG simulation. We quantify the performance of each generative model, using the Wasserstein distance between the distributions of morphological properties (e.g. the Gini-coefficient, the asymmetry, and ellipticity), the surface brightness distribution on various scales (as encoded by the power spectrum), the bulge statistic, and the colour for the generated and source data sets. With an average Wasserstein distance (Frechet Inception Distance) of 7.19 x 10(-2) (0.55), 5.98 x 10(-2) (1.45), and 5.08 x 10(-2) (7.76) for the Sersic, COSMOS and SKIRT data set, respectively, our best models convincingly reproduce even the most complicated galaxy properties and create images that are visually indistinguishable from the source data. We demonstrate that by supplementing the training data set with generated data, it is possible to significantly improve the robustness against domain-shifts and out-of-distribution data. In particular, we train a convolutional neural network to denoise a data set of mock observations. By mixing generated images into the original training data, we obtain an improvement of 11 and 45 per cent in the model performance regarding domain-shifts in the physical pixel size and background noise level, respectively.",Not About Sufficiency
ML4STEM Professional Development Program: Enriching K-12 STEM Teaching with Machine Learning,"The advances of machine learning (ML) in scientific discovery (SD) reveal exciting opportunities to utilize it as a cross-cutting tool for inquiry-based learning in K-12 STEM classrooms. There are, however, limited efforts on providing teachers with sufficient knowledge and skills to integrate ML into teaching. Our study addresses this gap by proposing a professional development (PD) program named ML4STEM. Based on existing research on supporting teacher learning in innovative technology integration, ML4STEM is composed of Teachers-as-Learners and Teachers-as-Designers sessions. It integrates an accessible ML learning platform designed for students with limited math and computing skills. We implemented this PD program and evaluated its effectiveness with 18 K-12 STEM teachers. Findings confirm that ML4STEM successfully develops teachers' understanding of teaching STEM with ML as well as fosters positive attitudes toward applying the ML as an in-class teaching technology. Discussions on the implications of our findings from ML4STEM are provided for future PD researchers and designers.",Not About Sufficiency
Improving morbidity information in Portugal: Evidence from data linkage of COVID-19 cases surveillance and mortality systems,"Background: COVID-19 rapidly spread around the world, putting health systems under unprecedented pressure and continuous adaptations. Well-established health information systems (HIS) are crucial in providing data to allow evidence-based policymaking and public health interventions in the pandemic response. This study aimed to compare morbidity information between two databases for COVID-19 management in Portugal and identify potential complementarities. Methods: This is an observational study using records from both COVID-19 cases surveillance (National Epidemiological Surveillance System; SINAVE) and related deaths (National e-Death Certificates Information System; SICO) systems, which were matched on sex, age, municipality of residence and date of death. After the linkage, morbidity reported in SINAVE and identified in SICO, through the application of Charlson and Elixhauser comorbidity indexes algorithms, were compared to evaluate agreement level. Results: Overall, 2285 matched cases were analyzed, including 53.9% males with a median age of 84 years. According to the method of data reporting assessment, the presence of any morbidity ranged between 26.3% and 62.5%. The reporting of ten morbidities could be compared between the information reported in SINAVE and SICO databases. The proportion of simultaneous reporting in both databases ranged between 5.7% for diabetes and 0.0% for human immunodeficiency virus infection or coagulopathy. Minimal or no agreement was found when assessing the similarity of the morbidity reporting in both databases, with neoplasms showing the highest level of agreement (0.352, 95% IC: 0.277-0.428; p < 0.001). Conclusion: Different information about reported morbidity could be found in two HIS used to monitor COVID-19 cases and related deaths, as data are independently collected. These results show that the interoperability of SICO and SINAVE databases would potentially improve available HIS and improve available information to decision-making and address COVID-19 pandemic management.",Not About Sufficiency
Effect of the Municipal Human Development Index on the results of the 2018 Brazilian presidential elections,"This paper explores the impact of the development indicators of Brazilian municipalities in the results of the 2018 Brazilian presidential elections. As the development indicator of a municipality we used the Municipal Human Development Index (MHDI), a known index derived from the Human Development Index (HDI) that has been adapted to Brazilian reality. The MHDI is composed by three sub-indices: education, income and longevity. Based on data publicly available, we used six different supervised machine learning classification algorithms for predicting the winner of the elections in municipalities. The results reached 87% of accuracy when using the education and income sub-indices as predictors of the elections. Based on the results obtained, we were able to confirm that municipalities with higher values for education and income sub-indices overwhelmingly preferred the right-wing candidate, while the left-wing option was preferred by those with lower values of the same sub indices.",Not About Sufficiency
Assisting Multitargeted Ligand Affinity Prediction of Receptor Tyrosine Kinases Associated Nonsmall Cell Lung Cancer Treatment with Multitasking Principal Neighborhood Aggregation,"A multitargeted therapeutic approach with hybrid drugs is a promising strategy to enhance anticancer efficiency and overcome drug resistance in nonsmall cell lung cancer (NSCLC) treatment. Estimating affinities of small molecules against targets of interest typically proceeds as a preliminary action for recent drug discovery in the pharmaceutical industry. In this investigation, we employed machine learning models to provide a computationally affordable means for computer-aided screening to accelerate the discovery of potential drug compounds. In particular, we introduced a quantitative structure-activity-relationship (QSAR)-based multitask learning model to facilitate an in silico screening system of multitargeted drug development. Our method combines a recently developed graph-based neural network architecture, principal neighborhood aggregation (PNA), with a descriptor-based deep neural network supporting synergistic utilization of molecular graph and fingerprint features. The model was generated by more than ten-thousands affinity-reported ligands of seven crucial receptor tyrosine kinases in NSCLC from two public data sources. As a result, our multitask model demonstrated better performance than all other benchmark models, as well as achieving satisfying predictive ability regarding applicable QSAR criteria for most tasks within the model's applicability. Since our model could potentially be a screening tool for practical use, we have provided a model implementation platform with a tutorial that is freely accessible hence, advising the first move in a long journey of cancer drug development.",Not About Sufficiency
Enhancing Data Space Semantic Interoperability through Machine Learning: a Visionary Perspective,"Our vision paper outlines a plan to improve the future of semantic interoperability in data spaces through the application of machine learning. The use of data spaces, where data is exchanged among members in a self-regulated environment, is becoming increasingly popular. However, the current manual practices of managing meta-data and vocabularies in these spaces are time-consuming, prone to errors, and may not meet the needs of all stakeholders. By leveraging the power of machine learning, we believe that semantic interoperability in data spaces can be signifcantly improved. This involves automatically generating and updating metadata, which results in a more fexible vocabulary that can accommodate the diverse terminologies used by diferent sub-communities. Our vision for the future of data spaces addresses the limitations of conventional data exchange and makes data more accessible and valuable for all members of the community.",Not About Sufficiency
Urban planning policy and clean energy development Harmony- evidence from smart city pilot policy in China,"Rapid urbanization is affecting the harmonious development of the environment and economy. Governments are developing new urban planning policies to achieve sustainable urban development. Using the Chinese smart city pilot policy as a natural experimental group, we use a difference-differences model to explore the relationship between smart cities and clean energy development. There are three findings in this paper. First, we find that smart city pilots drive clean energy development. Second, we find that digital technologies can reinforce the positive impact of smart city pilots and that innovation and financial inclusion have a positive mediating role. Third, we find that the impact of smart city pilots has a heterogeneous impact. Namely, pilots in coastal, large, and resource-based cities have a more positive impact on clean energy development. Existing smart city pilots significantly impact energy security and sustainable development. Our study provides a new perspective on urban planning and clean energy development in emerging countries such as China.",Not About Sufficiency
Construction and comparison of short-term prognosis prediction model based on machine learning in acute ischemic stroke,"Objective: To construct and compared the short-term prognosis prediction models of acute ischemic stroke (AIS) by machine learning (ML). Methods: Retrospectively study. The group W (mRS <= 3) was clustered, and combined with group P (mRS>3) to form the post -clustering dataset for modeling. The ""glmnet"", ""rpart"", ""xgboost"", ""randomForest"", ""neuralnet"" packages were used to construct ML models. The accuracy, sensitivity, specificity, positive predict value (PPV), negative predict value (NPV) among the models were compared. Four external clinical datasets were used for external clinical validation. The optimal prediction model was determined by variable screening ability, model visualization, and external clinical validation performance. Results: The post -clustering dataset contains 139 patients (group W) and 122 patients (group P). The neutrophil multiplied by D-dimer (NDM) has predictive value in all ML prediction models in this study. In the decision tree model, NDMQ occupies the first tree node, When NDM <= 5.62 and the age<74.5, the probability of poor prognosis of AIS is less than 20 %. When NDM>5.62 and accompanied by pneumonia, the incidence of poor prognosis of AIS is about 90 %. In the Random Forest (RF) model, NDMQ had the highest Gini index. The variable combination screened by the RF model had the best performance in the neural network, and the accuracy, sensitivity, specificity, PPV, and NPV of the external validation were 0.800, 0.774, 0.833, 0.857, and 0.741, respectively. The RF model had the best performance in the external clinical validation datasets, with accuracies of 0.646, 0.697, 0.695, and 0.713, respectively. Conclusions: NDM shows predictive value for AIS short-term prognosis in all ML models in this study. The optimal model in screening characteristic variables and the performance of in external clinical datasets was RF model. In the analysis of medical data with small sample size and outcome as categorical variables, RF could be used as the main algorithm to build a model.",Not About Sufficiency
The Greatest Challenge to Using AI/ML for Primary Health Care: Mindset or Datasets?,"The global vision for primary health care (PHC) is defined by regular access to quality care for comprehensive services throughout the course of life. However, this is not what typically happens, especially in low- and middle-income countries, where many people access the formal health system only for emergent needs. Yet, even episodic care is nearly impossible to attain due to infrastructure barriers, critical shortages of health care providers, and low-quality care. Artificial intelligence and machine learning (AI/ML) can help us revolutionize the current reality of health care into the vision of continuous health care that promotes individuals to maintain a constant healthy state. AI/ML can deliver precise recommendations to the individual, transforming patients from a passive receiver of health services into an active participant of their own care. By accounting for each individual, AI/ML can also ensure equitable coverage for entire populations with an ongoing data exchange between personal health, genomic data, public health, and environmental factors. The greatest challenge to enlisting AI/ML in the quest toward the PHC vision will be instilling a sense of responsibility with global citizens to recognize health data for the global good while prioritizing protected, individually owned data sets. Only when individuals start taking a collective approach to health data, shifting the mindset toward the goal of prevention, will the potential of AI/ML for PHC be realized. Until we overcome this challenge, the paradigm shift of the global community away from our ad hoc, reactive health system culture will not be achieved.",Not About Sufficiency
Policy brief: Improving national vaccination decision-making through data,"Life course immunisation looks at the broad value of vaccination across multiple generations, calling for more data power, collaboration, and multi-disciplinary work. Rapid strides in artificial intelligence, such as machine learning and natural language processing, can enhance data analysis, conceptual modelling, and real-time surveillance. The GRADE process is a valuable tool in informing public health decisions. It must be enhanced by real-world data which can span and capture immediate needs in diverse populations and vaccination administration scenarios. Analysis of data from multiple study designs is required to understand the nuances of health behaviors and interventions, address gaps, and mitigate the risk of bias or confounding presented by any single data collection methodology. Secure and responsible health data sharing across European countries can contribute to a deeper understanding of vaccines.",Not About Sufficiency
Data-driven framework for the adaptive exit selection problem in pedestrian flow: Visual information based heuristics approach,"Pedestrian behavior during evacuation has been formulated using various arbitrary microscopic methods to investigate the performance of crowd dynamics while their custom rules result in low visual realism in simulation due to the complexity of intrinsic decision logic of human. Statistical analysis is an effective way to reveal the motion pattern and path planning behavior of pedestrians whose main idea is to approach the trajectory and social attributes data of pedestrians extracted from evacuation drills as much as possible. In this study, we present a data-driven based microscopic pedestrian-simulation model with continuous-space representation to explore the potential of integrating empirical analysis into crowd simulation to enhance the authenticity of decision making. This method extracts the pedestrian's decision mode and smoothly applies it in the crowd dynamics model. Instead of navigating agents by arbitrary regulations, the desired direction of pedestrians during the motion is arranged by machine learning (ML) algorithms. The path decision module trained with actual pedestrian data improves the compatibility of the model in the application of various spatial scenarios and no longer suffers from tedious parameter fine-tuning work. To completely describe the information precepted by pedestrians, a polygon segmentation module is developed to divide the visual field of pedestrians and identify the mutual visibility among them. This module filters out the information that can be perceived by pedestrians in real situations, thereby bridges the gap between statistical analysis and numerical simulation methods. We compare different ML approaches for route-choice behavior prediction and discuss the relative importance of its influencing variables under different scenarios. Inferring the perception of social interactions from disaggregate choice data, the scope of effectiveness of conformity behavior and crowd-aversion are also discussed. The simulation results are compared with experimental data, illustrating the model's capability to accurately reproduce the observed flow motion in various scenarios with moderate modification in physical environment initialization. (c) 2021 Elsevier B.V. All rights reserved.",Not About Sufficiency
Evaluation of alternative standardized terminologies for medical conditions within a network of observational healthcare databases,"Large electronic databases of health care information, such as administrative claims and electronic health records, are available and are being used in a number of public health settings, including drug safety surveillance. However, because of a lack of standardization, clinical terminologies may differ across databases. With the aid of existing resources and expert coders, we have developed mapping tables to convert ICD-9-CM diagnosis codes used in some existing databases to SNOMED-CT and MedDRA. In addition, previously developed definitions for specific health outcomes of interest were mapped to the same standardized vocabularies. We evaluated how vocabulary mapping affected (1) the retention of clinical data from two test databases, (2) the semantic space of outcome definitions, (3) the prevalence of each outcome in the test databases, and (4) the reliability of analytic methods designed to detect drug-outcome associations in the test databases. Although vocabulary mapping affected the semantic space of some outcome definitions, as well as the prevalence of some outcomes in the test databases, it had only minor effects on the analysis of drug-outcome associations. Furthermore, both SNOMED-CT and MedDRA were viable for use as standardized vocabularies in systems designed to perform active medical product surveillance using disparate sources of observational data. (c) 2012 Elsevier Inc. All rights reserved.",Not About Sufficiency
Using deep learning to identify recent positive selection in malaria parasite sequence data,"Background: Malaria, caused by Plasmodium parasites, is a major global public health problem. To assist an understanding of malaria pathogenesis, including drug resistance, there is a need for the timely detection of underlying genetic mutations and their spread. With the increasing use of whole-genome sequencing (WGS) of Plasmodium DNA, the potential of deep learning models to detect loci under recent positive selection, historically signals of drug resistance, was evaluated. Methods: A deep learning-based approach (called ""DeepSweep"") was developed, which can be trained on haplotypic images from genetic regions with known sweeps, to identify loci under positive selection. DeepSweep software is available from . Results: Using simulated genomic data, DeepSweep could detect recent sweeps with high predictive accuracy (areas under ROC curve > 0.95). DeepSweep was applied to Plasmodium falciparum (n = 1125; genome size 23 Mbp) and Plasmodium vivax (n = 368; genome size 29 Mbp) WGS data, and the genes identified overlapped with two established extended haplotype homozygosity methods (within-population iHS, across-population Rsb) (similar to 60-75% overlap of hits at P < 0.0001). DeepSweep hits included regions proximal to known drug resistance loci for both P. falciparum (e.g. pfcrt, pfdhps and pfmdr1) and P. vivax (e.g. pvmrp1). Conclusion: The deep learning approach can detect positive selection signatures in malaria parasite WGS data. Further, as the approach is generalizable, it may be trained to detect other types of selection. With the ability to rapidly generate WGS data at low cost, machine learning approaches (e.g. DeepSweep) have the potential to assist parasite genome-based surveillance and inform malaria control decision-making.",Not About Sufficiency
The Effects of Urban Living Conditions on Subjective Well-Being: The Case of German Foreign Service Employees,"In an increasingly urbanized world, understanding the determinants of urban well-being will continue to grow in importance. Although the effects of different indicators of living conditions on well-being have been widely studied individually, little is known about their relative impact when examined jointly. In this study, we use a unique multi-source dataset that allows us to investigate the effect and relative importance of a variety of subjectively and objectively assessed aspects of urban living conditions on the subjective well-being (SWB) of German Foreign Service expatriates. The study captures living conditions in metropolises around the world at different stages of development, and assesses living conditions in a culturally comparably homogeneous set of participants, thus being potentially less confounded with cultural differences. Using linear regression and dominance analysis, we find that 'quality of and access to nature' (i.e., green space), 'quality of housing', and 'quality of public goods' (i.e., water, air, and sewage systems) have the strongest associations with SWB. Subjectively rated characteristics show stronger associations with SWB than externally assessed characteristics. Additionally, we examine whether the size of a city or the level of development of a country has an effect on SWB. Both living in a megacity (>= 10 million inhabitants) and a lower development status have negative effects on SWB. However, these effects disappear when the various indicators of living conditions are controlled for. Our findings can inform organisations sending employees abroad as well as urban planners seeking to improve their policies and decision-making.",Not About Sufficiency
Quantifying transport and electrocatalytic reaction processes in a gastight rotating cylinder electrode reactor via integration of Computational Fluid Dynamics modeling and experiments,"Understanding the complexity of the multiple processes of mass, momentum, charge, and heat transport, and how these affect reaction kinetics at the electrode/electrolyte interface is one of the major challenges in the field of energy and catalysis. The rapid and rational scale-up of electrocatalytic systems to industrial scales require a detailed understanding of nonlinear transport-reaction processes, accessible only through the building of multi-physics models that capture with high fidelity the complexity of real-world devices. The gastight rotating cylinder electrode (RCE) reactor is a promising lab-scale tool that can decouple transport from intrinsic kinetics to generate data for first-principle models useful in the design of industrial, electrochemical reactors. Computational Fluid Dynamics (CFD) studies have previously been used to investigate the bulk flow in RCE reactors for simple corrosion and electroplating processes. However, the quantification of changes in local concentration within the viscous layer where catalysis takes place requires capturing the correct flow conditions inside the hydrodynamic boundary layer near the surface of the electrode. This requires simulations with spatial resolution in the nm and mu m scale and temporal resolutions between ms and s scales that are similar to the timescales for reactions on the electrode surface. In this study, experimental electrocatalysis is combined with CFD modeling to elucidate and parameterize the hydrodynamics in a gastight RCE reactor. CFD simulations of the electrochemical ferricyanide reduction reaction under mass transport limited conditions are used to evaluate the validity of the CFD model parameters by comparing calculated dimensionless mass transport descriptors to dimensionless correlations obtained experimentally. Justifications for assumptions and details of the simulation methods used in this study are presented to provide a detailed understanding of the effect that each model parameter has on the ability to accurately simulate electrocatalysis in RCE systems. The simulation methodology reported here is a first step towards the development of multi-scale models for the study of transport dependent electrocatalytic processes, such as the electrochemical transformation of CO2 to fuels and chemicals.",Not About Sufficiency
"Evaluating population vulnerability to volcanic risk in a data scarcity context: The case of Goma city, Virunga volcanic province (DRCongo)","Goma city, at the eastern border of DRCongo, is highly exposed to natural hazards, especially from Nyiragongo volcano, located directly North of it. In January 2002, the city centre of Goma was devastated by lava flows and several thousands of people were temporarily displaced. Defining and quantifying population vulnerability to natural hazards, and lava flow hazards in particular, is a crucial element to evaluate and manage the risk. This paper aims at assessing the vulnerability of the population facing volcanic hazards in Goma, and its spatial variation across the city, in order to support volcanic risk prevention and management at the local levels. In this data scarcity context, two parallel methodologies are tested based on data collected through a large-scale household survey: the Social Vulnerability Index (SoVI) as defined by Cutter et al. (2003) based on a statistical data reduction; the other using fewer significant indicators in order to develop an Operational Vulnerability Index (OVI). Results show that the spatial distribution of the vulnerability levels with both approaches is quite similar, but the construction of an OVI can help to communicate the message more easily to political authorities for risk management actions - e.g., to target neighborhoods where to develop priority prevention programs - but also in terms of spatial urban planning - e.g., to identify areas where to act. Population vulnerability assessment, together with the lava flow hazard invasion probability and population exposure, is one of the crucial steps towards the lava flow risk assessment.",Not About Sufficiency
Ensemble empirical mode decomposition-based preprocessing method with Multi-LSTM for time series forecasting: a case study for hog prices,"Drastic hog price fluctuations have a great impact on the welfare of hog farmers, people's living standards, and the macroeconomy. To stabilise the hog price, hog price forecasting has become an increasingly hot issue in the research literature. Existing papers have neglected the benefits of decomposition and instead directly utilise models to predict hog prices by capturing raw data. Motivated by this issue, the authors introduce a new robust forecasting approach for hog prices that combines ensemble empirical mode decomposition (EEMD) and multilong short-term memory neural networks (Multi-LSTMs). First, EEMD decomposes the volatile raw sequence into several smoother subsequences. Second, the decomposed subsequences are predicted separately using a parallel structure model consisting of several LSTMs. Finally, the fuse function combines all the subresults to yield the final result. The empirical results suggest that the proposed method only has minor errors and proves the effectiveness and reliability in experiments on real datasets (2.55207, 4.816, and 0.332 on MAE, MAPE and RMSLE, respectively). Reliable forecasting of hog prices is beneficial to farmers and people to allow optimisation of their production and booking rates and to moderate the adverse effects of potential shocks.",Not About Sufficiency
Prioritizing urban nature-based solutions to support scaling-out strategies: A case study in Las Palmas de Gran Canaria,"Nature-based solutions (NbS) can synergistically improve human well-being and biodiversity in urban areas by enhancing ecosystem functions and services. Scaling out NbS, understood as the widespread application of NbS in multiple contexts, is an important policy ambition to increase the number of people and regions benefiting from NbS. However, designing scaling-out strategies requires considering critical aspects of the nexus between NbS and ecosystem services (ES) supply to yield large-scale benefits effectively. The aim of the study is twofold: (i) to simulate the stepwise implementation of multiple NbS and (ii) to quantify their cumulative impacts on ES supply and beneficiaries. The NbS implementation was guided by a method that prioritizes different NbS types in multiple sites, by combining spatial information on ES demand and land suitability with a qualitative description of ES supply by NbS type. The cumulative impacts were computed stepwise using GIS modeling. The study was conducted in Las Palmas de Gran Canaria (Spain), focusing on five types of NbS (urban forests, urban parks, community gardens, infiltration ponds), and five ES (runoff mitigation, stormwater treatment, soil erosion control, recreation, and food supply). Overall, 179 NbS were simulated in 130 sites distributed in the city. The findings showed an improvement in all services, albeit significant increments occurred only for certain services. We observed a non-linear relationship between NbS and impacts, including constant, positive, and negative trends. We discussed several factors that were determinants of impacts, providing insights on how to design scaling-out strategies from the biophysical perspective. Moreover, we reflected on the role of multifunctionality, social preferences, and economic feasibility as prioritization criteria for NbS. Finally, we addressed the imple-mentation of NbS scaling-out strategies from a broader perspective, deepening governance, economic and planning challenges, and potential actions for the case study.",Not About Sufficiency
Causal inference for time series,"Many research questions in Earth and environmental sciences are inherently causal, requiring robust analyses to establish whether and how changes in one variable cause changes in another. Causal inference provides the theoretical foundations to use data and qualitative domain knowledge to quantitatively answer these questions, complementing statistics and machine learning techniques. However, there is still a broad language gap between the methodological and domain science communities. In this Technical Review, we explain the use of causal inference frameworks with a focus on the challenges of time series data. Domain-adapted explanations, method guidance and practical case studies provide an accessible summary of methods for causal discovery and causal effect estimation. Examples from climate and biogeosciences illustrate typical challenges, such as contemporaneous causation, hidden confounding and non-stationarity, and some strategies to address these challenges. Integrating causal thinking into data-driven science will facilitate process understanding and more robust machine learning and statistical models for Earth and environmental sciences, enabling the tackling of many open problems with relevant environmental, economic and societal implications. Earth sciences often investigate the causal relationships between processes and events, but there is confusion about the correct use of methods to learn these relationships from data. This Technical Review explains the application of causal inference techniques to time series and demonstrates its use through two examples of climate and biosphere-related investigations.",Not About Sufficiency
Machine Learning-Enabled NIR Spectroscopy. Part 2: Workflow for Selecting a Subset of Samples from Publicly Accessible Data,"An increasingly large dataset of pharmaceutics disciplines is frequently challenging to comprehend. Since machine learning needs high-quality data sets, the open-source dataset can be a place to start. This work presents a systematic method to choose representative subsamples from the existing research, along with an extensive set of quality measures and a visualization strategy. The preceding article (Muthudoss et al.. in AAPS PharmSciTech 23, 2022) describes a workflow for leveraging near infrared (NIR) spectroscopy to obtain reliable and robust data on pharmaceutical samples. This study describes the systematic and structured procedure for selecting subsamples from the historical data. We offer a wide range of in-depth quality measures, diagnostic tools, and visualization techniques. A real-world, well-researched NIR dataset was employed to demonstrate this approach. This open-source tablet dataset (http://www.models.life.ku.dk/Tablets) consists of different doses in milligrams, different shapes, and sizes of dosage forms, slots in tablets, three different manufacturing scales (lab, pilot, production), coating differences (coated vs uncoated), etc. This sample is appropriate; that is, the model was developed on one scale (in this research, the lab scale), and it can be great to investigate how well the top models are transferable when tested on new data like pilot-scale or production (full) scale. A literature review indicated that the PLS regression models outperform artificial neural network-multilayer perceptron (ANN-MLP). This work demonstrates the selection of appropriate hyperparameters and their impact on ANN-MLP model performance. The hyperparameter tuning approaches and performance with available references are discussed for the data under investigation. Model extension from lab-scale to pilot-scale/production scale is demonstrated.",Not About Sufficiency
Scaling fuel sprays for different size diesel engines,"Diesel engines are widely used not only for heavy-duty vehicles such as ships and trucks but also for light-duty passenger cars, with a wide range of bore diameters. Since development of a new diesel engine is usually expensive and time consuming, the ability to accurately predict engine performance from existing models can reduce the resources and time required in the engine development procedure. However, so far knowledge on scaling diesel engines is far from adequacy, particularly for spray combustion processes. In this paper, two injectors with nozzle hole diameters of 0.11 mm and 0.14 mm are adopted for scaling fuel sprays under the non evaporating conditions based on the three similarly rules. Firstly, the existence of similarly is theoretically analyzed in diesel fuel sprays of both free injection and impinging upon a wall. Then, the similarly of diesel injection rate is experimentally verified by using the Bosch long tube method. Finally, the similarly of spray angle, tip penetration length, excess air ratio and impinging spray characteristics are investigated under various injection pressures and ambient densities in an optically accessible constant volume vessel by the high-speed shadowgraphy. The theoretical analysis and experimental results reveal that the similarly rule keeping the injection pressure constant is more preferable for scaling the spray angle, tip penetration length and excess air ratio, while the other two similarly rules with reduced injection pressure result in narrowed spray angle and increased spray tip penetration length. With respect to the post-impingement behavior, although the spray from the large-hole nozzle impinges upon the wall later than those of the small-hole nozzles with the speed rule and the lift-off rule, the rebound height of the large-hole nozzle is higher than those by the small holes with the speed and the lift-off rule. The mechanism of the above phenomena is discussed in depth.",Not About Sufficiency
SAFIR - The Voice of Services,"The SAFIR project has a double objective. On the one hand, it intends to give professional users access to ICT technology in those situations where traditional ICT devices cannot be used or access to applications or databases is not possible and on the other hand, it intends to considerably contribute to the removal or at least the reduction of the Digital Divide for residential users. The SAFIR consortium has developed a technological platform, based on the use of human speech, that allows those users, be it professional or residential ones, to get in touch with the required applications and databases anytime, anywhere in a hands-free and eyes-free manner. Various pilot tests have proven that these objectives are reached and that the SAFIR technology will dramatically change our lives. once implemented at large scale.",Not About Sufficiency
Rule Discovery in Milk Content towards Mastitis Diagnosis: Dealing with Farm Heterogeneity over Multiple Years through Classification Based on Associations,"Simple Summary Invisible (subclinical) mastitis decreases milk quality and production. Invisible mastitis is linked to an increased use of antimicrobials. The risk of the emergence of antimicrobial-resistant bacteria is a major public health concern worldwide. Therefore, early detection of infected cows is of great importance. Machine learning has opened a new avenue for early mastitis prediction based on simple and accessible milking parameters, such as milk volume, fat, protein, lactose, electrical conductivity (EC), milking time, and milking peak flow. However, farm heterogeneity is a major challenge where multiple patterns can predict mastitis. Here, we employed a classification based on associations and scaling approach for multiple pattern discovery over multiple years. The approach we have developed helps to address farm heterogeneity and generalise machine learning-based diagnosis of mastitis worldwide. Subclinical mastitis, an economically challenging disease of dairy cattle, is associated with an increased use of antimicrobials which reduces milk quantity and quality. It is more common than clinical mastitis and far more difficult to detect. Recently, much attention has been paid to the development of machine-learning expert systems for early detection of subclinical mastitis from milking features. However, differences between animals within a farm as well as between farms, particularly across multiple years, are major obstacles to the generalisation of machine learning models. Here, for the first time, we integrated scaling by quartiling with classification based on associations in a multi-year study to deal with farm heterogeneity by discovery of multiple patterns towards mastitis. The data were obtained from one farm comprising Holstein Friesian cows in Ongaonga, New Zealand, using an electronic automated monitoring system. The data collection was repeated annually over 3 consecutive years. Some discovered rules, such as when the milking peak flow is low, electrical conductivity (EC) of milk is low, milk lactose is low, milk fat is high, and milk volume is low, the cow has subclinical mastitis, reached high confidence (>70%) in multiple years. On averages, over 3 years, low level of milk lactose and high value of milk EC were part of 93% and 83.8% of all subclinical mastitis detecting rules, offering a reproducible pattern of subclinical mastitis detection. The scaled year-independent combinational rules provide an easy-to-apply and cost-effective machine-learning expert system for early detection of hidden mastitis using milking parameters.",Not About Sufficiency
"Robust learning from noisy, incomplete, high-dimensional experimental data via physically constrained symbolic regression","Machine learning offers an intriguing alternative to first-principle analysis for discovering new physics from experimental data. However, to date, purely data-driven methods have only proven successful in uncovering physical laws describing simple, low-dimensional systems with low levels of noise. Here we demonstrate that combining a data-driven methodology with some general physical principles enables discovery of a quantitatively accurate model of a non-equilibrium spatially extended system from high-dimensional data that is both noisy and incomplete. We illustrate this using an experimental weakly turbulent fluid flow where only the velocity field is accessible. We also show that this hybrid approach allows reconstruction of the inaccessible variables - the pressure and forcing field driving the flow. Reinbold et al. propose a physics-informed data-driven approach that successfully discovers a dynamical model using high-dimensional, noisy and incomplete experimental data describing a weakly turbulent fluid flow. This approach is relevant to other non-equilibrium spatially-extended systems.",Not About Sufficiency
Urban Green Space Pattern in Core Cities of the Greater Bay Area Based on Morphological Spatial Pattern Analysis,"Urban green spaces (UGSs) play a crucial role in supporting urban ecological systems and improving human well-being in cities. The spatial patterns of UGS are vital bases for analyzing various ecological processes. However, few studies have investigated morphological UGS patterns, especially in high-density cities. The Guangdong-Hong Kong-Macao Greater Bay Area (GBA) in China is one of the four major bay areas in the world. The aim of this study was to investigate the patterns and distributions of UGS in the core GBA cities (Guangzhou, Shenzhen, Zhuhai, Hong Kong, and Macao), and discuss the shortcomings and potential environmental impacts of the contemporary patterns of UGS. Morphological spatial pattern analysis (MSPA) was used to analyze the spatial UGS pattern. Seven MSPA metrics (core, islet, perforation, edge, loop, bridge, and branch) were assessed to measure morphological UGS patterns. The results showed that: (1) Hong Kong has the highest quality habitat, with a large and continuous distribution of UGSs, and a few smaller green spaces scattered in built-up areas; (2) Guangzhou's UGSs are unevenly distributed, with large green spaces concentrated in the northern part of the city and many small, scattered green spaces distributed in built-up areas, demonstrating the most prominent pattern of green space fragmentation; (3) green space patches in the Shenzhen-Hong Kong region exhibit a relatively complex form; and (4) the UGS in Zhuhai-Macao is relatively discrete, and its connectivity is relatively low. These findings not only improve the depth of understanding of the spatial pattern of UGS in the GBA, but also confirm the applicability of MSPA in the analysis of spatial patterns of UGS.",Not About Sufficiency
A Learn-to-Rank Approach for Predicting Road Cycling Race Outcomes,"Professional road cycling is a very competitive sport, and many factors influence the outcome of the race. These factors can be internal (e.g., psychological preparedness, physiological profile of the rider, and the preparedness or fitness of the rider) or external (e.g., the weather or strategy of the team) to the rider, or even completely unpredictable (e.g., crashes or mechanical failure). This variety makes perfectly predicting the outcome of a certain race an impossible task and the sport even more interesting. Nonetheless, before each race, journalists, ex-pro cyclists, websites and cycling fans try to predict the possible top 3, 5, or 10 riders. In this article, we use easily accessible data on road cycling from the past 20 years and the Machine Learning technique Learn-to-Rank (LtR) to predict the top 10 contenders for 1-day road cycling races. We accomplish this by mapping a relevancy weight to the finishing place in the first 10 positions. We assess the performance of this approach on 2018, 2019, and 2021 editions of six spring classic 1-day races. In the end, we compare the output of the framework with a mass fan prediction on the Normalized Discounted Cumulative Gain (NDCG) metric and the number of correct top 10 guesses. We found that our model, on average, has slightly higher performance on both metrics than the mass fan prediction. We also analyze which variables of our model have the most influence on the prediction of each race. This approach can give interesting insights to fans before a race but can also be helpful to sports coaches to predict how a rider might perform compared to other riders outside of the team.",Not About Sufficiency
Exploring the development of scientific research on Marine Protected Areas: From conservation to global ocean sustainability,"Marine Protected Areas (MPAs) are playing a central role in the achievement of ocean sustainability and, since 2000, their global coverage has increased over ten times. The success of MPAs, and therefore the delivery of their potential outcomes for human well-being and global sustainability, requires multi-disciplinary, holistic, and comprehensive approaches for its achievement. In this study, the global scientific literature on MPAs was quantitatively reviewed through bibliometrics approaches, investigating patterns and trends in its development over time. In particular, bibliometric network and citation burst analyses of keywords were performed using VOSviewer and CiteSpace software. The bibliographic search on the Scopus and Web of Science databases resulted in a total number of 5908 and 6036 scientific documents published on MPAs. The network analysis of the keywords co-occurrence produced four main clusters whose connections and overlapping showed a multidisciplinary structure of MPA science, in which the ecological, social, and economic domains of research are strongly interlinked. Temporal analyses showed a recent focus on topics related to the social-ecological systems theory (e.g., ecosystem services, marine spatial planning, governance, and small-scale fisheries) suggesting that newer research lines recognize the importance of integrating the ?human dimension? in conservation and sustainability studies. Overall, the results of both the performed bibliometric analyses pointed out the evolution of MPA science from the conventional concept of ?marine reserves? to a broader scope integrating ecological, economic, and social aspects. In conclusion, MPA research is timely responding to the identification of MPAs as ocean sustainability tools, opening MPA science to multi-disciplinary research lines by linking the ecological and socio-economic dimensions of sustainability. MPA research is expected to play a crucial role in generating the interdisciplinary scientific knowledge needed to fully contribute to global ocean sustainability and human wellbeing.",Not About Sufficiency
Functional Magnetic Resonance Imaging Data Manipulation - A new approach,"With constant technological advances, the organizations responsible for developing healthcare standards have made efforts to improve interoperability and information exchange between different hospital information systems. In medical imaging, images are the inputs and reports the outputs. In 2000, part 23 of the DICOM standard (Digital Imaging and Communications in Medicine Structured Report) was released as an official extension. Structured Report merges the radiologists' and physicians' text report and relevant images into a single report. Functional Magnetic Resonance Imaging (fMRI) is a technique of medical imaging that can make great use of the structured reports, 3D potentialities, studies comparisons and also from structuring the information. In this work we present a technique for displaying, organizing, managing and storing fMRI studies. This approach is straightforward, accessible and standardized, and aims to reduce the required time and associated costs with fMRI studies analysis.",Not About Sufficiency
Implementation of Machine Learning Algorithms in Arabic Sentiment Analysis Using N-Gram Features,"Sentiment analysis (SA) is a scholarly process of extricating and classifying individuals' emotions and feedbacks expressed in source text content. It is one of the pursued subfields of Computational Linguistics (CL) and Natural Language Processing (NLP). The evolution of social media based applications has generated a big amount of personalized reviews of different related information on the Web in the form of tweets, status updates, and many others. Several approaches have come into the spotlight in recent years to accomplish SA, the most part of SA researches have been applied utilizing the English language. SA in Arabic online social media may be slacking behind commonly because of the difficulties with handling the morphologically complex Arabic natural language and the lack and absence of accessible tools and assets for extracting Arabic opinions from the text. This research is aimed to analyze the collected twitter posts in different Arabic Dialects and a comparison between the various algorithms used for SA with various n-gram as a feature extraction method. The measurement of the performance of different algorithms is evaluated in terms of recall, precision, f-measure, and accuracy. The experiment results show that unigram with Passive Aggressive (PA) or Ridge Regression (RR) gives the highest accuracy 99.96 %. (C) 2019 The Authors. Published by Elsevier Ltd.",Not About Sufficiency
PVP-SVM: Sequence-Based Prediction of Phage Virion Proteins Using a Support Vector Machine,"Accurately identifying bacteriophage virion proteins from uncharacterized sequences is important to understand interactions between the phage and its host bacteria in order to develop new antibacterial drugs. However, identification of such proteins using experimental techniques is expensive and often time consuming: hence, development of an efficient computational algorithm for the prediction of phage virion proteins (PVPs) prior to in vitro experimentation is needed. Here, we describe a support vector machine (SVM)-based PVP predictor, called PVP-SVM, which was trained with 136 optimal features. A feature selection protocol was employed to identify the optimal features from a large set that included amino acid composition, dipeptide composition, atomic composition, physicochemical properties, and chain-transition-distribution. PVP-SVM achieved an accuracy of 0.870 during leave-one-out cross-validation, which was 6% higher than control SVM predictors trained with all features, indicating the efficiency of the feature selection method. Furthermore, PVP-SVM displayed superior performance compared to the currently available method, PVPred, and two other machine-learning methods developed in this study when objectively evaluated with an independent dataset. For the convenience of the scientific community, a user-friendly and publicly accessible web server has been established at www.thegleelab.org/PVP-SVM/PVP-SVM.html.",Not About Sufficiency
Effect of Housing Type on Subjective Well-Being: Focus on New Town Developments in South Korea,"As urbanization has matured in many areas, interest in qualitative values such as residents' residential satisfaction, social capital, and subjective well-being (SWB) has increased. However, few studies have investigated the influence of different urban environments, including housing types, on residential satisfaction, social capital, and SWB. The present study compares residential satisfaction, social capital, and SWB levels of two different housing types: high-rise apartments and low-rise dwellings. To this end, the authors analyze the large-scale survey data collected from 20,000 residents in Gyeonggi-do, Republic of Korea, using structural equation modeling. The analysis found that the satisfaction level of apartment residents tends to be higher, but their social capital level tends to be lower than their counterparts in low-rise dwellings. Because residential satisfaction and social capital dimensions are positively associated with the dimensions of SWB, the model identified apartments' positive indirect effects via residential satisfaction and negative indirect effects via social capital on subjective well-being. The results imply countervailing effects of apartment developments on residents' SWB: although offering more satisfactory residential environments, high-rise apartments may discourage social capital formation. These results call for urban planning and policy approaches that encourage social ties and interactions, thereby eventually improving residents' SWB.",Not About Sufficiency
Impact of Land Use Diversity on Daytime Social Segregation Patterns in Santiago de Chile,"Latin American cities are known for their high levels of marginality, segregation and inequality. As such, these issues have been the subject of substantial discussions in academia, with the predominant approach being the study of residential segregation, or what we call ""nighttime segregation"". Another dimension of urban sociability, related to labor, is what we call ""daytime segregation"", which has been far less studied. This article makes an original methodological contribution to the measurement of non-residential or daytime segregation based on data from mobility surveys. It seeks to explain this segregation measurement according to the diversity and distribution of land uses, as well as other characteristics of the built stock, such as land price and built-up density. We measured daytime social mix in urban spaces, and we show how it highly relates to land use diversity in a Latin American megacity, such as Santiago, Chile. We found that land use diversity plays a key role in enhancing the daytime social diversity of urban spaces, contributing to generate a more heterogeneous city and social gatherings during working days. This research is not only a contribution to the understanding of sociability patterns in cities but is also a contribution to public policy and the work of urban planners, as it informs the development of more diverse and integrated cities, which is a key tool for strengthening democracy, the exchange of ideas, the economy and social welfare.",Not About Sufficiency
Strengthening the Surveillance of Antimicrobial Resistance in India Using Integrative Technologies,"Background:The antimicrobial resistance (AMR) situation in India is alarming. In the absence of newer antibiotics, the best possible approach is to efficiently use the existing antimicrobials through surveillance of resistance. The data generated by AMR surveillance across the country has immense potential to drive policy decisions. However, this data is available in a variety of sources. It is imperative to have tools to integrate the data generated across the country into a single data repository. MethodsAn ensemble of tools (i-AMRSS, i-DIA, and i-AMRIT) have been designed and developed by the data management team at the Indian Council of Medical Research (ICMR) to strengthen surveillance of antimicrobial resistance in India. ResultsThe i-AMRSS is a web-based ICMR's AMR surveillance system, collecting data from tertiary care centers across the country and sending it to the one-stop data repository. The i-DIA is a web-based API that simplifies the AMR data interoperability by seamlessly importing most of the LIS / HIS data from CSV files into a central, one-stop data repository. The i-AMRIT is a standalone ICMR's AMR surveillance system using integrative technologies, collecting data from all the labs across the country and sending the lab-specific cumulative data to the one-stop data repository. DiscussionThe tools are being used in ICMR's AMR Network and have collected over 0.4 million patient records to date. The complete system is presently being used to capture human susceptibility testing data and can be extended for capturing data using the 'One Health' approach. The authors plan to make the system compliant with FHIR standards to enable interoperability with other countries.",Not About Sufficiency
Use of Artificial Intelligence in the Identification and Management of Frailty: A Scoping Review Protocol,"IntroductionRapid population ageing and associated health issues such as frailty are a growing public health concern. While early identification and management of frailty may limit adverse health outcomes, the complex presentations of frailty pose challenges for clinicians. Artificial intelligence (AI) has emerged as a potential solution to support the early identification and management of frailty. In order to provide a comprehensive overview of current evidence regarding the development and use of AI technologies including machine learning and deep learning for the identification and management of frailty, this protocol outlines a scoping review aiming to identify and present available information in this area. Specifically, this protocol describes a review that will focus on the clinical tools and frameworks used to assess frailty, the outcomes that have been evaluated and the involvement of knowledge users in the development, implementation and evaluation of AI methods and tools for frailty care in clinical settings.Methods and analysisThis scoping review protocol details a systematic search of eight major academic databases, including Medline, Embase, PsycInfo, Cumulative Index to Nursing and Allied Health Literature (CINAHL), Ageline, Web of Science, Scopus and Institute of Electrical and Electronics Engineers (IEEE) Xplore using the framework developed by Arksey and O'Malley and enhanced by Levac et al and the Joanna Briggs Institute. The search strategy has been designed in consultation with a librarian. Two independent reviewers will screen titles and abstracts, followed by full texts, for eligibility and then chart the data using a piloted data charting form. Results will be collated and presented through a narrative summary, tables and figures.Ethics and disseminationSince this study is based on publicly available information, ethics approval is not required. Findings will be communicated with healthcare providers, caregivers, patients and research and health programme funders through peer-reviewed publications, presentations and an infographic.Registration detailsOSF Registries (https://doi.org/10.17605/OSF.IO/T54G8).",Not About Sufficiency
Scalable spatiotemporal prediction with Bayesian neural fields,"Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in diverse applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As the scale of modern datasets increases, there is a growing need for statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle many observations. This article introduces the Bayesian Neural Field (BayesNF), a domain-general statistical model that infers rich spatiotemporal probability distributions for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust predictive uncertainty quantification. Evaluations against prominent baselines show that BayesNF delivers improvements on prediction problems from climate and public health data containing tens to hundreds of thousands of measurements. Accompanying the paper is an open-source software package (https://github.com/google/bayesnf) that runs on GPU and TPU accelerators through the Jax machine learning platform. Spatiotemporal data consisting of measurements gathered at different times and locations is challenging to analyse due to variability and noise impact across different scales. The authors propose a statistical approach that delivers models of large-scale spatiotemporal datasets applicable to data-analysis tasks of forecasting and interpolation.",Not About Sufficiency
Quantitative approaches for including equity in risk and resilience infrastructure planning analyses,"Risk and resilience assessments for critical infrastructure focus on myriad objectives, from natural hazard evaluations to optimizing investments. Although research has started to characterize externalities associated with current or possible future states, incorporation of equity priorities at project inception is increasingly being recognized as critical for planning related activities. However, there is no standard methodology that guides development of equity-informed quantitative approaches for infrastructure planning activities. To address this gap, we introduce a logic model that can be tailored to capture nuances about specific geographies and community priorities, effectively incorporating them into different mathematical approaches for quantitative risk assessments. Specifically, the logic model uses a graded, iterative approach to clarify specific equity objectives as well as inform the development of equations being used to support analysis. We demonstrate the utility of this framework using case studies spanning aviation fuel, produced water, and microgrid electricity infrastructures. For each case study, the use of the logic model helps clarify the ways that local priorities and infrastructure needs are used to drive the types of data and quantitative methodologies used in the respective analyses. The explicit consideration of methodological limitations (e.g., data mismatches) and stakeholder engagements serves to increase the transparency of the associated findings as well as effectively integrate community nuances (e.g., ownership of assets) into infrastructure assessments. Such integration will become increasingly important to ensure that planning activities (which occur throughout the lifecycle of the infrastructure projects) lead to long-lasting solutions to meet both energy and sustainable development goals for communities.",Not About Sufficiency
The Disproportionate Impact of COVID-19 among Undocumented Immigrants and Racial Minorities in the US,"Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the virus responsible for coronavirus disease 2019 (COVID-19), has had an unprecedented effect, especially among under-resourced minority communities. Surveillance of those at high risk is critical for preventing and controlling the pandemic. We must better understand the relationships between COVID-19-related cases or deaths and characteristics in our most vulnerable population that put them at risk to target COVID-19 prevention and management efforts. Population characteristics strongly related to United States (US) county-level data on COVID-19 cases and deaths during all stages of the pandemic were identified from the onset of the epidemic and included county-level socio-demographic and comorbidities data, as well as daily meteorological modeled observation data from the North American Regional Reanalysis (NARR), and the NARR high spatial resolution model to assess the environment. Advanced machine learning (ML) approaches were used to identify outbreaks (geographic clusters of COVID-19) and included spatiotemporal risk factors and COVID-19 vaccination efforts, especially among vulnerable and underserved communities. COVID-19 outcomes were found to be negatively associated with the number of people vaccinated and positively associated with age, the prevalence of cardiovascular disease, diabetes, and the minority population. There was also a strong positive correlation between unauthorized immigrants and the prevalence of COVID-19 cases and deaths. Meteorological variables were also investigated, but correlations with COVID-19 were relatively weak. Our findings suggest that COVID-19 has had a disproportionate impact across the US population among vulnerable and minority communities. Findings also emphasize the importance of vaccinations and tailored public health initiatives (e.g., mask mandates, vaccination) to reduce the spread of COVID-19 and the number of COVID-19 related deaths across all populations.",Not About Sufficiency
Understanding the accountability issues of the immunization workforce for the Expanded Program on Immunization (EPI) in Balochistan: An exploratory study,"Background Among all provinces of Pakistan, immunization coverage is poorest in Balochistan. There is no provincial immunization policy for Balochistan including a lack of human resource management policy. Maladministration and lack of accountability leading to health workforce demotivation and poor performance can be a crucial reason behind an inefficient and ineffective immunization program in Balochistan. The objective of this study was to better understand the accountability issues of EPI workforce at provincial and district level leading to poor program performance and to identify governance strategies for management of inefficiency, demotivation and absenteeism. Methods An exploratory qualitative study was carried out to explore issues related to human resource (HR) accountability within immunization program of Balochistan for developing strategies to improve performance of the program. Five districts were selected using purposive sampling based on the comparative poor and good routine immunization coverages and Human Development Index (HDI). Interviews were conducted with EPI Staff and District Health Officers (DHOs) in each district including provincial EPI Staff. A semi-structured and open-ended questionnaire was used. Thematic analysis was used to analyze the qualitative data. Results Major barriers to HR accountability included lack of a written HR policy, proper service structure including promotions and benefits and understanding of accurate job description coupled with inadequate HR development budget and activities. Most important demotivating factors were inadequate number of vaccinators, deficient budget with delayed wage and salary disbursements resulting in poor immunization coverage and a lack of appreciation/feedback from senior management for the frontline workers. Key challenge for vaccinators was poor community orientation and mobilization. Although, the participants proposed some solutions based on their perspective, none were elaborate or precise. Conclusions Adaptation of National Immunization Policy tailored to the provincial context and proper implementation is much needed. Review of current allocations of vaccinators and need based relocation along with recruitment of new vaccinators with clear job description and terms of reference is desirable. Review of current incentive structure is required. Finally, trust building between community and the vaccination program and social mobilization about the benefits of vaccinations through community influential is vital.",Not About Sufficiency
Lessons from Rodent Models for Genetic and Age-Related Hearing Loss,"This chapter compares the six most heavily studied rodent models with regard to hearing-in-aging and the availability of mutant lines that recapitulate human genetic hearing loss. Four of the six models are available only as outbreds, and much of that work has been based on genetically nonstandard animals of unclear origin. Some of these (guinea pigs and chinchillas) may no longer resemble their wild counterparts. Some results from outbred models may not be reproducible, since it may be impossible for experimenters to know if they are testing the same genetic models. Likewise, engineered or induced mutations onto outbred lines may not be productive because characterization can be confounded by variable and unknown modifier genes. Naturally arising coat color-related mutations may influence hearing through an absence of melanin or melanocytes. These lines may not be commercially available, however. Hamsters are not well described with respect to detailed hearing or aging studies. Gerbils, guinea pigs, and chinchillas are well explored both as general hearing models and as aging models. Inbred mice and rats have become the primary models for most research over the last 20 years. Inbred models offer a high degree of genetic standardization and reproducibility of results. Their short lifespans and the availability of lines with progressive hearing loss have made mice and rats popular for aging research. They also foster transgenic methods and gene discovery, but mice and rats may not be optimal for studies that require low-frequency hearing or readily accessible inner ear fluid spaces.",Not About Sufficiency
Machine Learning-Based Prediction Models for Depression Symptoms Among Chinese Healthcare Workers During the Early COVID-19 Outbreak in 2020: A Cross-Sectional Study,"BackgroundThe 2019 novel coronavirus (COVID-19)-related depression symptoms of healthcare workers have received worldwide recognition. Although many studies identified risk exposures associated with depression symptoms among healthcare workers, few have focused on a predictive model using machine learning methods. As a society, governments, and organizations are concerned about the need for immediate interventions and alert systems for healthcare workers who are mentally at-risk. This study aims to develop and validate machine learning-based models for predicting depression symptoms using survey data collected during the COVID-19 outbreak in China. MethodSurveys were conducted of 2,574 healthcare workers in hospitals designated to care for COVID-19 patients between 20 January and 11 February 2020. The patient health questionnaire (PHQ)-9 was used to measure the depression symptoms and quantify the severity, a score of >= 5 on the PHQ-9 represented depression symptoms positive, respectively. Four machine learning approaches were trained (75% of data) and tested (25% of data). Cross-validation with 100 repetitions was applied to the training dataset for hyperparameter tuning. Finally, all models were compared to evaluate their predictive performances and screening utility: decision tree, logistics regression with least absolute shrinkage and selection operator (LASSO), random forest, and gradient-boosting tree. ResultsImportant risk predictors identified and ranked by the machine learning models were highly consistent: self-perceived health status factors always occupied the top five most important predictors, followed by worried about infection, working on the frontline, a very high level of uncertainty, having received any form of psychological support material and having COVID-19-like symptoms. The area under the curve [95% CI] of machine learning models were as follows: LASSO model, 0.824 [0.792-0.856]; random forest, 0.828 [0.797-0.859]; gradient-boosting tree, 0.829 [0.798-0.861]; and decision tree, 0.785 [0.752-0.819]. The calibration plot indicated that the LASSO model, random forest, and gradient-boosting tree fit the data well. Decision curve analysis showed that all models obtained net benefits for predicting depression symptoms. ConclusionsThis study shows that machine learning prediction models are suitable for making predictions about mentally at-risk healthcare workers predictions in a public health emergency setting. The application of multidimensional machine learning models could support hospitals' and healthcare workers' decision-making on possible psychological interventions and proper mental health management.",Not About Sufficiency
Advancing safe mobility: A global analysis of research trends in safe route planning,"Safe route planning has become an increasingly important area of research in recent years due to growing concerns about pedestrian and traffic safety, rising traffic volumes and densities in urban areas, and advancements in smart vehicle and transportation technologies. This study conducted a bibliometric analysis of publications on safe route planning retrieved from the Web of Science database between January 2000 and January 2023 to understand the state of the field. A total of 1546 publications authored by 5423 researchers from 84 countries were analyzed. The findings identified the United States, China, India, South Korea, and Spain as the most productive countries, while the University of North Carolina emerged as the most productive organization. Engineering, computer science, transportation, public health, and automation were revealed to be the dominant initial research areas, although interest grew from other domains like urban planning and the environment over time. Analysis of publications by year showed a steady rise in output starting from 2008. Notable influential publications and highly cited authors in the field were also identified. Several research themes and terms like path planning, safety, walking, and route to school were highlighted through keyword analysis. This study provided novel insights into the evolving international landscape, topics, and influential contributors in safe route planning research over the past two decades. Limitations in database coverage and analytical techniques necessitate future work to enhance understanding in this critical domain.",Not About Sufficiency
Standard for improving emergency information interoperability: the HL7 data elements for emergency department systems,"Background Emergency departments in the United States service over 130 million visits per year. The demands for information from these visits require interoperable data exchange standards. While multiple data exchange specifications are in use, none have undergone rigorous standards review. This paper describes the creation and balloting of the Health Level Seven (HL7) Data Elements for Emergency Department Systems (DEEDS). Methods Existing data exchange specifications were collected and organized into categories reflecting the workflow of emergency care. The concepts were then mapped to existing standards for vocabulary, data types, and the HL7 information model. The HL7 community then processed the specification through the normal balloting process addressing all comments and concerns. The resulting specification was then submitted for publication as an HL7 informational standard. Results The resulting specification contains 525 concepts related to emergency care required for operations and reporting to external agencies. An additional 200 of the most commonly ordered laboratory tests were included. Each concept was given a unique identifier and mapped to Logical Observation Identifiers, Names, and Codes (LOINC). HL7 standard data types were applied. Discussion The HL7 DEEDS specification represents the first set of common ED related data elements to undergo rigorous standards development. The availability of this standard will contribute to improved interoperability of emergency care data.",Not About Sufficiency
Green infrastructure and socioeconomic dynamics in London low-income neighbourhoods: A 120-year perspective,"Green infrastructure (GI) and nature-based solutions (NBS) are increasingly adopted as urban planning and development solutions to enable sustainable and healthy urban transitions. However, urban green(ing) has featured as an instrument of urban planning for several centuries. The extent and causal effect of these instruments in delivering environmental and social sustainability outcomes are, however, often unclear, but raise concerns of green gentrification. This paper presents a 120-year analysis of GI in London low-income neighbourhoods drawing on below (soils) and above (urban greenery) components of GI. Three testable relationships are analysed in a long-term perspective (1881-2001): soils, geology and initial socio-spatial structures; impact of urban greenery in comparable low-income neighbourhoods; and the impact of urban greenery in low-income neighbourhoods set in their wider urban systems adjustments. The results suggest that new greenery in comparable low-income neighbourhoods had little independent effect on neighbourhood socioeconomic characteristics. Where gentrification does occur, wider processes of social, economic, and technological adjustment, rather than urban greening, is likely causal. The non-random distribution of soils is found to anchor socio-spatial structures. Future productivity of GI and NBS, e.g., sponginess or mass of green that can be sustained, will likely also vary spatially, and continue to anchor socio-spatial structures.",Not About Sufficiency
Application of machine learning approaches to predict the impact of ambient air pollution on outpatient visits for acute respiratory infections,"With a remarkable increase in industrialization among fast-developing countries, air pollution is rising at an alarming rate and has become a public health concern. The study aims to examine the effect of air pollution on patient's hospital visits for respiratory diseases, particularly Acute Respiratory Infections (ARI). Outpatient hospital visits, air pollution and meteorological parameters were collected from March 2018 to October 2021. Eight machine learning algorithms (Random Forest model, K-Nearest Neighbors regression model, Linear regression model, LASSO regression model, Decision Tree Regressor, Support Vector Regression, X.G. Boost and Deep Neural Network with 5-layers) were applied for the analysis of daily air pollutants and outpatient visits for ARI. The evaluation was done by using 5-cross-fold confirmations. The data was randomly divided into test and training data sets at a scale of 1:2, respectively. Results show that among the studied eight machine learning models, the Random Forest model has given the best performance with R2 = 0.606, 0.608 without lag and 1-day lag respectively on ARI patients and R2 = 0.872, 0.871 without lag and 1-day lag respectively on total patients. All eight models did not perform well with the lag effect on the ARI patient dataset but performed better on the total patient dataset. Thus, the study did not find any significant association between ARI patients and ambient air pollution due to the intermittent availability of data during the COVID-19 period. This study gives insight into developing machine learning programs for risk prediction that can be used to predict analytics for several other diseases apart from ARI, such as heart disease and other respiratory diseases.",Not About Sufficiency
The discovery BPD (D-BPD) program: study protocol of a prospective translational multicenter collaborative study to investigate determinants of chronic lung disease in very low birth weight infants,"BackgroundPremature birth is a growing and serious public health problem affecting more than one of every ten infants worldwide. Bronchopulmonary dysplasia (BPD) is the most common neonatal morbidity associated with prematurity and infants with BPD suffer from increased incidence of respiratory infections, asthma, other forms of chronic lung illness, and death (Day and Ryan, Pediatr Res 81: 210-213, 2017; Isayama et la., JAMA Pediatr 171:271-279, 2017). BPD is now understood as a longitudinal disease process influenced by the intrauterine environment during gestation and modulated by gene-environment interactions throughout the neonatal and early childhood periods. Despite of this concept, there remains a paucity of multidisciplinary team-based approaches dedicated to the comprehensive study of this complex disease.MethodsThe Discovery BPD (D-BPD) Program involves a cohort of infants <1,250g at birth prospectively followed until 6years of age. The program integrates analysis of detailed clinical data by machine learning, genetic susceptibility and molecular translation studies.DiscussionThe current gap in understanding BPD as a complex multi-trait spectrum of different disease endotypes will be addressed by a bedside-to-bench and bench-to-bedside approach in the D-BPD program. The D-BPD will provide enhanced understanding of mechanisms, evolution and consequences of lung diseases in preterm infants. The D-BPD program represents a unique opportunity to combine the expertise of biologists, neonatologists, pulmonologists, geneticists and biostatisticians to examine the disease process from multiple perspectives with a singular goal of improving outcomes of premature infants.Trial registrationDoes not apply for this study.",Not About Sufficiency
Measuring accessibility and utilization of public spaces in Famagusta,"Public spaces have a central role, both physically and functionally, in urban planning and development. Many urban theorists state their significant role as one of the principal components of a healthy urban setting. This is in addition to their functional role, when they increase a sense of community when intensive social interaction takes place in these areas. However, recently, they have started to lose significance, when they are neglected in the urban planning process, or when existing spaces are lost. Additionally, accessibility and utilization of these areas decreases, since public spaces are neglected in urban planning and development processes. In this study, public spaces are assessed in terms of accessibility and utilization, regarding the effects of rapid urban growth on their physical and functional structure. This study first evaluates the significance of public spaces in an urban setting; second, determines the variables effective in terms of their accessibility and utilization; third, assesses the factors affecting the accessibility and utilization of public spaces through a questionnaire survey on the role of public spaces in social interaction, and concludes with an evaluation of the results and suggestions for further research. (C) 2004 Elsevier Ltd. All rights reserved.",Not About Sufficiency
Psychosocial experiences modulate asthma-associated genes through gene-environment interactions,"Social interactions and the overall psychosocial environment have a demonstrated impact on health, particularly for people living in disadvantaged urban areas. Here, we investigated the effect of psychosocial experiences on gene expression in peripheral blood immune cells of children with asthma in Metro Detroit. Using RNA-sequencing and a new machine learning approach, we identified transcriptional signatures of 19 variables including psychosocial factors, blood cell composition, and asthma symptoms. Importantly, we found 169 genes associated with asthma or allergic disease that are regulated by psychosocial factors and 344 significant gene-environment interactions for gene expression levels. These results demonstrate that immune gene expression mediates the link between negative psychosocial experiences and asthma risk.",Not About Sufficiency
Attenuated Total Reflection Fourier Transform Infrared Spectral Identification of Bioaerosol Based on 1D-CNN,"Objective As an important component of the atmospheric environment, bioaerosols have a profound effect on environmental quality, climate change, and human health. As environmental and public health problems intensify, the monitoring and identification of bioaerosols have attracted widespread attention. However, traditional bioaerosol identification methods, such as microbial culture and molecular biology techniques, are slow and complex. We combine attenuated total reflection Fourier transform infrared (ATR-FTIR) spectroscopy with one-dimensional convolutional neural network (1D- CNN) to leverage the high sensitivity, non-invasive and real-time advantages of spectroscopic technology, as well as deep learning powerful capabilities in feature extraction and classification of complex spectral data, and build an efficient and accurate bioaerosol identification model. Methods Bioaerosol samples, including three types of bacteria and three types of fungi, are used as the research object, and high- quality infrared absorption spectrum data are collected using a Fourier transform infrared spectrometer with an attenuated total reflection (ATR) accessory. To improve data quality, preprocessing techniques such as wavelet packet transform and Savitzky-Golay filtering are used for baseline correction and noise filtering. On this basis, a 1D- CNN model, including a convolution layer, a pooling layer, a dropout layer, and a fully connected layer, is constructed to utilize its powerful feature extraction and classification capabilities for the fast and accurate identification of bioaerosols. The effectiveness and superiority of the model are fully verified through reasonable data set division, multi- angle performance evaluation, and comparison with traditional machine learning methods. A mixed sample test plan of different concentrations is designed to further evaluate the model's generalization ability in complex environments. Results and Discussions Through comparative analysis of test set recognition accuracy, the 1D- CNN model proposed in this paper performs exceptionally well in the bioaerosol recognition task, significantly better than the traditional support vector machine (SVM) method. In identifying six bioaerosol samples, the accuracy of the 1D- CNN model reaches 100%, while the SVM achieves only 95%, fully demonstrating the advantages of convolutional neural networks in feature extraction and classification of complex spectral data. The generalization ability and robustness of the 1D- CNN model are further evaluated through methods such as confusion matrix analysis (Fig. 4) and cross- validation (Table 2). We also design tests with mixed samples of Aspergillus at different concentrations to simulate the real- world complexities. Experimental results show that the proposed method performs well in recognition tasks with subtle features, maintaining high accuracy and demonstrating the practicability and scalability of the method. Conclusions To achieve rapid and accurate identification of bioaerosols, we propose a new method based on 1D- CNN and ATR-FTIR. By applying the 1D- CNN deep learning model to feature extraction and classification of ATR-FTIR spectral data, the method achieves 100% accuracy in identifying six common bioaerosol samples, demonstrating significantly better performance than the traditional SVM method. In addition, the constructed model shows high recognition accuracy in cross- validation and low- concentration sample testing. This study illustrates the great potential of combining deep learning technology with ATR-FTIR spectroscopy for rapid and accurate bioaerosol identification, providing a new technical approach for environmental monitoring and public health protection.",Not About Sufficiency
Quantitative Spatial Economics,"The observed uneven distribution of economic activity across space is influenced by variation in exogenous geographical characteristics and endogenous interactions between agents in goods and factor markets. Until the past decade, the theoretical literature on economic geography had focused on stylized settings that could not easily be taken to the data. This article reviews more recent research that has developed quantitative models of economic geography. These models are rich enough to speak to first-order features of the data, such as many heterogeneous locations and gravity equation relationships for trade and commuting. At the same time, these models are sufficiently tractable to undertake realistic counterfactual exercises to study the effect of changes in amenities, productivity, and public policy interventions such as transport infrastructure investments. We provide an extensive taxonomy of the different building blocks of these quantitative spatial models and discuss their main properties and quantification.",Not About Sufficiency
"Modeling Europe's role in the global LNG market 2040: Balancing decarbonization goals, energy security, and geopolitical tensions","This study examines the LNG trade in 2040, focusing on the role of Europe as an LNG importer. The conducted analysis reveals the complexities of Europe's strategy of simultaneously achieving decarbonization objectives and resolving energy security concerns regarding LNG. The study proposes an optimization model to determine the optimal global LNG trade between exporters and importers. As an alternative for Europe to solely rely on imports, the potential substitution of imports with domestic natural gas production equipped with carbon capture and storage is considered. Herein, two scenarios (low and high LNG demand) are examined. The findings indicate that Europe plays a pivotal role in the global LNG market solely in the ambitious sustainable scenario, whereas its significance diminishes in the high -demand scenario. Examining the volumes of LNG sent to Europe, African exporters appear to be notably significant in meeting the demand. However, as global LNG demand rises, the discernibility of genuinely stable trends or patterns in trade declines. The value of long-term contracts may experience a resurgence in the future. Future work should include long-term contracts, allowing for fixed volumes of LNG to be traded. Furthermore, our obtained LNG supply costs for Europe can be seen as valuable inputs for large-scale energy system models aiming to optimize the sustainable transition of Europe's energy infrastructure.",Not About Sufficiency
Thermal management materials for energy-efficient and sustainable future buildings,"Thermal management plays a key role in improving the energy efficiency and sustainability of future building envelopes. Here, we focus on the materials perspective and discuss the fundamental needs, current status, and future opportunities for thermal management of buildings. First, we identify the primary considerations and evaluation criteria for high-performance thermal materials. Second, state-of-the-art thermal materials are reviewed, ranging from conventional thermal insulating fiberglass, mineral wool, cellulose, and foams, to aerogels and mesoporous structures, as well as multifunctional thermal management materials. Further, recent progress on passive regulation and thermal energy storage systems are discussed, including sensible heat storage, phase change materials, and radiative cooling. Moreover, we discuss the emerging materials systems with tunable thermal and other physical properties that could potentially enable dynamic and interactive thermal management solutions for future buildings. Finally, we discuss the recent progress in theory and computational design from first-principles atomistic theory, molecular dynamics, to multiscale simulations and machine learning. We expect the rational design that combines data-driven computation and multiscale experiments could bridge the materials properties from microscopic to macroscopic scales and provide new opportunities in improving energy efficiency and enabling adaptive implementation per customized demand for future buildings.",Not About Sufficiency
Assessing the influence of landscape conservation and protected areas on social wellbeing using random forest machine learning,"The urgency of interconnected social-ecological dilemmas such as rapid biodiversity loss, habitat loss and fragmentation, and the escalating climate crisis have led to increased calls for the protection of ecologically important areas of the planet. Protected areas (PA) are considered critical to address these dilemmas although growing divides in wellbeing can exacerbate conflict around PAs and undermine effectiveness. We investigate the influence of proximity to PAs on wellbeing outcomes. We develop a novel multi-dimensional index of wellbeing for households and across Africa and use Random Forest Machine Learning techniques to assess the importance score of households' proximity to protected areas on their wellbeing outcomes compared with the importance scores of an array of other social, environmental, and local and national governance factors. This study makes important contributions to the conservation literature, first by expanding the ways in which wellbeing is measured and operationalized, and second, by providing additional empirical support for recent evidence that proximity to PAs is an influential factor affecting observed wellbeing outcomes, albeit likely through different pathways than the current literature suggests.",Not About Sufficiency
AspectAnalyzer-Distributed System for Bi-clustering Analysis,"In this study we describe a software package accessible as standalone application for performing various bi-clustering methods. System has already implemented 5 algorithms and there is a possibility to add further. In addition to the algorithms from literature, the software includes an original method developed by the authors. System is fully distributed, and has a module to create a computer farm consisting of the computational equipment on which this software were installed. It also allows for creating highly efficient computational environment that will solve wide range of problems related to bi-clustering. The software has been released to the public on the Internet, along with extensive service organized in the form of a blog. At the address http://aspectanalyzer.foszner.pl a ready to use installer has been posted, along with a complete user manual. In addition, the portal allows reporting bugs, new features, and questions about the software. All software is provided free of charge and will include a complete, ready-to-run package.",Not About Sufficiency
Acute cognitive deficits after traumatic brain injury predict Alzheimer's disease-like degradation of the human default mode network,"Traumatic brain injury (TBI) and Alzheimer's disease (AD) are prominent neurological conditions whose neural and cognitive commonalities are poorly understood. The extent of TBI-related neurophysiological abnormalities has been hypothesized to reflect AD-like neurodegeneration because TBI can increase vulnerability to AD. However, it remains challenging to prognosticate AD risk partly because the functional relationship between acute posttraumatic sequelae and chronic AD-like degradation remains elusive. Here, functional magnetic resonance imaging (fMRI), network theory, and machine learning (ML) are leveraged to study the extent to which geriatric mild TBI (mTBI) can lead to AD-like alteration of resting-state activity in the default mode network (DMN). This network is found to contain modules whose extent of AD-like, posttraumatic degradation can be accurately prognosticated based on the acute cognitive deficits of geriatric mTBI patients with cerebral microbleeds. Aside from establishing a predictive physiological association between geriatric mTBI, cognitive impairment, and AD-like functional degradation, these findings advance the goal of acutely forecasting mTBI patients' chronic deviations from normality along AD-like functional trajectories. The association of geriatric mTBI with AD-like changes in functional brain connectivity as early as similar to 6 months post-injury carries substantial implications for public health because TBI has relatively high prevalence in the elderly.",Not About Sufficiency
How Can We Adapt Together? Bridging Water Management and City Planning Approaches to Climate Change,"Different dynamics of climate change, population growth, and urbanisation challenge water service providers (WSPs) and those managing urban planning. The scientific community has been evidencing the concept of sustainable urban water management (SUWM) as a driver to foster the integration of the urban water cycle with its environmental, economic, and social sustainability dimensions. This article studies the approaches addressed by recent research on sustainable urban water management, focusing on the attention given by the scientific community to the way WSPs and city planners address the new challenges brought by climate change. A systematic review of existing literature shows how emergent challenges address the articulation between urban water cycle management and city planning. The results underline the need for the technical and economic evaluation of the overarching concept of SUWM systems, integrating values that go beyond financial issues; the need to address water scarcity not only from the supply side but also from the demand point of view; and the deepening of the relationship between new sources of water, such as the reuse, with the city planning in a context of climate change. Nevertheless, strategies for collaboration are still poorly addressed. The insights and gaps emerging from the analysis suggest new paths for research and practice in the field.",Not About Sufficiency
Prediction of acute organophosphate poisoning severity using machine learning techniques,"Poisoning with organophosphate compounds is a significant public health risk, especially in developing countries. Considering the importance of early and accurate prediction of organophosphate poisoning prognosis, the aim of this study was to develop a machine learning-based prediction model to predict the severity of organophosphate poisoning. The data of patients with organophosphate poisoning were retrospectively extracted and split into training and test sets in a ratio of 70:30. The feature selection was done by least absolute shrinkage and selection operator method. Selected features were fed into five machine learning techniques, including Histogram Boosting Gradient, eXtreme Gradient Boosting, K-Nearest Neighborhood, Support Vector Machine (SVM) (kernel = linear), and Random Forest. The Scikit-learn library in Python programming language was used to implement the models. Finally, the performance of developed models was measured using ten-fold cross-validation methods and some evaluation criteria with 95 % confidence intervals. A total of 1237 patients were used to train and test the machine learning models. According to the criteria determining severe organophosphate poisoning, 732 patients were assigned to group 1 (patients with mild to moderate poisoning) and 505 patients were assigned to group 2 (patients with severe poisoning). With an AUC value of 0.907 (95 % CI 0.89-0.92), the model developed using XGBoost outperformed other models. Feature importance evaluation found that venous blood gas-pH, white blood cells, and plasma cholinesterase activity were the top three variables that contribute the most to the prediction performance of the prognosis in patients with organophosphate poisoning. XGBoost model yield an accuracy of 90.1 % (95 % CI 0.891-0.918), specificity of 91.4 % (95 % CI 0.90-0.92), a sensitivity of 89.5 % (95 % CI 0.87-0.91), F-measure of 91.2 % (95 % CI 0.90-0.921), and Kappa statistic of 91.2 % (95 % CI 0.90-0.92). The machine learning-based prediction models can accurately predict the severity of organophosphate poisoning. Based on feature selection techniques, the most important predictors of organophosphate poisoning were VBG-pH, white blood cell count, plasma cholinesterase activity, VBG-BE, and age. The best algorithm with the highest predictive performance was the XGBoost classifier.",Not About Sufficiency
Partial Contribution of Socioeconomic Factors to the Mortality Rate of the Working-Age Population in Russia,"This study's relevance lies in the need to assess the role of socioeconomic, medical, and demographic factors on working-age population mortality in Russia. The purpose of this study is to substantiate the methodological tools for the assessment of the partial contribution of the most important factors that determine the dynamics of the mortality of the working-age population. Our hypothesis is that the factors determining the socioeconomic situation in the country affect the level and dynamics of mortality of the working-age population, but to a different extent in each separate period. To analyse the impact of the factors, we used official Rosstat data for the period from 2005 to 2021. We used the data that reflect the dynamics of socioeconomic and demographic indicators, including the dynamics of mortality of the working-age population in Russia as a whole and in its 85 regions. First, we selected 52 indicators of socioeconomic development and then grouped them into four factor blocks (working conditions, health care, life security, living standards). To reduce the level of statistical noise, we carried out a correlation analysis, which allowed us to narrow down the list to 15 key indicators with the strongest association with the mortality rate of the working-age population. The total period of 2005-2021 was divided into five segments of 3-4 years each, characterising the picture of the socioeconomic state of the country during the period under consideration. The socioeconomic approach used in the study made it possible to assess the extent to which the mortality rate was influenced by the indicators adopted for analysis. The results of this study show that over the whole period, life security (48%) and working conditions (29%) contributed most to the level and dynamics of mortality in the working-age population, while factors determining living standards and the state of the healthcare system accounted for much smaller shares (14% and 9%, respectively). The methodological apparatus of this study is based on the application of methods of machine learning and intelligent data analysis, which allowed us to identify the main factors and their share in the total influence on the mortality rate of the working-age population. The results of this study show the need to monitor the impact of socioeconomic factors on the dynamics and mortality rate of the working-age population in order to improve the effectiveness of social programme. When developing and adjusting government programmes to reduce mortality in the working-age population, the degree of influence of these factors should be taken into account.",Not About Sufficiency
"FHIR FLI: An Open Source Platform for Storing, Sharing and Analysing Lifestyle Data","Consumers and healthcare organisations alike are increasingly interested in using digital health solutions to reduce the risk of chronic conditions or to help manage these conditions outside hospitals. Equally, there is a strong public health benefit in helping individuals adopt and improve healthy lifestyle behaviours. The first step in this direction is the ability to record and analyse lifestyle data. Currently, lifestyle logging platforms use proprietary data formats. Data is segregated among different platforms, impacting consumers, service providers, research institutes and public health bodies. Our aim is to facilitate the transfer of information between individuals and organisations that hold or require their lifestyle data. We demonstrate that an open source platform based on a clinically recognised interoperability standard - Fast Healthcare Interoperability Resources (FHIR) - can meet both consumers and industry needs. We use as an example the case of people managing arthritis. Our contributions are: (i) an extension of the FHIR standard for lifestyle data, (ii) a reference architecture for a Personal Lifestyle Record, (iii) integration with voice-enabled digital assistants for lifestyle data capture and (iv) an open source implementation of this architecture that retrieves, saves and analyses lifestyle data from wearable devices.",Not About Sufficiency
Early Identification of Cognitive Impairment in Community Environments Through Modeling Subtle Inconsistencies in Questionnaire Responses: Machine Learning Model Development and Validation,"Background: The underdiagnosis of cognitive impairment hinders timely intervention of dementia. Health professionals working in the community play a critical role in the early detection of cognitive impairment, yet still face several challenges such as a lack of suitable tools, necessary training, and potential stigmatization. Objective: This study explored a novel application integrating psychometric methods with data science techniques to model subtle inconsistencies in questionnaire response data for early identification of cognitive impairment in community environments. Methods: This study analyzed questionnaire response data from participants aged 50 years and older in the Health and Retirement Study (waves 8-9, n=12,942). Predictors included low-quality response indices generated using the graded response model from four brief questionnaires (optimism, hopelessness, purpose in life, and life satisfaction) assessing aspects of overall well-being, a focus of health professionals in communities. The primary and supplemental predicted outcomes were current cognitive impairment derived from a validated criterion and dementia or mortality in the next ten years. Seven predictive models were trained, and the performance of these models was evaluated and compared. Results: The multilayer perceptron exhibited the best performance in predicting current cognitive impairment. In the selected four questionnaires, the area under curve values for identifying current cognitive impairment ranged from 0.63 to 0.66 and was improved to 0.71 to 0.74 when combining the low-quality response indices with age and gender for prediction. We set the threshold for assessing cognitive impairment risk in the tool based on the ratio of underdiagnosis costs to overdiagnosis costs, and a ratio of 4 was used as the default choice. Furthermore, the tool outperformed the efficiency of age or health-based screening strategies for identifying individuals at high risk for cognitive impairment, particularly in the 50- to 59-year and 60- to 69-year age groups. The tool is available on a portal website for the public to access freely. Conclusions: We developed a novel prediction tool that integrates psychometric methods with data science to facilitate ""passive or backend"" cognitive impairment assessments in community settings, aiming to promote early cognitive impairment detection. Thistool simplifies the cognitive impairment assessment process, making it more adaptable and reducing burdens. Our approach also presents a new perspective for using questionnaire data: leveraging, rather than dismissing, low-quality data.",Not About Sufficiency
Derivation of a Novel Diabetes Risk Score Using Semantic Discretization for Indian Population,"The objective of this study is to derive a simple, yet effective type 2 Diabetes Risk Score Tool for Indian population using semantic discretization and machine learning techniques. The dataset used for training and validation is taken from Annual Health Survey, containing over 1.65 million people's health-related information from 284 districts of India. This is the first study of its kind that truly represents the Indian population. A combination of feature selections techniques is used to find the minimal subset of attributes that optimally contribute in determining the class attribute. Continuous independent variables (various diabetes risk factors) are discretized using semantic discretization technique. The discretized dataset is then used in deriving Weighted Diabetes Risk Score for each risk factor. An optimal cutoff value for Total Weighted Diabetes Risk Score (TWDRS) is determined based on the evaluation parameters such as sensitivity, specificity, prediction accuracy, and proportion of population kept in high risk. The dataset used for this study contains 16,38,923 records. Records (7,42,605) that meet our criteria are selected for this study. Experimental results show that, at optimal cut point, TWDRS >=19, sensitivity is 72.55%, specificity is 61.99%, and proportion of population at high risk is 39.29%.",Not About Sufficiency
Key Technology Considerations in Developing and Deploying Machine Learning Models in Clinical Radiology Practice,"The use of machine learning to develop intelligent software tools for the interpretation of radiology images has gained widespread attention in recent years. The development, deployment, and eventual adoption of these models in clinical practice, however, remains fraught with challenges. In this paper, we propose a list of key considerations that machine learning researchers must recognize and address to make their models accurate, robust, and usable in practice. We discuss insufficient training data, decentralized data sets, high cost of annotations, ambiguous ground truth, imbalance in class representation, asymmetric misclassification costs, relevant performance metrics, generalization of models to unseen data sets, model decay, adversarial attacks, explainability, fairness and bias, and clinical validation. We describe each consideration and identify the techniques used to address it. Although these techniques have been discussed in prior research, by freshly examining them in the context of medical imaging and compiling them in the form of a laundry list, we hope to make them more accessible to researchers, software developers, radiologists, and other stakeholders.",Not About Sufficiency
Electrochemical label-free pathogen identification for bloodstream infections diagnosis: Towards a machine learning based smart blood culture bottle,"Bloodstream infections are a growing public health concern. Current pathogen identification systems are based on complex and expensive devices, intended for use in centralized laboratories. Subsequent identification re-quires harmful chemical reagents, specialized personnel and time. Here we describe a new approach for rapid and decentralized diagnosis of positive blood cultures using electrochemical sensors. By implementing a multi -material potentiometric platform in a blood culture bottle, we have developed a portable system for pathogen identification of Gram and of genus. Bacterial growth in human blood generates a specific label-free multiplex electrochemical fingerprint according to the detected species. Analysis of these fingerprints using homemade machine learning algorithms allow for rapid identification of the pathogen after detection (14 species and 9 genus) (GRAM= 99 % accuracy 5.75 h after detection, GENUS= 85 % accuracy 7.8 h after positivity) without further handling of the contaminated sample.",Not About Sufficiency
Spatial characteristics and multifactorial driving analysis of fly-tipping bulky waste in Beijing based on the random forest model,"The phenomenon of fly-tipping of bulky waste is becoming increasingly serious with the development of economy and the improvement of living standards. The fly-tipping of bulky waste causes a waste stream with a high proportion of good quality recyclable materials and a wide range of social and environmental problems including damaging the environment and aggravating traffic in a megacity like Beijing. In this study, we analyzed the quantitative spatial distribution of fly-tipping bulky waste in Beijing for the first time and identified its driving forces to explore the fly-tipping rule of bulky waste. We used Anselin's Local Moran I method to reveal the spatial characteristics, and Random Forest machine learning method to examine the driving factors based on multiple data sources such as geographical, population and survey data. The results showed that the spatial agglomeration of fly-tipping presented a typical core edge diffusion spatial distribution pattern. High-High clusters of cases were found in most regions of Dongcheng District and the east part of Xicheng District. In the multifactorial drivers, the three most important driving factors were found to be accommodation, floating population and income level. Surprisingly, any kind of road and educational level had small effect on the fly-tipping case. These results provide a theoretical basis for the government to advance comprehensive prevention and control strategies of bulky waste fly-tipping. And it is helpful to formulate management policies of fly-tipping bulky waste and provide targeted guidance and suggestions for senior decision makers and managers.",Not About Sufficiency
Artificial intelligence techniques for predicting cardiorespiratory mortality caused by air pollution,"Air pollution (AP) has risen as one of the biggest challenges of the 21st century, and it has adverse health effects for humans. The effects of health effects, including cardiorespiratory health effects of various air pollutants, are well documented. This research work presents the modeling and analysis of cardiorespiratory mortality attributed to AP. The modeling and predictions are also completed using four Artificial Intelligence (AI) techniques for comparison. The AI techniques utilized for comparison are (1) Enhanced Long Short-Term Memory (ELSTM), (2) Vector Autoregressive (VAR) (3) Deep Learning (DL), and (4) Long Short-Term Memory (LSTM). The research work is carried out at seven locations in Klang Valley, Malaysia. The five study locations i.e., Cheras, Petaling Jaya, Putrajaya, Shah Alam, and Klang have data from January 2006 to December 2016 and two relatively new monitoring stations, i.e., Banting and Batu Muda have data from April 2010 to December 2016 and January 2009 to December 2016, respectively. The comparison of results indicates that the ELSTM model predicts the cardiorespiratory mortality caused by AP significantly better than other AI models utilized in the study. The best Root-Mean-Squared Error (RMSE) results are obtained at Batu Muda and Klang study locations (ELSTM: 0.004, VAR: 0.03, DL: 0.0081, LSTM: 0.006) and (ELSTM: 0.005, VAR: 0.114, DL: 0.076, LSTM: 0.020), respectively. Based on the results, we can conclude that we can predict cardiorespiratory mortality based on air pollution in Klang Valley, Malaysia, using the AI techniques utilized in the study, especially ELSTM.",Not About Sufficiency
The Politics and Policies of Regulating Generics in Latin America: A Survey of Seventeen States,"When patents expire, are equivalent generic alternatives available to citizens? This article contributes to current discussion on access to medicine in the aftermath of the World Trade Organization's Agreement on Trade-Related Aspects of Intellectual Property Rights (TRIPS). The focus is on off-patent or ""generic"" medicines: their product definitions, quality standards and prescription procedures. Drawing from a survey conducted of seventeen countries across the Latin American region, this article examines the differences in definition of off-patent products and the paradox of their relatively lower consumption across multiple developing states. The findings point to pathways for improving standards, consumer information, and access in off patent pharmaceutical markets.",Not About Sufficiency
High-Resolution Urban Air Quality Mapping for Multiple Pollutants Based on Dense Monitoring Data and Machine Learning,"Spatially explicit urban air quality information is important for urban fine-management and public life. However, existing air quality measurement methods still have some limitations on spatial coverage and system stability. A micro station is an emerging monitoring system with multiple sensors, which can be deployed to provide dense air quality monitoring data. Here, we proposed a method for urban air quality mapping at high-resolution for multiple pollutants. By using the dense air quality monitoring data from 448 micro stations in Lanzhou city, we developed a decision tree model to infer the distribution of citywide air quality at a 500 m x 500 m x 1 h resolution, with a coefficient of determination (R-2) value of 0.740 for PM2.5, 0.754 for CO and 0.716 for SO2. Meanwhile, we also show that the deployment density of the monitoring stations can have a significant impact on the air quality inference results. Our method is able to show both short-term and long-term distribution of multiple important pollutants in the city, which demonstrates the potential and feasibility of dense monitoring data combined with advanced data science methods to support urban atmospheric environment fine-management, policy making, and public health studies.",Not About Sufficiency
The Effect of a Resourcefulness Training Intervention on Relocation Adjustment and Adaptive Functioning among Older Adults in Retirement Communities,"The population of older adults is increasing rapidly and is expected to reach 83.7 million by the year 2050. Previous research demonstrates that greater resourcefulness is associated with better quality of life and life satisfaction. The purpose of this pilot study was to evaluate the effects of a resourcefulness training intervention on positive cognitions, resourcefulness, relocation adjustment, and adaptive functioning among older adults who have relocated to retirement communities. Resourcefulness theory provided the theoretical framework for this study. Forty older adults who relocated to three retirement communities in Milwaukee, WI were randomly assigned to either a diversional activity group or to a resourcefulness training (RT) intervention group. Two older adults dropped out of the study (one from the diversional activity group and one from the RT group), leaving 38 elders. The results of the study indicated that there were slight increases (a trend) in the mean of positive cognitions, relocation adjustment, adaptive functioning, and personal resourcefulness in the expected direction for the RT intervention group as compared to the diversional group. Recommendations for future research include the use of larger and more diverse samples over a longer periods of time (6 weeks and 12 weeks post-intervention) as well as the use of cut scores on the resourcefulness scale so that the RT training intervention is taught to those who need it.",Not About Sufficiency
MEASLES VACCINATION Before the Measles-Mumps-Rubella Vaccine,"At the beginning of the 1960s, it was clear that a vaccine against measles would soon be available. Although measles was (and remains) a killer disease in the developing world, in the United States and Western Europe this was no longer so. Many parents and many medical practitioners considered measles an inevitable stage of a child's development. Debating the desirability of measles immunization, public health experts reasoned differently. In the United States, introduction of the vaccine fit well with Kennedy's and Johnson's administrations' political commitments. European policymakers proceeded cautiously, concerned about the acceptability of existing vaccination programs. In Sweden and the Netherlands, recent experience in controlling polio led researchers to prefer an inactivated virus vaccine. Although in the early 1970s attempts to develop a sufficiently potent inactivated vaccine were abandoned, we have argued that the debates and initiatives of the time during the vaccine's early history merit reflection in today's era of standardization and global markets.",Not About Sufficiency
Recent progress in ammonia fuel cells and their potential applications,"Conventional technologies are largely powered by fossil fuel exploitation and have ultimately led to extensive environmental concerns. Hydrogen is an excellent carbon-free energy carrier, but its storage and long-distance transportation remain big challenges. Ammonia, however, is a promising indirect hydrogen storage medium that has well-established storage and transportation links to make it an accessible fuel source. Moreover, the notion of 'green ammonia' synthesised from renewable energy sources is an emerging topic that may open significant markets and provide a pathway to decarbonise a variety of applications reliant on fossil fuels. Herein, a comparative study based on the chosen design, working principles, advantages and disadvantages of direct ammonia fuel cells is summarised. This work aims to review the most recent advances in ammonia fuel cells and demonstrates how close this technology type is to integration with future applications. At present, several challenges such as material selection, NOx formation, CO2 tolerance, limited power densities and long term stability must still be overcome and are also addressed within the contents of this review.",Not About Sufficiency
Geospatial Modeling of Deep Neural Visual Features for Predicting Obesity Prevalence in Missouri: Quantitative Study,"Background: The global obesity epidemic demands innovative approaches to understand its complex environmental and social determinants. Spatial technologies, such as geographic information systems, remote sensing, and spatial machine learning, offer new insights into this health issue. This study uses deep learning and spatial modeling to predict obesity rates for census tracts Objective: This study aims to develop a scalable method for predicting obesity prevalence using deep convolutional neural networks applied to satellite imagery and geospatial analysis, focusing on 1052 census tracts in Missouri. Methods: Our analysis followed 3 steps. First, Sentinel-2 satellite images were processed using the Residual Network-50 model to extract environmental features from 63,592 image chips (224x224 pixels). Second, these features were merged with obesity rate data from the Centers for Disease Control and Prevention for Missouri census tracts. Third, a spatial lag model was used to predict obesity rates and analyze the association between deep neural visual features and obesity prevalence. Spatial autocorrelation was used to identify clusters of obesity rates. Results: Substantial spatial clustering of obesity rates was found across Missouri, with a Moran I value of 0.68, indicating similar obesity rates among neighboring census tracts. The spatial lag model demonstrated strong predictive performance, with an R2 of 0.93 and a spatial pseudo R2 of 0.92, explaining 93% of the variation in obesity rates. Local indicators from a spatial association analysis revealed regions with distinct high and low clusters of obesity, which were visualized through choropleth Conclusions: This study highlights the effectiveness of integrating deep convolutional neural networks and spatial modeling to predict obesity prevalence based on environmental features from satellite imagery. The model's high accuracy and ability to capture spatial patterns offer valuable insights for public health interventions. Future work should expand the geographical scope and include socioeconomic data to further refine the model for broader applications in obesity research.",Not About Sufficiency
Mapping global distributions of clay-size minerals via soil properties and machine learning techniques,"Clay-size mineral is a vital ingredient of soil that influences various environment behaviors. It is crucial to establish a global distribution map of clay-size minerals to improve the recognition of environment variations. However, there is a huge gap of lacking some mineral contents in poorly accessible remote areas. In this work, machine learning (ML) approaches were conducted to predict the mineral contents and analyze their global abundance changes through the relationship between soil properties and mineral distributions. The average content of kaolinite, illite, smectite, vermiculite, chlorite, and feldspar were predicated to be 28.69 %, 22.30 %, 12.42 %, 5.43 %, 5.03 %, and 1.44 % respectively. Model interpretation showed that topsoil bulk density and drainage class were the most significant factors for predicting all six minerals. It could be seen from the feature importance analysis that bulk density notably reflected the distribution of 2:1 layered minerals more than that of 1:1 mineral. High drainage favored secondary minerals development, while low drainage was more benefited for primary minerals. Moreover, the content variation of different minerals aligned with the distribution of corresponding soil properties, which affirmed the accuracy of established models. This study proposed a new approach to predict mineral contents through soil properties, which filled a necessary step of understanding the geochemical cycles of soil-related processes.",Not About Sufficiency
Evaluating early communicative development.,"The communicative context is a key aspect of development in early childhood; it is therefore of critical importance to develop instruments for evaluating young children's competence in communication and social interaction. The ECSP scale (the acronym for Echelle d'evaluation de la Communication Sociale Precoce - the standardized and adapted French version of the American Early Social Communication Scales [1], by Guidetti and Tourrette [2]) is one of the most promising tools currently available for the evaluation of early social communication in all its complexity. This paper presents the data for the standardization and validation of the ECSP in two different countries (France and Italy), as well as two further studies testing the scale's effectiveness in evaluating child development in atypical populations (composed of deaf and autistic children, respectively). Overall, we found that the ECSP is an effective tool with rich potential for the evaluation of early communication in both typical and atypical populations.",Not About Sufficiency
Antibiotic resistance genes prevalence prediction and interpretation in beaches affected by urban wastewater discharge,"Background: The annual death toll of over 1.2 million worldwide is attributed to infections caused by resistant bacteria, driven by the significant impact of antibiotic misuse and overuse in spreading these bacteria and their associated antibiotic resistance genes (ARGs). While limited data suggest the presence of ARGs in beach environments, efficient prediction tools are needed for monitoring and detecting ARGs to ensure public health safety. This study aims to develop interpretable machine learning methods for predicting ARGs in beach waters, addressing the challenge of black-box models and enhancing our understanding of their internal mechanisms. Methods: In this study, we systematically collected beach water samples and subsequently isolated bacteria from these samples using various differential and selective media supplemented with different antibiotics. Resistance profiles of bacteria were determined by using Kirby-Bauer disk diffusion method. Further, ARGs were enumerated by using the quantitative polymerase chain reaction (qPCR) to detect and quantify ARGs. The obtained qPCR data and hydro-meteorological were used to create an ML model with high prediction performance and we further used two explainable artificial intelligence (xAI) model-agnostic interpretation methods to describe the internal behavior of ML model. Results: Using qPCR, we detected blaCTX_ M, blaNDM, blaCMY, blaOXA, blatetX, blasul1, and blaaac(6 '-Ib-cr) in the beach waters. Further, we developed ML prediction models for blaaac(6 '-Ib-cr), blasul1, and blatetX using the hydrometrological and qPCR-derived data and the models demonstrated strong performance, with R2 values of 0.957, 0.997, and 0.976, respectively. Conclusions: Our findings show that environmental factors, such as water temperature, precipitation, and tide, are among the important predictors of the abundance of resistance genes at beaches.",Not About Sufficiency
Tai Chi Expertise Classification in Older Adults Using Wrist Wearables and Machine Learning,"Tai Chi is a Chinese martial art that provides an adaptive and accessible exercise for older adults with varying functional capacity. While Tai Chi is widely recommended for its physical benefits, wider adoption in at-home practice presents challenges for practitioners, as limited feedback may hamper learning. This study examined the feasibility of using a wearable sensor, combined with machine learning (ML) approaches, to automatically and objectively classify Tai Chi expertise. We hypothesized that the combination of wrist acceleration profiles with ML approaches would be able to accurately classify practitioners' Tai Chi expertise levels. Twelve older active Tai Chi practitioners were recruited for this study. The self-reported lifetime practice hours were used to identify subjects in low, medium, or highly experienced groups. Using 15 acceleration-derived features from a wearable sensor during a self-guided Tai Chi movement and 8 ML architectures, we found multiclass classification performance to range from 0.73 to 0.97 in accuracy and F1-score. Based on feature importance analysis, the top three features were found to each result in a 16-19% performance drop in accuracy. These findings suggest that wrist-wearable-based ML models may accurately classify practice-related changes in movement patterns, which may be helpful in quantifying progress in at-home exercises.",Not About Sufficiency
Predicting habitat suitability and connectivity for management and conservation of urban wildlife: A real-time web application for grassland water voles,"Natural habitats in urban areas provide benefits for both humans and biodiversity. However, to achieve biodiversity gains, we require new techniques to determine habitat suitability and ecological connectivity that will inform urban planning and development. Using an example of an urban population of water voles Arvicola amphibius, we developed a habitat suitability model and a resistance-surface-based model of landscape connectivity to identify potential connectivity between areas of suitable habitat. We then updated the environmental variables according to new urban development plans and used our models to generate spatially explicit predictions of both habitat suitability and connectivity. To make models accessible to urban and conservation planners, we developed an interactive mapping tool that provided users with a graphical user interface (GUI) to inform conservation planning for this species. The model found that habitat suitability for water voles was related to the proportion and distance from key environmental variables, such as built-up areas and urban green spaces, while the connectivity model identified important corridors connecting areas of potential distribution for this species. Future development plans altered the potential spatial distribution of the water vole population, reducing the extent of suitable habitat in some core areas. The interactive mapping tool made available suitable habitat and connectivity maps for conservation managers to assess new planning applications and for the development of a conservation action plan for water voles. Synthesis and applications. We believe this approach provides a framework for future development of nature conservation tools that can be used by planners to inform ecological decision-making, increase biodiversity and reduce human-wildlife conflict in urban environments.",Not About Sufficiency
An approach to application-layer DoS detection,"With the massive resources and strategies accessible to attackers, countering Denial of Service (DoS) attacks is getting increasingly difficult. One of these techniques is application-layer DoS. Due to these challenges, network security has become increasingly more challenging to ensure. Hypertext Transfer Protocol (HTTP), Domain Name Service (DNS), Simple Mail Transfer Protocol (SMTP), and other application protocols have had increased attacks over the past several years. It is common for application-layer attacks to concentrate on these protocols because attackers can exploit some weaknesses. Flood and ""low and slow"" attacks are examples of application-layer attacks. They target weaknesses in HTTP, the most extensively used application-layer protocol on the Internet. Our experiment proposes a generalized detection approach to identify features for application-layer DoS attacks that is not specific to a single slow DoS attack. We combine four application-layer DoS attack datasets: Slow Read, HTTP POST, Slowloris, and Apache Range Header. We perform a feature-scaling technique that applies a normalization filter to the combined dataset. We perform a feature extraction technique, Principal Component Analysis (PCA), on the combined dataset to reduce dimensionality. We examine ways to enhance machine learning techniques for detecting slow application-layer DoS attacks that employ these methodologies. The machine learners effectively identify multiple slow DoS attacks, according to our findings. The experiment shows that classifiers are good predictors when combined with our selected Netflow characteristics and feature selection techniques.",Not About Sufficiency
Reconciling art and science in the era of personalised medicine: the legacy of George Canguilhem,"BackgroundBiomedicine, i.e. the application of basic sciences to medicine, has become the cornerstone for the study of etiopathogenesis and treatment of diseases. Biomedicine has enormously contributed to the progress of medicine and healthcare and has become the preferred approach to medical problems in the West. The developments in statistical inference and machine learning techniques have provided the foundation for personalised medicine where clinical management can be fully informed by biomedicine. The deployment of precision medicine may impact the autonomy and self-normativity of the patients. Understanding the relationship between biomedicine and medical practice can help navigate the benefits and challenges offered by precision medicine.MethodsConventional content analysis was applied to ""Le Normal and le Pathologique"" (Canguilhem G. The Normal and the Pathological. Princeton: Princeton University Press; 1991) and further investigated with respect to its relationship with techne and precision medicine using PubMed and Google Scholar and the Standford Encyclopedia of Philosophy to search for the following keywords singularly or in combination: ""Canguilhem"", ""techne"", ""episteme"", ""precision medicine"", ""machine learning AND medicine"".ResultsThe Hippocratic concept of techne accounts for many characteristics of medical knowledge and practice. The advances of biomedicine, experimental medicine and, more recently, machine learning offer, in contrast, the model of a medicine based purely on episteme. I argue that Canguilhem medical epistemology establishes a framework where episteme and data-driven medicine is compatible with the promotion of patient's autonomy and self-normativity.ConclusionsCanguilhem's medical epistemology orders the relationship of applied medicine with experimental sciences, ethics and social sciences. It provides guidance to define the scope of medicine and the boundaries of medicalization of healthy life. Finally, it sets an agenda for a safe implementation of machine learning in medicine.",Not About Sufficiency
Urban spatial structure and equity for urban services through the lens of accessibility,"Urban development, equity and sustainability cannot be concluded based on access to one type of urban service. This study goes beyond the usual approach of analysing a single type of urban service and provides a comprehensive overview of accessibility-related equity. A modified accessibility measure is used, which, in addition to travel time impedance, incorporates the aspect of connectivity of services with the transport infrastructure. This study evaluates mode-wise accessibility for jobs, schools, medical care units, hospitals, parks, supermarkets, and restaurants in Greater Mumbai. The spatial association and clustering of threshold-based accessibility are assessed using Global Moran's I. This reveals that the degree of spatial association and clustering is highest at 30 minutes travel time threshold for private mode and 45 mintues for public transport for urban services. The Gini index values indicate that spatial inequity is relatively higher for hospital services accessibility than other services. Social equity is assessed using the average accessibility ratio values for the urban services, which favours regions with moderate car ownership over low, indicating one of the undesirable urban development trends. This study helps understand the urban spatial structure and assesses the spatial and social inequities from the transportation/urban planning perspective. The developed decision framework can aid policymakers in prioritising the suggested measures effectively to address accessibility related inequities for different urban services.",Not About Sufficiency
Pitfalls in Radiology Informatics when deploying an Enterprise Solution,"In the Region Vastra Gotaland (VGR), Sweden, sharing of data from 4 PACS system has been done through the Radiology Information Infrastructure that where deployed in 2007, ([1,2,3]) and during 2008 and 2009 also including the information obtained from 3 different RIS systems installed in the region. The RIS information stored in the Radiology Information Infrastructure is Structured Reports (SR) ([4]) objects that derivatives from the regional information model. In practice, the Enterprise solution now offers new ways of social collaboration through information sharing within a region. Interoperability was developed according to the IHE mission, i.e. applying standards such as digital imaging and communication in medicine (DICOM) and Health Level 7 (HL7) to address specific clinical communication needs and support optimal patient care. ([5,6,7]) Applying standards and information has shown to be suitable for interoperability, but not appropriate for implementing social collaboration i.e. first and second opinion, as there is no user services related to the standards. The need for social interaction leads to a common negotiated interface and in contrary with interoperability the approach will be a common defined semantic model. Radiology informatics is the glue between the technical standards, information models, semantics, social ruleworks and regulations used within radiology and their customers to share information and services. The lessons learned be means of pitfalls in Radiology Informatics are: Syntax Harmonization of the syntax Standards",Not About Sufficiency
Machine learning and data science in soft materials engineering,"In many branches of materials science it is now routine to generate data sets of such large size and dimensionality that conventional methods of analysis fail. Paradigms and tools from data science and machine learning can provide scalable approaches to identify and extract trends and patterns within voluminous data sets, perform guided traversals of high-dimensional phase spaces, and furnish data-driven strategies for inverse materials design. This topical review provides an accessible introduction to machine learning tools in the context of soft and biological materials by 'de-jargonizing' data science terminology, presenting a taxonomy of machine learning techniques, and surveying the mathematical underpinnings and software implementations of popular tools, including principal component analysis, independent component analysis, diffusion maps, support vector machines, and relative entropy. We present illustrative examples of machine learning applications in soft matter, including inverse design of self-assembling materials, nonlinear learning of protein folding landscapes, high throughput antimicrobial peptide design, and data-driven materials design engines. We close with an outlook on the challenges and opportunities for the field.",Not About Sufficiency
Machine Learning Guides Peptide Nucleic Acid Flow Synthesis and Sequence Design,"Peptide nucleic acids (PNAs) are potential antisense therapies for genetic, acquired, and viral diseases. Efficiently selecting candidate PNA sequences for synthesis and evaluation from a genome containing hundreds to thousands of options can be challenging. To facilitate this process, this work leverages machine learning (ML) algorithms and automated synthesis technology to predict PNA synthesis efficiency and guide rational PNA sequence design. The training data is collected from individual fluorenylmethyloxycarbonyl (Fmoc) deprotection reactions performed on a fully automated PNA synthesizer. The optimized ML model allows for 93% prediction accuracy and 0.97 Pearson's r. The predicted synthesis scores are validated to be correlated with the experimental high-performance liquid chromatography (HPLC) crude purities (correlation coefficient R-2 = 0.95). Furthermore, a general applicability of ML is demonstrated through designing synthetically accessible antisense PNA sequences from 102 315 predicted candidates targeting exon 44 of the human dystrophin gene, SARS-CoV-2, HIV, as well as selected genes associated with cardiovascular diseases, type II diabetes, and various cancers. Collectively, ML provides an accurate prediction of PNA synthesis quality and serves as a useful computational tool for informing PNA sequence design.",Not About Sufficiency
Revealing the Unknown: Real-Time Recognition of Galapagos Snake Species Using Deep Learning,"Simple Summary The snakes in Galapagos are the least studied group of vertebrates in the archipelago. The conservation status of only four out of nine recognized species has been formally evaluated, and preliminary evidence suggests that some of the species may be entirely extinct on some islands. Moreover, nearly all park ranger reports and citizen/science photographic identifications of Galapagos snakes are spurious, given that the systematics of the snakes in the archipelago have just recently been clarified. Our solution is to provide park rangers and tourists with easily accessible applications for species identification in real time through automatic object recognition. We used deep learning algorithms on collected images of the snake species to develop the artificial intelligence platform, an application software, that is able to recognize a species of a snake using a user's uploaded image. The application software works in the following way: once a user uploads an image of a snake into the application, the algorithm processes it, classifies it into one of the nine snake species, gives the class of the predicted species, as well as educates users by providing them with information about the distribution, natural history, conservation, and etymology of the snake. Abstract Real-time identification of wildlife is an upcoming and promising tool for the preservation of wildlife. In this research project, we aimed to use object detection and image classification for the racer snakes of the Galapagos Islands, Ecuador. The final target of this project was to build an artificial intelligence (AI) platform, in terms of a web or mobile application, which would serve as a real-time decision making and supporting mechanism for the visitors and park rangers of the Galapagos Islands, to correctly identify a snake species from the user's uploaded image. Using the deep learning and machine learning algorithms and libraries, we modified and successfully implemented four region-based convolutional neural network (R-CNN) architectures (models for image classification): Inception V2, ResNet, MobileNet, and VGG16. Inception V2, ResNet and VGG16 reached an overall accuracy of 75%.",Not About Sufficiency
NEW ADVANCED TEACHING TOOLS FOR SMART CREATIVE DESIGN,"During the last years, new trends and changes appeared in the interior design. All these changes are accelerated by using 3D printing, advanced intelligent materials and different automation features, which are, nowadays, accessible on a large scale and at lower costs. This paper is a comparative analysis of different didactic projects that have been carried out by the students from the Faculty of Architecture and Urban Planning, during their Interior Design working classes. The whole study is focused on the two particularities in technological approach, related to the design of a ""green wall"", which is the centered subject. This design feature could be realized either by living plants, or by a reinterpretation, using some kinetic systems. The results of the analysis indicate the existence of a few major and different approaches, ranging from total ignorance to extensive use of automation equipment. Interesting phenomena could be observed also when using some simple materials, which appear totally disconnected from the environmental reality and often reduced to classical textures, mapped to diverse CAD generated geometries. As a reaction to these important observed issues, our study proposes a series of didactic aids in the form of dedicated responsive systems based on the intrinsic physical properties of the simple constituent materials. Reducing active components should become a mandatory trend of future smart passive design over the use of extensive automation. All the physical properties of these materials must be carefully studied during the design classes. The main teaching objectives are to encourage alternative thinking, to stimulate a combination of hand sketch and computer aided drawing and to know some innovative design issues. The methodology includes design procedures, but even a research study on the behavior of responsive materials as function of temperature. The whole result is a complete study about how to perform a didactic project by using alternative design features and advanced materials, alternating classic and innovative methods.",Not About Sufficiency
"Excess commuting, rail access and subjective wellbeing","While existing studies have largely focused on the impacts from transit-oriented development on commuting patterns, there is limited evidence on the effect of rail access on subjective wellbeing with explicit considerations about the presence of excess commuting. This study uses the Bayesian multilevel model to investigate the relationship between rail access, excess commuting and subjective wellbeing in Beijing metropolitan areas. We identify actual commuting times as re-ported by individuals and obtain optimized commuting times through the online map route in-formation between work and residential locations of survey respondents. We find that excess commuting imposes significantly negative effects on life satisfaction of residents, conditional upon social, spatial and contextual characteristics. Better rail access generates positive comple-mentary effects with excess commuting in benefiting life satisfaction of residents. Findings of this study suggest the importance of optimizing the transit-oriented development to mitigate social -spatial inequalities in urban quality of life concerns.",Not About Sufficiency
Service Learning in Public Health: A Critical Assessment of Potential Benefits and Unintended Consequences,"Service learning is a pedagogical strategy in which students complete a project at a community-based organization where they can apply classroom skills in a real-world context. Service learning can benefit student learning outcomes and continues to expand in public health education; yet there is the potential for ethical concerns. In particular, service learning may reinforce inequities between academic institutions and community organizations by positioning academic institutions as holding significant power and resources and inadvertently promoting this dynamic to students. This perspective provides an overview of service learning in public health and related fields, discusses the benefits of service learning as well as the gaps in standardization and evaluation, and highlights situations in which inequities may emerge. Recommendations are provided, with the goal of helping future health promotion professionals better meet the objective of fostering social justice through public health.",Not About Sufficiency
Investigating mental health outcomes of undergraduates and graduate students in Taiwan during the COVID-19 pandemic,"Objective: This study is an exploration of the major stressors associated with the COVID-19 for students in higher education in Taiwan. Participants: The sample comprised 838 higher education students studying at various Taiwanese universities. Methods: A cross-sectional online survey was administered at different postsecondary institutions during the semi-lockdown period of COVID-19, which mandated online instruction. Machine learning was employed to determine the variables that most highly predicted students' mental health using R. Results: The findings revealed that COVID-19-related experiences, including social interactions, financial conditions, and educational experiences, were significantly associated with mental health outcomes. Particularly, loneliness are significantly related to social interactions and educational experiences. Conclusions: Findings revealed that Covid-19 impacted Taiwanese students' financial conditions, educational experiences, and social interactions, which were significant predictors of their mental health outcomes such as anxiety, loneliness and depression. The current study contributes to the gap in knowledge about mental health issues among postsecondary students during the pandemic.",Not About Sufficiency
High-resolution traffic flow data from the urban traffic control system in Glasgow,"Traffic flow data has been used in various disciplines, including geography, transportation, urban planning, and public health. However, existing datasets often have limitations such as low spatiotemporal resolution and inconsistent quality due to data collection methods and the need for an adequate data cleaning process. This paper introduces a long-term traffic flow dataset at an intra-city scale with high spatio-temporal granularity. The dataset covers the Glasgow City Council area for four consecutive years spanning the COVID-19 pandemic, from October 2019 to September 2023, providing comprehensive temporal and spatial coverage. Such detailed information facilitates diverse applications, including traffic dynamic analysis, traffic management, infrastructure planning, and urban environment improvement. Also, it provides a valuable dataset to understand traffic flow change during a once-in-a-lifetime pandemic event.",Not About Sufficiency
A user-friendly tool for cloud-based whole slide image segmentation with examples from renal histopathology,"Plain language summaryArtificial intelligence (AI) is the ability of a computer to conduct complex tasks that humans are capable of performing. AI is useful in the field of pathology, which involves analyzing images of the microscopic structure of different tissues. However, AI can be difficult to set up and apply to the task. One specific task, segmentation, involves picking specific structures out of tissue images and is a prime candidate for automation with AI. In our study, we have created a tool for pathology image segmentation which runs in the cloud (is accessible over the web). We demonstrate the tool by using it to segment various structures from kidney tissue. Our experiments show that the tool is easy to use, accurate, and can estimate the presence of one type of scarring as reliably as human experts. Lutnick et al. develop a cloud-based deep learning tool for whole slide image segmentation. The authors provide several examples of its application in renal pathology, for segmenting glomeruli, interstitial fibrosis and other features of interest. BackgroundImage-based machine learning tools hold great promise for clinical applications in pathology research. However, the ideal end-users of these computational tools (e.g., pathologists and biological scientists) often lack the programming experience required for the setup and use of these tools which often rely on the use of command line interfaces.MethodsWe have developed Histo-Cloud, a tool for segmentation of whole slide images (WSIs) that has an easy-to-use graphical user interface. This tool runs a state-of-the-art convolutional neural network (CNN) for segmentation of WSIs in the cloud and allows the extraction of features from segmented regions for further analysis.ResultsBy segmenting glomeruli, interstitial fibrosis and tubular atrophy, and vascular structures from renal and non-renal WSIs, we demonstrate the scalability, best practices for transfer learning, and effects of dataset variability. Finally, we demonstrate an application for animal model research, analyzing glomerular features in three murine models.ConclusionsHisto-Cloud is open source, accessible over the internet, and adaptable for segmentation of any histological structure regardless of stain.",Not About Sufficiency
Genomic benchmarks: a collection of datasets for genomic sequence classification,"Background Recently, deep neural networks have been successfully applied in many biological fields. In 2020, a deep learning model AlphaFold won the protein folding competition with predicted structures within the error tolerance of experimental methods. However, this solution to the most prominent bioinformatic challenge of the past 50 years has been possible only thanks to a carefully curated benchmark of experimentally predicted protein structures. In Genomics, we have similar challenges (annotation of genomes and identification of functional elements) but currently, we lack benchmarks similar to protein folding competition. Results Here we present a collection of curated and easily accessible sequence classification datasets in the field of genomics. The proposed collection is based on a combination of novel datasets constructed from the mining of publicly available databases and existing datasets obtained from published articles. The collection currently contains nine datasets that focus on regulatory elements (promoters, enhancers, open chromatin region) from three model organisms: human, mouse, and roundworm. A simple convolution neural network is also included in a repository and can be used as a baseline model. Benchmarks and the baseline model are distributed as the Python package `genomicbenchmarks', and the code is available at https://github.com/ML-Bioinfo- CEITEC/genomic_benchmarks. Conclusions Deep learning techniques revolutionized many biological fields but mainly thanks to the carefully curated benchmarks. For the field of Genomics, we propose a collection of benchmark datasets for the classification of genomic sequences with an interface for the most commonly used deep learning libraries, implementation of the simple neural network and a training framework that can be used as a starting point for future research. The main aim of this effort is to create a repository for shared datasets that will make machine learning for genomics more comparable and reproducible while reducing the overhead of researchers who want to enter the field, leading to healthy competition and new discoveries.",Not About Sufficiency
A Comparison of Ambulance Redeployment Systems on Real-World Data,"Modern Emergency Medical Services (EMS) benefit from real-time sensor information in various ways as they provide up-to-date location information and help assess current local emergency risks. A critical part of EMS is dynamic ambulance redeployment, i.e., the task of assigning idle ambulances to base stations throughout a community. Although there has been a considerable effort on methods to optimize emergency response systems, a comparison of proposed methods is generally difficult as reported results are mostly based on artificial and proprietary test beds. In this paper, we present a benchmark simulation environment for dynamic ambulance redeployment based on real emergency data from the city of San Francisco. Our proposed simulation environment is highly scalable and is compatible with modern reinforcement learning frameworks. We provide a comparative study of several state-of-the-art methods for various metrics. Results indicate that even simple baseline algorithms can perform considerably well in close-to-realistic settings. The code of our simulator is openly available at https://github.com/niklasdbs/ambusim.",Not About Sufficiency
Development and validation of machine learning models to predict MDRO colonization or infection on ICU admission by using electronic health record data,"BackgroundMultidrug-resistant organisms (MDRO) pose a significant threat to public health. Intensive Care Units (ICU), characterized by the extensive use of antimicrobial agents and a high prevalence of bacterial resistance, are hotspots for MDRO proliferation. Timely identification of patients at high risk for MDRO can aid in curbing transmission, enhancing patient outcomes, and maintaining the cleanliness of the ICU environment. This study focused on developing a machine learning (ML) model to identify patients at risk of MDRO during the initial phase of their ICU stay.MethodsUtilizing patient data from the First Medical Center of the People's Liberation Army General Hospital (PLAGH-ICU) and the Medical Information Mart for Intensive Care (MIMIC-IV), the study analyzed variables within 24 h of ICU admission. Machine learning algorithms were applied to these datasets, emphasizing the early detection of MDRO colonization or infection. Model efficacy was evaluated by the area under the receiver operating characteristics curve (AUROC), alongside internal and external validation sets.ResultsThe study evaluated 3,536 patients in PLAGH-ICU and 34,923 in MIMIC-IV, revealing MDRO prevalence of 11.96% and 8.81%, respectively. Significant differences in ICU and hospital stays, along with mortality rates, were observed between MDRO positive and negative patients. In the temporal validation, the PLAGH-ICU model achieved an AUROC of 0.786 [0.748, 0.825], while the MIMIC-IV model reached 0.744 [0.723, 0.766]. External validation demonstrated reduced model performance across different datasets. Key predictors included biochemical markers and the duration of pre-ICU hospital stay.ConclusionsThe ML models developed in this study demonstrated their capability in early identification of MDRO risks in ICU patients. Continuous refinement and validation in varied clinical contexts remain essential for future applications.",Not About Sufficiency
Does hosting a professional sports team benefit the local community? Evidence from property assessments,"Local governments often justify subsidizing sports stadiums as economic development projects that have positive returns on investment. If this is true, economic and quality-of-life spillovers that are capitalized in local property values ought to generate additional tax revenue for host municipalities through increased property assessments. This analysis uses the synthetic control method to estimate the effect of a new publicly-funded professional baseball stadium and team relocation on property assessments in Cobb County, Georgia. Cobb assessment values did not increase relative to other metro-Atlanta counties following the stadiums' announcement or opening, which is inconsistent with the stadium having a positive fiscal impact, even with its desirable location and accompanying mixed-used development. The findings are consistent with past economic studies and are likely generalizable to other stadium projects.",Not About Sufficiency
SUSTAINABLE TRANSPORT IN HONG KONG,"Hong Kong has a population of seven million but with an area of only 1 100 square kilometres, which is characterized by hilly terrain and extensive area of countryside and green belts and that leaves only about 20% of usable land. Moving a population of 7 million people everyday on a landmass of roughly 200 square kilometres to meet their transport needs for work, school and other activities on the one hand, and to develop and maintain a sustainable transport system on the other, is a challenging task. Hong Kong has a developed and well utilized public transport system. About 90% of commuters use various modes of public transport. My speech gives an account of the transport system in Hong Kong, our ways of encouraging commuters to use public transport, and our future challenges in sustaining and developing our transport system.",Not About Sufficiency
"The Manure Model: manure, minerals (N, P and K), ammonia emission, heavy metals and the use of fertiliser in Dutch agriculture","The Manure Model for Dutch Agriculture (MESTAMM) has been developed by the Agricultural Economics Research Institute to provide insight into present and future levels of manure and mineral surpluses, and ammonia emissions, in the Netherlands. The Manure Model combines data from the Agricultural Census with data from agricultural and environmental research, included are manure and mineral excretion per animal, emission coefficients, and costs of - and legislation for - the application of manure. Results of model calculations made at the individual farm level are aggregated to local and national scales. Results include: manure and mineral production; manure and mineral surpluses; ammonia emission from stables, manure storage, grazing cattle and the application of manure and fertilisers; application rates of soil with minerals from manure and fertilisers, and costs of legislative measures. Various Dutch institutions like the Ministry of Agriculture, Nature Management and Fisheries, the Ministry of Housing, Spatial Planning and the Environment, and the Institute of Public Health and the Environment, as well as agricultural organisations and provinces, have made use of the Manure Model in their approach to several research issues.",Not About Sufficiency
Will smart cities enhance the social capital of residents? The importance of smart neighborhood management,"Social capital is a concept that indicates the strength of the relationship among people inside and outside a community. It is a factor that facilitates policy for community formation and regeneration and is also an indicator of the outcome of a policy for improving a community. This study examined the hypothesis that the regional efforts to create smart cities using advanced ICT technologies enhance the social capital of the city and identified the factors that enhance social capital. Urban policy in Japan is shifting from hardware-oriented development to software-oriented area management; the Minato Mirai 21 district in the city of Yokohama is a typical example of area management. The essence of smart transition is to enhance various management functions by using smart social technologies, such as advanced information communication. The attractiveness of smart cities with sophisticated area management provided by such technologies will likely be high. Thus, this study's results will provide useful implications for better area management in smart cities.",Not About Sufficiency
An Annotated Huge Dataset for Standard and Colloquial Arabic Reviews for Subjective Sentiment Analysis,"Sentiment analysis is getting increasingly popular as it facilitates gaining an indication of the wider public opinions or attitudes towards certain products, services, articles, etc. Many researchers have shown considerable interest in this field. Most of these studies have focused on English and other Indo-European languages. Very few studies have addressed the problem for the Arabic language. This is, mostly, due to the rare or nonexistent huge and free Arabic datasets that contains both Modern Standard Arabic (MSA) as well as Dialectal Arabic (DA). Generally, one of the main challenges for developing robust sentiment analysis systems is the availability of such large-scale datasets. Such datasets exist in abundance for English language, while it is not the case for a low-resource language such as the Arabic language. Recently, there have been some efforts for providing relatively large-scale Arabic datasets dedicated for sentiment analysis such as LABR and most recently BRAD 1.0, which is considered as the largest Arabic Book Reviews dataset for sentiment analysis and machine learning applications. In this work, we present BRAD 2.0, an extension to BRAD 1.0 with more than 200K extra records to account for several Arabic dialects. BRAD 2.0 has a total number of 692586 annotated reviews; each represents a single review along with the reviewer's rating ranging from 1 to 5 of a certain book. The most interesting property of BRAD 2.0 is that it combines both MSA and DA. To verify and validate the proposed dataset, we implement several state-of-the-art supervised and unsupervised classifiers to categorize book reviews. For the unsupervised classifiers, we implemented several models of CNN and RNN classifiers utilizing GloVe-based word embeddings. Although all classifiers performed well, the highest accuracies attained are between 90% and 91%. Experimental results show that BRAD 2.0 is rich and robust. Our key contribution is to make this benchmark-dataset available and accessible to promote further research in the field of Arabic computational linguistic. (C) 2018 The Authors. Published by Elsevier B.V.",Not About Sufficiency
Leveraging AI and Machine Learning to Develop and Evaluate a Contextualized User-Friendly Cough Audio Classifier for Detecting Respiratory Diseases: Protocol for a Diagnostic Study in Rural Tanzania,"Background: Respiratory diseases, including active tuberculosis (TB), asthma, and chronic obstructive pulmonary disease (COPD), constitute substantial global health challenges, necessitating timely and accurate diagnosis for effective treatment and management. Objective: This research seeks to develop and evaluate a noninvasive user-friendly artificial intelligence (AI)-powered cough audio classifier for detecting these respiratory conditions in rural Tanzania. Methods: This is a nonexperimental cross-sectional research with the primary objective of collection and analysis of cough sounds from patients with active TB, asthma, and COPD in outpatient clinics to generate and evaluate a noninvasive cough audio classifier. Specialized cough sound recording devices, designed to be nonintrusive and user-friendly, will facilitate the collection of diverse cough sound samples from patients attending outpatient clinics in 20 health care facilities in the Shinyanga region. The collected cough sound data will undergo rigorous analysis, using advanced AI signal processing and machine learning techniques. By comparing acoustic features and patterns associated with TB, asthma, and COPD, a robust algorithm capable of automated disease discrimination will be generated facilitating the development of a smartphone-based cough sound classifier. The classifier will be evaluated against the calculated reference standards including clinical assessments, sputum smear, GeneXpert, chest x-ray, culture and sensitivity, spirometry and peak expiratory flow, and sensitivity and predictive values. Results: This research represents a vital step toward enhancing the diagnostic capabilities available in outpatient clinics, with the potential to revolutionize the field of respiratory disease diagnosis. Findings from the 4 phases of the study will be presented as descriptions supported by relevant images, tables, and figures. The anticipated outcome of this research is the creation of a reliable, noninvasive diagnostic cough classifier that empowers health care professionals and patients themselves to identify and differentiate these respiratory diseases based on cough sound patterns. Conclusions: Cough sound classifiers use advanced technology for early detection and management of respiratory conditions, offering a less invasive and more efficient alternative to traditional diagnostics. This technology promises to ease public health burdens, improve patient outcomes, and enhance health care access in under-resourced areas, potentially transforming respiratory disease management globally.",Not About Sufficiency
Harnessing Big Data for Systems Pharmacology,"Systems pharmacology aims to holistically understand mechanisms of drug actions to support drug discovery and clinical practice. Systems pharmacology modeling (SPM) is data driven. It integrates an exponentially growing amount of data at multiple scales (genetic, molecular, cellular, organismal, and environmental). The goal of SPM is to develop mechanistic or predictive multiscale models that are interpretable and actionable. The current explosions in genomics and other omics data, as well as the tremendous advances in big data technologies, have already enabled biologists to generate novel hypotheses and gain new knowledge through computational models of genome-wide, heterogeneous, and dynamic data sets. More work is needed to interpret and predict a drug response phenotype, which is dependent on many known and unknown factors. To gain a comprehensive understanding of drug actions, SPM requires close collaborations between domain experts from diverse fields and integration of heterogeneous models from biophysics, mathematics, statistics, machine learning, and semantic webs. This creates challenges in model management, model integration, model translation, and knowledge integration. In this review, we discuss several emergent issues in SPM and potential solutions using big data technology and analytics. The concurrent development of high-throughput techniques, cloud computing, data science, and the semantic web will likely allow SPM to be findable, accessible, interoperable, reusable, reliable, interpretable, and actionable.",Not About Sufficiency
Ontology for experimentation of human-building interactions using virtual reality,"Scientific experiments significantly enhance the understanding of human-building interactions in building and engineering research. Recently, conducting virtual reality (VR) experiments has gained acceptance and popu-larity as an approach to studying human-building interactions. However, little attention has been given to the standardization of the experimentations. Proper standardization can promote the reusability, replicability, and repeatability of VR experiments and accelerate the maturity of this emerging experimentation method. Responding to such needs, the authors proposed a virtual human-building interaction experimentation ontology (VHBIEO). It is an ontology at the domain level, extending the ontology of scientific experiments (EXPO) to standardize virtual human-building interaction experimentation. It was developed based on state-of-the-art ontology development approaches. Competency questions (CQs) were used to derive requirements and regu-late the development. Semantic Web technologies were applied to make VHBIEO machine-readable, accessible, and processable. VHBIEO incorporates an application view (APV) to support the inclusion of unique information for particular applications. The authors performed taxonomy evaluations to assess the consistency, completeness, and redundancy, affirming no occurrence of errors in its structure. Application evaluations were applied for investigating its ability to standardize and support generating of machine-readable, accessible, and processable information. Application evaluations also verified the capability of APV to support the inclusion of unique information.",Not About Sufficiency
"Environmental conflict, urbanization and dissent: Perspectives on democratic practices in territorial planning","This article examines the notion of environmental conflict in order to contribute to contemporary discussions about just socio-ecological transitions in times of climate and social crisis and pandemics. Based on a conceptual review and information about cases of democratic practices in spatial planning processes and urbanization, the article proposes that the environmental conflict is a process of production and socio-ecological dispute of multiple territorialities. This process is in turn crossed by different socio-ecological inequalities. Therefore, in light of a recent interest in the depoliticization of the environmental issue, the article also proposes that conflicts can be understood as a rupture of a normalized order to force new arrangements towards environmental justice respecting the difference instead of controlling or eliminating dissent. This will allow promoting more democratic practices of spatial planning.",Not About Sufficiency
Max-Cut Linear Binary Classifier Based on Quantum Approximate Optimization Algorithm,"The rapid development of quantum computing has opened up entirely new possibilities for the field of machine learning. However, for the implementation of many existing quantum classification algorithms, a large number of qubits and quantum circuits with high complexity are still required. To effectively solve this problem, the Quantum Approximate Optimization Algorithm (QAOA) arises as a promising solution due to its comparative advantages. In particular, it can be realized in the case of shallow quantum circuits and a finite small number of qubits. Along these lines, in this work, a Max-Cut linear binary classifier based on QAOA (QAOA-MaxCut-LBC) was proposed. First, the data set was constructed into an undirected weighted graph, and the binary classification task was transformed into a Max-Cut problem. Then, a Variational Quantum Circuit (VQC) was built by using QAOA, and the expected value of the target Hamiltonian was transformed into a loss function. Finally, the circuit parameters were iteratively updated to make the loss function converge. The computational basis state with the maximum probability was taken as the classification result after the measurement. In the experimental study, our algorithm was validated on various datasets and compared with classical linear classifiers. Our scheme can be flexibly adjusted for the number of qubits, possessing the potential to scale to multi-classification tasks. The source code is accessible at the URL: https://github.com/Dullne/QAOA-MaxCut-LBC.",Not About Sufficiency
Creating informative experiences through a visual and interactive representation of health and social care data,"Association rule mining is an established machine learning tool for finding patterns ('rules') in big datasets. The algorithm can easily produce a large number of 'rules' of how items in a dataset are related to one another. This can pose a significant challenge to the interpretability and usefulness of the results obtained. In this paper, we present how to support decision making through 'playable mechanics' powered by a video games engine. The premise is to create aesthetic and informative experiences through a visual and interactive representation of a problem space such as the association rules mined from a health and social care dataset. The Unity game engine is used to create a force-directed graph to be rendered and explored in real-time using the Barnes-Hut method to accelerate computations. A Boolean AND/OR/NOT selection function was implemented, enabling the graph to be explored and pruned to the data points specified by the search query. As a result, users can obtain an overview of the large-scale structure of the dataset, with the option of performing targeted explorations around points of interest. To evaluate the effectiveness of the application, a series of online user testing workshops were conducted. The resultant thematic analysis found the incorporated features to be well-integrated, but a difference was found between the responses of users with high or low technical proficiency within the commissioning organization. The technical users were able to quickly grasp the operation of the system but were unclear about its purpose or practical application. Conversely, the health and social care professionals saw the potential value of the tool but were unsure of their personal ability to use it effectively. Finally, System Usability Scale (SUS) scores were obtained from participants in a final in-person workshop, with excellent results overall (mean 84, top quartile).",Not About Sufficiency
Poster:Stock Price Prediction using Machine Learning,"Many research highlights the incorporation of various dimensions in stock price prediction, encompassing fundamental, technical, float, and news/community-based information analysis. However, differing opinions may hinder rapid investment decision-making. Therefore, this research aims to utilize stock price trend prediction models to assist investors in making more effective decisions upon receiving news/community-based information since it is more accessible and comprehensible to many investors. This research gathers stock and technical data from Yahoo Finance!, fundamental data from Macro Trends, text data for employee reviews from Glassdoor for various companies in the analysis. The study employs Random Forest, Extreme Gradient Boosting, Recurrent Neural Network, and Long Short-Term Memory to develop stock price trend prediction models, aiming to provide future guidance for investors' decision-making processes.",Not About Sufficiency
FALCON: Feature-Label Constrained Graph Net Collapse for Memory-Efficient GNNs,"Graph neural network (GNN) ushered in a new era of machine learning with interconnected datasets. While traditional neural networks can only be trained on independent samples, GNN allows for the inclusion of intersample interactions in the training process. This gain, however, incurs additional memory cost, rendering most GNNs unscalable for real-world applications involving vast and complicated networks with tens of millions of nodes (e.g., social circles, web graphs, and brain graphs). This means that storing the graph in the main memory can be difficult, let alone training the GNN model with significantly less GPU memory. While much of the recent literature has focused on either mini-batching GNN methods or quantization, graph reduction methods remain largely scarce. Furthermore, present graph reduction approaches have several drawbacks. First, most graph reduction focuses only on the inference stage (e.g., condensation, pruning, and distillation) and requires full graph GNN training, which does not reduce training memory footprint. Second, many methods focus solely on the graph's structural aspect, ignoring the initial population feature-label distribution, resulting in a skewed postreduction label distribution. Here, we propose a feature-label constrained graph net collapse (FALCON) to address these limitations. Our three core contributions lie in: 1) designing FALCON, a topology-aware graph reduction technique that preserves feature-label distribution by introducing a $K$ -means clustering with a novel dimension-normalized Euclidean distance; 2) implementation of FALCON with other state-of-the-art (SOTA) memory reduction methods (i.e., mini-batched GNN and quantization) for further memory reduction; and 3) extensive benchmarking and ablation studies against SOTA methods to evaluate FALCON memory reduction. Our comprehensive results show that FALCON can significantly collapse various public datasets (e.g., PPI and Flickr to as low as 34% of the total nodes) while keeping equal prediction quality across GNN models. Our FALCON code is available at https://github.com/basiralab/FALCON.",Not About Sufficiency
Ranking Influential Non-Content Factors on Scientific Papers' Citation Impact: A Multidomain Comparative Analysis,"The influence of scientific papers is measured by their citations. Although predicting the papers' citation impact based on non-content factors has garnered extensive attention, the influence of such factors is rarely compared. In this article, we compare the influence of non-content factors on the citation counts of academic publications across three fields, i.e., math, computer science, and management. We consider different methods in this study, including three machine learning approaches, namely, XGBoost, Gradient Boosting Decision Tree, and Random Forest, along with statistical techniques such as linear regression and quantile analysis. Our findings reveal that no matter the field or analytical method applied, author prestige and the number of references consistently stand out as the most influential factors, while the breadth of categories covered by the paper has minimal impact. In mathematics, the first citation date and article length are almost equally important as author prestige, while the number of authors and the journal impact factor are crucial for computer science papers. In management, the number of collaborating countries is relatively influential with respect to the paper's citations. The results of the quantile regression indicate that at higher quantile levels, the impact of author prestige and the number of authors on the papers' citation impact are more pronounced across all three disciplines, while the journal impact factor and paper length have the greatest influence at low and medium quantile levels. Our findings indicate that the reliance of academic citations on author prestige and journal impact factors not only highlights the unequal distribution of resources within the current academic system but also further exacerbates citation inequality.",Not About Sufficiency
Machine Learning Methods for Spacecraft Telemetry Mining,"Spacecrafts are critical systems that have to survive space environment effects. Due to its complexity, these types of systems are designed in away to mitigate errors and maneuver the critical situations. Spacecraft delivers to the ground operator an abundance data related to system status telemetry; the telemetry parameters are monitored to indicate spacecraft performance. Recently, researchers proposed using Machine Learning (ML)/Telemetry Mining (TM) techniques for telemetry parameters forecasting. Telemetry processing facilitates the data visualization to enable operators understanding the behavior of the satellite in order to reduce failure risks. In this paper, we introduce a comparison between the different machine learning techniques that can be applied for low earth orbit satellite telemetry mining. The techniques are evaluated on the bases of calculating the prediction accuracy using mean error and correlation estimation. We used telemetry data received from Egyptsat-1 satellite including parameters such as battery temperature, power bus voltage and load current. The research summarizes the performance of processing telemetry data using autoregressive integrated moving average (ARIMA), Multilayer Perceptron (MLP), Recurrent Neural Network (RNN), Long Short-Term Memory Recurrent Neural Network (LSTM RNN), Deep Long Short-Term Memory Recurrent Neural Networks (DLSTM RNNs), Gated Recurrent Unit Recurrent Neural Network (GRU RNN), and Deep Gated Recurrent Unit Recurrent Neural Networks (DGRU RNNs).",Not About Sufficiency
Explainable deep learning on multi-target time series forecasting: An air pollution use case,"Urban air pollution represents a significant threat to public health and the environment, with nitrogen oxides, ozone, and particulate matter being among the most harmful pollutants. These contribute to respiratory and cardiovascular diseases, particularly in urban areas with high traffic and elevated temperatures. Machine learning, especially deep learning, shows promise in enhancing the prediction accuracy of prediction of pollutant's concentrations. However, the ""black box"" nature of these models often limits their interpretability, which is crucial for informed decision-making. Our study introduces a Temporal Selection Layer technique within deep learning models for time series forecasting to tackle this issue. This technique not only improves prediction accuracy by embedding feature selection directly into the neural network, but also enhances interpretability and reduces computational costs. In particular, we applied this method to hourly concentration data of pollutants, including particulate matter, ozone, and nitrogen oxides, from five urban monitoring sites in Graz, Austria. These concentrations were used as target variables to predict, while identifying the most relevant features and periods that affect prediction accuracy. Comparative analysis with other embedded feature selection methods showed that the Temporal Selection Layer significantly enhances both model effectiveness and transparency. Additionally, we applied explainable techniques to evaluate the impact of weather and time-related factors on air pollution, which also helped assess feature importance. The results show that our approach improves both prediction accuracy and model interpretability, leading finally to more effective pollution management strategies.",Not About Sufficiency
MPXV: Update on Morphological and Morphogenesis Aspects Through Transmission and Scanning Electron Microscopies and 3D Reconstruction,"An unprecedented global outbreak caused by the monkeypox virus (MPXV) prompted the World Health Organization to declare a public health emergency of international concern on July 23, 2022. Therapeutics and vaccines for MPXV are not widely available, necessitating further studies, particularly in drug repurposing area. To this end, the standardization of in vitro infection systems is essential. The most robust in vitro studies on poxviruses concern the Vaccinia virus, and there are significant gaps in understanding the replicative cycle of MPXV. Herein, we conducted ultrastructural studies using transmission and scanning electron microscopies and 3D reconstruction to describe and elucidate the step-by-step morphogenesis of MPXV. Vero cells, derived from the kidney lineage of Cercopithecus aethiops monkeys, were infected with a strain isolated from an oropharyngeal swab of a patient with suspected Mpox, collected during an observational cohort study conducted between June 12 and August 19, 2022, in Rio de Janeiro, Brazil. Infected Vero cells exhibited several morphological alterations, including cell lysis plaque formation, nuclei with altered chromatin profiles, thickening of the rough endoplasmic reticulum (RER), presence of myelin figures, disorganization of mitochondrial cristae, and the formation of a granular and fibrous matrix (viral factory) surrounded by mitochondria and RER cisternae in a perinuclear space. Viral entry into cells occurred via endocytosis MPXV particles were observed adhering to cytoskeletal filaments, and viral progeny extrusion occurred through exocytosis. This article presents novel data on the morphogenesis of MPXV that have not been previously documented in the literature.",Not About Sufficiency
How to improve collaboration between the public health sector and other policy sectors to reduce health inequalities? - A study in sixteen municipalities in the Netherlands,"Background: The causes of health inequalities are complex. For the reduction of health inequalities, intersectoral collaboration between the public health sector and both social policy sectors (e.g. youth affairs, education) and physical policy sectors (e.g. housing, spatial planning) is essential, but in local practice difficult to realize. The aim of this study was to examine the collaboration between the sectors in question more closely and to identify opportunities for improvement. Method: A qualitative descriptive analysis of five aspects of collaboration within sixteen Dutch municipalities was performed to examine the collaboration between the public health sector and other policy sectors: 1) involvement of the sectors in the public health policy network, 2) harmonisation of objectives, 3) use of policies by the relevant sectors, 4) formalised collaboration, and 5) previous experience. Empirical data on these collaboration aspects were collected based on document analysis, questionnaires and interviews. Results: The study found that the policy workers of social sectors were more involved in the public health network and more frequently supported the objectives in the field of health inequality reduction. Both social policy sectors and physical policy sectors used policies and activities to reduce health inequalities. More is done to influence the determinants of health inequality through policies aimed at lifestyle and social setting than through policies aimed at socioeconomic factors and the physical environment. Where the physical policy sectors are involved in the public health network, the collaboration follows a very similar pattern as with the social policy sectors. All sectors recognise the importance of good relationships, positive experiences, a common interest in working together and coordinated mechanisms. Conclusion: This study shows that there is scope for improving collaboration in the field of health inequality reduction between the public health sector and both social policy sectors and physical policy sectors. Ways in which improvement could be realised include involving physical policy sectors in the network, pursuing widely supported policy goals, making balanced efforts to influence determinants of health inequalities, and increasing the emphasis on a programmatic approach.",Not About Sufficiency
Factors contributing to urban isolation: A mixed-methods analysis of three new towns in Tehran,"This study investigates the extent and determinants of urban isolation in three planned towns near Tehran-Andisheh, Pardis, and Parand-using a mixed-methods approach. A structured questionnaire was distributed online via social media and in-person at community centers, collecting responses from 1152 residents. The questionnaire assessed isolation levels, contributing factors, and personal experiences through a combination of closed-ended and open-ended questions. MICMAC structural analysis was then employed to identify the key systemic drivers of urban isolation. Findings reveal significant variations in urban isolation across the three towns. Andisheh, benefiting from a more structured urban design, exhibits the lowest levels of isolation, while Parand records the highest due to socio-economic segregation, inadequate public services, and weak community networks. Factor analysis highlights socio-spatial barriers, including poor public transportation, restricted access to public spaces, and insecure neighborhoods, alongside social fragmentation driven by marital status, declining family sizes, rising divorce rates, and generalized distrust, as well as behavioral and lifestyle changes such as increased reliance on digital communication, longer work hours, and reduced leisure time. Residents' narratives highlight diverse drivers of isolation social disparities in Andisheh amenity deficiencies in Pardis and infrastructural shortcomings in Parand. MICMAC analysis identifies Employment Status, Generalized distrust, Increasing divorce and Social withdrawal as the most influential variables shaping urban isolation. Policy recommendations emphasize the urgent need for participatory urban planning, the development of inclusive and well-connected public spaces, enhanced transportation networks, and community-building initiatives. Addressing socio-spatial inequalities and fostering social interaction through integrated urban policies will be essential in mitigating urban isolation and improving the quality of life in these new town developments.",Not About Sufficiency
Real world performance of the 21st Century Cures Act population-level application programming interface,"Objective To evaluate the real-world performance of the SMART/HL7 Bulk Fast Health Interoperability Resources (FHIR) Access Application Programming Interface (API), developed to enable push button access to electronic health record data on large populations, and required under the 21st Century Cures Act Rule.Materials and Methods We used an open-source Bulk FHIR Testing Suite at 5 healthcare sites from April to September 2023, including 4 hospitals using electronic health records (EHRs) certified for interoperability, and 1 Health Information Exchange (HIE) using a custom, standards-compliant API build. We measured export speeds, data sizes, and completeness across 6 types of FHIR.Results Among the certified platforms, Oracle Cerner led in speed, managing 5-16 million resources at over 8000 resources/min. Three Epic sites exported a FHIR data subset, achieving 1-12 million resources at 1555-2500 resources/min. Notably, the HIE's custom API outperformed, generating over 141 million resources at 12 000 resources/min.Discussion The HIE's custom API showcased superior performance, endorsing the effectiveness of SMART/HL7 Bulk FHIR in enabling large-scale data exchange while underlining the need for optimization in existing EHR platforms. Agility and scalability are essential for diverse health, research, and public health use cases.Conclusion To fully realize the interoperability goals of the 21st Century Cures Act, addressing the performance limitations of Bulk FHIR API is critical. It would be beneficial to include performance metrics in both certification and reporting processes.",Not About Sufficiency
"Job accessibility and male teenage employment, 1980-1990: The declining significance of space?","The spatial mismatch hypothesis, which argues that job decentralization has had a major impact on the economic fortunes of inner-city minorities, has been a popular argument in academic and policy circles. it is possible, however, that employment decentralization was a temporary shock to inner-city labor markets and that labor supply has successfully adjusted by residential relocation and alterations in job search patterns. This paper examines this issue with an empirical analysis of the 1980 and 1990 employment probabilities of black and white male teenagers living in the largest metropolitan areas of the United States. Findings indicate that the impact of job accessibility on employment probabilities declined between 1980 and 1990, especially for black male teenagers not enrolled in school. Accessibility had a declining effect for this group because of losing the advantage of accessibility rather than overcoming the disadvantage of inaccessibility. By 1990, black male teens living in job-accessible areas no longer enjoyed as much of an employment advantage relative to teens living in job-inaccessible areas as they did in 1980. We should thus be cautious about ascribing too much theoretical or policy importance to job accessibility factors.",Not About Sufficiency
Modelling of land surface temperature changes as determinant of urban heat island and risk of heat-related conditions in the Wassa West Mining Area of Ghana,"In recent times, land surface temperature and its associated effect urban heat island (UHI) has attracted massive research and public attention due to their effects on the environment, climate and public health. Specifically, UHI adversely affects the quality of life of city dwellers in the form of thermal discomfort, resulting in a rise in energy consumption to mitigate the effects of the increase in surface temperature. Therefore, studying the presence, extent, spatial distribution and changes in surface temperature is essential as it could generate meaningful and useful information for sustainable urban planning. This research used four multi-temporal Landsat images covering three decades to examine surface temperature changes and urban heat islands in the Wassa West areas of south-western Ghana. The research was implemented through the application of several mathematical models, remote sensing and GIS techniques to determine land surface temperature variations over the study period. The results of the study showed a consistent rise in surface temperature over the 30-year period. For instance, in 1990, the minimum and maximum surface temperatures were 14.96 degrees C and 27.17 degrees C, respectively. In 2020, the minimum and maximum temperatures had increase to 24.22 degrees C and 31.23 degrees C, respectively. A consistent decline in the vegetation of the study was equally observed over the period, and this was attributed to the increase in urban settlements and large-scale mining activities in the study area. The results further indicated that low surface temperatures were observed in rural areas with high vegetation, whereas high surface temperature values were consistently observed in areas with impervious surface covers such as mining and urban areas, suggesting the presence of the UHI phenomenon. The study identifies environmentally critical areas in the study areas warranting the urgent attention of society and specific measures that could contribute to reducing the fast increase in surface temperature and mitigate the risk of heat-related conditions has been discussed.",Not About Sufficiency
Machine Learning of Coarse-Grained Molecular Dynamics Force Fields,"Atomistic or ab initio molecular dynamics simulations are widely used to predict thermodynamics and kinetics and relate them to molecular structure. A common approach to go beyond the time- and length-scales accessible with such computationally expensive simulations is the definition of coarse-grained molecular models. Existing coarse-graining approaches define an effective interaction potential to match defined properties of high-resolution models or experimental data. In this paper, we reformulate coarse-graining as a supervised machine learning problem. We use statistical learning theory to decompose the coarse-graining error and cross-validation to select and compare the performance of different models. We introduce CGnets, a deep learning approach, that learns coarse-grained free energy functions and can be trained by a force-matching scheme. CGnets maintain all physically relevant invariances and allow one to incorporate prior physics knowledge to avoid sampling of unphysical structures. We show that CGnets can capture all-atom explicit-solvent free energy surfaces with models using only a few coarse-grained beads and no solvent, while classical coarse-graining methods fail to capture crucial features of the free energy surface. Thus, CGnets are able to capture multibody terms that emerge from the dimensionality reduction.",Not About Sufficiency
The rural founding villages of the Italian Agrarian Reform in Basilicata (1950-1970): urban planning and 'modern' vernacular architecture to the test of contemporaneity. The case of Borgo Taccone (MT),"The contribution aims at providing an overview on urban planning and on 'modern' vernacular architecture of the rural founding villages built during the Agrarian Reform (1950-1970) in Italy, in the inland areas of Basilicata Region. In particular there are settlements not yet sufficiently known, in which the important of inventorying the considerable built heritage must be the objective of a necessary, urgent safeguarding. With the 'Agrarian Reform' (Law 841/1950), the Italian government carried out a redistribution to settlers of the lands of uncultivated or abandoned large estates. The purpose was to increase productivity in the reformed areas, as long as a better profitability of labor and an adequate 'social equity'. As a consequence, new villages were created that had to fulfil the task of reorganizing rural centers of socio-economic concentrations, able to reconstitute environments similar to the agglomerations from which the laborers, once employed in the latifundiums, came. Among the numerous centers built in Basilicata, Borgo Taccone is representative of this system of agrarian colonization of the Lucanian territory. The settlement, in which the modern construction techniques were broadly experimented, is the service center for farmers living in farmhouses in the surrounding funds and for this reason it was equipped with core services such as the church, the school, the post office, the clinic, cinema/theater, etc. After an initial period of demographic expansion, in the seventies the 'Borgo' began to depopulate and is now in a state of abandonment and decay. Despite this, this settlement, surrounded by agricultural land in a well-preserved landscape, still retains a strong formal character in both its urban and architectural layout. The contribution traces the physical, social and cultural transformation line that led this rich asset to the contemporary world, outlining a possible future cultural theoretical debate on its safeguard and sustainable enhancement.",Not About Sufficiency
Predicting life satisfaction based on the emotion words in self-statement texts,"Measuring people's life satisfaction in real time on a large scale is quite valuable for monitoring and promoting public mental health; however, the traditional questionnaire method cannot fully meet this need. This study utilized the emotion words in self-statement texts to train machine learning predictive models to identify an individual's life satisfaction. The SVR model was found to have the best performance, with the correlation between predicted scores and self-reported questionnaire scores achieving 0.42 and the split-half reliability achieving 0.939. This result demonstrates the possibility of identifying life satisfaction through emotional expressions and provides a method to measure the public's life satisfaction online. The word categories selected through the modeling process were happy (PA), sorrow (NB), boredom (NE), reproach (NN), glad (MH), aversion (ME), and N (negation + positive), which reveal the specific emotions in self-expression relevant to life satisfaction.",Not About Sufficiency
A multi-stage localization framework for accurate and precise docking of autonomous mobile robots (AMRs),"Autonomous navigation has been a long-standing research topic, and researchers have worked on many challenging problems in indoor and outdoor environments. One application area of navigation solutions is material handling in industrial environments. With Industry 4.0, the simple problem in traditional factories has evolved into the use of autonomous mobile robots within flexible production islands in a self-decision-making structure. Two main stages of such a navigation system are safe transportation of the vehicle from one point to another and reaching destinations at industrial standards. The main concern in the former is roughly determining the vehicle's pose to follow the route, while the latter aims to reach the target with high accuracy and precision. Often, it may not be possible or require extra effort to satisfy requirements with a single localization method. Therefore, a multi-stage localization approach is proposed in this study. Particle filter-based large-scale localization approaches are utilized during the vehicle's movement from one point to another, while scan-matching-based methods are used in the docking stage. The localization system enables the appropriate approach based on the vehicle's status and task through a decision-making mechanism. The decision-making mechanism uses a similarity metric obtained through the correntropy criterion to decide when and how to switch from large-scale localization to precise localization. The feasibility and performance of the developed method are corroborated through field tests. These evaluations demonstrate that the proposed method accomplishes tasks with sub-centimeter and sub-degree accuracy and precision without affecting the operation of the navigation algorithms in real time.",Not About Sufficiency
Accessibility measurements for urban parks considering age-grouped walkers? sectorial travel behavior and built environment,"A walking-accessible urban park system is essential to the physical and mental health of urban residents. However, it is difficult to accurately describe the walking behavior of elderly and young visitors using empirical models (i.e., empirical threshold distance and empirical decay function), because of the differences in walking ability and distance decay among visitors of different ages. To address this challenge, this study used survey data to extract the travel behavior of elderly and younger visitors from a survey of 3625 walking respondents using a sectional threshold distance in central Wuhan. A multiple linear regression model was then built to discover how the factors influence the sectional threshold distance for different age groups. The results show that the travel time threshold for elderly visitors was approximately 0.47-1 min shorter than for other visitors, and the dif-ference became more obvious for the 85th percentile of visitors' segment travel time. The correlation analysis and regression results suggest that the surrounding built environment, such as restaurants, mostly had to a positive impact on sectional threshold distances, while park facilities were negatively correlated with the threshold distance. This consequently led to different accessibility levels for visitors by age, and elderly people presented an accessibility 0.0012 person/ha lower than young visitors. These findings provide a more accurate and refined decision-making basis for urban planners and managers to optimize park layouts.",Not About Sufficiency
Comparison of different microarray data analysis programs and description of a database for microarray data management,"Data analysis and management represent a major challenge for gene expression studies using microarrays. Here, we compare different methods of analysis and demonstrate the utility of a personal microarray database. Gene expression during HIV infection of cell lines was studied using Affymetrix U-133 A and B chips. The data were analyzed using Affymetrix Microarray Suite and Data Mining Tool, Silicon Genetics Gene-Spring, and dChip from Harvard School of Public Health. A small-scale database was established with File-Maker Pro Developer to manage and analyze the data. There was great variability among the programs in the lists of significantly changed genes constructed from the same data. Similarly choices of different parameters for normalization, comparison, and standardization greatly affected the outcome. As many probe sets on the U133 chip target the same Unigene clusters, the Unigene information can be used as an internal control to confirm and interpret the probe set results. Algorithms used for the determination of changes in gene expression require further refinement and standardization. The use of a personal database powered with Unigene information can enhance the analysis of gene expression data.",Not About Sufficiency
"Research on Urban Public Green Space Planning Based on Taxi Data: A Case Study on Three Districts of Shenzhen, China","Urban public green space (UPGS) plays an important role in sustainable development. In China, the planning, classification, and management of green spaces are based on the Standard for Classification of Urban Green Space (SCUGS). However, limitations to the UPGS exist due to the over-emphasis on quantitative standards and insufficient consideration of the actual access mode of residents. Though the taxi trajectory data are widely selected to study public service facilities, its adoption in UPGSs research remains limited. Based on the case of UPGSs in the three districts of Shenzhen, we used the taxi (including cruise taxis and Didi cars, which are like Uber) trajectory data to investigate the spatial layout and the allocation of management resource of the UPGSs from the spatial interaction perspective. By rasterizing and visualizing the percentage of pick-up and drop-off points in the UPGSs' buffer, the service scope of UPGSs was defined, which reflected the spatial distribution and activity intensity of the visitors. Then, an unsupervised classification method was introduced to reclassify the twenty two municipal parks in the three districts. Compared to the traditional planning method, the results show that the service scope of the same type of UPGS in the traditional classification is not the same as the one obtained by the study. Visitors to all UPGSs are distributed as a quadratic function and decay as the distance increases. In addition, the attenuation rates of the same type of UPGSs are similar. The findings of this study are expected to assist planners in improving the spatial layout of UPGSs and optimizing the allocation of UPGS management resources based on new classifications.",Not About Sufficiency
IS INTELLECTUAL PROPERTY THE COVID-19 BAD GUY? LESSONS WE COULD LEARN FROM THE PANDEMIC,"At the time the COVID-19 pandemic was declared there was no vaccine and other medical products were insufficient to meet demands. At the time intellectual property was considered a limitation to an effective pandemic response and the World Trade Organization considered a waiver of intellectual property addressed by the Agreement on Trade -Related Aspects of Intellectual Property Rights (TRIPS). The lesson from the COVID-19 pandemic and TRIPS waiver is that given enough time sufficient medical products will be delivered, albeit there remain some complicated delivery challenges and vaccine hesitancy issues. This column addresses the moment before that medical product saturation and the inherent limitation imposed by industry policies. The column concludes that the private sectors' motivating factors need to be integrated into the design of global public health pandemic responses from the start.",Not About Sufficiency
Investigating the Automatic Classification of Algae Using the Spectral and Morphological Characteristics via Deep Residual Learning,"Under the impact of global climate changes and human activities, harmful algae blooms (HABs) have become a growing concern due to negative impacts on water related industries, such as tourism, fishing and safe water supply. Many jurisdictions have introduced specific water quality regulations to protect public health and safety. Therefore reliable and cost effective methods of quantifying the type and concentration of algae cells has become critical for ensuring successful water management. In this work we present an innovative system to automatically classify multiple types of algae by combining standard morphological features with their multi-wavelength signals. To accomplish this we use a custom-designed microscopy imaging system which is configured to image water samples at two fluorescent wavelengths and seven absorption wavelengths using discrete-wavelength high-powered light emitting diodes (LEDs). We investigate the effectiveness of automatic classification using a deep residual convolutional neural network and achieve a classification accuracy of 96% in an experiment conducted with six different algae types. This high level of accuracy was achieved using a deep residual convolutional neural network that learns the optimal combination of spectral and morphological features. These findings illustrate the possibility of leveraging a unique fingerprint of algae cell (i.e. spectral wavelengths and morphological features) to automatically distinguish different algae types. Our work herein demonstrates that, when coupled with multi-band fluorescence microscopy, machine learning algorithms can potentially be used as a robust and cost-effective tool for identifying and enumerating algae cells.",Not About Sufficiency
HMMRATAC: a Hidden Markov ModeleR for ATAC-seq,"ATAC-seq has been widely adopted to identify accessible chromatin regions across the genome. However, current data analysis still utilizes approaches initially designed for ChIP-seq or DNase-seq, without considering the transposase digested DNA fragments that contain additional nucleosome positioning information. We present the first dedicated ATAC-seq analysis tool, a semi-supervised machine learning approach named HMMRATAC. HMMRATAC splits a single ATAC-seq dataset into nucleosome-free and nucleosome-enriched signals, learns the unique chromatin structure around accessible regions, and then predicts accessible regions across the entire genome. We show that HMMRATAC outperforms the popular peak-calling algorithms on published human ATAC-seq datasets. We find that single-end sequenced or size-selected ATAC-seq datasets result in a loss of sensitivity compared to paired-end datasets without size-selection.",Not About Sufficiency
Enhancement of a Single-Axis Femtosecond Laser Scanning System by Using Two Galvanometers to Improve the Telecentricity and the Effective Scanning Length on Laser Process,"The galvanometer scanning system plays a crucial role in modern laser material processing. With the development of this industry, the requirements for galvanometer scanners are getting higher and higher, especially to overcome the inherent disadvantages that still exist, such as image distortion, marking speed and accuracy in state-of-the-art scanning systems. In this paper, a single-axis optical scanner using two galvanometers in combination with one f-theta telecentric lens and a 343 nm femtosecond pulse laser source is proposed as a new approach for enhancing the precision of laser micromachining technology. The additional second galvanometer is used to manipulate the output laser beam of the first galvanometer to the path with less lens aberration to enhance the telecentricity correction and the effective scanning area. This is based on the international standard regulation ISO (the International Organization for Standardization) 11145:2018 requirements in optics and photonics, in which an important criterion is for the roundness of the focused beam spot to be greater than 87% to determine the effective working length of the proposed scanning system compared to the conventional scanning system. It is demonstrated by optical simulations and real optical experiments that the effective working length can be increased by 3.6 mm, corresponding to 8.1% of the effective scanning field, to achieve a laser material processing system with ISO standard. The damped least squares (DLS) algorithm in optical design software ZEMAX is used to optimize the deflected angle of the two galvanometers to obtain the optimal incident position of the f-theta lens.",Not About Sufficiency
Spirituality and Religiosity in Rheumatic Diseases: A Systematic Review,"Several studies have demonstrated the influence of religiosity and spirituality (R/S) on patients with chronic diseases. However, few studies have explored the influence of R/S on autoimmune/non-autoimmune rheumatic diseases. We conducted a systematic review of the literature on the impact of religiosity and spirituality (R/S) on the health of patients with (autoimmune and non-autoimmune) rheumatic diseases. Systematic review of the literature according to the PRISMA protocol. Articles published between January 1912 and September 2024 in the Virtual Health Library (VHL), PubMed, Web of Science, Cochrane Library, Scopus, and PsychInfo databases were included. Few studies have explored the influence of R/S on non-autoimmune rheumatic diseases. Most studies have evaluated the impact of R/S on central sensitization pain syndromes, such as fibromyalgia, and degenerative diseases, such as osteoarthritis. Only two studies have been conducted in patients with autoimmune rheumatic diseases, rheumatoid arthritis, and idiopathic inflammatory myopathies. Among the 1614 articles found, 17 met the eligibility criteria. In the quality analysis of the studies, 76.5% were classified as ""good."" The findings were then divided into ""psychological and physical impacts of R/S in patients with autoimmune rheumatic diseases"" and ""psychological and physical impacts of R/S in patients with non-autoimmune rheumatic diseases"". The literature demonstrates the impact of R/S on improving quality of life, especially on lower pain scores, reducing stress, and improving mood and life satisfaction. This evidence is the most robust for non-autoimmune rheumatic diseases. The effects of R/S on non-psychological aspects are uncertain. Additionally, studies have small samples, and most are not longitudinal. Therefore, longitudinal studies that consider differences such as the type of religion, standardization of R/S level mapping methods, and larger samples are necessary.",Not About Sufficiency
INVESTIGATION OF COGNITIVE NEIGHBORHOODSIZE BY AGENT-BASED SIMULATION,"Different social groups tend to settle in different parts of cities leading over time to social segregation. Neighborhood obviously plays an important role in this process and what constitutes neighborhood is a cognitive notion. In segregation analysis neighborhood borders are often drawn arbitrarily or simple assumptions are used to weight neighbor influences. Some authors have developed ideas to overcome such approaches by more detailed models. In this work we investigate the size of a cognitive neighborhood on the base of a continuous, geographically unlimited definition of neighborhood, using a distance-dependent function as such neighborhood ""size"" definition. We use agent-based simulation of the choice of residence as our primary investigation tool. Tobler's first law of geography tells us that close things are more related than far ones. Extrapolating this thought and applying it to the question discussed here one could expect that closer neighbors have on their own and in sum more influence than those living further apart. The ""sum"" in the last sentence would lead to a neighborhood weighting of less than the inverse square of distance. The results of this investigation confirm that this is the case.",Not About Sufficiency
Visual representation of safety in urban spaces: a tale of two neighbourhoods,"Urban studies have addressed the role of media with regard to the ways in which the production of news affects the practicing of urban life. Media promotes urban growth, but also plays a role in the creation of negative images ascribed to places and neighbourhoods. This paper explores discourses of safety and security, illustrated in Swedish newspaper representations of two neighbourhoods in Malmo, Sweden, with different social status. Content analysis and discourse analysis methods were used to contribute to the understanding of potentiality as well as hindrances for a just urban planning. The overall result shows a negative image of Bellevuegarden as a problematic area, whereas Vastra Hamnen is presented as an attractive and safe neighbourhood. The study also identifies the interrelations and intertextuality between each neighbourhood and the city of Malmo as a whole. It is highlighted how Malmo has struggled with its negative reputation as a dangerous city. To favour one and disfavour another area does not contribute to equality and integration, which is the vision involved in the comprehensive planning of Malmo city. It has been concluded that the news media's representation of neighbourhoods has influenced how they are perceived, both by inhabitants and by society at large.",Not About Sufficiency
"Investigating of transportation systems development for urban districts, costs and social equity: a case of Sanandaj, Kurdistan","The transportation system is one of the essential subjects for any country because it directly concerns the daily costs and social problems. Researchers have proposed different methods to develop transportation systems, each with its strengths and weaknesses. This paper has proposed a new bi-objective mathematical model based on three social equity theories, the first of which considers infrastructure development costs for a transportation system and includes operational, environmental, and pollution costs. The second objective focuses on enhancing social equity in investment distributions in the transportation system for a long time. Three theories, including Utilitarianism, Rawl's, and Sadr's, were modeled and used to assess social equity. The model was examined for an original case of data in Sanandaj city, and an improved version of the augmented e-constraint method was developed to solve it. According to the general results, each social equity theory has its own merits. The results show that Rawls' theory, with minimum infrastructure costs, establishes the best social equity (70%) after 5 years of application; therefore, the Sanandaj city would be a sustainable city based on Rawls' theory which enhances region 1, and the three regions would be in balance after 5 years. Urban planners and transport network managers can use the developed model for equitable investment in an urban transportation system.",Not About Sufficiency
A digital image colorimetry system based on smart devices for immediate and simultaneous determination of enzyme-linked immunosorbent assays,"Standard enzyme-linked immunosorbent assays based on microplates are frequently utilized for various molecular sensing, disease screening, and nanomedicine applications. Comparing this multi-well plate batched analysis to non-batched or non-standard testing, the diagnosis expenses per patient are drastically reduced. However, the requirement for rather big and pricey readout instruments prevents their application in environments with limited resources, especially in the field. In this work, a handheld cellphone-based colorimetric microplate reader for quick, credible, and novel analysis of digital images of human cancer cell lines at a reasonable price was developed. Using our in-house-developed app, images of the plates are captured and sent to our servers, where they are processed using a machine learning algorithm to produce diagnostic results. Using FDA-approved human epididymis protein of ovary IgG (HE4), prostate cancer cell line (PC3), and bladder cancer cell line (5637) ELISA tests, we successfully examined this mobile platform. The accuracies for the HE4, PC3, and 5637 tests were 93%, 97.5%, and 97.2%, respectively. By contrasting the findings with the measurements made using optical absorption EPOCH microplate readers and optical absorption Tecan microplate readers, this approach was found to be accurate and effective. As a result, digital image colorimetry on smart devices offered a practical, user-friendly, affordable, precise, and effective method for quickly identifying human cancer cell lines. Thus, healthcare providers might use this portable device to carry out high-throughput illness screening, epidemiological investigations or monitor vaccination campaigns.",Not About Sufficiency
On the Use of Decision Tree Regression for Predicting Vibration Frequency Response of Handheld Probes,"This article focuses on the prediction of the vibration frequency response of handheld probes. A novel approach that involves machine learning and readily available data from probes was explored. Vibration probes are efficient and affordable devices that provide information about testing airborne sound insulation in building acoustics. However, fixing a probe to a vibrating surface downshifts sensor resonancesi and underestimates levels. Therefore, the calibration response of the sensor included in a probe differs from the frequency response of that same probe. Simulation techniques of complex mechanical systems may describe this issue, but they include hardly obtainable parameters, ultimately restricting the model. Thus, this study discusses an alternative method, which comprises different parts. Firstly, the vibration frequency responses of 85 probes were measured and labelled according to six features. Then, Linear Regression, Decision Tree Regression and Artificial Neural Networks algorithms were analysed. It was revealed that decision tree regression is the more appropriate technique for this data. The best decision tree models, in terms of scores and model structure, were fine-tuned. Eventually, the final suggested model employs only four out of the six original features. A trade-off solution that involved a simple structure, an interpretable model and accurate predictions was accomplished. It showed a maximum average deviation from test measurements ranging from 0.6 dB in low- frequency to 3 dB in high-frequency while remaining at a low computational load. This research developed an original and reliable prediction tool that provides the vibration frequency response of handheld probes.",Not About Sufficiency
Diagnostic accuracy of artificial intelligence assisted clinical imaging in the detection of oral potentially malignant disorders and oral cancer: a systematic review and meta-analysis,"Background:The objective of this study is to examine the application of artificial intelligence (AI) algorithms in detecting oral potentially malignant disorders (OPMD) and oral cancerous lesions, and to evaluate the accuracy variations among different imaging tools employed in these diagnostic processes.Materials and methods:A systematic search was conducted in four databases: Embase, Web of Science, PubMed, and Scopus. The inclusion criteria included studies using machine learning algorithms to provide diagnostic information on specific oral lesions, prospective or retrospective design, and inclusion of OPMD. Sensitivity and specificity analyses were also required. Forest plots were generated to display overall diagnostic odds ratio (DOR), sensitivity, specificity, negative predictive values, and summary receiver operating characteristic (SROC) curves. Meta-regression analysis was conducted to examine potential differences among different imaging tools.Results:The overall DOR for AI-based screening of OPMD and oral mucosal cancerous lesions from normal mucosa was 68.438 (95% CI= [39.484-118.623], I 2=86%). The area under the SROC curve was 0.938, indicating excellent diagnostic performance. AI-assisted screening showed a sensitivity of 89.9% (95% CI= [0.866-0.925]; I 2=81%), specificity of 89.2% (95% CI= [0.851-0.922], I 2=79%), and a high negative predictive value of 89.5% (95% CI= [0.851-0.927], I 2=96%). Meta-regression analysis revealed no significant difference among the three image tools. After generating a GOSH plot, the DOR was calculated to be 49.30, and the area under the SROC curve was 0.877. Additionally, sensitivity, specificity, and negative predictive value were 90.5% (95% CI [0.873-0.929], I 2=4%), 87.0% (95% CI [0.813-0.912], I 2=49%) and 90.1% (95% CI [0.860-0.931], I 2=57%), respectively. Subgroup analysis showed that clinical photography had the highest diagnostic accuracy.Conclusions:AI-based detection using clinical photography shows a high DOR and is easily accessible in the current era with billions of phone subscribers globally. This indicates that there is significant potential for AI to enhance the diagnostic capabilities of general practitioners to the level of specialists by utilizing clinical photographs, without the need for expensive specialized imaging equipment.",Not About Sufficiency
Robust effect of justice for others: Evidence from word embedding association test,"Maintenance of control and non-randomness is the fundamental need of human beings, and people are motivated to see social and physical environments as stable and orderly, so that the belief about the controlled and nonrandom world is protected. According to just motive theory, people tend to believe in a just world, in which good deeds get rewarded and bad evils get punished, and this belief helps individuals get the sense of control. Otherwise, it is difficult for individuals to pursuit long-term goals and obey social rules in ordinary lives. Recent evidence shows that there are self-others' distinctions regarding belief in a just world (BJW). In particular, BJW for others is found to be processed more primitively and influenced more possibly by cultural norms, whereas BJW for the self is processed more elaborately and influenced more possibly by personal experiences. And BJW for others is more related to the indicators of human values and social attitudes, such as world assumption and justice restoration, whereas BJW for the self is more related to the indicators of personal wellness, such as self-esteem and mental health. More important, justice motive is an implicit process, and in particular justice motive for others (vs. for the self) plays a critical role in shaping human values and social attitudes, which is often hidden in the explicit measurement because of social desirability and sample bias. However, it is difficult for researchers to measure the implicit process of justice motive, especially for others' lives and the whole society. Many studies, especially those conducted among college students and middle class populations, yielded that BJW for others was endorsed lower than BJW for the self. Therefore, more and more researchers recommend that BJW(s) should be measured through implicit tests and large-scale samples. Based on machine learning and Word Embedding Association Test (WEAT), the present research was to test the robust effect of justice for others in terms of the word or sentence vector similarity, with a greater similarity indicating a higher level of semantic association. First, in the word vector model of the Sina Weibo corpus, the word vector similarity of ""others"" and ""justice"" was significantly greater than that of ""self"" and ""justice"", and this pattern was replicated through the word vector model of a comprehensive Weibo corpus. Further, we calculated the sentence vector through the Google BERT model, and found that the sentence vector similarity of ""others"" and ""justice"" was also significantly greater than that of ""self"" and ""justice"". In conclusion, both results via word vector and sentence vector demonstrate the robust effect of others regarding implicit just-world beliefs, in that as compared with one's own experiences, people are more likely to believe that others' world or the whole society is a just place. Furthermore, this study also suggests that internet users' fundamental need for social order is automatically embedded in the cyberspace which is seemingly random and disordered, and the WEAT provides a new method to measure human's social attitude behind the unstructured texts.",Not About Sufficiency
"""Tax Discrimination District"" The Intersection of Race and Transit Value Capture","Problem, research strategy and findings Recent efforts to promote land value capture as a solution to the fiscal crisis of local governments have gained traction, increasing the use of value capture tools across a variety of contexts. This research provides a case study examining how the politics of transit value capture district designation intersect with racialized patterns of disinvestment. Using the case of Kansas City (MO), I illustrate how the attempts of planners and policymakers to expand transit value capture were met with resistance from both low-income and wealthy neighborhoods. I explore how differentiated response to neighborhood concerns by planners combined with stakeholder frameworks about social equity and perceptions of historical domination to produce results exactly counter to the stated purpose of ""not building transit purely for the White people."" The case illustrates the importance of considering racial inequities and historic patterns of marginalization in transit value capture to achieve racial equity and challenge existing patterns of segregation. Takeaway for practice Designating value capture district boundaries is a sociotechnical process that interacts with and potentially reinforces existing patterns of domination and oppression. To ensure that value capture implementation does not reproduce and exacerbate existing inequities, urban planning professionals should centralize issues of race, segregation, and marginality in their efforts to create, capture, and distribute land value. Value capture schemes need to be tailored to the local built environment, sociodemographic history, and the needs of specific communities to be effective at addressing spatial inequities.",Not About Sufficiency
A mini-review on data science approaches in crop yield and disease detection,"Agriculture constitutes a sector with a considerable environmental impact, a concern that is poised to increase with the projected growth in population, thereby amplifying implications for public health. Effectively mitigating and managing this impact demands the implementation of intelligent technologies and data-driven methodologies collectively called precision agriculture. While certain methodologies enjoy widespread acknowledgement, others, despite their lesser prominence, contribute meaningfully. This mini-review report discusses the prevalent AI technologies within precision agriculture over the preceding five years, with a specific emphasis on crop yield prediction and disease detection domains extensively studied within the current literature. The primary objective is to give a comprehensive overview of AI applications in agriculture, spanning machine learning, deep learning, and statistical methods. This approach aims to address a notable gap wherein existing reviews predominantly focus on singular aspects rather than presenting a unified and inclusive perspective.",Not About Sufficiency
"Cities of the imagination: Science fiction, urban space, and community engagement in urban planning","Stories, dreams, histories and myths, Michel de Certeau argues, connect people to particular places and makes place concrete and inhabitable. These narratives generate an imaginary, poetic geography that haunts the abstract city of street maps and development plans, and makes it socially meaningful. This paper is concerned with one particular kind of story-telling - science fiction - and its relationship with the city, urban planning, and questions of community engagement. The paper argues that the 'cities of the imagination' generated by science fiction and other forms of narrative provide a powerful means of understanding, communicating and enriching the connections to place in urban communities. Moreover, science fiction is often characterised by its ability to explore the future of cities. This gives the genre a fascinating and potentially useful resonance with urban planning as a discourse and set of practices; and, in particular, strategies for engaging communities in the design process and, thus, designing for future social sustainability. These ideas will be tested through a reading of near-future urban spatiality in the cyberpunk stories of William Gibson. The theorisation of the relationship between urban space and narrative in the work of de Certeau and other theorists will be used to help frame this discussion. (C) 2011 Elsevier Ltd. All rights reserved.",Not About Sufficiency
Feasibility of assessing cognitive impairment via distributed camera network and privacy-preserving edge computing,"INTRODUCTION: Mild cognitive impairment (MCI) involves cognitive decline beyond normal age and education expectations. It correlates with decreased socialization and increased aimless motion. We aim to automate detection of these behaviors for improved longitudinal monitoring. METHODS: We used a privacy-preserving distributed camera network to collect data from MCI patients in an indoor space. Movement and social interaction features were developed using this data to train machine learning algorithms to differentiate between higher and lower cognitive functioning MCI groups. RESULTSA Wilcoxon rank-sum test showed significant differences between high- and low-functioning cohorts in the movement and social interaction features. Despite the absence of data linking each person's identity to their specific level of cognitive decline, a machine learning model using key features achieved 71% accuracy. DISCUSSION: We show that an edge computing-based privacy-preserving camera network can differentiate between levels of cognitive impairment based on movements and social interactions during group activities. Highlights Movement and social interaction features showed significant differences in high- and low-functioning cohorts. Significant features included linear path lengths, walking speed, direction change and velocity entropies, and number of group formations, among others. Differences were observed despite the presence of healthy individuals and the lack of individual identifiers. Data were collected using a 39-camera privacy-preserving edge computing network covering a 1700-m2 indoor space.",Not About Sufficiency
"The USA's ""Education Equity"" under Its Official Discourse Framework - Based on the Critical Discourse Analysis of President Obama's Speeches and Remarks","Objective: Education equity has always been the mainstream discourse in the USA's public education, and also the crucial idea for promoting its political democracy and social justice. The political elites spare no efforts ever to induce the public to acknowledge and accept its values system by political persuasion and indoctrination under its official discourse framework. As the representative of the political elites, the US President plays an important role. Based on J.P. Gee's analytic theory, this article has made a critical discourse analysis of Obama's speeches and remarks about public education, trying to reveal that it is the neoliberalist ideology that drives the Obama's education reform. Methods: Using J.P. Gee's analytic theory to do quantitative Study of President Obama's speeches and remarks about public education. Results: Getting Rid out of the US Economic Dilemma is the Original Intention of Obama's New Education Reform; Neoliberalism-driven Educational Equity is A Disguise of Racist Rationality. Values: it can provide a means for the using of J.P. Gee's critical discourse analysis. Conclusion: Obama's education reform has adopted the market principles of marketization of education, enterprization of schools, standardization of academy and productization of students, and hence draws a conclusion that the so-called education equity demonstrated in the USA's official discourse framework is in fact to cover its institutional inequity. This article is a try for the application of J.P. Gee's critical discourse analysis on official discourse and is meaningful for the language policy development in multilingual countries.",Not About Sufficiency
"Income-related inequalities in health care utilization in Mongolia, 2007/2008-2012","Background: Although health strategies and policies have addressed equitable distribution of health care in Mongolia, few studies have been conducted on this topic. Rapid socio-economic changes have recently occurred; however, there is no evidence as to how horizontal inequity has changed. The aim of this paper is to evaluate income related-inequalities in health care utilizations and their changes between 2007/2008 and 2012 in Mongolia. Methods: The data used in this study was taken from the nationwide cross-sectional data sets, the Household Socio-Economic Survey, collected in 2007/2008 and 2012 by the National Statistical Office of Mongolia. We employed the Erreygers' concentration index to measure inequality in health service utilization. Horizontal inequity was estimated by a difference between actual and predicted use of health services using the indirect standardization method. Results: The results show that the concentration indices for tertiary level, private outpatient and inpatient services were significantly positive, the contrary for family group practice/soum hospital outpatient services, in both years. After controlling for need, pro-rich inequity (p < 0.01) was observed in the tertiary level, private outpatient, and general inpatient, services in both years. Pro-poor inequity (p < 0.01) existed in family group practice/soum hospital outpatient services in both years. Degrees of inequity in tertiary level hospital and private hospital outpatient services became more pro-rich, whereas in family group practice/soum hospital outpatient services became more pro-poor from 2007/2008 to 2012. Pro-rich inequity in inpatient services remained the same from 2007/2008 to 2012. Conclusions: Equitable distribution of health care has been well documented in health strategies and policies; however, the degree of inequity in delivery of health services has a tendency to increase in Mongolia. Therefore, there is a need to consider implementation issues of the strategies and refocus on policy prioritizations. It is necessary to strengthen primary health care services, particularly by diminishing obstacles for lower income and higher need groups.",Not About Sufficiency
Food allergen analysis: A review of current gaps and the potential to fill them by matrix-assisted laser desorption/ionization,"Food allergy remains a public health, business, and regulatory challenge. Risk analysis (RA) and risk management (RM) of food allergens are of great importance and analysis for food allergens is necessary for both. The current workhorse techniques for allergen analysis (enzyme linked immunosorbent assay [ELISA] and real-time polymerase chain reaction) exhibit recognized challenges including variable and antibody specific responses and detection of species DNA rather than allergen protein, respectively. Liquid chromatography-tandem mass spectrometry (LC-MS/MS) enables protein identification, with potential for multiplex analysis and traceability to the System of International units (SI), aiding global measurement standardization. In this review, recent literature has been systematically reviewed to assess progress in LC-MS/MS and define the potential and benefits of matrix-assisted laser desorption/ionization-time-of-flight MS (MALDI-ToF-MS) technology for allergen analysis. MALDI-ToF-MS of initially intact protein is already applied to verify in silico-derived peptide sequences for LC-MS/MS analysis. We describe the origins of MALDI and its future perspectives, including affinity bead-assisted assays coupled to MALDI. Based on the proliferation of reliable and reproducible MALDI-based clinical applications, the technique should emulate the detection capability (sensitivity) of established allergen detection techniques, whilst reducing technical support and having equivalent multiplexing potential to competing techniques, for example, LC-MS/MS and ELISA. Although unlikely to offer inherent SI traceability, MALDI-based allergen analysis will complement existing MS approaches for allergens. Affinity bead-MALDI appears capable of higher throughput at lower cost per sample than almost any existing technique, enabling repeated sub-sampling as a way to reduce representative sampling issues.",Not About Sufficiency
Children's Green Infrastructure: Children and Their Rights to Nature and the City,"The development of green spaces in cities has corresponded to a need to deal with a series of socio-environmental and health problems felt in urban spaces. However, these are often fragmented or somewhat disconnected interventions that leave out vulnerable and subaltern groups like children, being also commonly based on strictly formatted designs, with more urban furniture than natural elements. In view of the need to make urban spaces healthier, safer, more resilient, and at the same time more child-friendly, in this Conceptual Analysis paper we build from the literature on Urban Green Spaces, Child-Friendly Cities and environments, and Children's Infrastructure to propose the concept of Children Green Infrastructure (CGI), and discuss its application to urban planning, foregrounding the need for fairer, more inclusive and participatory approaches. GGI derives from the Children Infrastructure concept but it puts at the center of the debate the idea of connecting children to nature where they live, learn and play. CGI is based on the assumption that nature should be transversal in urban planning processes, and that it must be perfectly integrated within urban infrastructures, ensuring access to all. Understanding children's needs and integrating their voices in urban planning and design processes are necessary conditions to moving forward to a fairer, more inclusive and truly collective urban project.",Not About Sufficiency
Anchoring of firms in the neighbourhood: Does local social and physical order affect local firms' investment strategies?,"An increasing number of small and medium-sized firms, whose low relocation propensity seems to point to strong local anchors, have moved to residential neighbourhoods. On one hand, the neighbourhood context, an attractive production milieu with rich social network potential, may enable entrepreneurs to increase investments in either human capital (personnel) or physical capital (premises, production goods). These investments further anchor firms in a neighbourhood, where they contribute to a thriving local economy. On the other hand, high crime rates, a deteriorated physical environment and a lack of social capital can discourage entrepreneurs from investing locally. In this study, we conducted a multilevel analysis on the relative contribution of neighbourhood characteristics to firm investment strategies and controlled for firm and entrepreneur characteristics. We found that neighbourhood cohesiveness and liveability-but not local market characteristics-make a small but significant contribution to explaining local firms' investments. This contribution is even stronger for the smallest firms. Therefore, a spatially-targeted policy aimed at improving social cohesion and safety is likely to benefit both residents and small local firms.",Not About Sufficiency
"Long Short-term Memory-Based Prediction of the Spread of Influenza-Like Illness Leveraging Surveillance, Weather, and Twitter Data: Model Development and Validation","Background: The potential to harness the plurality of available data in real time along with advanced data analytics for the accurate prediction of influenza-like illness (ILI) outbreaks has gained significant scientific interest. Different methodologies based on the use of machine learning techniques and traditional and alternative data sources, such as ILI surveillance reports, weather reports, search engine queries, and social media, have been explored with the ultimate goal of being used in the development of electronic surveillance systems that could complement existing monitoring resources. Objective: The scope of this study was to investigate for the first time the combined use of ILI surveillance data, weather data, and Twitter data along with deep learning techniques toward the development of prediction models able to nowcast and forecast weekly ILI cases. By assessing the predictive power of both traditional and alternative data sources on the use case of ILI, this study aimed to provide a novel approach for corroborating evidence and enhancing accuracy and reliability in the surveillance of infectious diseases. Methods: The model's input space consisted of information related to weekly ILI surveillance, web-based social (eg, Twitter) behavior, and weather conditions. For the design and development of the model, relevant data corresponding to the period of 2010 to 2019 and focusing on the Greek population and weather were collected. Long short-term memory (LSTM) neural networks were leveraged to efficiently handle the sequential and nonlinear nature of the multitude of collected data. The 3 data categories were first used separately for training 3 LSTM-based primary models. Subsequently, different transfer learning (TL) approaches were explored with the aim of creating various feature spaces combining the features extracted from the corresponding primary models'LSTM layers for the latter to feed a dense layer. Results: The primary model that learned from weather data yielded better forecast accuracy (root mean square error [RMSE]=0.144; Pearson correlation coefficient [PCC]=0.801) than the model trained with ILI historical data (RMSE=0.159; PCC=0.794). The best performance was achieved by the TL-based model leveraging the combination of the 3 data categories (RMSE=0.128; PCC=0.822). Conclusions: The superiority of the TL-based model, which considers Twitter data, weather data, and ILI surveillance data, reflects the potential of alternative public sources to enhance accurate and reliable prediction of ILI spread. Despite its focus on the use case of Greece, the proposed approach can be generalized to other locations, populations, and social media platforms to support the surveillance of infectious diseases with the ultimate goal of reinforcing preparedness for future epidemics.",Not About Sufficiency
Breathing unequal air: environmental disadvantage and residential sorting of immigrant minorities in England and Germany,"Despite ongoing debates on environmental justice, the link between selective residential migration and the unequal exposure to environmental hazards remains underexplored. Previous research has often relied on spatially aggregated data and focused on single-country analyses, limiting our understanding of broader patterns. We address this gap using longitudinal household-level data from the UK Household Longitudinal Study and the German Socio-Economic Panel linked to air pollution estimates (NO2, PM2.5, and SO2). We find that immigrant minorities are exposed to higher levels of air pollution at their place of residence. The overall disadvantage faced by immigrant minorities in England is three times as large as in Germany. Given that immigrant households start under initially higher levels of air pollution, one would expect convergence with non-immigrant populations over time due to residential moves. However, immigrants face a substantial penalty when moving. If native households started in similar neighborhoods as immigrants-the relevant counterfactual-they would experience higher gains from relocation. Socio-economic factors cannot explain these differences. The pattern holds in both England and Germany, although inequalities in residential mobility are more pronounced in England. In particular, racial and ethnic minorities, such as Bangladeshi, Caribbean, and African migrants in England and Turkish migrants in Germany, experience the largest environmental disadvantages.",Not About Sufficiency
DEM Extraction from ALS Point Clouds in Forest Areas via Graph Convolution Network,"It is difficult to extract a digital elevation model (DEM) from an airborne laser scanning (ALS) point cloud in a forest area because of the irregular and uneven distribution of ground and vegetation points. Machine learning, especially deep learning methods, has shown powerful feature extraction in accomplishing point cloud classification. However, most of the existing deep learning frameworks, such as PointNet, dynamic graph convolutional neural network (DGCNN), and SparseConvNet, cannot consider the particularity of ALS point clouds. For large-scene laser point clouds, the current data preprocessing methods are mostly based on random sampling, which is not suitable for DEM extraction tasks. In this study, we propose a novel data sampling algorithm for the data preparation of patch-based training and classification named T-Sampling. T-Sampling uses the set of the lowest points in a certain area as basic points with other points added to supplement it, which can guarantee the integrity of the terrain in the sampling area. In the learning part, we propose a new convolution model based on terrain named Tin-EdgeConv that fully considers the spatial relationship between ground and non-ground points when constructing a directed graph. We design a new network based on Tin-EdgeConv to extract local features and use PointNet architecture to extract global context information. Finally, we combine this information effectively with a designed attention fusion module. These aspects are important in achieving high classification accuracy. We evaluate the proposed method by using large-scale data from forest areas. Results show that our method is more accurate than existing algorithms.",Not About Sufficiency
Ocean FAIR Data Services,"Well-founded data management systems are of vital importance for ocean observing systems as they ensure that essential data are not only collected but also retained and made accessible for analysis and application by current and future users. Effective data management requires collaboration across activities including observations, metadata and data assembly, quality assurance and control (QA\QC), and data publication that enables local and interoperable discovery and access and secures archiving that guarantees long-term preservation. To achieve this, data should be findable, accessible, interoperable, and reusable (FAIR). Here, we outline how these principles apply to ocean data and illustrate them with a few examples. In recent decades, ocean data managers, in close collaboration with international organizations, have played an active role in the improvement of environmental data standardization, accessibility, and interoperability through different projects, enhancing access to observation data at all stages of the data life cycle and fostering the development of integrated services targeted to research, regulatory, and operational users. As ocean observing systems evolve and an increasing number of autonomous platforms and sensors are deployed, the volume and variety of data increase dramatically. For instance, there are more than 70 data catalogs that contain metadata records for the polar oceans, a situation that makes comprehensive data discovery beyond the capacity of most researchers. To better serve research, operational, and commercial users, more efficient turnaround of quality data in known formats and made available through Web services is necessary. In particular, automation of data workflows will be critical to reduce friction throughout the data value chain. Adhering to the FAIR principles with free, timely, and unrestricted access to ocean observation data is beneficial for the originators, has obvious benefits for users, and is an essential foundation for the development of new services made possible with big data technologies.",Not About Sufficiency
Process and impact evaluation of the Greater Christchurch Urban Development Strategy Health Impact Assessment,"Background: despite health impact assessment (HIA) being increasingly widely used internationally, fundamental questions about its impact on decision-making, implementation and practices remain. In 2005 a collaboration between public health and local government authorities performed an HIA on the Christchurch Urban Development Strategy Options paper in New Zealand. The findings of this were incorporated into the Greater Christchurch Urban Development Strategy; Methods: using multiple qualitative methodologies including key informant interviews, focus groups and questionnaires, this study performs process and impact evaluations of the Christchurch HIA including evaluation of costs and resource use; Results: the evaluation found that the HIA had demonstrable direct impacts on planning and implementation of the final Urban Development Strategy as well as indirect impacts on understandings and ways of working within and between organisations. It also points out future directions and ways of working in this successful collaboration between public health and local government authorities. It summarises the modest resource use and discusses the important role HIA can play in urban planning with intersectoral collaboration and enhanced relationships as both catalysts and outcomes of the HIA process; Conclusion: as one of the few evaluations of HIA that have been published to date, this paper makes a substantial contribution to the literature on the impact, utility and effectiveness of HIA.",Not About Sufficiency
Detection and Evaluation of Machine Learning Bias,"Machine learning models are built using training data, which is collected from human experience and is prone to bias. Humans demonstrate a cognitive bias in their thinking and behavior, which is ultimately reflected in the collected data. From Amazon's hiring system, which was built using ten years of human hiring experience, to a judicial system that was trained using human judging practices, these systems all include some element of bias. The best machine learning models are said to mimic humans' cognitive ability, and thus such models are also inclined towards bias. However, detecting and evaluating bias is a very important step for better explainable models. In this work, we aim to explain bias in learning models in relation to humans' cognitive bias and propose a wrapper technique to detect and evaluate bias in machine learning models using an openly accessible dataset from UCI Machine Learning Repository. In the deployed dataset, the potentially biased attributes (PBAs) are gender and race. This study introduces the concept of alternation functions to swap the values of PBAs, and evaluates the impact on prediction using KL divergence. Results demonstrate females and Asians to be associated with low wages, placing some open research questions for the research community to ponder over.",Not About Sufficiency
Ability of Procalcitonin and C-Reactive Protein for Discriminating between Bacterial and Enteroviral Meningitis in Children Using Decision Tree,"Bacterial meningitis (BM) is a public health burden in developing countries, including Central Asia. This disease is characterized by a high mortality rate and serious neurological complications. Delay with the start of adequate therapy is associated with an increase in mortality for patients with acute bacterial meningitis. Cerebrospinal fluid culture, as a gold standard in bacterial meningitis diagnosis, is time-consuming with modest sensitivity, and this is unsuitable for timely decision-making. It has been shown that bacterial meningitis differentiation from viral meningitis could be done through different parameters such as clinical signs and symptoms, laboratory values, such as PCR, including blood and cerebrospinal fluid (CSF) analysis. In this study, we proposed the method for distinguishing the bacterial form of meningitis from enteroviral one. The method is based on the machine learning process deriving making decision rules. The proposed fast-and-frugal trees (FFTree) decision tree approach showed an ability to determine procalcitonin and C-reactive protein (CRP) with cut-off values for distinguishing between bacterial and enteroviral meningitis (EVM) in children. Such a method demonstrated 100% sensitivity, 96% specificity, and 98% accuracy in the differentiation of all cases of bacterial meningitis in this study. These findings and proposed method may be useful for clinicians to facilitate the decision-making process and optimize the diagnostics of meningitis.",Not About Sufficiency
Comprehensive Analysis of the Biomechanical Research of Pelvic Organ Prolapse: A Scientometric Approach,"Background: Pelvic organ prolapse (POP) has become a significant public health issue, with its prevalence increasing proportionally with age. Despite the considerable number of biomechanical studies reported on POP, there remains a lack of a systematic approach to summarize and synthesize all existing research. Methods: The Web of Science Core Collection (WoSCC) database was used as the data source to select literature published from 2003 to 2023 related to biomechanical research of POP. We employed various visualization software to generate scientific knowledge maps, facilitating data analysis and visual representation. Results: This study included 292 publications, comprising 252 research articles (86.3%) and 40 review articles (13.7%). The United States has emerged as a leading nation in terms of productivity, with the University of Porto making significant contributions. Robust partnerships are maintained by all countries and institutions involved. Moali PA stands out as the most prolific author, while Deprest J exhibits exemplary levels of collaboration. Notably, the journal Int Urogynecol J has the highest publication rate and citation frequency, making a significant contribution and demonstrating considerable academic influence in the field. Keyword and cluster analysis reveal that key research areas include validating finite element (FE) models of pelvic floor structures, studying interactions among pelvic support systems, evaluating the impact of vaginal delivery, assessing the effects of various mesh or 3D-printed materials on POP repair, remodeling vaginal connective tissue in POP patients, and biomechanical performance evaluations of pelvic floor tissues. Future research will likely focus on the development of personalized and regenerative treatment strategies. Moreover, advancements in machine learning, various regenerative medicine approaches, and multimodal large-scale FE modeling offer promising insights for development. Conclusion: This study presents a comprehensive analysis of the knowledge system and research directions of the biomechanics of POP, providing valuable guidance for future research endeavors.",Not About Sufficiency
MRI Image Based Classification Model for Lung Tumor Detection Using Convolutional Neural Networks,"Lung tumor is a dangerous disease with the most noteworthy effects and causing more deaths around the world. Medical diagnosis of lung tumor growth can essentially lessen the death rate, on the grounds that powerful treatment alternatives firmly rely upon the particular phase of disease. Medical diagnosis considers to the use of innovation in science with the end goal of analyzing the interior structure of the organs of the human body. It is an approach to improve the nature of the patient's life through a progressively exact and fast detection, and with restricted symptoms, prompting a powerful generally treatment methodology. The main goal of the proposed work is to design a Lung Tumor Detection Model using Convolution Neural Networks (LTD-CNN) with machine learning technique that spread both miniaturized scale and full scale image surfaces experienced in Magnetic Resonance Imaging (MRI) and advanced microscopy modalities separately. Image pixels can give critical data on the abnormality of tissue and performs classification for accurate tumor detection. The advancement of Computer-Aided Diagnosing (CAD) helps the doctors and radiologists to analyze the lung disease precisely from CT images in its beginning phase. Different methods are accessible for the lung disease recognition, however numerous methodologies give not so much exactness but rather more fake positives. The proposed method is compared with the traditional models and the results exhibit that the proposed model detects the tumor effectively and more accurately.",Not About Sufficiency
Data-driven aeolian dust emission scheme for climate modelling evaluated with EMAC 2.55.2,"Aeolian dust has significant impacts on climate, public health, infrastructure and ecosystems. Assessing dust concentrations and the impacts ischallenging because the emissions depend on many environmental factors and can vary greatly with meteorological conditions. We present a data-drivenaeolian dust scheme that combines machine learning components and physical equations to predict atmospheric dust concentrations and quantify thesources. The numerical scheme was trained to reproduce dust aerosol optical depth retrievals by the Infrared Atmospheric Sounding Interferometer onboard the MetOp-A satellite. The input parameters included meteorological variables from the fifth-generation atmospheric reanalysis of the EuropeanCentre for Medium-Range Weather Forecasts. The trained dust scheme can be applied as an emission submodel to be used in climate and Earth systemmodels, which is reproducibly derived from observational data so that a priori assumptions and manual parameter tuning can be largely avoided. Wecompared the trained emission submodel to a state-of-the-art emission parameterisation, showing that it substantially improves the representation ofaeolian dust in the global atmospheric chemistry-climate model EMAC.",Not About Sufficiency
Application of a socio-environmental vulnerability index for disasters through a Geographic Information System (GIS): a case study in Blumenau (SC),"Objective: To map and classify the socio-environmental vulnerability of regions in the municipality of Blumenau (SC) to socio-environmental disasters (floods and landslides) through the construction of a socio-environmental vulnerability index (SEVI) for the period 2000-2020. Method: The SEVI map was developed from a multicriteria analysis with the aid of a geographic information system (GIS). For the SEVI to operate, it was necessary to prepare the indices of environmental susceptibility (ES) and social vulnerability (SV), which were then crossreferenced in an impact matrix. Relevance: To develop a methodology for assessing the socio-environmental vulnerability to disasters, which considers issues related both to the local natural environment and to the ability of social groups to resist/respond to disasters. Results: The results have indicated an increase in informal settlements in permanent preservation areas, environmental degradation, social inequality and consequently a population exposed to risk. In short, Blumenau presents a pattern of high susceptibility and an exacerbated increase in social vulnerability, which thereby configures a highly vulnerable scenario for socioenvironmental disasters. Contributions: Advances made in evaluation research and in formulating integrated management proposals for risk and resilience in the municipalities of Vale do Itajai/SC/Brazil. In this context, it is essential to understand the scenarios with the greatest impact of socioenvironmental disasters, as a tool for urban planning and disaster risk management in the municipality of Blumenau (SC).",Not About Sufficiency
"The COVID-19 pandemic: Impacts on cities and major lessons for urban planning, design, and management","Since the early days of the COVID-19 crisis the scientific community has constantly been striving to shed light on various issues such as the mechanisms driving the spread of the virus, its environmental and socio-economic impacts, and necessary recovery and adaptation plans and policies. Given the high concentration of population and economic activities in cities, they are often hotspots of COVID-19 infections. Accordingly, many researchers are struggling to explore the dynamics of the pandemic in urban areas to understand impacts of COVID-19 on cities. In this study we seek to provide an overview of COVID-19 research related to cities by reviewing literature published during the first eight months after the first confirmed cases were reported in Wuhan, China. The main aims are to understand impacts of the pandemic on cities and to highlight major lessons that can be learned for post-COVID urban planning and design. Results show that, in terms of thematic focus, early research on the impacts of COVID-19 on cities is mainly related to four major themes, namely, (1) environmental quality, (2) socioeconomic impacts, (3) management and governance, and (4) transportation and urban design. While this indicates a diverse research agenda, the first theme that covers issues related to air quality, meteorological parameters, and water quality is dominant, and the others are still relatively underexplored. Improvements in air and water quality in cities during lockdown periods highlight the significant environmental impacts of anthropogenic activities and provide a wake-up call to adopt environmentally friendly development pathways. The paper also provides other recommendations related to the socio-economic factors, urban management and governance, and transportation and urban design that can be used for post-COVID urban plann ing and design. Overall, existing knowledge shows that the COVID-19 crisis entails an excellent opportunity for planners and policy makers to take transformative actions towards creating cities that are more just, resilient, and sustainable. (C) 2020 Elsevier B.V. All rights reserved.",Not About Sufficiency
Bias in machine learning applications to address non-communicable diseases at a population-level: a scoping review,"BackgroundMachine learning (ML) is increasingly used in population and public health to support epidemiological studies, surveillance, and evaluation. Our objective was to conduct a scoping review to identify studies that use ML in population health, with a focus on its use in non-communicable diseases (NCDs). We also examine potential algorithmic biases in model design, training, and implementation, as well as efforts to mitigate these biases.MethodsWe searched the peer-reviewed, indexed literature using Medline, Embase, Cochrane Central Register of Controlled Trials and Cochrane Database of Systematic Reviews, CINAHL, Scopus, ACM Digital Library, Inspec, Web of Science's Science Citation Index, Social Sciences Citation Index, and the Emerging Sources Citation Index, up to March 2022.ResultsThe search identified 27 310 studies and 65 were included. Study aims were separated into algorithm comparison (n = 13, 20%) or disease modelling for population-health-related outputs (n = 52, 80%). We extracted data on NCD type, data sources, technical approach, possible algorithmic bias, and jurisdiction. Type 2 diabetes was the most studied NCD. The most common use of ML was for risk modeling. Mitigating bias was not extensively addressed, with most methods focused on mitigating sex-related bias.ConclusionThis review examines current applications of ML in NCDs, highlighting potential biases and strategies for mitigation. Future research should focus on communicable diseases and the transferability of ML models in low and middle-income settings. Our findings can guide the development of guidelines for the equitable use of ML to improve population health outcomes.",Not About Sufficiency
Siting of Proposed Areas for Urban Agriculture in the City of Baghdad,"Urban agriculture is the practice of growing, processing and distributing food in a village, town, town or in its vicinity. Urban agriculture can include animal husbandry, aquaculture, agroforestry and horticulture. These activities are also carried out in semi-urban areas as well. Urban agriculture is generally targeted for income-generating activities or food production, although in some societies the main motivation is entertainment and recreation. The definition of urban agriculture as a sector that provides the food needs of a city, from within the city itself, using the resources of the city after recycling. Urban agriculture is a complex system involving a series of concerns, from traditional activities associated with production, processing, marketing, distribution and consumption to many other features and services that are not widely recognized and documented. In the city of Baghdad. The aim of the research is to demonstrate the role and importance of urban agriculture in achieving sustainability in the city with its environmental, economic and social dimensions. Presumably, urban agriculture achieves sustainability, environmental and food security, entertainment and recreation in the city, and is a means of improving the standard of living of people living in cities. As a general methodology for the research it was adopted descriptive analytical method, as GIS program was also used through a satellite image of the city of Baghdad that has identified areas of cultivation, empty areas and the calculation of space in each. This research comes from the framework of traditional and systematic modernity, and closure to the postmodern framework and rejection of some of its aspects. It deals with openness to the world's experiences and its application. It does not adhere to the methodological or traditional criteria. It has a contrary and opposite concept. It calls for disagreement against what is customary in the city through Urban agriculture and the diversity of research in terms of experiences in urban agriculture, urban integration and urban planning. The practical framework of the research was analyzing the reality of Baghdad city, identifying the cultivated and empty areas in the city of Baghdad and calculating its areas through the use of the GIS program through a satellite image of the city of Baghdad, In the city of Baghdad, and the research reached the most important estates: 1. The total area of cultivated and uncultivated land in Baghdad is (159,378) Acres, which is equivalent to only 9% of the total requirement for the city of Baghdad from cultivated land (urban agriculture). 2. Planting vegetables and fruit in home gardens is important for people in Baghdad. Many people love raising domestic animals in homes, which are very important for people whose standard of living is low or insufficient to benefit economically. 3. Most people support the cultivation of public, abandoned and empty fields and the cultivation of roofs of houses and public buildings such as schools and hospitals, and support participation in agriculture by boys in schools, and support participation and teamwork in agriculture.",Not About Sufficiency
Auto-QChem: an automated workflow for the generation and storage of DFT calculations for organic molecules,"This perspective describes Auto-QChem, an automatic, high-throughput and end-to-end DFT calculation workflow that computes chemical descriptors for organic molecules. Tailored toward users without extensive programming experience, Auto-QChem has facilitated more than 38 000 DFT calculations for 17 000 molecules as of January 2022. Starting from string representations of molecules, Auto-QChem automatically (a) generates conformational ensembles, (b) submits and manages DFT calculations on a high-performance computing (HPC) cluster, (c) extracts production-ready features that are suitable for statistical analysis and machine learning model development, and (d) stores resulting calculations in a cloud-hosted and web-accessible database. We describe in detail the design and implementation of Auto-QChem, as well as its current functionalities. We also review three case studies where Auto-QChem was applied to our recent efforts in combining data science approaches in organic chemistry methodology development: (a) the design of a diverse and unbiased aryl bromide substrate scope for a Ni/photoredox catalyzed alkylation reaction, (b) mechanistic studies on the effect of bioxazoline (BiOx) and biimidazoline (BiIm) ligands on enantioselectivity in a Ni/photoredox catalyzed cross-electrophile coupling of epoxides and aryl iodides, (c) the development of a reaction condition optimization framework using Bayesian optimization. In addition, we discuss limitations and future directions of Auto-QChem and similar automated DFT calculation systems.",Not About Sufficiency
"Preservation without Representation: Making CLG Programs Vehicles for Inclusive Leadership, Historic Preservation, and Engagement","This article examines public historic preservation agencies' ability to support social inclusion aims within the context of the Certified Local Government (CLG) program. Though administered by the Texas Historical Commission, Texas' State CLG program is federally-funded and makes available special access to technical assistance, grants, and loans to qualifying communities contingent on compliance. Program surveys the state staff administered to city and county historical commissions with the CLG designation indicate challenges around diversifying their leadership and identifying training opportunities. This article reviews those surveys to detect insights into how the state CLG program can create spaces in which local commissions can increase their ""representativeness"" through changes in assessment and training content. Specifically, I analyze two government assessment tools used to evaluate local CLGs' ability to meet federal and state training and participation expectations. I compare these survey results to self-assessment activities and questionnaires collected during a pilot training on implicit bias, outreach, and cultural resource surveying I conducted with multiple CLGs in Gonzales, Texas. Findings suggest more creatively designed training and capacity building is necessary around inclusion, identifying structural barriers to participation, and foundational knowledge of historic preservation and planning practice, and ethics.",Not About Sufficiency
A novel computational green infrastructure design framework for hydrologic and human benefits,"Increased storm-water runoff and flooding and poor ecosystem health have brought increasing attention to catchment-wide implementation of green infrastructure (e.g., bioswales, rain gardens, permeable pavements, tree box filters, urban wetlands and forests, stream buffers, and green roofs) to replace or supplement conventional storm water management practices and create more sustainable urban water systems. Current green infrastructure (GI) practice aims at mitigating the negative effects of urbanization by restoring pre-development hydrology and ultimately addressing water quality issues at an urban catchment scale. However, the benefits of GI extend well beyond local storm water management, as urban green spaces are also major contributors to human health. Considerable research in the psychological sciences have shown significant human health benefits from appropriately designed green spaces, yet impacts on human wellbeing have not yet been formally considered in GI design frameworks. This work develops a novel computational green infrastructure (GI) design framework that integrates storm water management requirements with criteria for human wellbeing. A supervised machine-learning model is created to identify specific patterns in urban green spaces that promote human wellbeing; the model is linked to RHESSYS hydrological model to evaluate GI designs in terms of both water resource and human health benefits. An application of the framework to tree-based GI design in Dead Run Watershed, Baltimore, MD, shows that image-mining methods are able to capture key elements of human preferences that could improve GI design. The results also show that hydrologic benefits associated with tree-based features are substantial, indicating that increased urban tree coverage and a more integrated GI design approach can significantly increase both human and hydrologic benefits.",Not About Sufficiency
Achieving Reliability in Cloud Computing by a Novel Hybrid Approach,"Cloud computing (CC) benefits and opportunities are among the fastest growing technologies in the computer industry. Cloud computing's challenges include resource allocation, security, quality of service, availability, privacy, data management, performance compatibility, and fault tolerance. Fault tolerance (FT) refers to a system's ability to continue performing its intended task in the presence of defects. Fault-tolerance challenges include heterogeneity and a lack of standards, the need for automation, cloud downtime reliability, consideration for recovery point objects, recovery time objects, and cloud workload. The proposed research includes machine learning (ML) algorithms such as naive Bayes (NB), library support vector machine (LibSVM), multinomial logistic regression (MLR), sequential minimal optimization (SMO), K-nearest neighbor (KNN), and random forest (RF) as well as a fault-tolerance method known as delta-checkpointing to achieve higher accuracy, lesser fault prediction error, and reliability. Furthermore, the secondary data were collected from the homonymous, experimental high-performance computing (HPC) system at the Swiss Federal Institute of Technology (ETH), Zurich, and the primary data were generated using virtual machines (VMs) to select the best machine learning classifier. In this article, the secondary and primary data were divided into two split ratios of 80/20 and 70/30, respectively, and cross-validation (5-fold) was used to identify more accuracy and less prediction of faults in terms of true, false, repair, and failure of virtual machines. Secondary data results show that naive Bayes performed exceptionally well on CPU-Mem mono and multi blocks, and sequential minimal optimization performed very well on HDD mono and multi blocks in terms of accuracy and fault prediction. In the case of greater accuracy and less fault prediction, primary data results revealed that random forest performed very well in terms of accuracy and fault prediction but not with good time complexity. Sequential minimal optimization has good time complexity with minor differences in random forest accuracy and fault prediction. We decided to modify sequential minimal optimization. Finally, the modified sequential minimal optimization (MSMO) algorithm with the fault-tolerance delta-checkpointing (D-CP) method is proposed to improve accuracy, fault prediction error, and reliability in cloud computing.",Not About Sufficiency
Sense of coherence moderates the relationship between life stress and natural killer cell activity in healthy older adults,"A sense of coherence (SOC) has been found to be a strong predictor of health outcomes and life satisfaction in older adults. This study investigated mood and immune effects of anticipated voluntary housing relocation in 30 healthy older adults and 28 age-matched controls and examined whether SOC would buffer effects of relocation on natural killer (NK) cell activity. Movers completed assessments and; had blood drawn 1 month before relocation to congregate living facilities; controls were assessed concurrently. Compared with the control group, movers showed decreased positive mood and NK activity and elevated thought intrusion. Positive mood mediated the relationship of moving with NK activity, whereas SOC moderated this relationship. Low SOC movers had the poorest NK activity; that of high SOC movers was less compromised. These findings are consistent with possible salutogenic contributions of SOC and positive mood to immune function in older adults facing stressful life transitions.",Not About Sufficiency
Improving Post-Relocation Support for People Resettled by Infrastructure Development INTRODUCTION,"Lagging other components, project-induced resettlement rarely, if ever, is completed after those resettled are compensated and replacement infrastructure handed-over. Initiating livelihood restoration programs may jumpstart but fall short of re-articulating dismantled local economies. Successful resettlement requires pre- and post-relocation actions that will help resellers and their hosts re-articulate new routine social and economic arrangements and improve their well-being. This Special Issue examines the distinct challenges of the post-relocation phase of resettlement. During this phase, the resettlement burdens shift from the relocation project to the resettlers, their hosts, and third parties; from individual to collective issues; and from mitigation to development. For decades, China has experienced with a variety of long-term, post-relocation policies, programs and methodologies. The contributors provide a glimpse of an extensive toolkit being crafted for use in this localized context-defined phase. Some are transferable. Others are not. Post-relocation support (PReS) adds value to improving the likelihood of successful outcomes.",Not About Sufficiency
Take the aTrain. Introducing an interface for the Accessible Transcription of Interviews,"Research in behavioral and experimental finance becomes more multifaceted and the analysis of data from speech interactions more important. This raises the need for technical support for researchers using qualitative data generated from speech interactions. aTrain serves this need and is an open-source, offline transcription tool with a graphical interface for audio data in multiple languages. It requires no programming skills, runs on most computers, operates without internet, and ensures data is not uploaded to external servers. aTrain combines OpenAI's Whisper transcription models with speaker recognition and provides output that integrates with MAXQDA and ATLAS.ti. Available on the Microsoft Store for easy installation, its source code is also accessible on GitHub. aTrain, designed for speed on local computers, transcribes audio files at 2-3 times the audio duration on mobile CPUs using the highest-accuracy Whisper transcription models. With an entry-level graphics card, this speed improves to 30% of the audio duration.",Not About Sufficiency
"Analysis of COVID-19 cases and comorbidities using machine learning algorithms: A case study of the Limpopo Province, South Africa","This study examined the biological, social, and clinical risk factors for mortality in coronavirus of the year 2019 (COVID-19) hospitalised patients. The population of the study is prone to COVID-19, thus understanding the most common traits and comorbidities of people who were affected is crucial in reducing its consequences. In this study, four supervised machine learning algorithms were implemented and compared to predict the mortality rate based on the explanatory variables across the five districts of Limpopo Province in South Africa. The data was obtained from Lim-popo Department of Health. Prediction about the chances of dying from COVID-19 disease was made using logistic regression, random forest, support vector machine, and decision tree algo-rithms on the dataset of 20,592 records with twenty-one attributes. Due to the imbalanced nature of the data, Random Over-Sampling Examples (ROSE) were employed to balance our data for more accurate classification effectively. The ROSE package provides functions to deal with binary classification problems in the presence of imbalanced classes. We used 70% of the data for training, while 30% was selected for testing the predictive algorithms. A technique called Step Akaike's Information Criterion (StepAIC) was deployed to reduce the insignificant variables from the full model of the logistic regression. According to the findings of the study, among the four algorithms tested, random forest had the highest recall rate for predicting mortality at roughly 79 percent compared to the other three algorithms. Accordingly, we conclude that random forest algorithm is appropriate for predicting the chances of patients dying from COVID-19 based on the attributes of the five districts of Limpopo Province. In terms of the features and their importance, a function called Variable Importance (VarImp) was used to check which of the attributes have predictive power on the outcome variable (discharged status). The findings revealed that age, ever ventilated, ever oxygenated, intensive ward upon admission, Waterberg district, and private facility type were among the risk factors that could be selected for the logistic regression model to predict mortality in hospitalised patients. This implies that special attention should be given to these identified variables. The random forest model is adequate to establish these factors since the findings reveal that a fairly considerable percentage of explained variation would correctly classify 79% of the cases. The novelty of conducting research on the analysis of COVID-19 cases and comorbidities using machine learning algorithms lies in the potential to uncover new insights and patterns that might not be immediately apparent through traditional statistical methods. Machine learning algorithms can quickly and accurately analyze large datasets and identify complex relationships between variables, which could provide valuable information for public health officials and policymakers.",Not About Sufficiency
"Solving ""Smart City"" Transport Problems by Designing Carpooling Gamification Schemes with Multi-Agent Systems: The Case of the So-Called ""Mordor of Warsaw""","To reduce energy consumption and improve residents' quality of life, ""smart cities"" should use not only modern technologies, but also the social innovations of the ""Internet of Things"" (IoT) era. This article attempts to solve transport problems in a smart city's office district by utilizing gamification that incentivizes the carpooling system. The goal of the devised system is to significantly reduce the number of cars, and, consequently, to alleviate traffic jams, as well as to curb pollution and energy consumption. A representative sample of the statistical population of people working in one of the biggest office hubs in Poland (the so-called ""Mordor of Warsaw"") was surveyed. The collected data were processed using spatial data mining methods, and the results were a set of parameters for the multi-agent system. This approach made it possible to run a series of simulations on a set of 100,000 agents and to select an effective gamification methodology that supports the carpooling process. The implementation of the proposed solutions (a ""serious game"" variation of urban games) would help to reduce the number of cars by several dozen percent, significantly reduce energy consumption, eliminate traffic jams, and increase the activity of the smart city residents.",Not About Sufficiency
Strategic Incorporation of Synthetic Data for Performance Enhancement in Deep Learning A Case Study on Object Tracking Tasks,"Obtaining training data for machine learning models can be challenging. Capturing or gathering the data, followed by its manual labelling, is an expensive and time-consuming process. In cases where there are no publicly accessible datasets, this can significantly hinder progress. In this paper, we analyze the similarity between synthetic and real data. While focusing on an object tracking task, we investigate the quantitative improvement influenced by the concentration of the synthetic data and the variation in the distribution of training samples induced by it. Through examination of three well-known benchmarks, we reveal guidelines that lead to performance gain. We quantify the minimum variation required and demonstrate its efficacy on prominent object-tracking neural network architecture.",Not About Sufficiency
Quantum Computing for High-School Students An Experience Report,"Quantum computing is an emerging field that can revolutionize our ability to solve problems and enable breakthroughs in many areas including optimization, machine learning, chemistry, and drug design. With the increasing computational power of quantum computers and the proliferation of quantum development kits, the demand for a skilled workforce in quantum computing increases significantly. The theory of quantum computing lies at the crossroads of quantum physics, mathematics, and computer science. The field of quantum computing has matured and can now be explored by all students. While today, quantum computers and simulators are readily accessible and programmable over the internet, quantum computing education is just ramping up. This paper describes our experiences in organizing and delivering quantum computing workshops for high-school students with little or no experience in the above-mentioned fields. We introduce students to the world of quantum computing in innovative ways, such as newly designed ""unplugged"" activities for teaching basic quantum computing concepts. Overall, we take a programmatic approach and introduce students to the IBM Q Experience using Qiskit and Jupyter notebooks. Our experiences and findings suggest that basic quantum computing concepts are palatable for high-school students, and-due to significant differences between classical and quantum computing-early exposure to quantum computing is a valuable addition to the set of problem-solving and computing skills that high-schoolers obtain before entering university.",Not About Sufficiency
DCNN: Deep Convolutional Neural Network With XAI for Efficient Detection of Specific Language Impairment in Children,"Assessing children for specific language impairment (SLI) or other communication impairments can be challenging for doctors due to the extensive battery of tests and examinations required. Artificial intelligence and computer-aided diagnostics have aided medical professionals in conducting rapid, reliable assessments of children's neurodevelopmental conditions concerning language comprehension and output. Previous research has shown differences between the vocal characteristics of typically developing (TD) children and those with SLI. This study aims to develop a natural language processing (NLP) system that can identify children's early impairments using specific conditions. Our dataset contains examples of disorders, and this study seeks to (1) demonstrate the effectiveness of several classifiers in this regard and (2) select the most effective model from the classifiers. We utilized various machine learning (ML), deep learning (DL), and transformer models to achieve our objective. Our deep convolutional neural network (DCNN) model yielded excellent results, outperforming the competition with an accuracy of 90.47%, making it the top-performing model overall. To increase the accuracy and credibility of our most likely output, we have incorporated explainable AI approaches like SHAP and LIME. These approaches aid in interpreting and explaining model predictions, considering the significance and sensitivity of the topic. Additionally, we believe that our work can contribute to developing more accessible, effective methods for diagnosing language impairments in young children.",Not About Sufficiency
Optimizing Smart City Strategies: A Data-Driven Analysis Using Random Forest and Regression Analysis,"This study investigates the critical factors influencing smart city program success through a comprehensive data-driven analysis of 140 urban centers. Advanced machine learning techniques, specifically random forest algorithms, in conjunction with regression analysis, were employed to examine the correlations between 45 distinct attributes and respective smart city rankings. The findings reveal that the human development index (HDI) is a key predictor of smart city performance. Furthermore, the regression analysis revealed that elements such as education, healthcare, infrastructure, and digital services significantly enhance achieving higher HDI scores. Similarly, factors like education, sanitation, healthcare, and government transparency are closely associated with successfully implementing sharing platforms. These findings highlight the importance of investing in human capital, developing digital infrastructure, and promoting community engagement to create sustainable and resilient smart cities. Policymakers can utilize these findings to prioritize investments and devise effective strategies to improve their city's ranking.",Not About Sufficiency
Multi-Source Data and Machine Learning-Based Refined Governance for Responding to Public Health Emergencies in Beijing: A Case Study of COVID-19,"The outbreak of COVID-19 in Beijing has been sporadic since the beginning of 2022 and has become increasingly severe since October. In China's policy of insisting on dynamic clearance, fine-grained management has become the focus of current epidemic prevention and control. In this paper, we conduct a refined COVID-19 risk prediction and identification of its influencing factors in Beijing based on neighborhood-scale spatial statistical units. We obtained geographic coordinate data of COVID-19 cases in Beijing and quantified them into risk indices of each statistical unit. Additionally, spatial autocorrelation was used to analyze the epidemic risk clustering characteristics. With the multi-source data, 20 influencing elements were constructed, and their spatial heterogeneity was explored by screening 8 for Multiscale Geographically weighted regression (MGWR) model analysis. Finally, a neural network classification model was used to predict the risk of COVID-19 within the sixth ring of Beijing. The MGWR model and the neural network classification model showed good performance: the R2 of the MGWR model was 0.770, and the accuracy of the neural network classification model was 0.852. The results of this study show that: (1) COVID-19 risk is uneven, with the highest clustering within the Fifth Ring Road of Beijing; (2) The results of the MGWR model show that population structure, population density, road density, residential area density, and living service facility density have significant spatial heterogeneity on COVID-19 risk; and (3) The prediction results show a high COVID-19 risk, with the most severe risk being in the eastern, southeastern and southern regions. It should be noted that the prediction results are highly consistent with the current epidemic situation in Shijingshan District, Beijing, and can provide a strong reference for fine-grained epidemic prevention and control in Beijing.",Not About Sufficiency
"Drone-Based Community Assessment, Planning, and Disaster Risk Management for Sustainable Development","Accessible, low-cost technologies and tools are needed in the developing world to support community planning, disaster risk assessment, and land tenure. Enterprise-scale geographic information system (GIS) software and high-resolution aerial or satellite imagery are tools which are typically not available to or affordable for resource-limited communities. In this paper, we present a concept of aerial data collection, 3D cadastre modeling, and disaster risk assessment using low-cost drones and adapted open-source software. Computer vision/machine learning methods are used to create a classified 3D cadastre that contextualizes and quantifies potential natural disaster risk to existing or planned infrastructure. Building type and integrity are determined from aerial imagery. Potential flood damage risk to a building is evaluated as a function of three mechanisms: undermining (erosion) of the foundation, hydraulic pressure damage, and building collapse due to water load. Use of Soil and Water Assessment Tool (SWAT) provides water runoff estimates that are improved using classified land features (urban ecology, erosion marks) to improve flow direction estimates. A convolutional neural network (CNN) is trained to find these flood-induced erosion marks from high-resolution drone imagery. A flood damage potential metric scaled by property value estimates results in individual and community property risk assessments.",Not About Sufficiency
"Atmosphere, mood, and scientific explanation","In this article, I consider how scientific theories may explain architectural atmosphere. Architects use atmosphere to refer to a holistic, emergent property of a space that partly determines the mood of inhabitants. It is said to be a ""subtle, intangible, ambient quality of a place"" that also significantly shapes the way we interact with a space. It is caused by the way light, texture, materials, layout, geometry, acoustics, smell, and other perceptual properties influence affect. But it goes beyond these individually because of non-linear interactions between them. In sections one and two, I explain what an externalist account of the atmosphere would look like. This is an interpretation that objectifies the atmosphere, treating it as a complex causal property of buildings and spaces, accessible to scientific study through ethnographic research, through quantifying and minutely observing and recording humans and the buildings they are in, and then using machine learning and statistical analyses to identify correlations. The goal is to push the identification of the underlying external attributes as far as possible, ultimately to where a machine might enter a room, move around, and then label its atmosphere. In section three, I explore an internalist or subjectivist account of the atmosphere. This is the position that pushes back on machine identification of atmospheres. A subjectivist interpretation is harder to study scientifically because it involves knowing so much about the inner state and the history of a person. Culture, incoming mood, prior experience and associations, interests, tasks, social interaction, and more may all affect mood. Section four explores the frequently underestimated role-on emotion and space comprehension-played by the tasks that occupants perform while in a space, and the way their surrounding social and technological context intrudes on their encounter. I introduce and defend the view that tasks, social context, and nearby technology situate a person in a different environment than when they are inactive. This complicates the search for atmosphere. Nonetheless, I end on an optimistic note that there may yet be a place for atmosphere in the neuroscience of architecture, but it will be much different than our current thinking.",Not About Sufficiency
Deep learning with image-based autism spectrum disorder analysis: A systematic review,"Autism spectrum disorder (ASD) is a collection of neuro-developmental disorders associated with social, communicational, and behavioral difficulties. Early detection thereof is necessary to mitigate the adverse effects of this disorder by initiating special education in schools and rehabilitation centers. Two methods are available for diagnosing and rehabilitating ASD. The first is the manual method (i.e., an observation -or interview-based approach), in which the disorder is diagnosed through observation or by interviewing a parent or caregiver. This method is time-consuming, subjective, and mostly comprises the examination of behavioral symptoms. The other method involves automatic diagnosis using traditional machine learning (ML)-and modern deep learning (DL)-based approaches that rely on image analysis. Indeed, the amount of research literature concerned with the evaluation of the usefulness of DL-based methods that process images or video data to diagnose ASD to improve patients' lives has increased significantly. This paper presents a systematic review of the DL-based approach involving the analysis of images or videos in autism research. The review covers studies that were published from 2017 to June 2023 and were indexed in PubMed, IEEE Xplore, ACM Digital Library, and Google Scholar. The results are reported according to the preferred reporting items for systematic reviews and meta-analyses (PRISMA) guidelines. A total of 130 studies were included in the analysis. Eligible papers were categorized based on the different features extracted as input for the DL-based approach. Existing well-known public and private datasets that include images or videos for autism research are extensively reviewed and discussed in this systematic review. In addition, different rehabilitation strategies that have been shown to be highly beneficial for ASD individuals are included. Finally, various challenges presented by the automated detection, classification, and rehabilitation of ASD are discussed. The review concludes that the use of DL for the precise and affordable diagnosis of autism is increasing substantially. Our findings are expected to significantly benefit researchers, therapists, psychologists, and relevant stakeholders to advance ASD screening, monitoring, and diagnosis with the aid of a DL-based approach that entails image or video analysis.",Not About Sufficiency
Current pharmacological models of social withdrawal in rats: relevance to schizophrenia,"Social dysfunction in schizophrenia is one of the core negative symptoms, which to date is not adequately addressed by treatment with both typical and atypical antipsychotics. A number of different pharmacological models of social withdrawal are used to mimic social dysfunction in rats, such as amphetamine, N-methyl-D-aspartic acid antagonists, cannabinergic and serotonergic receptor ligands. The purpose of this review is to discuss and compare these models of social withdrawal with a focus on their face, construct and predictive validities. Various techniques and strategies used to observe and analyze rodent social behaviour and other factors that are of relevance to this paradigm have also been examined. After comparing the reports, we are of the opinion that to improve replicability of any given model and its antipsychotic screening potential and the reliability of comparisons made, efforts need to be directed towards cross-laboratory standardization of variables that may confound experimental outcomes and cause discrepancies in results reported. In keeping with an earlier suggestion this may be facilitated through the creation of an online consortium for behavioural neuroscientists to share and compare methodologies, laboratory layouts and perhaps even raw data. Behavioural Pharmacology 21:690-709 (C) 2010 Wolters Kluwer Health vertical bar Lippincott Williams & Wilkins.",Not About Sufficiency
Twitter discourse reveals geographical and temporal variation in concerns about COVID-19 vaccines in the United States,"The speed at which social media is propagating COVID-19 misinformation and its potential reach and impact is growing, yet little work has focused on the potential applications of these data for informing public health communication about COVID-19 vaccines. We used Twitter to access a random sample of over 78 million vaccine-related tweets posted between December 1, 2020 and February 28, 2021 to describe the geographical and temporal variation in COVID-19 vaccine discourse. Urban suburbs posted about equitable distribution in communities, college towns talked about in-clinic vaccinations near universities, evangelical hubs posted about operation warp speed and thanking God, exurbs posted about the 2020 election, Hispanic centers posted about concerns around food and water, and counties in the ACP African American South posted about issues of trust, hesitancy, and history. The graying America ACP community posted about the federal government's failures; rural middle American counties posted about news press conferences. Topics related to allergic and adverse reactions, misinformation around Bill Gates and China, and issues of trust among Black Americans in the healthcare system were more prevalent in December, topics related to questions about mask wearing, reaching herd immunity and natural infection, and concerns about nursing home residents and workers increased in January, and themes around access to black communities, waiting for appointments, keeping family safe by vaccinating and fighting online misinformation campaigns were more prevalent in February. Twitter discourse around COVID-19 vaccines in the United States varied significantly across different communities and changed over time; these insights could inform targeted messaging and mitigation strategies. (c) 2021 Elsevier Ltd. All rights reserved.",Not About Sufficiency
"A multi-country One Health foodborne outbreak simulation exercise: cross-sectoral cooperation, data sharing and communication","IntroductionThe awareness of scientists and policy makers regarding the requirement for an integrated One Health (OH) approach in responding to zoonoses has increased in recent years. However, there remains an overall inertia in relation to the implementation of practical cross-sector collaborations. Foodborne outbreaks of zoonotic diseases continue to affect the European population despite stringent regulations, evidencing the requirement for better 'prevent, detect and response' strategies. Response exercises play an essential role in the improvement of crisis management plans, providing the opportunity to test practical intervention methodologies in a controlled environment. MethodsThe One Health European Joint Programme simulation exercise (OHEJP SimEx) aimed at practicing the OH capacity and interoperability across public health, animal health and food safety sectors in a challenging outbreak scenario. The OHEJP SimEx was delivered through a sequence of scripts covering the different stages of a Salmonella outbreak investigation at a national level, involving both the human food chain and the raw pet feed industry. ResultsA total of 255 participants from 11 European countries (Belgium, Denmark, Estonia, Finland, France, Italy, Norway, Poland, Portugal, Sweden, the Netherlands) took part in national level two-day exercises during 2022. National evaluations identified common recommendations to countries aiming to improve their OH structure to establish formal communication channels between sectors, implement a common data sharing platform, harmonize laboratory procedures, and reinforce inter-laboratory networks within countries. The large proportion of participants (94%) indicated significant interest in pursuing a OH approach and desire to work more closely with other sectors. DiscussionThe OHEJP SimEx outcomes will assist policy makers in implementing a harmonized approach to cross-sector health-related topics, by highlighting the benefits of cooperation, identifying gaps in the current strategies and suggesting actions required to better address foodborne outbreaks. Furthermore, we summarize recommendations for future OH simulation exercises, which are essential to continually test, challenge and improve national OH strategies.",Not About Sufficiency
Marketing plant-based versus animal-sourced foods in online grocery stores: A comparative content analysis of sustainability and other product claims in the United States,"The market share of e-commerce grocery is swiftly rising, but online product listings may present different environmental sustainability and public health information. We analyzed product marketing content available on retailer websites and product images, comparing plant-based (PBFs) and animal-sourced foods (ASFs). Over sixteen thousand marketing and labeling content observations were gathered from seven leading U.S. e-commerce grocery retailers for top-selling ASFs and closely-matched PBFs (N = 134; n = 68 PBFs, n = 66 ASFs), including sustainability, mandatory information, health, sourcing, and hedonic/emotional claims. PBFs averaged 47 total claims, ASFs 28. On-web claims were substantially higher than on-pack (PBFs 34 on-web, 12 on-pack; ASFs 19 and nine, respectively). On-web information for the same products varied considerably across retailers, especially for sustainability and mandatory information. PBFs presented mostly sustainability claims on-pack and health claims on-web. ASFs led emotional claims and made almost no sustainability claims across settings. Across settings, mandatory information compliance was greater among PBFs whereas ASFs notably lacked disclosures for foods with major allergens. Online grocery lacks standardization-businesses should regard differences as opportunities for marketing sustainable foods.",Not About Sufficiency
Design and Usability Assessment of a Multi-Device SOA-Based Telecare Framework for the Elderly,"Telemonitoring is a branch of telehealth that aims at remotely monitoring vital signs, which is important for chronically ill patients and the elderly living alone. The available standalone devices and applications for the self-monitoring of health parameters largely suffer from interoperability problems; meanwhile, telemonitoring medical devices are expensive, self-contained, and are not integrated into user-friendly technological platforms for the end user. This paper presents the technical aspects and usability assessment of the telemonitoring features of the HEREiAM platform, which supports heterogeneous information technology systems. By exploiting a service-oriented architecture, the measured parameters collected by off-the-shelf Bluetooth medical devices are sent as XML documents to a private cloud that implements an interoperable health service infrastructure, which is compliant with the most recent healthcare standards and security protocols. This Android-based system is designed to be accessible both via TV and portable devices, and includes other utilities designed to support the elderly living alone. Four usability assessment sessions with quality-validated questionnaires were performed to accurately understand the ease of use, usefulness, acceptance, and quality of the proposed system. The results reveal that our system achieved very high usability scores even at its first use, and the scores did not significantly change over time during a field trial that lasted for four months, reinforcing the idea of an intuitive design. At the end of such a trial, the user-experience questionnaire achieved excellent scores in all aspects with respect to the benchmark. Good results were also reported by general practitioners who assessed the quality of their remote interfaces for telemonitoring.",Not About Sufficiency
An Individualized Prediction Model for Long-term Lung Function Trajectory and Risk of COPD in the General Population,"BACKGROUND: Prediction of future lung function will enable the identification of individuals at high risk of developing COPD, but the trajectory of lung function decline varies greatly among individuals. This study involved the development and validation of an individualized prediction model of lung function trajectory and risk of airflow limitation in the general population. METHODS: Data were obtained from the Framingham Offspring Cohort, which included 4,167 participants >= 20 years of age and who had >= 2 valid spirometry assessments. The primary outcome was prebronchodilator FEV1; the secondary outcome was the risk of airflow limitation (defined as FEV1/FVC less than the lower limit of normal). Mixed effects regression models were developed for individualized prediction, and a machine learning algorithm was used to determine essential predictors. The model was validated in two large, independent multicenter cohorts (N = 2,075 and 12,913, respectively). RESULTS: With 20 common predictors, the model explained 79% of the variation in FEV1 decline in the derivation cohort. In two validation datasets, the model had low error in predicting FEV1 decline (root mean square error range, 0.18-0.22 L) and high discriminative power in predicting risk of airflow limitation (C-statistic range, 0.86-0.87). This model was implemented in a freely accessible website-based application, which allows prediction based on flexible sets of predictors (littp://respxoreubc.caiipress/FramingharnITV1). CONCLUSIONS: The individualized predictor is an accurate tool to predict long-term lung function trajectories and risk of airflow limitation in the general population. This model enables identifying individuals at higher risk of COPD, who can then be targeted for preventive therapies.",Not About Sufficiency
"Assessing Trade-Offs and Optimal Ranges of Density for Life Expectancy and 12 Causes of Mortality in Metro Vancouver, Canada, 1990-2016","Background: Understanding and managing the impacts of population growth and densification are important steps for sustainable development. This study sought to evaluate the health trade-offs associated with increasing densification and to identify the optimal balance of neighbourhood densification for health. Methods: We linked population density with a 27-year mortality dataset in Metro Vancouver that includes census-tract levels of life expectancy (LE), cause-specific mortalities, and area-level deprivation. We applied two methods: (1) difference-in-differences (DID) models to study the impacts of densification changes from the early 1990s on changes in mortality over a 27-year period; and (2) smoothed cubic splines to identify thresholds of densification at which mortality rates accelerated. Results: At densities above ~9400 persons per km(2), LE began to decrease more rapidly. By cause, densification was linked to decreased mortality for major causes of mortality in the region, such as cardiovascular diseases, neoplasms, and diabetes. Greater inequality with increasing density was observed for causes such as human immunodeficiency virus and acquired immunodeficiency syndrome (HIV/AIDS), sexually transmitted infections, and self-harm and interpersonal violence. Conclusions: Areas with higher population densities generally have lower rates of mortality from the major causes, but these environments are also associated with higher relative inequality from largely preventable causes of death.",Not About Sufficiency
AI and Spatial Planning for Sustainable Socio-Ecosystems,"The conservation and the restoration of biodiversity, in accordance with human well-being, is a necessary condition for the realization of several Sustainable Development Goals. However, there is still an important gap between biodiversity research and the management of natural areas. This research project aims to reduce this gap by proposing spatial planning methods that robustly and accurately integrate socio-ecological issues. Artificial intelligence, and notably Constraint Programming, will play a central role and will make it possible to remove the methodological obstacles that prevent us from properly addressing the complexity and heterogeneity of sustainability issues in the management of ecosystems. The whole will be articulated in three axes: (i) integrate socio-ecological dynamics into spatial planning, (ii) rely on adequate landscape metrics in spatial planning, (iii) scaling up spatial planning methods performances. The main study context of this project is the sustainable management of tropical forests, with a particular focus on New Caledonia and West Africa.",Not About Sufficiency
Growth over resilience: how Canadian municipalities frame the challenge of reducing carbon emissions,"In response to anthropogenic climate change, many governments are adopting policies to reduce carbon emissions. In Canada, federal and provincial governments have implemented carbon pricing. One of the effects of putting a price on carbon is increasing the cost of using private vehicles, which may reduce mobility and increase the risk of social exclusion, especially in contexts where car dependence is high. In this article, we analyse how municipal governments in Canada frame the challenges of climate change and reducing emissions, and examine whether they link these challenges to issues of mobility and social exclusion. Focusing on policies from four of Canada's largest cities - Calgary, Edmonton, Winnipeg and Vancouver - we identify four main frames used in planning documents: ""the Growing City"", ""If You Build It, They Will Come"", ""Better City for All"", and ""the Resilient City"". The Growing City frame is used to support status quo urban development, with climate mitigation options (including sustainable travel modes) optionally included for more concerned residents. This is the dominant frame in Calgary, Edmonton, and Winnipeg. Conversely, Vancouver uses the Resilient City frame to indicate that climate mitigation and adaption strategies are essential, and all citizens must be prepared for change. Vancouver presents changes to mobility as necessary for all, rather than an option for some. Social exclusion is not explicitly addressed in the frames, though it is presented as a reason to support building alternative transportation or more public spaces. Social exclusion receives little consideration as a potential consequence of climate mitigation policies.",Not About Sufficiency
Development of prognostic model for preterm birth using machine learning in a population-based cohort of Western Australia births between 1980 and 2015,"Preterm birth is a global public health problem with a significant burden on the individuals affected. The study aimed to extend current research on preterm birth prognostic model development by developing and internally validating models using machine learning classification algorithms and population-based routinely collected data in Western Australia. The longitudinal retrospective cohort study involved all births in Western Australia between 1980 and 2015, and the analytic sample contains 81,974 (8.6%) preterm births (< 37 weeks of gestation). Prediction models for preterm birth were developed using regularised logistic regression, decision trees, Random Forests, extreme gradient boosting, and multi-layer perceptron (MLP). Predictors included maternal socio-demographics and medical conditions, current and past pregnancy complications, and family history. Class weight was applied to handle imbalanced outcomes and stratified tenfold cross-validation was used to reduce overfitting. Close to half of the preterm births (49.1% at 5% FPR, 95% CI 48.9%,49.5%) were correctly classified by the best performing classifier (MLP) for all women when current pregnancy information was available. The sensitivity was boosted to 52.7% (95% CI 52.1%,53.3%) after including past obstetric history in a sub-population of births from multiparous women. Around half of the preterm birth can be identified antenatally at high specificity using population-based routinely collected maternal and pregnancy data. The performance of the prediction models depends on the available predictor pool that is individual and time specific.",Not About Sufficiency
Advancing civil engineering: The transformative impact of neuromorphic computing on infrastructure resilience and sustainability,"The resilience and adaptability of civil infrastructure are paramount for societal well-being and economic stability. This paper examines the transformative impact of neuromorphic computing on enhancing the resilience of infrastructure systems and advancing sustainable engineering practices. Drawing inspiration from the human brain's processing capabilities, neuromorphic computing has been demonstrated to significantly enhance the efficiency of real-time data analysis, learning, and prediction of structural integrity. We detail the fundamental principles of neuromorphic computing, its practical applications within civil engineering, and notable implementations, underscoring its substantial contributions to predictive maintenance, disaster preparedness, and environmental conservation. Our findings reveal that neuromorphic systems, through their capability to continuously learn and adapt, have effectively improved the accuracy of predictive models for infrastructure behavior under various environmental stressors, thereby facilitating more robust disaster response strategies and reducing maintenance costs by up to 30 %. Despite facing technological and integration challenges, neuromorphic computing emerges as a pioneering force in the field of civil engineering, offering a future where infrastructure is not only more durable but also inherently intelligent and responsive to changing conditions. The integration of neuromorphic computing holds the promise of revolutionizing civil engineering practices by fostering greater interdisciplinary collaboration and driving a commitment to environmental sustainability. This study advocates for broader adoption of neuromorphic technologies to cultivate a resilient, adaptive, and sustainable built environment.",Not About Sufficiency
"Guidance to 2018 good practice: ARIA digitally-enabled, integrated, person-centred care for rhinitis and asthma","AimsMobile Airways Sentinel NetworK (MASK) belongs to the Fondation Partenariale MACVIA-LR of Montpellier, France and aims to provide an active and healthy life to rhinitis sufferers and to those with asthma multimorbidity across the life cycle, whatever their gender or socio-economic status, in order to reduce health and social inequities incurred by the disease and to improve the digital transformation of health and care. The ultimate goal is to change the management strategy in chronic diseases.MethodsMASK implements ICT technologies for individualized and predictive medicine to develop novel care pathways by a multi-disciplinary group centred around the patients.StakeholdersInclude patients, health care professionals (pharmacists and physicians), authorities, patient's associations, private and public sectors.ResultsMASK is deployed in 23 countries and 17 languages. 26,000 users have registered.EU grants (2018)MASK is participating in EU projects (POLLAR: impact of air POLLution in Asthma and Rhinitis, EIT Health, DigitalHealthEurope, Euriphi and Vigour).Lessons learnt(i) Adherence to treatment is the major problem of allergic disease, (ii) Self-management strategies should be considerably expanded (behavioural), (iii) Change management is essential in allergic diseases, (iv) Education strategies should be reconsidered using a patient-centred approach and (v) Lessons learnt for allergic diseases can be expanded to chronic diseases.",Not About Sufficiency
Soft Sensor Transferability: A Survey,"Soft Sensors (SSs) are inferential dynamical models employed in industries to perform prediction of process hard-to-measure variables based on their relation with easily accessible ones. They allow implementation of real-time control and monitoring of the plants and present other advantages in terms of costs and efforts. Given the complexity of industrial processes, these models are generally designed with data-driven black-box machine learning (ML) techniques. ML methods work well only if the data on which the prediction is performed share the same distribution with the one on which the model was trained. This is not always possible, since plants can often show new working conditions. Even similar plants show different data distributions, making SSs not scalable between them. Models should then be created from scratch with highly time-consuming procedures. Transfer Learning (TL) is a field of ML that re-uses the knowledge from one task to learn a new different, but related, one. TL techniques are mainly used for classification tasks. Only recently TL techniques have been adopted in the SS field. The proposed survey reports the state of the art of TL techniques for nonlinear dynamical SSs design. Methods and applications are discussed and the new directions of this research field are depicted.",Not About Sufficiency
Prediction of Thermostability of Enzymes Based on the Amino Acid Index (AAindex) Database and Machine Learning,"The combination of wet-lab experimental data on multi-site combinatorial mutations and machine learning is an innovative method in protein engineering. In this study, we used an innovative sequence-activity relationship (innov'SAR) methodology based on novel descriptors and digital signal processing (DSP) to construct a predictive model. In this paper, 21 experimental (R)-selective amine transaminases from Aspergillus terreus (AT-ATA) were used as an input to predict higher thermostability mutants than those predicted using the existing data. We successfully improved the coefficient of determination (R2) of the model from 0.66 to 0.92. In addition, root-mean-squared deviation (RMSD), root-mean-squared fluctuation (RMSF), solvent accessible surface area (SASA), hydrogen bonds, and the radius of gyration were estimated based on molecular dynamics simulations, and the differences between the predicted mutants and the wild-type (WT) were analyzed. The successful application of the innov'SAR algorithm in improving the thermostability of AT-ATA may help in directed evolutionary screening and open up new avenues for protein engineering.",Not About Sufficiency
How to extend the capabilities of space systems for long duration space exploration systems,"For sustainable Exploration Missions the need exists to assemble systems-of-systems in space, on the Moon or on other planetary surfaces. To fulfill this need new and innovative system architectures must be developed to be modularized and launched with the present lift capability of existing rocket technology. To enable long duration missions with minimal redundancy and mass, system software and hardware must be reconfigurable. This will enable increased functionality and multiple use of launched assets while providing the capability to quickly overcome components failures. Additional required capability includes the ability to dynamically demate and reassemble individual system elements during a mission in order to recover from failed hardware or to adapt to changes in mission requirements. To meet the Space Exploration goals of Interoperability and Reconfigurability, many challenges must be addressed to transform the traditional static avionics architectures into architectures with dynamic capabilities. The objective of this paper is to introduce concepts associated with reconfigurable computer systems; to review the various needs and challenges associated with reconfigurable avionics space systems; to provide an operational example that illustrates the application to both the Crew Exploration Vehicle and a collection of ""Habot-like"" mobile surface elements; to summarize the approaches that address key challenges to the acceptance of a Flexible, Intelligent, Modular, Affordable and Reconfigurable avionics space system.",Not About Sufficiency
kMoL: an open-source machine and federated learning library for drug discovery,"Machine learning is quickly becoming integral to drug discovery pipelines, particularly quantitative structure-activity relationship (QSAR) and absorption, distribution, metabolism, and excretion (ADME) tasks. Graph Convolutional Network (GCN) models have proven especially promising due to their inherent ability to model molecular structures using graph-based representations. However, maximizing the potential of such models in practice is challenging, as companies prioritize data privacy and security over collaboration initiatives to improve model performance and robustness. kMoL is an open-source machine learning library with integrated federated learning capabilities developed to address such challenges. Its key features include state-of-the-art model architectures, Bayesian optimization, explainability, and federated learning mechanisms. It demonstrates extensive customization possibilities, advanced security features, straightforward implementation of user-specific models, and high adaptability to custom datasets without additional programming requirements. kMoL is evaluated through locally trained benchmark settings and distributed federated learning experiments using various datasets to assess the features and flexibility of the library, as well as the ability to facilitate fast and practical experimentation. Additionally, results of these experiments provide further insights into the performance trade-offs associated with federated learning strategies, presenting valuable guidance for deploying machine learning models in a privacy-preserving manner within drug discovery pipelines. kMoL is available on GitHub at https://github.com/elix-tech/kmol.Scientific contribution The primary scientific contribution of this research project is the introduction and evaluation of kMoL, an open-source machine learning library with integrated federated learning capabilities. By demonstrating advanced customization and security capabilities without additional programming requirements, kMoL represents an accessible yet secure open-source platform for collaborative drug discovery projects. Additionally, the experiment results provide further insights into the performance trade-offs associated with federated learning strategies, presenting valuable guidance for deploying machine learning models in a privacy-preserving manner within drug discovery pipelines.",Not About Sufficiency
"The grey-green divide: multi-temporal analysis of greenness across 10,000 urban centres derived from the Global Human Settlement Layer (GHSL)","The presence of green spaces within city centres has been recognized as a valuable component of the city landscape. Vegetation provides a variety of benefits including energy saving, improved air quality, reduced noise pollution, decreased ambient temperature and psychological restoration. Evidence also shows that the amount of vegetation, known as 'greenness', in densely populated areas, can also be an indicator of the relative wealth of a neighbourhood. The 'grey-green divide', the contrast between built-up areas with a dominant grey colour and green spaces, is taken as a proxy indicator of sustainable management of cities and planning of urban growth. Consistent and continuous assessment of greenness in cities is therefore essential for monitoring progress towards the United Nations Sustainable Development Goal 11. The availability of multi-temporal greenness information from Landsat data archives together with data derived from the city centres database of the Global Human Settlement Layer (GHSL) initiative, offers a unique perspective to quantify and analyse changes in greenness across 10,323 urban centres all around the globe. In this research, we assess differences between greenness within and outside the built-up area for all the urban centres described by the city centres database of the GHSL. We also analyse changes in the amount of green space over time considering changes in the built-up areas in the periods 1990, 2000 and 2014. The results show an overall trend of increased greenness between 1990 and 2014 in most cities. The effect of greening is observed also for most of the 32 world megacities. We conclude that using simple yet effective approaches exploiting open and free global data it is possible to provide quantitative information on the greenness of cities and its changes over time. This information is of direct interest for urban planners and decision-makers to mitigate urban related environmental and social impacts.",Not About Sufficiency
DCC Terminology Service-An Automated CI/CD Pipeline for Converting Clinical and Biomedical Terminologies in Graph Format for the Swiss Personalized Health Network,"One goal of the Swiss Personalized Health Network (SPHN) is to provide an infrastructure for FAIR (Findable, Accessible, Interoperable and Reusable) health-related data for research purposes. Semantic web technology and biomedical terminologies are key to achieving semantic interoperability. To enable the integrative use of different terminologies, a terminology service is a important component of the SPHN Infrastructure for FAIR data. It provides both the current and historical versions of the terminologies in an SPHN-compliant graph format. To minimize the usually high maintenance effort of a terminology service, we developed an automated CI/CD pipeline for converting clinical and biomedical terminologies in an SPHN-compatible way. Hospitals, research infrastructure providers, as well as any other data providers, can download a terminology bundle (currently composed of SNOMED CT, LOINC, UCUM, ATC, ICD-10-GM, and CHOP) and deploy it in their local terminology service. The distributed service architecture allows each party to fulfill their local IT and security requirements, while still having an up-to-date interoperable stack of SPHN-compliant terminologies. In the future, more terminologies and mappings will be added to the terminology service according to the needs of the SPHN community.",Not About Sufficiency
Research on Ontology Integration Combined with Machine learning,"Recently ontologies are playing very important part in many areas, such as intelligent information retrieve, knowledge management and organization, electronic commerce and so on, however, several drawbacks must be overcome before ontologies become useful and practical tools. As the number of ontologies are made publicly available and accessible on the web increases steadily, a single ontology is no longer enough to support the tasks envisaged by a distributed environment like the semantic web. Multiple ontologies need to be accessed for several applications. A critical issue is ontology integration, which can largely improve the efficiency to enrich such a domain ontology with less time and lower cost for obtaining related knowledge. This paper has deeply studied the principles of ontology integration, then proposes a procedure model for ontology construction and a new framework for ontology integration based on machine learning through analyzing the characteristics and problems in the process of ontology integration.",Not About Sufficiency
Hybrid Classification for Tweets Related to Infection with Influenza,"Traditional public health surveillance methods such as those employed by the CDC (United States Centers for Disease Control and Prevention) rely on regular clinical reports, which are almost always manual and labor intensive. Twitter, a popular micro-blogging service, provides the possibility of automated public health surveillance. Tweets, however, are less than 140 characters, and do not provide sufficient word occurrences for conventional classification methods to work reliably. Moreover, natural language is complex. This makes health-related classification more challenging. In this study, we use flu-related classification as a demonstration to propose a hybrid classification method, which combines two classification approaches: manually-defined features and auto-generated features by machine learning approaches. Preprocessing based on Natural Language Processing (NLP) is used to help extract useful information, and to eliminate noise features. Our simulations show an improved accuracy.",Not About Sufficiency
Blood smear imagery dataset for malaria parasite detection: A case of Tanzania,"Malaria is a major public health issue in many regions of Africa, including Tanzania. The Tanzania Malaria National Strategic Plan (2021-2025) emphasizes on high-quality testing services availability, high coverage of timely diagnosis of malaria, and availability of innovative diagnostic systems for effective detection, treatment and control of malaria. This would be achieved by employing state of the art technologies like Machine learning. However, Machine learning requires diverse dataset to work effectively and efficiently. Therefore, this paper presents blood smear imagery dataset that can be used by researchers to develop computer vision systems for malaria parasite detection. The imagery dataset were acquired by setting up a 40X-2500X Real 4 K compound microscope with a 4k SONY IMX334 sensor camera mounted to it in five health centres of Tanga region. Blood samples taken according to normal routine of diagnosing patients in health care, were stained using Giemsa reagent and examined under microscope. Following these procedures, the study collected and annotated Thick infected blood smear images ( n = 1139) ; Thick uninfected blood smear images ( n = 1071 ); Thin uninfected blood smear images ( n = 270 ); and Thin infected blood smear images ( n = 1064 ). Furthermore, the curated dataset have been uploaded in a public Harvard data verse repository. In summary, the dataset aims to support the creation of diagnostic tools that improve malaria detection, thereby advancing health outcomes and aiding malaria con- trol initiatives in Tanzania and other regions impacted by the disease. (c) 2024 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/)",Not About Sufficiency
"Recent Developments in Heavy Metals Detection: Modified Electrodes, Pretreatment Methods, Prediction Models and Algorithms","Heavy metal pollution has become an increasingly serious environmental issue, making the detection of heavy metals essential for safeguarding public health and the environment. This review aims to highlight the commonly used methods for detecting heavy metals (such as atomic absorption spectroscopy (AAS), atomic emission spectroscopy (AES), inductively coupled plasma-mass spectrometry (ICP-MS), square-wave anodic stripping voltammetry (SWASV), etc.), with a particular focus on electrochemical detection and electrode modification materials. Metal nanomaterials (such as titanium dioxide (TiO2), copper oxide (CuO), ZIF-8, MXene, etc.) are emphasized as promising candidates for enhancing the performance of sensors due to their high surface area and excellent catalytic properties. However, challenges such as interference from non-target heavy metal ions and the formation of organometallic complexes with organic compounds can complicate the detection process. To address these issues, two potential solutions have been proposed: the development of advanced algorithms (such as machine learning (ML), back-propagation neural network (BPNN), support vector machines (SVM), random forests (RF), etc.) for signal processing and the use of pretreatment methods (such as Fenton oxidation (FO), ozone oxidation, and photochemical oxidation) to suppress such interferences. This paper aims to review commonly used methods for detecting heavy metals, with a particular emphasis on electrochemical techniques. It will also highlight the challenges faced in these methods, such as interference and sensitivity limitations, and propose innovative solutions, including the use of metal nanomaterials for improved sensor performance and the integration of advanced algorithms and pretreatment techniques to address interference and enhance detection accuracy.",Not About Sufficiency
A services management system for small and disadvantaged communities,"Several kinds of citizens in Italy face significant obstacles during their interactions with public administrations and governments, having to cope with bureaucracy, ambiguous procedures, functional disintegration, vague and/or overlapping authority structures and information fragmentation. We present in this paper a modular system for e-Government services distribution that can be easily updated, extended and integrated with the existing information systems of public administrations (PAs). Our Services Management System is a framework that automatically generates the infrastructure for the classification and the distribution of selected services on the basis of the preferences expressed by the administrator. This architecture as the potential to achieve operational integration into one-stop e-government allowing organizations that deliver e-Government services to realize specific web portals for the distribution of personalized contents. Moreover, services description are taken from a single repository managed by a central administration. With this system we try to satisfy all the requirements needed to build e-Government system around citizens and to reach the population as much as possible to overcome digital divide in disadvantage territories and communities.",Not About Sufficiency
FAIR health data in the national and international data space,"Health data are extremely important in today's data-driven world. Through automation, healthcare processes can be optimized, and clinical decisions can be supported. For any reuse of data, the quality, validity, and trustworthiness of data are essential, and it is the only way to guarantee that data can be reused sensibly. Specific requirements for the description and coding of reusable data are defined in the FAIR guiding principles for data stewardship. Various national research associations and infrastructure projects in the German healthcare sector have already clearly positioned themselves on the FAIR principles: both the infrastructures of the Medical Informatics Initiative and the University Medicine Network operate explicitly on the basis of the FAIR principles, as do the National Research Data Infrastructure for Personal Health Data and the German Center for Diabetes Research. To ensure that a resource complies with the FAIR principles, the degree of FAIRness should first be determined (so-called FAIR assessment), followed by the prioritization for improvement steps (so-called FAIRification). Since 2016, a set of tools and guidelines have been developed for both steps, based on the different, domain-specific interpretations of the FAIR principles. Neighboring European countries have also invested in the development of a national framework for semantic interoperability in the context of the FAIR (Findable, Accessible, Interoperable, Reusable) principles. Concepts for comprehensive data enrichment were developed to simplify data analysis, for example, in the European Health Data Space or via the Observational Health Data Sciences and Informatics network. With the support of the European Open Science Cloud, among others, structured FAIRification measures have already been taken for German health datasets.",Not About Sufficiency
Data-driven framework with graphical user interface for predicting flexural behavior of FRCM strengthened RC beams,"In this study, the flexural capacity of reinforced concrete (RC) beams strengthened with fiber-reinforced cementitious matrices (FRCM) was computed using machine learning (ML) algorithms including: (i) adaptive neuro-fuzzy inference system (ANFIS), (ii) artificial neural network (ANN), and (iii) extreme gradient boosting (XGBoost). A total of 198 pertinent experimental datasets were compiled and included six types of FRCM composites (PBO, carbon, glass, basalt, coated carbon, and combined glass and carbon). The considered input parameters comprise the beam cross-sectional details, area of tensile and compressive steel reinforcement, mechanical properties of FRCM composite, and concrete compressive strength. To assess the reliability of ML models, four existing analytical models and one established standard guideline were used for comparison. Moreover, six statistical metrics were employed, along with an overfitting analysis, to determine the best-fitting model. Graphical fitting of the optimal model was depicted using the Taylor diagram, violin plot, as well as multi-panel histogram plot. Based on both graphical and statistical metrics, the XGBoost model attained the highest precision compared to all analytical and ML-based models. The correlation coefficient and MAPE of the XGBoost model were 0.9977% and 2.98%, respectively. To interpret the influence of individual parameters on the flexural strength of the FRCM-strengthened RC beams, a feature importance plot based on SHAP explanatory theory was deployed. Ultimately, a user-friendly graphical interface was developed and made accessible to aid practicing engineers in estimating the flexural strength of FRCM-strengthened RC beams, offering an effective alternative to complex design procedures.",Not About Sufficiency
Research of Traffic Velocity in Urban Transport Network using Programs VISUM and VISSIM,"The increasing motorization level draws attention of urban planners and transport experts to the street network performance problem. It is necessary to make the street network to ensure reliable service. The paper deals with evaluation of of traffic flows and their characteristics. It is analysed how congestions formation impacted on the velocity of traffic, which leads to the infrastructure system (streets throughput) incompatible with the transport needs and the inappropriate traffic control system. Results of traffic velocity in Kaunas urban traffic transport network are presented. It is observed that traffic velocity can be increased by control traffic lights according to actual traffic flows.",Not About Sufficiency
Can ChatGPT Replace an Otolaryngologist in Guiding Parents on Tonsillectomy?,"Background: ChatGPT is an artificial intelligence tool, which utilizes machine learning to analyze and generate human-like text. The user-friendly accessibility of this tool enables patients conveniently access medical information without intricate terminology challenges. The objective of this study was to assess the accuracy of ChatGPT in providing insights into indications and management of complications after tonsillectomy, a common pediatric otolaryngology procedure. Methods: The responses generated by ChatGPT were compared to the ""Clinical practice guidelines: tonsillectomy in children-executive summary"" developed by the American Academy of Otolaryngology-Head and Neck Surgery Foundation (AAO-HNSF). An assessment was carried out by presenting predetermined questions regarding indications and complications post tonsillectomy to ChatGPT, followed by a comparison of its responses with the established guideline by 2 otolaryngology experts. The responses of both parties were reviewed by the senior author. Results: A total of 16 responses generated by ChatGPT were assessed. After a comprehensive review, it was concluded that 15 out of 16 (93.8%) responses demonstrated a high degree of reliability and accuracy, closely adhering to the standard established by the AAO-HNSF guideline. Conclusion: The results validate the potential of using ChatGPT to enhance healthcare delivery making guidelines more accessible to patients while also emphasizing the importance of ensuring the provision of accurate and reliable medical advice to patients.",Not About Sufficiency
Urban heat island mitigation by green infrastructure in European Functional Urban Areas,"The Urban Heat Island (UHI) effect is one of the most harmful environmental hazards for urban dwellers. Climate change is expected to increase the intensity of the UHI effect. In this context, the implementation of Urban Green Infrastructure (UGI) can partially reduce UHI intensity, promoting a resilient urban environment and contributing to climate change adaptation and mitigation. In order to achieve this result, there is a need to systematically integrate UGI into urban planning and legislation, but this process is subject to the availability of widely applicable, easily accessible and quantitative evidence. To offer a big picture of urban heat intensity and opportunities to mitigate high temperatures, we developed a model that reports the Ecosystem Service (ES) of microclimate regulation of UGI in 601 European cities. The model simulates the temperature difference between a baseline and a no-vegetation scenario, extrapolating the role of UGI in mitigating UHI in different urban contexts. Finally, a practical, quantitative indicator that can be applied by policymakers and city administrations has been elaborated, allowing to estimate the amount of urban vegetation that is needed to cool summer temperatures by a certain degree. UGI is found to cool European cities by 1.07 degrees C on average, and up to 2.9 degrees C, but in order to achieve a 1 degrees C drop in urban temperatures, a tree cover of at least 16% is required. The microclimate regulation ES is mostly dependent on the amount of vegetation inside a city and by transpiration and canopy evaporation. Furthermore, in almost 40% of the countries, more than half of the residing population does not benefit from the microclimate regulation service provided by urban vegetation. Widespread implementation of UGI, in particular in arid regions and cities with insufficient tree cover, is key to ensure healthy urban living conditions for citizens.",Not About Sufficiency
"How good is your synthetic data? SynthRO, a dashboard to evaluate and benchmark synthetic tabular data","BackgroundThe exponential growth in patient data collection by healthcare providers, governments, and private industries is yielding large and diverse datasets that offer new insights into critical medical questions. Leveraging extensive computational resources, Machine Learning and Artificial Intelligence are increasingly utilized to address health-related issues, such as predicting outcomes from Electronic Health Records and detecting patterns in multi-omics data. Despite the proliferation of medical devices based on Artificial Intelligence, data accessibility for research is limited due to privacy concerns. Efforts to de-identify data have met challenges in maintaining effectiveness, particularly with large datasets. As an alternative, synthetic data, that replicate main statistical properties of real patient data, are proposed. However, the lack of standardized evaluation metrics complicates the selection of appropriate synthetic data generation methods. Effective evaluation of synthetic data must consider resemblance, utility and privacy, tailored to specific applications. Despite available metrics, benchmarking efforts remain limited, necessitating further research in this area.ResultsWe present SynthRO (Synthetic data Rank and Order), a user-friendly tool for benchmarking health synthetic tabular data across various contexts. SynthRO offers accessible quality evaluation metrics and automated benchmarking, helping users determine the most suitable synthetic data models for specific use cases by prioritizing metrics and providing consistent quantitative scores. Our dashboard is divided into three main sections: (1) Loading Data section, where users can locally upload real and synthetic datasets; (2) Evaluation section, in which several quality assessments are performed by computing different metrics and measures; (3) Benchmarking section, where users can globally compare synthetic datasets based on quality evaluation.ConclusionsSynthetic data mitigate concerns about privacy and data accessibility, yet lacks standardized evaluation metrics. SynthRO provides an accessible dashboard helping users select suitable synthetic data models, and it also supports various use cases in healthcare, enhancing prognostic scores and enabling federated learning. SynthRO's accessible GUI and modular structure facilitate effective data evaluation, promoting reliability and fairness. Future developments will include temporal data evaluation, further broadening its applicability.",Not About Sufficiency
Cyto-Safe: A Machine Learning Tool for Early Identification of Cytotoxic Compounds in Drug Discovery,"Cytotoxicity is essential in drug discovery, enabling early evaluation of toxic compounds during screenings to minimize toxicological risks. In vitro assays support high-throughput screening, allowing for efficient detection of toxic substances while considerably reducing the need for animal testing. Additionally, AI-based Quantitative Structure-Activity Relationship (AI-QSAR) models enhance early stage predictions by assessing the cytotoxic potential of molecular structures, which helps prioritize low-risk compounds for further validation. We present a freely accessible web application designed for identifying potential cytotoxic compounds utilizing QSAR models. This application utilizes machine learning techniques and is built on a data set of approximately 90,000 compounds, evaluated against two cell lines, 3T3 and HEK 293. Users can interact with the app by inputting a SMILES representation, uploading CSV or SDF files, or sketching molecules. The output includes a binary prediction for each cell line, a confidence percentage, and an explainable AI (XAI) analysis. Cyto-Safe web-app version 1.0 is available at http://insightai.labmol.com.br/.",Not About Sufficiency
Deep Learning for HABs Prediction with Multimodal Fusion,"Harmful Algal Blooms (HABs) present significant environmental and public health threats. Recent machine learning-based HABs monitoring methods often rely solely on unimodal data, e.g., satellite imagery, overlooking crucial environmental factors such as temperature. Moreover, existing multi-modal approaches grapple with real-time applicability and generalizability challenges due to the use of ensemble methodologies and hard-coded geolocation clusters. Addressing these gaps, this paper presents a novel deep learning model using a single-model-based multi-task framework. This framework is designed to segment water bodies and predict HABs severity levels concurrently, enabling the model to focus on areas of interest, thereby enhancing prediction accuracy. Our model integrates multimodal inputs, i.e., satellite imagery, elevation data, temperature readings, and geolocation details, via a dual-branch architecture: the Satellite-Elevation (SE) branch and the TemperatureGeolocation (TG) branch. Satellite and elevation data in the SE branch, being spatially coherent, assist in water area detection and feature extraction. Meanwhile, the TG branch, using sequential temperature data and geolocation information, captures temporal algal growth patterns and adjusts for temperature variations influenced by regional climatic differences, ensuring the model's adaptability across different geographic regions. Additionally, we propose a geometric multimodal focal loss to further enhance representation learning. On the Tick-Tick Bloom (TTB) dataset, our approach outperforms the SOTA methods by 15.65%.",Not About Sufficiency
A hybrid ensemble machine learning model for detecting APT attacks based on network behavior anomaly detection,"A persistent, targeted cyber attack is called an advanced persistent threat (APT) attack. The attack is mainly launched to gain sensitive information, take over the system, and for financial gain, which creates nowadays more hurdles and challenges for the organization in preventing, detecting, and recovering from such attacks. Due to the nature of APT attacks, it is difficult to detect them quickly. Therefore machine learning techniques come into these research areas. This study uses deep and machine learning models such as random forest, decision tree, convolutional neural network, multilayer perceptron and so forth to categorize and effectively detect APT attacks by utilizing publicly accessible datasets. The datasets used in this study are CSE-CIC-IDS2018, CIC-IDS2017, NSL-KDD, and UNSW-NB15. This study proposes the hybrid ensemble machine learning model, a mixed approach of random forest and XGBoost classifiers. It has obtained the maximum prediction accuracy of 98.92%, 99.91%, 99.24%, and 97.11% for datasets CSE-CIC-IDS2018, CIC-IDS2017, NSL-KDD, and UNSW-NB15, with a false positive rate of 0.52%, 0.12%, 0.62%, and 5.29% respectively. These results are compared to other closely related recent studies in the literature. Our experiment's findings show that our model has performed significantly better for all datasets.",Not About Sufficiency
Bringing Big Data to Bear in Environmental Public Health: Challenges and Recommendations,"Understanding the role that the environment plays in influencing public health often involves collecting and studying large, complex data sets. There have been a number of private and public efforts to gather sufficient information and confront significant unknowns in the field of environmental public health, yet there is a persistent and largely unmet need for findable, accessible, interoperable, and reusable (FAIR) data. Even when data are readily available, the ability to create, analyze, and draw conclusions from these data using emerging computational tools, such as augmented and artificial intelligence (AI) and machine learning, requires technical skills not currently implemented on a programmatic level across research hubs and academic institutions. We argue that collaborative efforts in data curation and storage, scientific computing, and training are of paramount importance to empower researchers within environmental sciences and the broader public health community to apply AI approaches and fully realize their potential. Leaders in the field were asked to prioritize challenges in incorporating big data in environmental public health research: inconsistent implementation of FAIR principles in data collection and sharing, a lack of skilled data scientists and appropriate cyber-infrastructures, and limited understanding of possibilities and communication of benefits were among those identified. These issues are discussed, and actionable recommendations are provided.",Not About Sufficiency
THEORETICAL PERSPECTIVE TO ANALYZE ORGANIZATIONAL STRATEGY FOR INFORMATION AND COMMUNICATIONS TECHNOLOGY STANDARDS,"Standardization of innovative technology can create great competitive advantage for organizations. In addition, the consequences of interactions between organizations as they implement their standards strategies can have tremendous socio-economic impact, especially with the current convergence in information and communications technology. The existing research on standards uses economic theories to explain how economic factors stimulate technology standardization and game theory to analyze how organizations collaborate on standardization, but these theories both overlook the issue of how organizations make decisions about their standards strategies and the results of their interactions with other actors. Thus, the research question presented in this paper is: how do organizations reach their strategies for standards during and after the standardization process? To answer this question, we need a holistic perspective to analyze the context of organizational standards strategy, including the situation of any given organization and how the organization interprets its situation to choose or develop a certain strategy. This research presents a framework based on combining two theories, ANT (Actor Network Theory) and Self-organized Complexity.",Not About Sufficiency
Wearable Sensor and Machine Learning Model-Based Fall Detection System for Safety of Elders and Movement Disorders,"Due to different working culture of people, elderly people's health gets neglected as they live alone at home. With the rising population, there is a pressing demand for the evolution of fall identification systems. People with age greater than 65 are suffering from highest number of fatal falls. Some of the difficulties and challenges faced by the elders and mobility disordered people can be over passed by implementing algorithms able to anticipate falls. It is possible to have a good to great quality of life for the affected, by providing living assistance through automatic fall detection and alarming systems. We propose implementation of a fall recognition system for real-time tracking of elderly people. The proposed system has wearable sensor unit for detecting falls and alert mechanism to intimate the concerned and the care takers in case of falls by means of messages. The acceleration data are collected by the system using triaxial accelerometer and use machine learning algorithms to detect the falls upon various feature calculations. Extensive computations are carried out to compare the performance of different machine learning algorithms with varying features, and the algorithm giving the highest accuracy with optimal features is identified. The system gains an accuracy up to 99% by using random forest algorithm with tenfold cross validation. Thus, with a secure and reliable fall detection and alarming system, one could reduce the fatal falls, improving social integration, productivity, and quality of life.",Not About Sufficiency
Upgrading historic cities by integrated and innovative solutions,"During the ongoing rapid urbanisation process, many Chinese cities have redeveloped their inner city with large-scale demolishment and relocation. Yangzhou has adopted a different approach, and is actively taking measures to save and improve its historic centre. A substantial historical area of 5.1 km(2) with about 110 000 residents, characterised by one-to two-storey traditional courtyard buildings and narrow lanes, remains. The old city is a rich heritage resource attractive to both tourists and residents. However, the houses are in disrepair, the infrastructure is insufficient and the historic area is facing an increasing risk of deterioration. To link heritage preservation with the improvement of living conditions, the Yangzhou municipal government, German Technical Cooperation (GTZ) and the Cities Alliance have embarked on a programme of sustainable urban conservation by upgrading traditional urban neighbourhoods and supporting self-help initiatives. In a pilot block, integrated concepts were developed. Innovative technical solutions appropriate to the narrow lanes were designed and tested. Infrastructure was installed and standards were developed. Concepts and measures of ecological water cycle management were introduced at the municipal level. Residents were involved through a community action planning (CAP) approach. They agreed on appropriate standards and became involved in upgrading their houses and lane facades. The initial successful experience is being planned for use throughout the entire historical area. Expanding the process as a model for other historic cities in China is also being considered.",Not About Sufficiency
Study of Resident Relocation in Village Flood Protection Projects in the Inner Floodplain of the Yellow River Downstream,"Resettlement environmental capacity refers to population that can be resettled in host area receiving the resettled under the certain productivity and living standard, enviromental quality and social conditions for the purpose of guaranteeing the environmental, social and economic Sustainable development of the host area in a period of time. Combined with the situation that the rural economic structure of present stage is in the key period for strategical readjustment and considering the natural geographical feature and flood characteristic in different river sections in the inner floodplain area synthetically, this paper puts forward that the residents in the Yellow River downstream floodplain area should be resettled in 3 ways, i.e. relocation Outside floodplain area, local resettlement in the same village and temporary withdraw. In addition, based on the actual situation of the Yellow River downstream area, this paper presents the environmental capacity analysis methods of resettlement in the host area (Moving outside floodplain) on the basis of county so as to prevent the uniform Moving Outside the floodplain or impractical excessive resettlement outside the floodplain area.",Not About Sufficiency
Design and Development of Wireless Sensor Network based data logger with ESP-NOW protocol,"This paper presents Wireless Sensor Network (WSN) based data logger system for collecting data accurately from agricultural field. Agriculture is the backbone of India. Though many technological advances have been made in various fields, not much of it is implemented in the field of agriculture. One of the fundamental aspects that we can improve in agricultural field is by getting more data from soil, atmosphere, plants, etc accurately. By collecting more data we can apply machine learning and prediction algorithms to figure out the amount of fertilizer, water and other manures required for crops to get better yield. But in developing countries like India, cost of the technology is quite not affordable for implementing in large scale. Hence an effective and low cost real time monitoring system is needed, to monitor and collect data accurately. Wireless Sensor Network (WSN) based data logger system with ESP-NOW protocol developed by us aims at doing just that and the project is discussed here.",Not About Sufficiency
Artificial intelligence-based suicide prevention and prediction: A systematic review (2019-2023)-2023),"Suicide is a major global public health concern, and the application of artificial intelligence (AI) methods, such as natural language processing (NLP), machine learning (ML), and deep learning (DL), has shown promise in advancing suicide prediction and prevention efforts. Recent advancements in AI - particularly NLP and DL have opened up new avenues of research in suicide prediction and prevention. While several papers have reviewed specific detection techniques like NLP or DL, there has been no recent study that acts as a one-stop-shop, providing a comprehensive overview of all AI-based studies in this field. In this work, we conduct a systematic literature review to identify relevant studies published between 2019 and 2023, resulting in the inclusion of 156 studies. We provide a comprehensive overview of the current state of research conducted on AI-driven suicide prevention and prediction, focusing on different data types and AI techniques employed. We discuss the benefits and challenges of these approaches and propose future research directions to improve the practical application of AI in suicide research. AI is highly capable of improving the accuracy and efficiency of risk assessment, enabling personalized interventions, and enhancing our understanding of risk and protective factors. Multidisciplinary approaches combining diverse data sources and AI methods can help identify individuals at risk by analyzing social media content, patient histories, and data from mobile devices, enabling timely intervention. However, challenges related to data privacy, algorithmic bias, model interpretability, and real-world implementation must be addressed to realize the full potential of these technologies. Future research should focus on integrating prediction and prevention strategies, harnessing multimodal data, and expanding the scope to include diverse populations. Collaboration across disciplines and stakeholders is essential to ensure that AI- driven suicide prevention and prediction efforts are ethical, culturally sensitive, and person-centered.",Not About Sufficiency
Multi-hazard risk assessment of rail infrastructure in India under local vulnerabilities towards adaptive pathways for disaster resilient infrastructure planning,"""Lifeline of the nation"" is the motto of Indian Railways as it connects through a common thread, billion plus population in one way or the other. The National Rail Plan for India - 2030 focuses on creating a 'future ready' Railway system by 2030 by suitably integrating new railway systems like high-speed rails. However, rail infrastructure is exposed to multi-hazards and disasters sometimes disrupt safe rail operations. This study explores rail infrastructure risk assessment at a national scale utilizing the UNDRR framework and synthesized application of geospatial technologies with a focus on disentanglement of local vulnerabilities of the rail infrastructure assets utilizing factors of health of bridges, visibility obstruction to level crossings, labour wages & their regions and GSDP under multi-hazard scenarios. The results revealed that the NR and NFR were identified as high-risk routes under the risk analysis of physical and social vulnerability scenarios, followed by CR Railways. The average annual frequencies of emergency cases in each zone show a correlation r (17) = 0.4758 with the combined mean risk ranks for each zone. In comparison to socioeconomic factors, which contribute to indirect losses, physical factors directly affect safety and contribute to direct losses. Further, outcomes depict more accidents on Indian Railways during the monsoon (nearly 50%) and cold weather (29%) seasons. The study suggests that with the participation of key stakeholders, including urban and transport planners, an integrated approach is helpful in identifying critical rail routes towards risk-informed adaptive disaster-resilient infrastructure planning for providing safety, continuity and reliability of essential rail services.",Not About Sufficiency
"Regulating public procurement in Brazil, India, and China: Toward the regulatory-developmental state","Since the 1990s, emerging economies such as Brazil, India, and China have adopted transparency-enhancing public procurement regulations in line with international norms. Yet they have hesitated to join the World Trade Organization's legally binding Government Procurement Agreement (GPA). Based on the Special Issue framework, this article scrutinizes the underlying domestic and international determinants, and how they influence emerging countries' positions in two overlapping international procurement regimes. In particular, reform-oriented state actors, societal pressure, and lesson-drawing from international templates have induced a strengthening of domestic procurement institutions and turned emerging countries into ""promoters"" of the international transparency regime. Conversely, the rising powers have remained, to varying degrees, reluctant ""spoilers"" of the GPA-based market access regime in order to keep policy space and use procurement for domestic development objectives. The article suggests that this regulatory-developmental layering of rule-based governance and interventionist ambitions characterizes the variegated regulatory state in emerging countries.",Not About Sufficiency
Heart Disease Prediction Using Logistic Regression Machine Learning Model,"Heart disease is a significant global health issue responsible for millions of deaths annually. Modifiable risk factors such as high cholesterol, smoking, physical inactivity, and high blood pressure can be tackled through lifestyle changes and medical interventions. Machine learning and artificial intelligence have the potential to enhance disease prediction and management, leading to better outcomes for patients. Python, along with its libraries such as Scikit-Learn and TensorFlow, provides a versatile platform for developing and deploying machine learning models. In this study, the logistic regression model from Scikit-Learn was employed to predict the likelihood of cardiovascular disease based on various risk factors. The Pickle library was used to store the trained model for future use. Future research could enhance the model's efficacy by developing a user-friendly graphical user interface, implementing more advanced machine learning techniques, expanding the database used for training, and incorporating additional risk factors. The study demonstrates the potential of machine learning models for predicting and managing cardiovascular disease, highlighting the need for further research and development to improve accuracy, applicability, and clinical utility. The development of effective prevention and treatment strategies using advanced technologies such as AI and machine learning is crucial in reducing the impact of this disease on individuals and society as a whole.",Not About Sufficiency
Application of Machine Learning for Insect Monitoring in Grain Facilities,"In this study, a basic insect detection system consisting of a manual-focus camera, a Jetson Nano-a low-cost, low-power single-board computer, and a trained deep learning model was developed. The model was validated through a live visual feed. Detecting, classifying, and monitoring insect pests in a grain storage or food facility in real time is vital to making insect control decisions. The camera captures the image of the insect and passes it to a Jetson Nano for processing. The Jetson Nano runs a trained deep-learning model to detect the presence and species of insects. With three different lighting situations: white LED light, yellow LED light, and no lighting condition, the detection results are displayed on a monitor. Validating using F1 scores and comparing the accuracy based on light sources, the system was tested with a variety of stored grain insect pests and was able to detect and classify adult cigarette beetles and warehouse beetles with acceptable accuracy. The results demonstrate that the system is an effective and affordable automated solution to insect detection. Such an automated insect detection system can help reduce pest control costs and save producers time and energy while safeguarding the quality of stored products.",Not About Sufficiency
Machine Learning for Detecting Virus Infection Hotspots Via Wastewater-Based Epidemiology: The Case of SARS-CoV-2 RNA,"Wastewater-based epidemiology (WBE) has been proven to be a useful tool in monitoring public health-related issues such as drug use, and disease. By sampling wastewater and applying WBE methods, wastewater-detectable pathogens such as viruses can be cheaply and effectively monitored, tracking people who might be missed or under-represented in traditional disease surveillance. There is a gap in current knowledge in combining hydraulic modeling with WBE. Recent literature has also identified a gap in combining machine learning with WBE for the detection of viral outbreaks. In this study, we loosely coupled a physically-based hydraulic model of pathogen introduction and transport with a machine learning model to track and trace the source of a pathogen within a sewer network and to evaluate its usefulness under various conditions. The methodology developed was applied to a hypothetical sewer network for the rapid detection of disease hotspots of the disease caused by the SARS-CoV-2 virus. Results showed that the machine learning model's ability to recognize hotspots is promising, but requires a high time-resolution of monitoring data and is highly sensitive to the sewer system's physical layout and properties such as flow velocity, the pathogen sampling procedure, and the model's boundary conditions. The methodology proposed and developed in this paper opens new possibilities for WBE, suggesting a rapid back-tracing of human-excreted biomarkers based on only sampling at the outlet or other key points, but would require high-frequency, contaminant-specific sensor systems that are not available currently. Viral outbreaks such as COVID-19 are commonly monitored using traditional surveillance methods (e.g., nose swabs). However, individuals may not follow such testing schemes for different reasons, such as if they do not have symptoms. When people are infected by COVID-19, they excrete SARS-CoV-2, the virus that causes the disease. Hence, analyzing sewage using wastewater-based epidemiology (WBE) methods could help monitor the virus outbreak. Recent literature has identified a gap in combining machine learning and hydraulic models with WBE to detect viral outbreaks. In this study, we combine a hydraulic model that simulates virus transport with a machine learning model that tracks hotspot areas within a sewer network. The methodology developed in this paper was applied to a hypothetical sewer network to detect COVID-19 disease hotspots rapidly. Results showed that the machine learning model has the potential to recognize hotspots but requires high time-resolution data and is dependent on the sewer system's characteristics, such as flow velocity and virus sampling procedure. The developed methodology opens new possibilities for WBE to potentially trace human-excreted viral genetic material based on only sampling at particular points. However, it would require high-frequency sampling sensor systems that are not available currently. Using a numerical model of wastewater network and machine learning has the potential for the early detection of viral outbreaksThe ability to recognize disease hotspots depends on sampling frequency and methodThe use of sewer models can improve the usefulness of wastewater-based epidemiology data",Not About Sufficiency
"CollembolAI, a macrophotography and computer vision workflow to digitize and characterize samples of soil invertebrate communities preserved in fluid","Soils are inhabited by communities of tiny invertebrates that participate in the essential functions of soils. Characterizing those communities in terms of species diversity and species abundance is part of investigating soil functions and response to perturbation. Dozens to hundreds of specimens can be extracted from a sample that need to be sorted, counted and identified. It involves an enormous amount of time, straining the workflow of soil zoologists. Deep learning-based computer vision approaches have become increasingly popular to monitor biodiversity, as they can be applied to detect, count and classify organisms. In this work, we present CollembolAI, an open-source prototype for a computer vision workflow. It includes a hardware system for acquiring high-resolution pictures of soil species samples in fluid and a deep learning-based application (Faster R-CNN with Slicing Aided Hyper Inference) to train and evaluate models for the detection and classification of animals on those pictures. We evaluated the workflow using a mix of specimens belonging to 12 species of springtail and mite, picked from our taxonomic collection. Specimens were photographed multiple times under various angle of views. The model was trained using 5671 views on specimens on 30 images and tested on 442 views from new specimens on six images. CollembolAI is affordable, simple to build and allows the rapid digitization of mesofauna samples. Our deep learning model achieved a Precision of 0.940, Recall of 0.918 and a mAP@0.5 (Pascal VOC) of 0.868. The model showed a lower Recall for one species, but was performant on all others. Our prototype offers an operational workflow for the creation of soil fauna picture datasets needed to develop efficient deep learning-based classifiers. The applications are numerous, for example, collection digitization, soil biodiversity analysis and monitoring, or automatic assessment of mesofauna-based bioindicators. Computer vision is a rapidly emerging tool to handle efficiently the complexity of biodiversity and is already successfully used for plants and vertebrates recognition of pictures. It is on the way to become a major asset for dealing with invertebrate diversity. The code and setup instructions are available on Github.",Not About Sufficiency
Characteristics of statewide prescription drug monitoring programs and potentially inappropriate opioid prescribing to patients with non-cancer chronic pain: A machine learning application,"Unnecessary/unsafe opioid prescribing has become a major public health concern in the U.S. Statewide prescription drug monitoring programs (PDMPs) with varying characteristics have been implemented to improve safe prescribing practice. Yet, no studies have comprehensively evaluated the effectiveness of PDMP characteristics in reducing opioid-related potentially inappropriate prescribing (PIP) practices. The objective of the study is to apply machine learning methods to evaluate PDMP effectiveness by examining how different PDMP characteristics are associated with opioid-related PIPs for non-cancer chronic pain (NCCP) treatment. This was a retrospective observational study that included 802,926 adult patients who were diagnosed NCCP, obtained opioid prescriptions, and were continuously enrolled in plans of a major U.S. insurer for over a year. Four outcomes of opioid-related PIP practices, including dosage >50 MME/day and >90 MME/day, days supply >7 days, and benzodiazepine-opioid co-prescription were examined. Machine learning models were applied, including logistic regression, least absolute shrinkage and selection operation regression, classification and regression trees, random forests, and gradient boost modeling (GBM). The SHapley Additive exPlanations (SHAP) method was applied to interpret model results. The results show that among 1,886,146 NCCP opioid-related claims, 22.8% had an opioid dosage >50 MME/day and 8.9% >90 MME/day, 70.3% had days supply >7 days, and 10.3% were when benzodiazepine was filled <7 days ago. GBM had superior model performance. We identified the most salient PDMP characteristics that predict opioid-related PIPs (e.g., broader access to patient prescription history, monitoring Schedule IV controlled substances), which could be informative to the states considering the redesign of PDMPs.",Not About Sufficiency
Strategies for Dealing with Urban Shrinkage: Issues and Scenarios in Taranto,"Shrinkage has increasingly become a ""standard pathway"" of urban and regional development in many European cities and regions. Shrinking is generally seen in the literature as a negative phenomenon: certain strategies may, however, trigger-off positive effects, such as social networking opportunities, affordable housing, and an increased sense of identity and opportunity change. Focusing on the effects of urban development should be seen as a priority, attempting to seize opportunities for the integration of a range of urban policies, making the most of scarce resources. This paper begins with a short introduction, a sort of ""reading guide"" clarifying in which steps the document's argument is developed in the following sections; then it presents a theoretical framework with some central questions on strategies for shrinking cities and neighbourhoods, with a short review of the literature. An empirical section follows describing the case of Taranto as a de-industrialized city and the main features of its shrinkage, attempting to understand the effects of regeneration policies and urban planning strategies already put into place to tackle shrinkage at various administrative levels. This is followed by an evaluation of existing theoretical knowledge, comparing key points with the main features of shrinkage in the case of Taranto. This is carried out to contribute to a better understanding of the questions addressed, highlighting various unsolved problems that are then dealt with in the concluding section, as research challenges that remain open-ended.",Not About Sufficiency
The cooperative urban land development model in Germany-An effective instrument to support affordable housing,"Several international studies have already mentioned various policy instruments to promote the sociallymixed housing policy that supports many cities to achieve inclusionary housing goals. One of the greatest challenges facing cities in Germany is the current housing shortage, especially of affordable housing and the provision of adequate social infrastructure. The main causes are the strong influx into the cities and the rapidly rising land prices. To solve these problems several German cities have developed and established cooperative urban land development (CULD) models. This study aims to review inclusionary housing policy and present a firm introduction on a land development policy instrument CULD that supports among other targets the creation of affordable housing in Germany. It includes a description of this model that has social, economic and planning goals. Our research approach relies on a content analysis incorporating related literature, city documents and practical projects. Under the CULD system, the increase in land value through public planning and development measures is used to cover the entire development and infrastructure costs. Furthermore, landowners or investors benefitting from urban planning are legally committed to provide a certain quota (e.g. 30%) of their developed property for social housing as well as to ensure socially mixed city districts. There has been some debate on whether this arrangement can produce a sufficient stock of affordable housing. In this article, we argue that local CULD models are powerful land management instruments to achieve socially equitable land-use, and increase the potential to transfer land in such a manner that it can enhance the prosperity of the urban regions. They can utilize the land value increase through urban planning and public infrastructure, for refinancing expenditures of urban developments, and can provide a sufficient stock of affordable housing to local residents to create social mix in cities. In this way, CULDs can make a significant contribution to the economic and social objectives of sustainable urban development.",Not About Sufficiency
Solar Irradiance Prediction Using an Optimized Data Driven Machine Learning Models,"For a higher degree of penetration of renewable energy into the controls of the existing power system, an accurate solar energy prediction is necessary. Data-driven algorithms may be used to enhance solar generation forecasts as data has now become readily accessible in large quantities. To address these predicting issues in this research article three machine learning models: Support Vector Regressor (SVR), Multilayer Perceptron (MLP) and Random Forest Regressor (RFR) have been incorporated to forecast the Global Horizontal Irradiance (GHI), Diffused Horizontal Irradiance (DHI), Diffused Normal Irradiance (DNI) based on the spatiotemporal factors. In order to improve the prediction accuracy, the parametric tuning of models has been carried out with the two met heuristic algorithms: Moth Flame Optimization (MFO) and Grey Wolf Optimization (GWO) and also validated with the novel application of Evolve Class Topper Optimization (ECTO) method. Corresponding performance measures, including Mean Square Error (MSE), Root Mean Square Error (RMSE), Mean Absolute Error (MAE), Max Error (ME), and Coefficient of Determination (R2), are employed to evaluate each model's performance. The results obtained through a comparative assessment of all machine learning models confirmed that the ECTO based models have outperformed others and the RFR-ECTO model is the best forecasting model having the highest R2 scores of 0.9441, 0.9107 and 0.8882 and the lowest RMSE value of 75.8613 W/m(2), 40.8714 W/m(2), 94.8916 W/m(2) for GHI, DHI and DNI respectively which ensures that the designed predictive model can be implemented for prediction of solar energy.",Not About Sufficiency
LeanDojo: Theorem Proving with Retrieval-Augmented Language Models,"Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection-a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's program analysis capability to identify accessible premises and hard negative examples, which makes retrieval much more effective. Furthermore, we construct a new benchmark consisting of 98,734 theorems and proofs extracted from Lean's math library. It features challenging data split requiring the prover to generalize to theorems relying on novel premises that are never used in training. We use this benchmark for training and evaluation, and experimental results demonstrate the effectiveness of ReProver over non-retrieval baselines and GPT-4. We thus provide the first set of open-source LLM-based theorem provers without any proprietary datasets and release it under a permissive MIT license to facilitate further research.",Not About Sufficiency
Social Vulnerability Segmentation Methodology Based on Key Performance Indicators,"Detecting social vulnerability is a complex process that involves many personal and environmental factors to take into account. In this paper, a new methodology is proposed to detect which individuals are more likely to live under vulnerable situations merging all those personal and environmental factors in one metric. This methodology assigns each person who asks The Red Cross for help different normalized key performance indicators (KPI) regarding several areas as personal, social, health, environmental, economic, labour and familiar. All these fields are combined to represent the person in two axes: the internal or personal vulnerability axis (composed by personal, health and economical indexes) and external axis (composed by environmental, labour, social and familiar indexes). Based on these axes and through unsupervised machine learning techniques, this methodology assigns each person a vulnerability group which may be related with a series of actions to cover their needs and act upon their situation. This way, the proposed methodology allows us to go from a high dimensionality to a reduced problem that considerably simplifies the study. This process permits The Red Cross act quicker in those high-vulnerable or more-likely-vulnerable situations, improving the assistance process and helping the estimation of needed resources.",Not About Sufficiency
Assessing the use of HL7 FHIR for implementing the FAIR guiding principles: a case study of the MIMIC-IV Emergency Department module,"Objectives To provide a real-world example on how and to what extent Health Level Seven Fast Healthcare Interoperability Resources (FHIR) implements the Findable, Accessible, Interoperable, and Reusable (FAIR) guiding principles for scientific data. Additionally, presents a list of FAIR implementation choices for supporting future FAIR implementations that use FHIR.Materials and methods A case study was conducted on the Medical Information Mart for Intensive Care-IV Emergency Department (MIMIC-ED) dataset, a deidentified clinical dataset converted into FHIR. The FAIRness of this dataset was assessed using a set of common FAIR assessment indicators.Results The FHIR distribution of MIMIC-ED, comprising an implementation guide and demo data, was more FAIR compared to the non-FHIR distribution. The FAIRness score increased from 60 to 82 out of 95 points, a relative improvement of 37%. The most notable improvements were observed in interoperability, with a score increase from 5 to 19 out of 19 points, and reusability, with a score increase from 8 to 14 out of 24 points. A total of 14 FAIR implementation choices were identified.Discussion Our work examined how and to what extent the FHIR standard contributes to FAIR data. Challenges arose from interpreting the FAIR assessment indicators. This study stands out for providing a real-world example of a dataset that was made more FAIR using FHIR.Conclusion To the best of our knowledge, this is the first study that formally assessed the conformance of a FHIR dataset to the FAIR principles. FHIR improved the accessibility, interoperability, and reusability of MIMIC-ED. Future research should focus on implementing FHIR in research data infrastructures. In this study, we explored how a healthcare data exchange standard called Health Level Seven Fast Healthcare Interoperability Resources (FHIR) helps make scientific data more reusable. We performed a case study on a deidentified real-world dataset called the Medical Information Mart for Intensive Care-IV Emergency Department (MIMIC-ED), which is freely available for research and education. After assessing both the original and the FHIR versions of this dataset, we found that the FHIR version was more Findable, Accessible, Interoperable, and Reusable (FAIR). The biggest improvements were found in interoperability (meaning it was easier to exchange the data between different systems) and reusability (meaning researchers could reuse the data more easily). We also identified 14 aspects of FHIR that make data more FAIR. This study is unique because it is the first to formally assess how well FHIR follows the FAIR principles. The results show that using FHIR can improve the quality of healthcare data. Readers should be able to use this case study as an example of how the FAIR principles can be implemented using FHIR.",Not About Sufficiency
Unsupervised machine learning highlights the challenges of subtyping disorders of gut-brain interaction,"Background: Unsupervised machine learning describes a collection of powerful techniques that seek to identify hidden patterns in unlabeled data. These techniques can be broadly categorized into dimension reduction, which transforms and combines the original set of measurements to simplify data, and cluster analysis, which seeks to group subjects based on some measure of similarity. Unsupervised machine learning can be used to explore alternative subtyping of disorders of gut-brain interaction (DGBI) compared to the existing gastrointestinal symptom-based definitions of Rome IV. Purpose: This present review aims to familiarize the reader with fundamental concepts of unsupervised machine learning using accessible definitions and provide a critical summary of their application to the evaluation of DGBI subtyping. By considering the overlap between Rome IV clinical definitions and identified clusters, along with clinical and physiological insights, this paper speculates on the possible implications for DGBI. Also considered are algorithmic developments in the unsupervised machine learning community that may help leverage increasingly available omics data to explore biologically informed definitions. Unsupervised machine learning challenges the modern subtyping of DGBI and, with the necessary clinical validation, has the potential to enhance future iterations of the Rome criteria to identify more homogeneous, diagnosable, and treatable patient populations.",Not About Sufficiency
Leveraging explainable artificial intelligence and big trip data to understand factors influencing willingness to ridesharing,"Carpool-style ridesharing, compared to traditional solo ride-hailing, can reduce traffic congestion, cut per-passenger carbon emissions, reduce parking infrastructure, and provide a more cost-effective way to travel. Despite these benefits, ridesharing only occupies a small percentage of the total ride-hailing trips in cities. This study integrates big trip data with machine learning and eXplainable AI (XAI) to understand the factors that influence willingness to take shared rides. We use the City of Chicago as a case study, and results show that users tend to adopt ridesharing for longer distance trips, and the cost of a trip remains the most important factor. We identify a strong diurnal pattern that people prefer to request shared trips during the morning and afternoon peak hours. We also find socio-economic disparities: users who requested trips from neighbourhoods with a high percentage of non-white, a low median household income, a low percentage of bachelor's degrees, and high vehicle ownership are more likely to share a ride. The findings and the XAI-based analytical framework presented in this study can help transportation network companies and local governments understand ridesharing behaviour and suggest new strategies and policies to promote the proportion of ridesharing for more sustainable and efficient city transportation.",Not About Sufficiency
Assessing bus users satisfaction using discrete choice models: a case of Bhopal,"The bus transportation system in Bhopal is ineffective, unreliable, and inefficient, struggling to meet the daily transportation needs of its large population. Thus, evaluating the quality of bus service from the perspective of customers is crucial for planning a sustainable transportation system and formulating relevant policies and regulations. While there have been studies on public transportation systems in India, little attention has been paid to how different attributes of service quality influence passenger satisfaction. This study aims to explore the interdependance between bus service quality and its determinants in Bhopal. Through a customer satisfaction survey involving 1313 respondents based on Cochran's formula, discrete choice models like multinomial logit and mixed logit were developed. Results show widespread dissatisfaction among residents with their bus services, with less than 10% rating service quality as excellent or Very Satisfied. Factors such as users' comfort level and safety emerged as the primary contributors to this dissatisfaction. Other significant factors include ontime, driver skills, boarding and alighting processes, waiting time, and vehicle condition. Interestingly, the multinomial logit model showed better fit for the sample data compared to the mixed logit model, suggesting that bus users in Bhopal may form a relatively homogeneous group with access to alternative modes of transportation. These findings can inform the development of policies and regulations aimed at enhancing bus transportation in Bhopal.",Not About Sufficiency
The role of market-based transformative service initiatives in service inclusion of refugees,"PurposeThis paper aims to empirically investigate the role of market-based transformative service initiatives (TSIs) during the refugee crisis and shed light on how such TSIs increase inclusion of refugees in service systems by using market forces while creating broader benefits for service organizations themselves. Design/methodology/approachThis paper uses the case of the World Food Program's (WFP) Dalili smartphone application targeting Syrian refugees in the context of Lebanon. A mixed-methods approach, including in-depth interviews with the retail managers of the local supermarkets and statistical cross- and intra-regional analysis on the retailing mix elements of the local supermarkets was adopted for the empirical investigation. FindingsThe results show that the WFP's Dalili TSI increases service inclusion of refugees by facilitating their access to the essential food services easier and at affordable prices and helps them integrate into the host community. Furthermore, such market-based TSIs were shown to have broader benefits for other stakeholders in the food retail ecosystem including retailers and nonrefugee shoppers as they are successful in improving the retailing management standards of the participating supermarkets by decreasing the average retail price of the merchandise, increasing their variety and assortment, increasing promotional offers and improving the customer service level. Research limitations/implicationsThis research fills the gap in the literature for empirical investigation on the impact mechanism of market-based TSIs on service inclusion and well-being of refugees. In contrast to the majority of TSIs studied in the literature that are designed by governments or nonprofit organizations in the areas such as higher education, health care and humanitarian aids, this study focuses on the case of TSIs developed by supranational organizations using market forces in the food retail ecosystem. Furthermore, the findings suggest that TSIs could also benefit the service organizations that offer such initiatives. Practical implicationsThe findings of this paper have implications for service organizations and policymakers and their ability to design effective market-based TSIs during the refugee crisis. Originality/valueThe studied case in the context of TSIs in the food retail ecosystem and the empirical approach used are academically novel. Moreover, focusing on the refugee crisis in the Middle East region is rather understudied in the service research literature.",Not About Sufficiency
Umami-MRNN: Deep learning-based prediction of umami peptide using RNN and MLP,"Umami components are an important part of food condiments, and the use of umami peptides in the condiment industry has received great attention. However, traditional methods for umami peptide identification are timeconsuming, labor-intensive, and difficult to achieve high throughput. Therefore, it is essential to develop an effective algorithm to identify potential umami peptides. In this study, we proposed a prediction method for umami peptides called Umami-MRNN. We constructed a merged model for the Multi-layer Perceptron and Recurrent Neural Network. We then developed predictors with six feature vectors as the input. We trained the neural networks using the training dataset and selected hyperparameters of machine learning models via a 10fold cross-validation. The independent tests showed that Umami-MRNN achieved an accuracy of 90.5% and a Matthews correlation coefficient value of 0.811. To assist the scientific community, we also developed a publicly accessible web server at https://umami-mrnn.herokuapp.com/.",Not About Sufficiency
Understanding smart mobility experiments in the Dutch automobility system: Who is involved and what do they promise?,"In this paper, we aim to understand what ICT-related automobility experiments are initiated in the Netherlands, who is involved and what promises they make, in order to get a better understanding of the magnitude and direction of change. We show an example of how to study a large variety of experiments to understand the emergence of niches before predetermining these as analytical constructs. By analyzing 118 experiments, we can identify the emergence of two niches: an automated mobility niche and a mobility services niche. The automated niche is characterized by large involvement of incumbents and a strong technological orientation. The services niche focuses more on organizational innovations and involves many new entrants. The involvement of a third actor category, 'mature entrants', applies to both niches and concern those actors that fall more or less in-between the common 'incumbent-new entrant' dichotomy. In general, experiments in the automated niche seem to strengthen the dominant role of the car in the automobility system, while services niche experiments mainly portray an altered role of the car in an alternative mobility system. We conclude that we gained a better understanding of the experiments and emerging niches, but at this stage, developments can still head in different directions. Nevertheless, the involvement of mature entrants in both niches, we argue, can be an important indication that more substantial change is likely to occur.",Not About Sufficiency
Determination of COVID-19 Patients Using Machine Learning Algorithms,"Coronavirus disease (COVID-19), also known as Severe acute respiratory syndrome (SARS-COV2) and it has imposed deep concern on public health globally. Based on its fast-spreading breakout among the people exposed to the wet animal market in Wuhan city of China, the city was indicated as its origin. The symptoms, reactions, and the rate of recovery shown in the coronavirus cases worldwide have been varied. The number of patients is still rising exponentially, and some countries are now battling the third wave. Since the most effective treatment of this disease has not been discovered so far, early detection of potential COVID-19 patients can help isolate them socially to decrease the spread and flatten the curve. In this study, we explore state-of-the-art research on coronavirus disease to determine the impact of this illness among various age groups. Moreover, we analyze the performance of the Decision tree (DT), K-nearest neighbors (KNN), Naive bayes (NB), Support vector machine (SVM), and Logistic regression (LR) to determine COVID-19 in the patients based on their symptoms. A dataset obtained from a public repository was collected and pre-processed, before applying the selected Machine learning (ML) algorithms on them. The results demonstrate that all the ML algorithms incorporated perform well in determining COVID-19 in potential patients. NB and DT classifiers show the best performance with an accuracy of 93.70%, whereas other algorithms, such as SVM, KNN, and LR, demonstrate an accuracy of 93.60%, 93.50%, and 92.80% respectively. Hence, we determine that ML models have a significant role in detecting COVID-19 in patients based on their symptoms.",Not About Sufficiency
Integrating object-based and pixel-based segmentation for building footprint extraction from satellite images,"Accurately delineating building footprints from optical satellite imagery presents a formidable challenge, particularly in urban settings characterized by intricate and diverse structures. Consequently, enhancing the utility of these images for geospatial data updates demands meticulous refinement. Machine learning algorithms have made notable contributions in this context, yet the pursuit of precision remains an ongoing challenge. This paper aims to enhance the accuracy of building footprint extraction through the integration of object-based and pixel-based segmentation techniques. Additionally, it evaluates the performance of machine learning methodologies, specifically LightGBM, XGBoost, and Neural Network (NN) approaches. The model's evaluation employed low spectral resolution optical images, widely accessible and cost-effective for acquisition. The study's outcomes demonstrate a substantial enhancement in extraction accuracy compared to extant literature. The proposed methodology attains an overall accuracy of 99.39%, an F1 measurement of 0.9935, and a Cohen Kappa index of 0.9870. Thus, the proposed approach signifies a noteworthy advancement over existing techniques for building footprint extraction from high-resolution optical imagery.",Not About Sufficiency
Digital Healthcare in the Metaverse: Insights into Privacy and Security,"In this article, metaverse healthcare systems are studied from the privacy and security perspectives. We address data communication security for the metaverse, and the privacy and security threats of employing machine learning and artificial intelligence (ML/AI) algorithms for metaverse healthcare. In addition, human-centric data privacy for the social interactions in the metaverse is studied. Our goal is to present new visions and approaches, including physical layer security, semantic metaverse communications, differential privacy, and adversarial machine learning. These approaches have shown promising results in the field of data communications and networking, as well as the computer science domain, showcasing a huge potential to be employed for the metaverse healthcare systems. As a case study, we propose distributed differential privacy for the metaverse healthcare systems, where each virtual clinic perturbs its medical model vector to safeguard privacy against malicious clients and honest-but-curious servers. Through our experiments on the Breast Cancer Wisconsin Dataset, we highlight the privacy-utility tradeoff for different adjustable levels of privacy.",Not About Sufficiency
A machine learning approach to predict healthcare-associated infections at intensive care unit admission: findings from the SPIN-UTI project,"Background: Identifying patients at higher risk of healthcare-associated infections (HAIs) in intensive care units (ICUs) represents a major challenge for public health. Machine learning could improve patient risk stratification and lead to targeted infection prevention and control interventions. Aim: To evaluate the performance of the Simplified Acute Physiology Score (SAPS) II for HAI risk prediction in ICUs, using both traditional statistical and machine learning approaches. Methods: Data for 7827 patients from the 'Italian Nosocomial Infections Surveillance in Intensive Care Units' project were used in this study. The Support Vector Machines (SVM) algorithm was applied to classify patients according to sex, patient origin, non-surgical treatment for acute coronary disease, surgical intervention, SAPS II at admission, presence of invasive devices, trauma, impaired immunity, and antibiotic therapy in 48 h preceding ICU admission. Findings: The performance of SAPS II for predicting HAI risk provides a receiver operating characteristic curve with an area under the curve of 0.612 (P<0.001) and accuracy of 56%. Considering SAPS II along with other characteristics at ICU admission, the SVM classifier was found to have accuracy of 88% and an AUC of 0.90 (P<0.001) for the test set. The predictive ability was lower when considering the same SVM model but with the SAPS II variable removed (accuracy 78%, AUC 0.66). Conclusions: This study suggested that the SVM model is a useful tool for early prediction",Not About Sufficiency
"Promoting a sustainable behavioral shift in commuting choices: the role of previous intention and ""personalized travel plan"" feedback","According to the European Environment Agency (European Environmental Agency, EEA, 2018), road transport is responsible for 72% of all transport-related greenhouse gas emissions in the European Union (EU), which accounts for 25% of total energy-related emissions (Eurostat, 2018). Thus, it is crucial to identify drivers and barriers to more sustainable transport behaviors. In this regard, the Norm Activation Model and Theory of Planned Behavior have often been used as conceptual frameworks for predicting such behaviors. The present study aimed to analyze the differential impact of both socio-psychological factors and persuasive messages sent through a Personalized Travel Plan (PTP) on Sustainable Transport Choices (STC). To reach this aim we administered a survey two times (T1: Oct./Dec. 2020; T2: March/May 2021) to 398 car users. Measures of constructs included in the Norm Activation Model and the Theory of Planned Behavior, such as behavioral intention, attitude, perceived behavioral control, beliefs, and personal and social norms, were detected. Participants were then exposed to a PTP built on feedback information regarding kilocalories, CO2 emissions, cost, and time savings when using sustainable transport compared to driving a car. Structural Equation Modeling (SEM) analysis shows that intention to use sustainable transport in T1 is on one side directly predicted by personal norm, perceived behavioral control, and attitude, and on the other side emerged as the main predictor of sustainable travel choices in T2, together with kcal spent, whereas time was the major barrier. Implications and future developments are discussed in the light of the conceptual framework.",Not About Sufficiency
The impact of preferential trade agreements on bilateral trade: A structural gravity model analysis,"Trade agreements are thought to raise trade integration, but existing preferential trade agreements (PTAs) are insufficient in measuring market access of products. This study develops a product-based coverage index of PTAs using the World Trade Organization (WTO) preferential trade agreements and calculates bilateral trade measures using the EORA multi-regional input-output (MRIO) tables covering 189 countries worldwide over the period 1990-2015; the structural gravity model is employed to test how PTAs affect bilateral trade. Our findings show that countries sharing a common PTA could boost the trade volume compared to those without PTAs, supporting the trade creation effect. However, the trade promotion effect of the product-based coverage index of PTAs is significant only if the member countries are low-and middle-income countries. Further, the wide range of product liberalization brought by PTAs can promote global production networks by stimulating the trade of intermediate goods. Our results are important for understanding the market access effect of PTAs with the increasing development of trade integration and global value chains (GVCs).",Not About Sufficiency
Precision Medicine in Inflammatory Bowel Disease: A Spotlight on Emerging Molecular Biomarkers,"Inflammatory bowel diseases (IBD) remain challenging in terms of understanding their causes and in terms of diagnosing, treating, and monitoring patients. Modern diagnosis combines biomarkers, imaging, and endoscopic methods. Common biomarkers like CRP and fecal calprotectin, while invaluable tools, have limitations and are not entirely specific to IBD. The limitations of existing markers and the invasiveness of endoscopic procedures highlight the need to discover and implement new markers. With an ideal biomarker, we could predict the risk of disease development, as well as the possibility of response to a particular therapy, which would be significant in elucidating the pathogenesis of the disease. Recent research in the fields of machine learning, proteomics, epigenetics, and gut microbiota provides further insight into the pathogenesis of the disease and is also revealing new biomarkers. New markers, such as BAFF, PGE-MUM, oncostatin M, microRNA panels, alpha v beta 6 antibody, and S100A12 from stool, are increasingly being identified, with alpha v beta 6 antibody and oncostatin M being potentially close to being presented into clinical practice. However, the specificity of certain markers still remains problematic. Furthermore, the use of expensive and less accessible technology for detecting new markers, such as microRNAs, represents a limitation for widespread use in clinical practice. Nevertheless, the need for non-invasive, comprehensive markers is becoming increasingly important regarding the complexity of treatment and overall management of IBD.",Not About Sufficiency
Digital-Twin-Based Management of Sewer Systems: Research Strategy for the KaSyTwin Project,"Sewer infrastructure is vital for flood prevention, environmental protection, and public health. As part of sewer infrastructure, sewer systems are prone to degradation. Traditional maintenance methods for sewer systems are largely manual and reactive and rely on inconsistent data, leading to inefficient maintenance. The KaSyTwin research project addresses the urgent need for efficient and resilient sewer system management methods in Germany, aiming to develop a methodology for the semi-automated development and utilization of digital twins of sewer systems to enhance data availability and operational resilience. Using advanced multi-sensor robotic platforms equipped with scanning and imaging systems, i.e., laser scanners and cameras, as well as artificial intelligence (AI), the KaSyTwin research project focuses on generating digital twin-enabled representations of sewer systems in real time. As a project report, this work outlines the research framework and proposed methodologies in the KaSyTwin research project. Digital twins of sewer systems integrated with AI technologies are expected to facilitate proactive maintenance, resilience forecasting against extreme weather events, and real-time damage detection. Furthermore, the KaSyTwin research project aspires to advance the digital management of sewer systems, ensuring long-term functionality and public welfare via on-demand structural health monitoring and non-destructive testing.",Not About Sufficiency
Long-term trends of atmospheric hot-and-polluted episodes (HPE) and the public health implications in the Pearl River Delta region of China,"Air pollution and extreme heat have been responsible for more than a million deaths in China every year, especially in densely urbanized regions. While previous studies intensively evaluated air pollution episodes and extreme heat events, a limited number of studies comprehensively assessed atmospheric hot-and-polluted-episodes (HPE) - an episode with simultaneously high levels of air pollution and temperature - which have potential adverse synergic impacts on human health. This study focused on the Pearl River Delta (PRD) region of China due to its high temperature in summer and poor air quality throughout a year. We employed geostatistical downscaling to model meteorology at a spatial resolution of 1 km, and applied a machine learning algorithm (XGBoost) to estimate a high-resolution (1 km) daily concentration of particulate matter with an aerodynamic diameter <= 2.5 mu m (PM2.5) and ozone (O-3) for June to October over 20 years (2000-2019). Our results indicate an increasing trend (similar to 50%) in the frequency of HPE occurrence in the first decade (2000-2010). Conversely, the annual frequency of HPE occurrence reduced (16.7%), but its intensity increased during the second decade (2010-2019). The northern cities in the PRD region had higher levels of PM2.5 and O-3 than their southern counterparts. During HPEs, regional daily PM2.5 exceeded the World Health Organization (WHO) and Chinese guideline levels by 75% and 25%, respectively, while the O-3 exceeded the WHO O-3 standard by up to 69%. Overall, 567,063 (95% confidence interval (CI): 510,357-623,770) and 52,231 (95%CI: 26,116-78,346) excessive deaths were respectively attributable to exposure to PM2.5 and O-3 in the PRD region. Our findings imply the necessity and urgency to formulate co-benefit policies to mitigate the region's air pollution and heat problems.",Not About Sufficiency
Home intrusion: smart security and live video surveillance system,"Security concerns are rampant in our world today. No location is immune to crime, whether it be public spaces, commercial areas, offices, or private property. While law enforcement agencies work to address crimes after they have taken place, there is no guarantee that they can prevent them from occurring. Communities and businesses often turn to security personnel and surveillance cameras to keep watch, but this only provides passive protection. Criminals may act while security is absent and cameras only record events, they do not prevent them. Although, there are few systems with face recognition and alerting mechanisms are existing, they are too expensive and not affordable to a common man. In response to this challenge, this work proposes a most economic, hassle free proactive home security system which can be used as a plug and play device for the already installed surveillance system employing CCTV cameras. This system features facial recognition technology, enabling it to identify authorized individuals by storing their images in a database, thus swiftly distinguishing between intruders and residents. In cases of unauthorized intrusions, the system promptly activates alerts and alarms. Additionally, it incorporates supplementary security measures utilizing sensors and actuators for heightened protection. Moreover, the system utilizes an IP camera, permitting remote monitoring via live streaming. An array of machine learning algorithms is assessed to ascertain the most effective method for facial recognition, integrated into a monitoring dashboard using RTSP streaming.",Not About Sufficiency
Towards safer streets: A framework for unveiling pedestrians' perceived road safety using street view imagery,"Road safety has become a global concern but its impact in low- and middle-income countries is widespread mainly due to lack of appropriate crash database system and under-reporting. In this context, the primary objective of this paper is to provide a scalable framework for unveiling pedestrians' perceived road safety that can also be applied in regions where accessible crash data are limited or near-crashes are left unreported. In the first step of our methodology, a deep learning architecture-based semantic segmentation model (HRNet+OCR) is trained using labeled Google Street View (GSV) images from specific study areas in Dhaka, Bangladesh, which facilitates the identification of both man-made components (such as roads, sidewalks, buildings, and vehicles) and natural elements (including trees and sky). The developed model showed excellent performance in identifying different features in an image by achieving high precision (0.95), recall (0.97), F1-score (0.96), and intersection over union (IoU) (91.86). Secondly, a group of trained raters scored the perceived road safety on an ordinal scale from 0 to 10 (extremely unsafe to extremely safe to walk in terms of road crashes) by assessing the GSV images. Then, several regression models have been used on features extracted from GSV images, and socio-demographic factors (i.e., population density, and relative wealth index) to estimate the perceived road safety, and random forest regression model was found to perform the best. Further, Shapley Additive Explanations (SHAP), a model-agnostic technique has been used for examining feature importance by computing the contribution of each feature to the random forest regression model output. The results show that sidewalk, road, population density, wall, and relative wealth index have higher impact on determining the perceived road safety rating. Additionally, the results of t-tests between the average perceived road safety scores for crash-prone and non crash-prone areas revealed the existence of significant differences. This study also provides perceived road safety rating map on a neighborhood scale, which can be a useful visualization tool for policy-makers and practitioners to identify the road safety deficiencies at specific locations, and formulate appropriate and strategic countermeasures to improve pedestrians' road safety.",Not About Sufficiency
Breathing unequal air: environmental disadvantage and residential sorting of immigrant minorities in England and Germany,"Despite ongoing debates on environmental justice, the link between selective residential migration and the unequal exposure to environmental hazards remains underexplored. Previous research has often relied on spatially aggregated data and focused on single-country analyses, limiting our understanding of broader patterns. We address this gap using longitudinal household-level data from the UK Household Longitudinal Study and the German Socio-Economic Panel linked to air pollution estimates (NO2, PM2.5, and SO2). We find that immigrant minorities are exposed to higher levels of air pollution at their place of residence. The overall disadvantage faced by immigrant minorities in England is three times as large as in Germany. Given that immigrant households start under initially higher levels of air pollution, one would expect convergence with non-immigrant populations over time due to residential moves. However, immigrants face a substantial penalty when moving. If native households started in similar neighborhoods as immigrants-the relevant counterfactual-they would experience higher gains from relocation. Socio-economic factors cannot explain these differences. The pattern holds in both England and Germany, although inequalities in residential mobility are more pronounced in England. In particular, racial and ethnic minorities, such as Bangladeshi, Caribbean, and African migrants in England and Turkish migrants in Germany, experience the largest environmental disadvantages.",Not About Sufficiency
Single atom alloys aggregation in the presence of ligands,"Single atom alloys (SAAs) have gained tremendous attention as promising materials with unique physicochemical properties, particularly in catalysis. The stability of SAAs relies on the formation of a single active dopant on the surface of a metal host, quantified by the surface segregation and aggregation energy. Previous studies have investigated the surface segregation of non-ligated and ligated SAAs to reveal the driving forces underlying such phenomena. In this work we address another key factor dictating the stability in non-ligated and ligated SAAs: the aggregation energy (Eagg) of dopants. Specifically, we examine how thiols and amines, commonly found ligands in colloidal bimetallic nanoparticle synthesis, affect the aggregation of dopants (forming dimers and trimers) on the surface of a metal host. Utilizing Density Functional Theory (DFT) and machine learning (ML), we explore the stability patterns of SAAs through the energetics of low-index surfaces, such as (111) and (100), consisting of d8-(Pt, Pd, Ni) and d9-(Ag, Au, Cu) metals, both in the presence and absence of ligands. Collecting rich and accurate DFT data, we developed a four-feature support vector regression using the radial basis function (SVR RBF) to predict the Eagg. The model revealed important and easily accessible (tabulated) thermodynamic stability features that drive metal aggregation in SAAs, such as the bulk cohesive energy of the metal considering the exposed coordination environment on the surface, the charge transfer represented by the difference in electron affinities of metals and the radii of the metals describing strain effects. Additional incorporated features include adsorbate properties, such as the binding energy of the ligand on a single atom considering the coordination environment of the adsorbate. Through our study, we have revealed that stable SAAs are formed in Ni-, Pd-, Pt-based SAAs in the presence of ligands, while Ag-, Au-, Cu- doped with Ni-, Pd-, Pt- lead to aggregation. Finally, we tested our model against several experimental studies and demonstrated its robustness in predicting the formation of SAAs, enabling rapid screening across the vast materials space of SAAs. Additionally, we suggest criteria for stabilization of SAAs, guiding experimental efforts. Overall, our study advances the understanding of thermodynamic stability of colloidal SAAs, paving the way for rational SAA design.",Not About Sufficiency
In-vivo data-driven parcellation of Heschl's gyrus using structural connectivity,"The human auditory cortex around Heschl's gyrus (HG) exhibits diverging patterns across individuals owing to the heterogeneity of its substructures. In this study, we investigated the subregions of the human auditory cortex using data-driven machine-learning techniques at the individual level and assessed their structural and functional profiles. We studied an openly accessible large dataset of the Human Connectome Project and identified the subregions of the HG in humans using data-driven clustering techniques with individually calculated imaging features of cortical folding and structural connectivity information obtained via diffusion magnetic resonance imaging tractography. We characterized the structural and functional profiles of each HG subregion according to the cortical morphology, microstructure, and functional connectivity at rest. We found three subregions. The first subregion (HG(1)) occupied the central portion of HG, the second subregion (HG(2)) occupied the medial-posterior-superior part of HG, and the third subregion (HG(3)) occupied the lateral-anterior-inferior part of HG. The HG(3) exhibited strong structural and functional connectivity to the association and paralimbic areas, and the HG(1) exhibited a higher myelin density and larger cortical thickness than other subregions. A functional gradient analysis revealed a gradual axis expanding from the HG(2) to the HG(3). Our findings clarify the individually varying structural and functional organization of human HG subregions and provide insights into the substructures of the human auditory cortex.",Not About Sufficiency
Macro-factors driving bicycle adoption as a primary transport mode across Europe,"This study provides the first analysis of geospatial factors on the seasonal decision to commute by bike or use a bike for errands across 31 European countries. The sample was drawn from a survey of 14,845 respondents and combined with geospatial variables that describe the respondents' local areas. Controlling for well-known de-mographic and subjective socio-cultural variables, statistical relationships from the less explored objective geospatial variables on the seasonal biking decisions were estimated using a bivariate probit model. We find that the large differences observed between the biking levels in European countries can not be explained by differences in structural characteristics alone. In line with literature our interpretation of this gap points towards biking as a social innovation, partly driven by cultural and societal levers. The cross-country comparison presented in this study improves our understanding of the dynamic social innovation process that encapsulates mobility choice and cycling adoption, and highlights the levers that policymakers and urban planners have to influence this process.",Not About Sufficiency
SciPipe: A workflow library for agile development of complex and dynamic bioinformatics pipelines,"Background: The complex nature of biological data has driven the development of specialized software tools. Scientific workflow management systems simplify the assembly of such tools into pipelines, assist with job automation, and aid reproducibility of analyses. Many contemporary workflow tools are specialized or not designed for highly complex workflows, such as with nested loops, dynamic scheduling, and parametrization, which is common in, e.g., machine learning. Findings: SciPipe is a workflow programming library implemented in the programming language Go, for managing complex and dynamic pipelines in bioinformatics, cheminformatics, and other fields. SciPipe helps in particular with workflow constructs common in machine learning, such as extensive branching, parameter sweeps, and dynamic scheduling and parametrization of downstream tasks. SciPipe builds on flow-based programming principles to support agile development of workflows based on a library of self-contained, reusable components. It supports running subsets of workflows for improved iterative development and provides a data-centric audit logging feature that saves a full audit trace for every output file of a workflow, which can be converted to other formats such as HTML, TeX, and PDF on demand. The utility of SciPipe is demonstrated with a machine learning pipeline, a genomics, and a transcriptomics pipeline. Conclusions: SciPipe provides a solution for agile development of complex and dynamic pipelines, especially in machine learning, through a flexible application programming interface suitable for scientists used to programming or scripting.",Not About Sufficiency
Hyperon-nucleon interaction constrained by light hypernuclei,"Ab initio structure calculations for p-shell hypernuclei have recently become accessible through extensions of nuclear many-body methods, such as the no-core shell model, in combination with hyperonnucleon interactions from chiral effective field theory. However, the low-energy constants in these hyperon-nucleon interactions are poorly constraint due to the very limited amount of experimental scattering data available. We present a hyperon-nucleon interaction that is additionally constrained by experimental ground-state and spectroscopic data for selected p-shell hypernuclei and, thus, optimized for hypernuclear structure calculations. We show that the previous overestimation of the hyperon separation energies in the p-shell is remedied and discuss the significantly improved description of the AHe isotopic chain. We further discuss the uncertainty quantification for hypernuclear observables on the many-body level, obtained through a novel machine-learning tool. (c) 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons .org /licenses /by /4 .0/). Funded by SCOAP3.",Not About Sufficiency
Rural-urban disparities of Alzheimer's disease and related dementias: A scoping review,"The rising age of the global population has made Alzheimer's disease and related dementias (ADRD) a critical public health problem, with significant health-related disparities observed between rural and urban areas. However, no previous reviews have examined the scope and determinant factors contributing to rural-urban disparities of ADRD-related health outcomes. This study aims to systematically collate and synthesize peer-reviewed articles on rural-urban disparities in ADRD, identifying key determinants and research gaps to guide future research. We conducted a systematic search using key terms related to rural-urban disparities and ADRD without restrictions on geography or study design. Five search engines-MEDLINE, CINAHL, Web of Science, PubMed, and Scopus-were used to identify relevant articles. The search was performed on August 16, 2024, and included English-language articles published from 2000 onward. Sixty-three articles met the eligibility criteria for data extraction and synthesis. Most articles were published after 2010 (85.7%) and were concentrated in the United States, China, and Canada (66.7%). A majority had cross-sectional (58.7%) or cohort study designs (23.8%), primarily examining prevalence (41.3%) or incidence (11.1%). Findings often indicated a higher prevalence and incidence in rural areas, although inconsistent rural-urban classification systems were noted. Common risk factors included female sex, lower education level, lower income, and comorbidities such as diabetes and cerebrovascular diseases. Environmental (12.7%) and lifestyle (14.3%) factors for ADRD have been less explored. The statistical methods used were mainly traditional analyses (e.g., logistic regression) and lacked advanced techniques such as machine learning or causal inference methods. The gaps identified in this review emphasize the need for future research in underexplored geographic regions and encourage the use of advanced methods to investigate understudied factors contributing to ADRD disparities, such as environmental, lifestyle, and genetic influences.",Not About Sufficiency
LATVIAN RENEWABLE ENERGY POLICY IN THE CONTEXT OF EU INITIATIVES,"Energy security and reduction of dependence on energy imports is a key issue on the EU's agenda in the context of today's global challenges. The resource potential of renewable energy expands the capabilities of EU member states to increase energy sustainability and attract additional investments to implement the green transformation of national economies. The EU has adopted a number of initiatives under Directive EU 2018/2001, REPowerEU, which are synchronized with ""The European Green Deal"". They are aimed at boosting the development of renewable energy. These EU initiatives and the need to increase energy security and sustainability make the development of renewable energy relevant for Latvia. The subject of the study is the content and features of Latvia's energy policy. The purpose of the scientific research is to study the key trends in the development of Latvia's renewable energy policy in the context of EU initiatives. Research methodology: systematic approach, methods of analysis and generalization. One of the key areas of the government's energy policy is the development of renewable energy. This is stated in the National Energy and Climate Plan 2021-2030 and the Sustainable Development Strategy until 2030. Key tasks include synchronising the efforts of the state, business, the public and stakeholders in developing energy security by increasing the use of energy from renewable sources, attracting investment in the energy sector to promote a green economic transformation, energy efficiency and creating a culture of responsible resource consumption. Latvia has identified key areas for the development of renewable energy in line with the content of its energy policy and relevant EU initiatives. Considerable attention is being paid to optimising the regulatory framework. The Green Channel online platform has been created to coordinate the efforts of the government and stakeholders in developing key sectors of the national economy, including renewable energy. The resource of cross-border cooperation is actively used in the implementation of projects for the construction of wind farms for the production of electricity and the necessary infrastructure for its supply to domestic and foreign consumers. The research phases of the Latvian-Estonian offshore wind project ELWIND and the construction of the Kurzeme wind farm with the assistance of the Swedish company Eolus and the German company PNE have been launched. Domestic stakeholders are interested in building solar parks and related infrastructure. Considerable attention is being paid to the digitalisation of the energy sector in order to optimise the management of the energy market and consumer services. Results of the study: the priority direction of Latvia's energy policy is the development of renewable energy in the context of increasing the level of energy security of the state and correlation with relevant EU initiatives; the national government promotes the optimisation of the content of the regulatory framework in the field of renewable energy, expands opportunities for cross-border cooperation and involves stakeholders' initiative in project implementation; measures are taken to digitalise the energy sector and create a culture of responsible energy consumption.",Not About Sufficiency
"Essential oil of Eucalyptus citriodora: Physio-Chemical analysis, Formulation with Hand Sanitizer Gel and Antibacterial Activity","Background: The essential oil-bearing plants are extensively being used in traditional systems of medicine due to the occurrence of the diversity of phytochemical constituents. The emerging crisis of developing resistance to conventional drugs has increased public health awareness and reliance on natural compounds as safer alternatives. Methods: The essential oil extracted from Eucalyptus citriodora (Hook.) leaves was characterized for physicochemical attributes, formulated with hand sanitizer gel, tested for organoleptic parameters, and antimicrobial activity against Escherichia coli, Staphylococcus aureus, and Bacillus subtilis. Results: E. citriodora essential oil (EEO) had a camphorous scent, and dark yellow coloration, while exhibiting 0.60% yield (v/w, 97% pure), 0.94 density, 1.47 refractive index, 11.10 viscosity, 0.92 specific gravity, 0.0-9.98o optical rotation, 11.20 acid number, 50.60 ester number, which satisfy the standards specified by ISO (The International Organization for Standardization). The chromatographic analysis of oil identified eucalyptol as the most abundant compound (80.08%) followed by alpha-terpinyl acetate, isopinocarveol, and globulol as the moderately abundant compounds (4.46-4.81%), while viridiflorol and terpinen-4-ol as less abundant compounds (3.06 and 2.69%, respectively). Formulated hand sanitizer with EEO exhibited physical and microbiological properties that were comparable with the market products. It also had a pleasant scent, was compatible with the skin, was easy to apply, and is acceptable to the users. Conclusion: The current study clearly shows that EEO could be utilized as a potential ingredient in alcohol -based gel hand sanitizer formulation for giving a pleasant smell, acceptable physical appearance and microbial quality parameters.",Not About Sufficiency
Implementing and Maintaining a SARS-CoV-2 Exposure Notification Application for Mobile Phones: The Finnish Experience,"Exposure notification applications (ENAs) or digital proximity tracing apps were used in several countries during the COVID-19 pandemic. In this viewpoint, we share our experience of implementing and running the Finnish ENA (Koronavilkku), one of the national ENAs with the highest proportion of users during the pandemic. With the aim of strengthening public trust and increasing app uptake, there was a strong prioritization of privacy and data security for the end user throughout the ENA development. This, in turn, limited the use of the app as a tool for health care professionals and deeper insight into its potential effectiveness. The ENA was designed to supplement conventional contact tracing, rather than replace it, and to serve as an early warning system and a trigger for action for the user in case of potential exposure. The predefined target of 40% uptake in the population was achieved within 3 months of the ENA launch. We consider easy-to-understand information produced together with communication experts crucial during the changing pandemic situation. This information educated people about the app as one component in mitigating the pandemic. As the pandemic and its mitigation evolved, the ENA also needed adapting and updating. A few months after its launch, Finland joined European interoperability, which allowed the ENA to share information with ENAs of other countries. We added automatic token issuing to the ENA as of mid-2021. If added earlier and more comprehensively, automatization could have more effectively saved resources in health care services and prevented overburdening contact tracing teams, while also notifying potentially exposed individuals quicker and more reliably. In the spring of 2021, the number of active apps started to gradually decline. Quarantine and testing practices for asymptomatic vaccinated individuals following exposure to the virus were eased and home tests became more common, eventually replacing laboratory testing for much of the population. Taken together, this led to decreased token issuance, which weakened the potential public health usefulness of the app. A self-service option for token issuance would likely have prolonged the lifespan of the app. The ENA was discontinued in mid-2022. Regularly conducted surveys would have helped gain timely knowledge on the use and effectiveness of the app for better responding to the changing needs during the pandemic.",Not About Sufficiency
Digital divide and Internet health information seeking among cancer survivors: A trend analysis from 2011 to 2017,"Objective The current study aims to explore the trend of Internet health information seeking (IHIS) in cancer survivors and the relationship between four dimensions of digital divide (eg, mental access, material access, skills access, and usage access) and IHIS. Methods Data from three iterations (2011, 2013, and 2017) of Health Information National Trends Survey were analyzed. Only cancer survivors (2011: n=563; 2013: n=459; 2017: n=504) were included. ANOVA and multivariate regressions were performed. Results The rate of IHIS among cancer survivors has increased from 53.5% in 2011 to 69.2% in 2017. Also, across the three survey years, material access (eg, physical Internet access), usage access (eg, eHealth activities), and mental access (eg, trust in online health information) have remained in a significant and positive relationship with IHIS. Skills access (eg, health information seeking skills) and the other mental access factor, self-efficacy in health information seeking, were not significantly associated with IHIS over time. Conclusions This research revealed a rising trend of IHIS adoption in cancer survivors, and demonstrated significant relationships between digital divide and IHIS. We call for targeted interventions to reduce the digital divide barriers for cancer survivors.",Not About Sufficiency
Des-q: a quantum algorithm to provably speedup retraining of decision trees,"Decision trees are widely adopted machine learning models due to their simplicity and explainability. However, as training data size grows, standard methods become increasingly slow, scaling polynomially with the number of training examples. In this work, we introduce Des-q, a novel quantum algorithm to construct and retrain decision trees for regression and binary classification tasks. Assuming the data stream produces small, periodic increments of new training examples, Des- q significantly reduces the tree retraining time. Des-q achieves a logarithmic complexity in the combined total number of old and new examples, even accounting for the time needed to load the new samples into quantum-accessible memory. Our approach to grow the tree from any given node involves performing piecewise linear splits to generate multiple hyperplanes, thus partitioning the input feature space into distinct regions. To determine the suitable anchor points for these splits, we develop an efficient quantum-supervised clustering method, building upon the qmeans algorithm introduced by Kerenidis et al. We benchmark the simulated version of Des-q against the state-of-the-art classical methods on multiple data sets and observe that our algorithm exhibits similar performance to the state-of-the-art decision trees while significantly speeding up the periodic tree retraining.",Not About Sufficiency
A Human Machine Hybrid Approach for Systematic Reviews and Maps in International Development and Social Impact Sectors,"The international development and social impact evidence community is divided about the use of machine-centered approaches in carrying out systematic reviews and maps. While some researchers argue that machine-centered approaches such as machine learning, artificial intelligence, text mining, automated semantic analysis, and translation bots are superior to human-centered ones, others claim the opposite. We argue that a hybrid approach combining machine and human-centered elements can have higher effectiveness, efficiency, and societal relevance than either approach can achieve alone. We present how combining lexical databases with dictionaries from crowdsourced literature, using full texts instead of titles, abstracts, and keywords. Using metadata sets can significantly improve the current practices of systematic reviews and maps. Since the use of machine-centered approaches in forestry and forestry-related reviews and maps are rare, the gains in effectiveness, efficiency, and relevance can be very high for the evidence base in forestry. We also argue that the benefits from our hybrid approach will increase in time as digital literacy and better ontologies improve globally.",Not About Sufficiency
The community land trust as a highway environmental impact mitigation tool,"The proposed Newtown Pike Extension in Lexington, Kentucky, has the potential to cause significant displacement of low-income residents. Following Executive Order 12898, federal agencies have been required to consider environmental justice impacts of their policies and activities. Highway planners in Lexington have crafted a mitigation strategy that includes a community land trust, which is intended to provide permanent protection for the low-income community to be affected by the highway extension. Although the community land trust may be an attractive option for maintaining housing affordability and community cohesion, the idea must overcome some significant hurdles if it is going to be useful as an environmental impact mitigation tool. One problem is selling the idea of communal land ownership to African American and low-income households. Part of the challenge is to design a method for sharing capital gains that gives an adequate return to home buyers while maintaining affordability. Another difficulty concerns renters who are eligible for only limited relocation subsidies that in many cases will not be enough to cover the cost of new rental housing.",Not About Sufficiency
Ethical considerations of urban ecological design and planning experiments,"Societal Impact Statement It is increasingly common for plant scientists and urban planning and design professionals to collaborate on interdisciplinary teams that integrate scientific experiments into public and social urban spaces. However, neither the procedural ethics that govern scientific experimentation, nor the professional ethics of urban design and planning practice, fully account for the possible impacts of urban ecological experiments on local residents and communities. Scientists that participate in design and planning teams act as decision-makers, and must expand their domain of ethical consideration accordingly. Conversely, practitioners who engage in ecological experiments take on the moral responsibilities inherent in generation of knowledge. To avoid potential harm to human and non-human inhabitants of cities while maintaining scientific and professional integrity in research and practice, an integrated ethical framework is needed for urban ecological planning and design. While there are many ethical and procedural guidelines for scientists who wish to inform decision-making and public policy, urban ecologists are increasingly embedded in planning and design teams to integrate scientific measurements and experiments into urban landscapes. These scientists are not just informing decision-making - they are themselves acting as decision-makers. As such, researchers take on additional moral obligations beyond scientific procedural ethics when designing and conducting ecological design and planning experiments. We describe the growing field of urban ecological design and planning and present a framework for expanding the ethical considerations of socioecological researchers and urban practitioners who collaborate on interdisciplinary teams. Drawing on existing ethical frameworks from a range of disciplines, we outline possible ways in which ecologists, social scientists, and practitioners should expand the traditional ethical considerations of their work to ensure that urban residents, communities, and non-human entities are not harmed as researchers and practitioners carry out their individual obligations to clients, municipalities, and scientific practice. We present an integrated framework to aid in the development of ethical codes for research, practice, and education in integrated urban ecology, socioenvironmental sciences, and design and planning.",Not About Sufficiency
Call for algorithmic fairness to mitigate amplification of racial biases in artificial intelligence models used in orthodontics and craniofacial health,"Machine Learning (ML), a subfield of Artificial Intelligence (AI), is being increasingly used in Orthodontics and craniofacial health for predicting clinical outcomes. Current ML/AI models are prone to accentuate racial disparities. The objective of this narrative review is to provide an overview of how AI/ML models perpetuate racial biases and how we can mitigate this situation. A narrative review of articles published in the medical literature on racial biases and the use of AI/ML models was undertaken. Current AI/ML models are built on homogenous clinical datasets that have a gross underrepresentation of historically disadvantages demographic groups, especially the ethno-racial minorities. The consequence of such AI/ML models is that they perform poorly when deployed on ethno-racial minorities thus further amplifying racial biases. Healthcare providers, policymakers, AI developers and all stakeholders should pay close attention to various steps in the pipeline of building AI/ML models and every effort must be made to establish algorithmic fairness to redress inequities.",Not About Sufficiency
Multi-Task Learning for Electricity Price Forecasting and Resource Management in Cloud Based Industrial IoT Systems,"Cloud computing has gained immense popularity in the logistics industry. This innovative technology optimizes computing operations by eliminating the requirement for physical equipment for calculations. Instead, specialized companies provide cloud-based computing services, relying heavily on computers and servers that consume substantial amounts of energy. Hence, ensuring the availability of affordable and dependable electricity is paramount for the efficient design and management of these logistics services. Cloud centers, which are power-intensive, face the challenge of reducing their energy consumption due to escalating power costs. To address this issue, efficient data placement and node management strategies are commonly employed in logistics operations. An AlexNet model has been designed to optimize storage relocation and predict power prices. The outcome of this initiative has resulted in a considerable reduction in energy consumption at data centres. The model uses Dwarf Mongoose Optimization Algorithm (DMOA) to produce an optimal solution for the AlexNet and increase its performance with a real-world dataset from IESO in Ontario, Canada. 75% of the available data was used for training to assure the model's precision, with the remaining 25% allocated to testing purposes. The model forecasts power prices with an MAE of 2.22% and an MSE of 6.33%, resulting in an average reduction of 22.21% in electricity expenses. Our proposed method has an accuracy of 97% compared to 11 benchmark algorithms, including CNN, DenseNet, and SVM having an accuracy of 89%, 88%, and 82%, respectively.",Not About Sufficiency
Improving Mortality Risk Prediction with Routine Clinical Data: A Practical Machine Learning Model Based on eICU Patients,"Purpose: Mortality risk prediction helps clinicians make better decisions in patient healthcare. However, existing severity scoring systems or algorithms used in intensive care units (ICUs) often rely on laborious manual collection of complex variables and lack sufficient validation in diverse clinical environments, thus limiting their practical applicability. This study aims to evaluate the performance of machine learning models that utilize routinely collected clinical data for short-term mortality risk prediction. Patients and Methods: Using the eICU Collaborative Research Database, we identified a cohort of 12,393 ICU patients, who were randomly divided into a training group and a validation group at a ratio of 9:1. The models utilized routine variables obtained from regular medical workflows, including age, gender, physiological measurements, and usage of vasoactive medications within a 24-hour period prior to patient discharge. Four different machine learning algorithms, namely logistic regression, random forest, extreme gradient boosting (XGboost), and artificial neural network were employed to develop the mortality risk prediction model. We compared the discrimination and calibration performance of these models in assessing mortality risk within 1-week time window. Results: Among the tested models, the XGBoost algorithm demonstrated the highest performance, with an area under the receiver operating characteristic curve (AUROC) of 0.9702, an area under precision and recall curves (AUPRC) of 0.8517, and a favorable Brier score of 0.0259 for 24-hour mortality risk prediction. Although the model's performance decreased when considering larger time windows, it still achieved a comparable AUROC of 0.9184 and AUPRC of 0.5519 for 3-day mortality risk prediction. Conclusion: The findings demonstrate the feasibility of developing a highly accurate and well-calibrated model based on the XGBoost algorithm for short-term mortality risk prediction with easily accessible and interpretative data. These results enhance confidence in the application of the machine learning model to clinical practice.",Not About Sufficiency
"Combining Unsupervised and Supervised Learning to Predict Poverty Households in Sakon Nakhon, Thailand","Poverty is a problem that various government agencies are attempting to address accurately and precisely. This solution relies on data and analysis of features affecting poverty. Machine Learning is a technique to analyze and focus on poverty features encompassing five livelihood capitals: human, physical, economic, natural, and social capital to understand the household context and environment. The dataset contains 1,598 poverty households from Kut Bak district, Sakon Nakhon, Thailand. Kprototype was used to group categorical and numerical dataset into four clusters and labelled as Destitute, Extreme poor, Moderate poor, and Vulnerable non -poor. The performances of the Decision tree classifier with feature selection algorithms, including MI, ReliefF, RFE, and SFS, are compared. The best performance is SFS with F -measure, precision, and recall at 74.6%, 74.8%, and 74.7%, respectively. The result is the decision tree rules to predict the poverty level of households, enabling the establishment of guidelines for resolving household issues, and addressing broader problems within the areas.",Not About Sufficiency
Prediction of machine learning-based hardness for the polycarbonate using additive manufacturing,"Introduction Additive manufacturing (AM) is a revolutionary technology transforming traditional production processes by providing exceptional mechanical characteristics.Methods The present study aims explicitly to predict the hardness of Polycarbonate (PC) parts produced using AM. The objectives of this study are: (1) To investigate the process parameters that impact the ability to estimate the hardness of PC materials accurately, and (2) To develop a best-performing ML model from a range of models that can reliably predict the hardness of additively manufactured PC parts. Initially, fused filament fabrication (FFF), the most affordable AM technique, was used for the manufacturing of parts. Four process parameters, infill density, print direction, raster angle, and layer thickness, are selected for investigation. A heatmap is generated to obtain the influence of process parameters on hardness. Then, machine learning (ML) techniques create a range of predictive models that can predict hardness value considering the level of process parameters.Results The developed ML models include Linear Regression, Decision Tree, Random Forest, K-nearest neighbor, Support Vector Regression, AdaBoost, and Artificial Neural Network. Further, an investigation has been done that includes choosing and improving ML algorithms and assessing the models' performance.Discussion Prediction plots, residual plots, and evaluation metrics plots are prepared to gauge the performance of the developed models. Thus, the research enhances AM capabilities by applying predictive modeling to process parameters and improving the quality and reliability of fabricated components.",Not About Sufficiency
Page Avenue health impact assessment: Building on diverse partnerships and evidence to promote a healthy community,"The Page Avenue health impact assessment (HIA) was focused on a redevelopment in Missouri. This case study describes a comprehensive HIA led by an interdisciplinary academic team with community partners, as well as compliance with North American HIA Practice Standards. Some of the key lessons learned included: (1) interdisciplinary teams are valuable but they require flexibility and organization: (2) engaging community stakeholders and decision-makers prior to, during, and following the HIA is critical to a successful HIA; and (3) HIA teams should not be too closely affiliated with decision-makers. It is hoped that this case study will inform future HIAs. (C) 2011 Elsevier Ltd. All rights reserved,",Not About Sufficiency
Aligning biodiversity conservation and ecosystem services in spatial planning: Focus on ecosystem processes,"Although the consideration of socio-economic demands with biodiversity conservation is now high on the environmental policy agenda, it is not yet standard practice in spatial planning. This is argued to be related, among others, to a lack of awareness among stakeholders and practitioners of the underpinning role of ecosystem functioning and biodiversity to support human well-being. Meanwhile, there is mounting critique on the absolute focus of biodiversity conservation on static properties such as species and habitats. The establishment of more ecologically sensible objectives that include ecosystem processes besides species and habitats is put forward as a more effective way of environmental conservation. Methodological approaches increasingly consider ecosystem processes. However, the processes that are included mostly relate to aspects of biodiversity such as dispersal and productivity, and rarely do they include abiolic mechanisms that underlie biodiversity. We here report on the development of a method that integrates two principles which we identify as key to advance the integration of ecosystem services with biodiversity conservation in planning practice: (1) consider the variety of ecosystem processes, biotic as well as abiolic, that support biodiversity and ecosystem services, and (2) link the ecosystem processes to biodiversity and to socio-economic benefits to identify the common ground between seemingly conflicting objectives. The methodology uses a stepwise approach and is based on an extensive review of available knowledge on ecosystem functioning, expert consultation and stakeholder involvement. We illustrate how the methodology supports the setting of strategic goals to accomplish a healthy coastal ecosystem in Belgium, and exemplify how this may affect spatial plans. The aim of this paper is to demonstrate how including processes opens opportunities to align biocliversity and ecosystem services and how this increases chances to provide long-term benefits for biodiversity and human well-being. The paper may provide inspiration to advance current spatial planning approaches. (C) 2020 Elsevier B.V. All rights reserved.",Not About Sufficiency
"Trends, Enablers, and Barriers for Car Ownership","Car ownership has been the object of interest in various research areas. Studies on the topic focus especially on the influence of factors such as environment, life, family events, use, and availability, usually restricted to a given city or region. This paper focuses on recent studies that identified enablers, barriers, and trends related to car ownership in the automotive sector. We use an exploratory-qualitative approach based on systematic mapping of the literature to select papers published between 2019 and March 2021 on the Scopus database. As a result, we first classified the factors that act as enablers or barriers to the acquisition and maintenance of cars. These factors were divided into the following six groups: personal, cultural, political, economic, technological, and environmental. Next, we identified car-ownership-related trends for the automotive sector: purchase replaced by effective use; restrictions on car ownership; autonomous driving; sustainable mobility; new mobility models; customized customer service; real-time integrated modal communication, systematic global traffic, and integrated urban planning. We consider our findings to be relevant contributions to the industry in relation to establishing strategies and public administrators acting in the transportation planning field.",Not About Sufficiency
Artificial Intelligence and Big Data in Public Health,"Artificial intelligence and automation are topics dominating global discussions on the future of professional employment, societal change, and economic performance. In this paper, we describe fundamental concepts underlying AI and Big Data and their significance to public health. We highlight issues involved and describe the potential impacts and challenges to medical professionals and diagnosticians. The possible benefits of advanced data analytics and machine learning are described in the context of recently reported research. Problems are identified and discussed with respect to ethical issues and the future roles of professionals and specialists in the age of artificial intelligence.",Not About Sufficiency
Big-Data-Driven Machine Learning for Enhancing Spatiotemporal Air Pollution Pattern Analysis,"Air pollution is an important problem for public health. The spatiotemporal analysis is a crucial step for understanding the complex characteristics of air pollution. Using many sensors and high-resolution time-step observations makes this task a big data challenge. In this study, unsupervised machine learning algorithms were applied to analyze spatiotemporal patterns of air pollution. The analysis was conducted using PM10 big data collected from almost 100 sensors located in Krakow, over a period of one year, with data being recorded at 1-h intervals. The analysis results using K-means and SKATER clustering revealed distinct differences between average and maximum values of pollutant concentrations. The study found that the K-means algorithm with Dynamic Time Warping (DTW) was more accurate in identifying yearly patterns and clustering in rapidly and spatially varying data, compared to the SKATER algorithm. Moreover, the clustering analysis of data after kriging greatly facilitated the interpretation of the results. These findings highlight the potential of machine learning techniques and big data analysis for identifying hot-spots, coldspots, and patterns of air pollution and informing policy decisions related to urban planning, traffic management, and public health interventions.",Not About Sufficiency
Identification of Review Helpfulness Using Novel Textual and Language-Context Features,"With the increase in users of social media websites such as IMDb, a movie website, and the rise of publicly available data, opinion mining is more accessible than ever. In the research field of language understanding, categorization of movie reviews can be challenging because human language is complex, leading to scenarios where connotation words exist. Connotation words have a different meaning than their literal meanings. While representing a word, the context in which the word is used changes the semantics of words. In this research work, categorizing movie reviews with good F-Measure scores has been investigated with Word2Vec and three different aspects of proposed features have been inspected. First, psychological features are extracted from reviews positive emotion, negative emotion, anger, sadness, clout (confidence level) and dictionary words. Second, readablility features are extracted; the Automated Readability Index (ARI), the Coleman Liau Index (CLI) and Word Count (WC) are calculated to measure the review's understandability score and their impact on review classification performance is measured. Lastly, linguistic features are also extracted from reviews adjectives and adverbs. The Word2Vec model is trained on collecting 50,000 reviews related to movies. A self-trained Word2Vec model is used for the contextualized embedding of words into vectors with 50, 100, 150 and 300 dimensions.The pretrained Word2Vec model converts words into vectors with 150 and 300 dimensions. Traditional and advanced machine-learning (ML) algorithms are applied and evaluated according to performance measures: accuracy, precision, recall and F-Measure. The results indicate Support Vector Machine (SVM) using self-trained Word2Vec achieved 86% F-Measure and using psychological, linguistic and readability features with concatenation of Word2Vec features SVM achieved 87.93% F-Measure.",Not About Sufficiency
"Disaster, relocation, and resilience: recovery and adaptation of Karamemedesane in Lily Tribal Community after Typhoon Morakot, Taiwan","After Typhoon Morakot struck Taiwan in the summer of 2009, government officials relocated the indigenous village communities of Kucapungane, Adiri, Karamemedesane, Kinulane, Dawadawan and Tikuvulu into sub-montane, permanent housing. Because villagers were accustomed to living in mountainous areas, they encountered many challenges while adapting their lifestyle and culture into a new setting. During the relocation process, government and post-disaster relief agencies disregarded, oversimplified, and concealed social vulnerability. Can indigenous communities recover from typhoon damage and continue to pass down their culture? Using in-depth interviews and participant observation, this research examined how Karamemedesane villagers organised and reconstructed themselves using their land for farming practices, culture, rituals, and livelihoods following the government-forced, community migration. The source of resilience for Karamemedesane turned out to be the cultivation of red quinoa, a traditional food crop. Villagers rediscovered the cultural value of food through small changes in farming practices and knowledge, social network and social learning, leadership, and innovation-aided recovery that resulted in establishing the Academy of Special Rukai Crops. Results suggested that post-disaster policies for indigenous communities should be land-based and culturally relevant to promote transformability.",Not About Sufficiency
Economic shocks and infant health: Evidence from a trade reform in Brazil,"Using a uniquely rich set of data sources spanning more than 3,000 Brazilian municipalities over a horizon of 25 years, we investigate the effects of changes in local economic conditions generated by a trade liberalization reform on infant mortality. We exploit variation in import tariff reductions, together with differences in the baseline industry composition across locations, for identification. We find that areas with greater exposure to the trade-induced economic shock experienced a larger decline in infant mortality. In our exploration of mechanisms, we find the most support for the hypothesis that worse labor market opportunities make it less costly undertake health-improving behaviors that are time intensive. Consistent with this hypothesis, we observe a significant decline in female employment rates and an increase in the use of basic health services among women of childbearing ages and infants. We also document that the rollout of a community-based intervention that brings basic health services to the home in a flexible fashion lowers the magnitude of the mortality effect, providing further evidence in favor of the parental time mechanism.",Not About Sufficiency
Heavy Duty Vehicle Fuel Consumption Modelling Based on Exploitation Data by Using Artificial Neural Networks,"One of the ways to improve the fuel economy of heavy duty trucks is to operate the combustion engine in its most efficient operating points. To do that, a mathematical model of the engine is required, which shows the relations between engine speed, torque and fuel consumption in transient states. In this paper, easy accessible exploitation data collected via CAN bus of the heavy duty truck were used to obtain a model of a diesel engine. Various polynomial regression, K-Nearest Neighbor and Artificial Neural Network models were evaluated, and based on RMSE the most relevant sets of parameters for the given algorithm were selected. Finally, the models were compared by using RMSE and Absolute Relative Error scores for 5 test samples. These represent the whole engine's operating area. Apart from goodness of fit, the models were analyzed in terms of sensitivity to the size of the training samples. ANN and KNN proved to be accurate algorithms for modeling fuel consumption by using exploitation data. The ANN model was ranked best, as it required less observations to be trained in order to achieve an absolute relative error which was lower than 5%. A conventional method, i.e. polynomial regression, performed significantly worse than either the ANN or the KNN models. The approach presented in this study shows the potential for using easy accessible exploitation data to modeling fuel consumption of heavy duty trucks. This leads to the reduction of fuel consumption having a clear positive impact on the environment.",Not About Sufficiency
Accurate species identification of food-contaminating beetles with quality-improved elytral images and deep learning,"Food samples are routinely screened for food-contaminating beetles (i.e., pantry beetles) due to their adverse impact on the economy, environment, public health and safety. If found, their remains are subsequently analyzed to identify the species responsible for the contamination; each species poses different levels of risk, requiring different regulatory and management steps. At present, this identification is done through manual microscopic examination since each species of beetle has a unique pattern on its elytra (hardened forewing). Our study sought to automate the pattern recognition process through machine learning. Such automation will enable more efficient identification of pantry beetle species and could potentially be scaled up and implemented across various analysis centers in a consistent manner. In our earlier studies, we demonstrated that automated species identification of pantry beetles is feasible through elytral pattern recognition. Due to poor image quality, however, we failed to achieve prediction accuracies of more than 80%. Subsequently, we modified the traditional imaging technique, allowing us to acquire high-quality elytral images. In this study, we explored whether high-quality elytral images can truly achieve near-perfect prediction accuracies for 27 different species of pantry beetles. To test this hypothesis, we developed a convolutional neural network (CNN) model and compared performance between two different image sets for various pantry beetles. Our study indicates improved image quality indeed leads to better prediction accuracy; however, it was not the only requirement for achieving good accuracy. Also required are many high-quality images, especially for species with a high number of variations in their elytral patterns. The current study provided a direction toward achieving our ultimate goal of automated species identification through elytral pattern recognition.",Not About Sufficiency
"Temporal Analysis of Urban-Suburban PET, mPET and UTCI Indices in Belgrade (Serbia)","The analysis of the bioclimatic conditions is becoming increasingly relevant in climate interpretations for human needs, particularly in spatial planning, tourism, public health, sports events, bio-prognosis, etc. In this context, our study presents general temporal bioclimatic conditions in Belgrade, defined based on the PET, mPET and UTCI heat budget indices. Monthly, seasonal and annual indices were analyzed for urban and suburban weather stations based on 43 annual sets of meteorological data obtained by hourly observations at 7 h and 14 h CET. This study aims to present the distribution of PET, mPET and UTCI indices to show the pattern of each index in a mild climate location and to examine annual and seasonal differences of each index in the Belgrade urban center and suburban part of the city. The study results indicate higher biothermal stress in the urban area compared to the suburban zone and that the indices are congruent during the summer. At the same time, during the winter, they are more difficult to compare due to their peculiarities becoming more noticeable. The results obtained of all mean monthly and mean annual values of all three indices clearly indicate the difference that follows the definition of the urban heat island (UHI), particularly those from morning observation and winter season. The UTCI index shows the most significant monthly, seasonal and annual difference between urban and suburban areas for both observations. The annual difference of Delta UTCI7h amounts to 1.5 degrees C is the same as the annual difference of minimum temperatures (Delta tmin). In contrast, the annual differences of Delta PET7h Delta mPET(7h) are degrees smaller (0.8 degrees C and 0.7 degrees C) and closer to the annual differences of maximum temperatures Delta tmax amounted of 0.6 degrees C.",Not About Sufficiency
Leveraging 3D CAD Data in Product Life Cycle: Exchange - Visualization - Collaboration,"With their practical introduction by the 1970's, virtual product data have emerged to a major technical source of intelligence in manufacturing. Modern organization have since then developed and continuously improved strategies, methods and tools to feed the individual needs of business domains, multidisciplinary teams and supply chain to master the growing complexity of virtual product data and manufacturing processes. Three principal activities are associated to the repurposing of virtual product data. These are Exchange, Visualization and Communication of the manufacturing intelligence from its virtual product representation perspective. One development approach alongside PLM, which declares the 3D CAD model as the record of authority and the source for which all other documentation flows is Model-Based-Design (MBD). By emphasizing digital CAD file use for collaboration at the beginning of development, it is the ground for a fully integrated and collaborative environment founded on a 3D model based definition detailed, documented and shared across the enterprise to enable rapid, seamless, and affordable deployment of products from concept to disposal. Since the practical introduction of virtual product data by the 1970's, several CAD interoperability and visualization formats have indeed been developed to support the aforementioned strategies. Most of them, however, have not yet provided the expected outcome mainly due to their lack of versatility and primary focus on only selected business need. This paper analyses methods and tools used in virtual product development to leverage 3D CAD data across the entire life cycle. It presents a set of versatile concepts for mastering exchange, aware and unaware visualization and collaboration from single technical packages fit purposely for different domains and disciplines.",Not About Sufficiency
Radiological incident: public health countermeasures,"Several measures in a response to a nuclear or radiological emergency have in common the aim of protecting human life and health, among these: to save lives; to avoid or to minimize severe deterministic effects; to provide fist aid, critical medical treatment and to manage the treatment of radiation injuries; to reduce the risk of stochastic effects. In the phase of the urgent response (the first hours or few days from the declaration of the emergency) mitigatory actions have to be taken by the operating personnel of a nuclear facility to prevent the escalation of the emergency and mitigate the consequences of radioactive releases and exposure; along with these, urgent protective actions have to be implemented. Examples of urgent protective actions are: sheltering, evacuation of people residing near the plant and iodine thyroid blocking (ITB): these actions can be also precautionary if taken before or immediately after the beginning of the radioactive release. In the second phase of the emergency (the early response phase) which can last days or weeks, early protective actions, like relocation, restrictions on the food chain and on water supply etc., should be taken. The mitigatory and protective actions should be part of a general protective strategy of the population, based on generic criteria and generic guidance values for restricting exposure of the emergency workers and of the general population.",Not About Sufficiency
The global sustainability footprint of sovereign wealth funds,"With the emergence of sovereign wealth funds (SWFs) around the world managing equity of over $8 trillion, their impact on the corporate landscape and social welfare is being scrutinized. This study investigates whether and how SWFs incorporate environmental, social, and governance (ESG) considerations in their investment decisions in publicly listed corporations, as well as the subsequent evolution of target firms' ESG performance. We find that SWF funds do consider the level of past ESG performance as well as recent ESG score improvement when taking ownership stakes in listed companies. These results are driven by the SWF funds that do have an explicit or implicit ESG policy and are most transparent, and by SWF originating from developed countries and countries with civil law origins. In relation to engagement, we find by means of two natural experiments with exogenous shocks (the Deepwater Horizon catastrophe and Volkwagen diesel scandal) that the ESG scores do not change significantly more for firms in which SWFs have ownership stakes. This potentially suggests that SWFs in general do not actively steer their target firms towards higher levels of ESG.",Not About Sufficiency
"Innovations in spatial planning as a social process - phases, actors, conflicts","The aim of this paper is to understand the social process of the emergence and institutionalization of innovations in spatial planning (which we describe as 'social innovations'). The paper is based on a recently finished empirical and comparative study conducted in four distinct areas of spatial planning in Germany: urban design, neighbourhood development, urban regeneration and regional planning. The empirical cases selected in these areas encompass different topics, historical periods, degrees of maturity and spatial scales of innovation. As a temporal structure of the innovation processes in the different cases we identified five phases: 'incubating, generating, formatting, stabilizing, adjusting'. In a cross-comparison of the case studies and along these phases, we furthermore found typical (groups of) actors, tensions and conflicts. In the focus of our case analyses are the following dimensions: (1) the content of the innovations, (2) actors, networks and communities involved as well as (3) institutions and institutionalization.",Not About Sufficiency
Entity Resolution with Recursive Blocking,"Entity resolution is a well-known challenge in data management for the lack of unique identifiers of records and various errors hidden in the data, undermining the identifiability of entities they refer to. To reveal matching records, every record potentially needs to be compared with all other records in the database, which is computationally intractable even for moderately-sized databases. To circumvent this quadratic challenge, blocking methods are typically employed to facilitate restricting promising comparisons of pairs within small subsets, called blocks, of records. Existing effective methods typically rely on blocking keys created by experts to capture matches, which inevitably involves a large amount of human labor and do not guarantee high-quality results. To reduce manual labor and promote accuracy, machine learning approaches are investigated to meet the challenge with limited success, due to high requirements of training data and inefficiency, especially for large databases. The exhaustive method produces exact results but suffers from efficiency problems. In this paper, we propose a paradigm of divide-and-conquer entity resolution, named recursive blocking, which derives comparatively good results while largely alleviating efficiency concerns. Specifically, recursive blocking refines blocks and traps matches in an iterative fashion to derive high-quality results, and we study two types of recursive blocking, i.e. redundancy- and partition-based approaches, and investigate their relative performance. Comprehensive experiments on both real-world and synthetic datasets verified the superiority of our approaches over the existing ones. (C) 2020 Elsevier Inc. All rights reserved.",Not About Sufficiency
"Spatial ecology of Bay of Quinte walleye (Sander vitreus): Annual timing, extent, and patterns of migrations in eastern Lake Ontario","Walleye (Sander vitreus) are the top nearshore predator in the Bay of Quinte and eastern Lake Ontario, where they have strong ecological and socio-economic impacts. The population is known to migrate seasonally; however, the precise timing and extent are not well defined. This study used acoustic telemetry to provide a fisheries-independent measure of timing, extent, and seasonal distribution of migration in Lake Ontario and to examine the influence of sex, spawning river, size, and year. Annual detection histories were used to determine the timing of migrations into and out of the spawning rivers, departure from the Bay of Quinte post-spawn, and the pre-spawn return to the Bay of Quinte. Sequence analysis was subsequently used to examine how fish occupy defined regions of eastern Lake Ontario annually and identify patterns in migration strategy. Spawning site fidelity was high for both rivers (91-97%) and annual residency within the Bay of Quinte was low (9.5%). Females spent less time in spawning rivers, migrated to the main lake earlier, and generally travelled further than males. Larger fish also migrated to the main lake first and travelled further, and differences in timing between spawning rivers were minor. Annual differences in timing were observed and were most likely related to environmental differences between years. Cluster analysis was used to identify groups of fish which utilized unique annual migration strategies and demonstrated sex and size had an important influence on the variability in annual spatial occupancy, but the importance of spawning river and year was minimal. (c) 2021 The Authors. Published by Elsevier B.V. on behalf of International Association for Great Lakes Research. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/ licenses/by-nc-nd/4.0/).",Not About Sufficiency
Automatic skin lesions detection from images through microscopic hybrid features set and machine learning classifiers,"Skin cancer occurrences increase exponentially worldwide due to the lack of awareness of significant populations and skin specialists. Medical imaging can help with early detection and more accurate diagnosis of skin cancer. The physicians usually follow the manual diagnosis method in their clinics but nonprofessional dermatologists sometimes affect the accuracy of the results. Thus, the automated system is required to assist physicians in diagnosing skin cancer at early stage precisely to decrease the mortality rate. This article presents an automatic skin lesions detection through a microscopic hybrid feature set and machine learning-based classification. The employment of deep features through AlexNet architecture with local optimal-oriented pattern can accurately predict skin lesions. The proposed model is tested on two open-access datasets PAD-UFES-20 and MED-NODE comprising melanoma and nevus images. Experimental results on both datasets exhibit the efficacy of hybrid features with the help of machine learning. Finally, the proposed model achieved 94.7% accuracy using an ensemble classifier.",Not About Sufficiency
Machine learning health-related applications in low-income and middle-income countries: a scoping review protocol,"Introduction Machine learning (ML) has been used in bio-medical research, and recently in clinical and public health research. However, much of the available evidence comes from high-income countries, where different health profiles challenge the application of this research to low/middle-income countries (LMICs). It is largely unknown what ML applications are available for LMICs that can support and advance clinical medicine and public health. We aim to address this gap by conducting a scoping review of health-related ML applications in LMICs. Methods and analysis This scoping review will follow the methodology proposed by Levac et al. The search strategy is informed by recent systematic reviews of ML health-related applications. We will search Embase, Medline and Global Health (through Ovid), Cochrane and Google Scholar; we will present the date of our searches in the final review. Idles and abstracts will be screened by two reviewers independently; selected reports will be studied by two reviewers independently. Reports will be included if they are primary research where data have been analysed, ML techniques have been used on data from LMICs and they aimed to improve health-related outcomes. We will synthesise the information following evidence mapping recommendations. Ethics and dissemination The review will provide a comprehensive list of health-related ML applications in LMICs. The results will be disseminated through scientific publications. We also plan to launch a website where ML models can be hosted so that researchers, policymakers and the general public can readily access them.",Not About Sufficiency
Green Enough? A dose-response curve of the impact of street greenery levels and types on perceived happiness,"Although research shows that individuals report higher levels of happiness when viewing green environments, the dose curve describing the impact of greenery on happiness remains undefined. Current literature only presents dose curves representing the associations between stress recovery and tree coverage, and does not explore how this fluctuates for different types of green infrastructure. Using an image-based randomised control trial with 401 participants, this study assesses the impact of levels and types of street greenery on people's perceptions of happiness. Participants were randomly assigned to rate one of six images representing proportional increments of street greenery coverage (from 0% to 45%) across three greenery configurations - ground level, between buildings, and vertical (on building fa & ccedil;ades). The results suggest that the highest levels of perceived happiness are obtained from green coverage of between 35% and 45%, at which point the effect levels off. Vertical greenery coverage has a larger impact on perceived happiness than the other two tested configurations, and has a positive linear rather than concave relationship. The study indicates that viewing greater amounts of greenery significantly enhances communities' perceived happiness, and shows that the relationship is dependent on the type of green infrastructure configuration used.",Not About Sufficiency
An integrated sustainability assessment of synergistic supply of energy and water in remote communities,"The success of deploying energy and water technologies in remote communities in developing countries can be improved by considering their synergistic relationships and their social, economic and environmental implications. This paper first evaluates social implications of current energy and water supply in a prototypical remote community against five future (2030) scenarios for synergistic provision of electricity, heat for cooking and water. This is followed by an integrated assessment of the social, environmental and economic life cycle sustainability through multicriteria decision analysis. The Business-as-usual (BAU) scenario shows high life cycle health impacts but low impacts from local air pollution. The contrary is true for the Independent and Advanced Independent scenarios which assume community self-sufficiency in energy and water supply. Greater access to electricity and water in the Advanced and Advanced Independent scenarios increases the potential for human development and security of supply, but there is an increase in the risk of accidents and decrease in social acceptability of the water supply. Similarly, a transition towards clean cooking fuels away from traditional solid biomass reduces local air pollution but increases reliance on imported fuels (BAU and Advanced scenarios). The Transition scenario is socially the most sustainable option, while Independent and Advanced Independent are the best options environmentally. They also have the lowest total operating costs, but have higher capital requirements than most other scenarios. Overall, unless extreme preferences for either environmental or social aspects are adopted, the Transition and Independent scenarios emerge as the most sustainable options. This suggests that current energy and water supply to remote communities can be transitioned sustainably to a self-sufficient system that does not depend on imported resources. The scenarios developed in this work present a framework for an integrated design and evaluation of energy and water supply in remote communities with the aim of aiding stakeholders in defining sustainable transition pathways. (C) 2020 The Authors. Published by Elsevier B.V. on behalf of Institution of Chemical Engineers.",Not About Sufficiency
Automatic Cyberbullying Detection: a Mexican case in High School and Higher Education students,"The social interaction among young students has been partially or totally transformed to mobile-based communication, specifically through the use of social networks. This new communication environment has allowed a more immediate, diverse and massive interaction, offering a faster and more effective situation when carrying out academic and recreational activities. However, this scenario has also promoted the phenomenon of social harassment known as bullying, exponentially increasing its scope and diversifying the types and forms of aggression. Machine learning and natural language processing techniques have been used to create models that detect bullying situations among students, using data corpus from mainly public social networks. However, generally, these data sources are not representative of the social networks commonly used by the students; generating classification models that do not consider the vocabulary used by this social group. This article describes the methodology used to create a representative data corpus of the interaction between Mexican high school and university students, and a comparative analysis on characteristics that influence the quality of the content of a corpus in this domain. In addition, the performance achieved by implementing various machine learning models to identify bullying situations is presented. The best result is reported for the Naive Bayesian classifier (F1-Score of 0.862), performing better than models based on deep learning such as Recurrent (F1-Score of 0.845) and Convolutional (F1-Score of 0.807) Neural Networks.",Not About Sufficiency
Bio-Signal Complexity Analysis in Epileptic Seizure Monitoring: A Topic Review,"Complexity science has provided new perspectives and opportunities for understanding a variety of complex natural or social phenomena, including brain dysfunctions like epilepsy. By delving into the complexity in electrophysiological signals and neuroimaging, new insights have emerged. These discoveries have revealed that complexity is a fundamental aspect of physiological processes. The inherent nonlinearity and non-stationarity of physiological processes limits the methods based on simpler underlying assumptions to point out the pathway to a more comprehensive understanding of their behavior and relation with certain diseases. The perspective of complexity may benefit both the research and clinical practice through providing novel data analytics tools devoted for the understanding of and the intervention about epilepsies. This review aims to provide a sketchy overview of the methods derived from different disciplines lucubrating to the complexity of bio-signals in the field of epilepsy monitoring. Although the complexity of bio-signals is still not fully understood, bundles of new insights have been already obtained. Despite the promising results about epileptic seizure detection and prediction through offline analysis, we are still lacking robust, tried-and-true real-time applications. Multidisciplinary collaborations and more high-quality data accessible to the whole community are needed for reproducible research and the development of such applications.",Not About Sufficiency
Virtual reality-assessment of social interactions and prognosis in depression,"Background: Freud proposed that excessive self-blame-related motivations such as self-punishing tendencies play a key role in depression. Most of the supporting evidence, however, is based on cross-sectional studies and questionnaire measures. Methods: In this pre-registered (NCT04593537) study, we used a novel Virtual Reality (VR) task to determine whether maladaptive self-blame-related action tendencies prospectively identify a subgroup of depression with poor prognosis when treated as usual over four months in primary care. Ninety-eight patients with depression (Patient Health Questionnaire-9 >= 15), screening negatively for bipolar and alcohol/substance use disorders, completed the VR-task at baseline (n = 93 completed follow-up). Results: Our pre-registered statistical/machine learning model prospectively predicted a cross-validated 19 % of variance in depressive symptoms. Contrary to our specific predictions, and in accordance with Freud's observations, feeling like punishing oneself emerged as prognostically relevant rather than feeling like hiding or creating a distance from oneself. Using a principal components analysis of all pre-registered continuous measures, a factor most strongly loading on feeling like punishing oneself for other people's wrongdoings (beta = 0.23, p = 0.01), a baseline symptom factor (beta = 0.30, p = 0.006) and Maudsley Staging Method treatment-resistance scores (beta = 0.28, p = 0.009) at baseline predicted higher depressive symptoms after four months. Limitations: Patients were not assessed with a diagnostic interview. Conclusions: Independently and apart from known clinical variables, feeling like punishing oneself emerged as a distinctly relevant prognostic factor and should therefore be assessed and tackled in personalised care pathways for difficult-to-treat depression.",Not About Sufficiency
Assessing growth potential of careers with occupational mobility network and ensemble framework,"The growth potential of a career reflects its future prospects and is an important consideration for individuals and organizations when career planning. There is still a lack of quantitative assessment tools for growth potential of careers. In this study, considering the key role of human capital in human resource management, as well as the excellent performance of complex network and machine learning in big data analysis and prediction, a career growth potential assessment model with human capital ensemble is proposed through human capital-based occupational mobility network and ensemble learning. First, an occupational mobility network is constructed based on online professional dataset to associate occupations with each other. Then, five dimensions of human capital measurements are designed to quantify human capital in terms of education, experience, social capital, occupational size, and concentration. These are then combined with the occupational mobility network to create a new network that depicts human capital flows among occupations. Finally, an ensemble framework for assessing career growth potential is constructed to integrate multidimensional human capital information in the network and obtain quantitative scores of growth potential. This study is the original attempt to adopt a data-driven idea and an intelligent approach to understand career growth potential. The experimental results show that it also makes a useful exploration for modeling human capital flows and intelligent assessment of career prospects.",Not About Sufficiency
Identification of prognostic coagulation-related signatures in clear cell renal cell carcinoma through integrated multi-omics analysis and machine learning,"Clear cell renal cell carcinoma is a threat to public health with high morbidity and mortality. Clinical evidence has shown that cancer-associated thrombosis poses significant challenges to treatments, including drug resistance and difficulties in surgical decision-making in ccRCC. However, the coagulation pathway, one of the core mechanisms of cancer-associated thrombosis, recently found closely related to the tumor microenvironment and immune-related pathway, is rarely researched in ccRCC. Therefore, we integrated bulk RNA-seq data, DNA mutation and methylation data, single-cell data, and proteomic data to perform a comprehensive analysis of coagulation-related genes in ccRCC. First, we demonstrated the importance of the coagulation-related gene set by consensus clustering. Based on machine learning, we identified 5 coagulation signature genes and verified their clinical value in TCGA, ICGC, and E-MTAB-1980 databases. It's also demonstrated that the specific expression patterns of coagulation signature genes driven by CNV and methylation were closely correlated with pathways including apoptosis, immune infiltration, angiogenesis, and the construction of extracellular matrix. Moreover, we identified two types of tumor cells in single-cell data by machine learning, and the coagulation signature genes were differentially expressed in two types of tumor cells. Besides, the signature genes were proven to influence immune cells especially the differentiation of T cells. And their protein level was also validated.",Not About Sufficiency
Reference dataset for rate of penetration benchmarking,"In recent years, there were multiple papers published related to rate of penetration prediction using machine learning vastly outperforming analytical methods. There are models proposed reportedly achieving R2 values as high as 0.996. Unfortunately, it is most often impossible to independently verify these claims as the input data is rarely accessible to others. To solve this problem, this paper presents a database derived from Equinor's public Volve dataset that will serve as a benchmark for rate of penetration prediction methods. By providing a partially processed dataset with unambiguous testing scenarios, scientists can perform machine learning research on a level playing field. This in turn will both discourage publication of methods tested in a substandard manner as well as promote exploration of truly superior solutions. A set of seven wells with nearly 200-000 samples and twelve common attributes is proposed together with reference results from common machine learning algorithms. Data and relevant source code are published on the pages of University of Stavanger and GitHub.",Not About Sufficiency
Machine learning formation enthalpies of intermetallics,"Developing fast and accurate methods to discover intermetallic compounds is relevant for alloy design. While density-functional-theory (DFT)-based methods have accelerated design of binary and ternary alloys by providing rapid access to the energy and properties of the stable intermetallics, they are not amenable for rapidly screening the vast combinatorial space of multi-principal element alloys (MPEAs). Here, a machine-learning model is presented for predicting the formation enthalpy of binary intermetallics and is used to identify new ones. The model uses easily accessible elemental properties as descriptors and has a mean absolute error of 0.025eV/atom in predicting the formation enthalpy of stable binary intermetallics reported in the Materials Project database. The model further predicts stable intermetallics to form in 112 binary alloy systems that do not have any stable intermetallics reported in the Materials Project database. DFT calculations confirm one such stable intermetallic identified by the model, NbV2, to be on the convex hull. Furthermore, an adaptive transfer learning method is used to generalize the model to predict ternary intermetallics with a similar accuracy as DFT, which suggests that it could be extended to identify compositionally complex intermetallics that may form in MPEAs.",Not About Sufficiency
Machine Learning for Network Resilience: The Start of a Journey,"Security is one of the main concerns facing the development of new projects in networking and communications. Another challenge is to verify that a system is working exactly as specified. On the other hand, advances in Artificial Intelligence (AI) technology have opened up new markets and opportunities for progress in critical areas such as network resiliency, health, education, energy, economic inclusion, social welfare, and the environment. AI is expected to play an increasing role in defensive and offensive measures to provide a rapid response to react to the landscape of evolving threats. Software Defined Networking (SDN), being centralized by nature, provides a global view of the network. It is the flexibility and robustness offered by programmable networking that lead us to consider the integration of these two concepts, SDN and AI. Inspired by the fascinating tactics of the human immunity system, we aim to design a general hybrid Artificial Intelligence Resiliency System (ARS) that strikes a good balance between centralized and distributed security systems that may be applicable to different network environments. In addition, we aim to investigate and leverage the latest AI techniques to improve network performance in general and resiliency in particular.",Not About Sufficiency
Discovery of potent Covid-19 main protease inhibitors using integrated drug-repurposing strategy,"The emergence and rapid spreading of novel SARS-CoV-2 across the globe represent an imminent threat to public health. Novel antiviral therapies are urgently needed to overcome this pandemic. Given the significant role of the main protease of Covid-19 for virus replication, we performed a drug-repurposing study using the recently deposited main protease structure, 6LU7. For instance, pharmacophore- and e-pharmacophore-based hypotheses such as AARRH and AARR, respectively, were developed using available small molecule inhibitors and utilized in the screening of the DrugBank repository. Further, a hierarchical docking protocol was implemented with the support of the Glide algorithm. The resultant compounds were then examined for their binding free energy against the main protease of Covid-19 by means of the Prime-MM/GBSA algorithm. Most importantly, the machine learning-based AutoQSAR algorithm was used to predict the antiviral activities of resultant compounds. The hit molecules were also examined for their drug-likeness and toxicity parameters through the QikProp algorithm. Finally, the hit compounds activity against the main protease was validated using molecular dynamics simulation studies. Overall, the present analysis yielded two potential inhibitors (DB02986 and DB08573) that are predicted to bind with the main protease of Covid-19 better than currently used drug molecules such as N3 (cocrystallized native ligand), lopinavir, and ritonavir.",Not About Sufficiency
"Presyncodon, a Web Server for Gene Design with the Evolutionary Information of the Expression Hosts","In the natural host, most of the synonymous codons of a gene have been evolutionarily selected and related to protein expression and function. However, for the design of a new gene, most of the existing codon optimization tools select the high-frequency-usage codons and neglect the contribution of the low-frequency-usage codons (rare codons) to the expression of the target gene in the host. In this study, we developed the method Presyncodon, available in a web version, to predict the gene code from a protein sequence, using built-in evolutionary information on a specific expression host. The synonymous codon-usage pattern of a peptide was studied from three genomic datasets (Escherichia coli, Bacillus subtilis, and Saccharomyces cerevisiae). Machine-learning models were constructed to predict a selection of synonymous codons (low- or high-frequency-usage codon) in a gene. This method could be easily and efficiently used to design new genes from protein sequences for optimal expression in three expression hosts (E. coli, B. subtilis, and S. cerevisiae). Presyncodon is free to academic and noncommercial users; accessible at http://www.mobioinfor.cn/presyncodon_www/index.html.",Not About Sufficiency
Machine learning and experiments A synergy for the development of functional materials,"With machine learning (ML) and artificial intelligence (AI) becoming increasingly refined and accessible, computer engineers and materials scientists are utilizing these data-driven techniques to design new functional materials more efficiently. Additionally, the advancement of simulation software and computing power has substantially lowered the cost of obtaining training data. However, using only simulation data presents a difficulty in the eventual realization of a material design due to possible misalignment of the simulation setup and physical laboratory conditions. Therefore, it is mutually beneficial to also improve the experimental aspect of functional materials development using ML and AI techniques. In this article, we survey the current state of ML/AI involvement in functional materials design, focusing specifically on acoustic/mechanical metamaterials, piezoelectric materials, and biological materials. The macroscopic nature of these functional materials lends well to additive manufacturing fabrication, which makes optimizing the synthesis process of these materials highly desirable. We conclude by pointing out a few promising directions for future investigation of functional materials and their place in societal applications.",Not About Sufficiency
Brief Communication: Concepts and Terms - ISKO's Major Challenge,"Starting from the premise that extant knowledge of the discipline of Knowledge Organization ought to be made accessible by its knowledge units (concepts) this article includes short descriptions of the work of E.Wuester (Austria) and F. Riggs (USA) who both had laid foundations in this field. A noematic concept of knowledge (Diemer 1962, 474) is used for the necessary work to be done. It is shown how a concept-theoretical approach (relying on the characteristics of concepts and their system-building capacity) can be applied for pertinent terminological work. Earlier work in this regard by standardization bodies is mentioned. Seven necessary steps towards accomplishment are outlined.",Not About Sufficiency
MPIGate: A Solution to use Heterogeneous Networks for Assisted Living Applications,"Existing sensors and actuators, that can be used in an AAL (ambient assisting living) environment, work on heterogeneous network protocols, e. g., WiFi, Bluetooth, EIB/KNX and Zigbee. For a given sensor/actuator, the choice of its underlying communication protocol is optimized according to its bandwidth and/or energy needs. However, integrating sensors/actuators of heterogeneous network protocols into an AAL system arises interoperability problems. In this paper, we present MPIGate, a multi-protocol gateway and interface for assisted living applications. Besides its multiple communication drivers for supporting the different protocols, MPIGate also proposes a database for storing the last updated sensor data as well as an user interface for both transparent data access and easy application development. MPIGate has been deployed in a smart home technical test bed at our LORIA labs. We presents some first results and discuss the lessons learnt.",Not About Sufficiency
Transforming Points of Single Contact Data into Linked Data,"Open data portals contain valuable information for citizens and business. However, searching for information can prove to be tiresome even in portals tackling domains similar information. A typical case is the information residing in the European Commission's portals supported by Member States aiming to facilitate service provision activities for EU citizens and businesses. The current work followed the FAIR principles (Findability, Accessibility, Interoperability, and Reuse of digital assets) as well as the GO-FAIR principles and tried to transform raw data into fair data. The innovative part of this work is the mapping of information residing in various governmental portals (Points of Single Contacts) by transforming information appearing in them in RDF format (i.e., as Linked data), in order to make them easily accessible, exchangeable, interoperable and publishable as linked open data. Mapping was performed using the semantic model of a single portal, i.e., the enriched Greek e-GIF ontology and by retrieving and analyzing raw, i.e., non-FAIR data, by defining the semantic model and by making data linkable. The Data mapping process proved to require a significant manual effort and revealed that data value remains unexplored due to poor data representation. It also highlighted the need for appropriately designing and implementing horizontal actions addressing an important number of recipients in an interoperable way.",Not About Sufficiency
Perspectives on current approaches to virtual screening in drug discovery,"Introduction: For the past two decades, virtual screening (VS) has been an efficient hit finding approach for drug discovery. Today, billions of commercially accessible compounds are routinely screened, and many successful examples of VS have been reported. VS methods continue to evolve, including machine learning and physics-based methods. Areas covered: The authors examine recent examples of VS in drug discovery and discuss prospective hit finding results from the critical assessment of computational hit-finding experiments (CACHE) challenge. The authors also highlight the cost considerations and open-source options for conducting VS and examine chemical space coverage and library selections for VS. Expert opinion: The advancement of sophisticated VS approaches, including the use of machine learning techniques and increased computer resources as well as the ease of access to synthetically available chemical spaces, and commercial and open-source VS platforms allow for interrogating ultra-large libraries (ULL) of billions of molecules. An impressive number of prospective ULL VS campaigns have generated potent and structurally novel hits across many target classes. Nonetheless, many successful contemporary VS approaches still use considerably smaller focused libraries. This apparent dichotomy illustrates that VS is best conducted in a fit-for-purpose way choosing an appropriate chemical space. Better methods need to be developed to tackle more challenging targets.",Not About Sufficiency
Predictive spreadsheet autocompletion with constraints,"Spreadsheets are arguably the most accessible data-analysis tool and are used by millions of people. Despite the fact that they lie at the core of most business practices, working with spreadsheets can be error prone, usage of formulas requires training and, crucially, spreadsheet users do not have access to state-of-the-art analysis techniques offered by machine learning. To tackle these issues, we introduce the novel task of predictive spreadsheet autocompletion, where the goal is to automatically predict the missing entries in the spreadsheets. This task is highly non-trivial: cells can hold heterogeneous data types and there might be unobserved relationships between their values, such as constraints or probabilistic dependencies. Critically, the exact prediction task itself is not given. We consider a simplified, yet non-trivial, setting and propose a principled probabilistic model to solve it. Our approach combines black-box predictive models specialized for different predictive tasks (e.g., classification, regression) and constraints and formulas detected by a constraint learner, and produces a maximally likely prediction for all target cells that is consistent with the constraints. Overall, our approach brings us one step closer to allowing end users to leverage machine learning in their workflows without writing a single line of code.",Not About Sufficiency
An analysis of COVID-19 vaccine hesitancy in the U.S.,"Reluctance or refusal to get vaccinated, commonly known as Vaccine Hesitancy (VH), poses a significant challenge to COVID-19 vaccination campaigns. Understanding the factors contributing to VH is essential for shaping effective public health strategies. This study proposes a novel framework for combining machine learning with publicly available data to generate a proxy metric that evaluates the dynamics of VH faster than the currently used survey methods. The metric is input to descriptive classification models that analyze a wide array of data, aiming to identify key factors associated with VH at the county level in the U.S. during the COVID-19 pandemic (i.e., January to October 2021). Both static and dynamic factors are considered. We use a Random Forest classifier that identifies political affiliation and Google search trends as the most significant factors influencing VH behavior. The model categorizes U.S. counties into five distinct clusters based on VH behavior. Cluster 1, with low VH, consists mainly of Democratic-leaning residents who, have the longest life expectancy, have a college degree, have the highest income per capita, and live in metropolitan areas. Cluster 5, with high VH, is predominantly Republican-leaning individuals in non-metropolitan areas. Individuals in Cluster 1 is more responsive to vaccination policies.",Not About Sufficiency
Urban or Rural: Where are people happier and why?,"Using data from a worldwide sample, we investigate how happy people look like and if these ""happiness characteristics"" are more present in big urban towns or in small rural villages. We found evidence that (i) people seem to be slightly happier in rural settlements, (ii) happier people have some particular characteristics (e.g., higher levels of trust in others and being more interested in politics) and (iii) these positive attitudes are slightly more present in rural contexts. Then, we discuss some conceivable explanations to what we have seen.",Not About Sufficiency
Promoting planning for housing development: What can Sweden learn from Germany?,"In the years 2006-2014, urban planning reform was seen as the major remedy against housing shortage in Sweden. The present government has the continuation of such reform on its agenda, but as of yet has made no proposals; instead, other housing policy measures have been introduced. In light of the uncertainty as to the future course of urban planning reform, possible future steps can be discussed. This article accordingly investigates whether German urban planning law and implementation could provide interesting reference points for discussion of further urban planning reform in Sweden, and if so, what parts of the German experience should be the center of attention. The article covers three aspects of German planning that influence the uncertainty, duration, and cost of residential planning as well as social goals addressed through planning: planning law, focusing on facilitated planning procedures of German planning law, measures taken in the organization of planning authorities to make development planning more efficient, and planning-related city demands for affordable housing. The conclusions encompass proposals for the further reform of the Swedish planning process in the form of a facilitated and accelerated development planning procedure for housing projects, as well as the introduction of private initiative in development planning. Further improvements to the organization and incentives of planning authorities are proposed. More research is required into municipal demands for affordable housing in the form of inclusionary zoning; such research should draw on the extensive international experience of such zoning, relating it to a Swedish pilot project. (C) 2017 Elsevier Ltd. All rights reserved.",Not About Sufficiency
Building the European Social Innovation Database with Natural Language Processing and Machine Learning,"Social innovation is widely defined as technological and non-technological new products, services or models that simultaneously meet social needs and create new social relationships or collaborations. Despite a significant interest in the concept, the lack of reliable and comprehensive data is a barrier for social science research. We created the European Social Innovation Database (ESID) to address this gap. ESID is based on the idea of large-scale collection of unstructured web site text to classify and characterise social innovation projects from around the world. We use advanced machine learning techniques to extract features such as social innovation dimensions, project locations, summaries, and topics, among others. Our models perform as high as 0.90 F1. ESID currently includes 11,468 projects from 159 countries. ESID data is available freely and also presented in a web-based app. Our future workplan includes expansion (i.e., increasing the number of projects), extension (i.e., adding new variables) and dynamic retrieval (i.e., retrieving and extracting information in regular intervals).",Not About Sufficiency
Toward a Global Genomic Epidemiology of Meningococcal Disease,"Whole-genome sequencing (WGS) is invaluable for studying the epidemiology of meningococcal disease. Here we provide a perspective on the use of WGS for meningococcal molecular surveillance and outbreak investigation, where it helps to characterize pathogens, predict pathogen traits, identify emerging pathogens, and investigate pathogen transmission during outbreaks. Standardization of WGS workflows has facilitated their implementation by clinical and public health laboratories (PHLs), but further development is required for metagenomic shotgun sequencing and targeted sequencing to be widely available for culture-free characterization of bacterial meningitis pathogens. Internet-accessible servers are being established to support bioinformatics analysis, data management, and data sharing among PHLs. However, establishing WGS capacity requires investments in laboratory infrastructure and technical knowledge, which is particularly challenging in resource-limited regions, including the African meningitis belt. Strategic WGS implementation is necessary to monitor the molecular epidemiology of meningococcal disease in these regions and construct a global view of meningococcal disease epidemiology.",Not About Sufficiency
Smart City Data Analysis,"Smart City is one of the vital issues in the next coming years as it is estimated that more number of people will he migrating towards city and by 2040 cities is populated by 70'N of the world's population. This will give raise to the city management problems like traffic congestion, parking queues, capacity planning and continuous depreciation in energy. Hence there is a need to understand the upcoming issues and find an efficient way to resolve them by resource planning. The evolution of city to Smart City demand information and communications technology (ICT) to gather data. The data is collected based on city infrastructure and conduct of people in the city. This big data is analyzed to make knowledgeable decisions to give quality of life to the citizens. It is referred as the evolution of intelligent city to the next generation city of information technology. ICT provides real time processing of data to gather useful information. Based on this information visual applications can be developed to have a more farsighted view of the Smart City. Online banking, online shopping and remote video communication are some of the applications of Smart City. As one of the concepts to improve city structure and its facilities, Smart City is vital in city management and development and uses information and communication technology (ICT) for city evolution and fast growth. These cities are planned to enhance more business and industrial growth thus increasing in economic development and raising living standard of the inhabitants [1]. It offers various facilities to the city like better transportation in the city, more job opportunities, less travelling time, healthy environment, better healthcare, less crime etc. Smart City is a concept which works with sensors and data analytics to enhance the standard and quality of life in the cities. Smart City deals with the challenges like traffic, parking, capacity planning, energy etc. and resolves them to provide better city facilities to their citizens [2]. 1..C.17 is becoming widespread in the cities and is being used as a tool to deal with Smart City application domains. Smart City is using big data to generate intelligent information systems which support decision making capabilities. With effective data sources and data analytics tool we can design high end services for the citizens in urban cities. Big data has changed the way we store data and process the data efficiently. Now this crucial data is not only used to get the historic trends information but also used to predict the future. It is significant in planning the future of the cities. Data is analyzed to develop action plans from the predicted results. in this paper, various datasets for traffic management and parking garage management have been tested and evaluated by using data analytics techniques in order to find a pattern in a historical information. Based on this research this information is further analyzed by machine learning tools and algorithms to predict the future behavior and help on taking decision. Machine learning and Analytics tools for Data Analytics from Hadoop and Elasticsearch environments as well as Mining algorithms from Python library are used for data prediction.",Not About Sufficiency
"No justice, no streets: The complex task of evaluating environmental justice on open streets in three US cities","In this paper, we study open street initiatives through a holistic definition of environmental justice, shedding light on three potential paradoxes of such initiatives: the engagement, hegemony, and displacement paradoxes. We use a mixed-methods approach integrating interviews and spatial analyses, focusing on three cities with permanent programs: Denver, Oakland, and Seattle. Our findings for the engagement paradox show that cities with existing equity planning relationships were better suited to address procedural justice tensions between the need to act swiftly due to the COVID-19 pandemic and the necessity to adequately engage racially/ethnically minoritized communities in planning open streets. For the hegemony paradox, we find a tension between distributional and recognitional justice, wherein open streets might have been available in minoritized communities but such streets did not meet their needs. In the displacement paradox, respondents suggested that green gentrification concerns were a barrier to the equitable implementation of open streets.",Not About Sufficiency
Automatic segmentation and classification of mice ultrasonic vocalizations,"This paper addresses the development of a system for classifying mouse ultrasonic vocalizations (USVs) present in audio recordings. The automatic labeling process for USVs is usually divided into two main steps: USV segmentation followed by the matching classification. Three main contributions can be highlighted: (i) a new segmentation algorithm, (ii) a new set of features, and (iii) the discrimination of a higher number of classes when compared to similar studies. The developed segmentation algorithm is based on spectral entropy analysis. This novel segmentation approach can detect USVs with 94% and 74% recall and precision, respectively. When compared to other methods/software, our segmentation algorithm achieves a higher recall. Regarding the classification phase, besides the traditional features from time, frequency, and time-frequency domains, a new set of contour-based features were extracted and used as inputs of shallow machine learning classification models. The contour-based features were obtained from the time-frequency ridge representation of USVs. The classification methods can differentiate among ten different syllable types with 81.1% accuracy and 80.5% weighted F1-score. The algorithms were developed and evaluated based on a large dataset, acquired on diverse social interaction conditions between the animals, to stimulate a varied vocal repertoire. C2022 Author(s). All article content, except where otherwise noted, is licensed under a Creative Commons Attribution (CC BY) license (http://creativecommons.org/licenses/by/4.0/).",Not About Sufficiency
Applications of artificial intelligence for energy efficiency throughout the building lifecycle: An overview,"The use of Artificial Intelligence (AI) technologies in buildings can assist in reducing energy consumption through enhanced control, automation, and reliability. This review aims to explore the use of AI to enhance energy efficiency throughout various stages of the building lifecycle, including building design, construction, operation and control, maintenance, and retrofit. The review encompasses multiple studies in the field published between 2018 and 2023. These studies were identified through keyword searches that best represent the topic, using various research databases. In addition to summarizing the technologies and approaches related to AI and energy efficiency, this review discusses future opportunities for the application of AI in energy efficiency within these lifecycle stages. The review highlights that AI-based solutions are currently employed in building design generation and optimization, decision-making, predictive and adaptive control, fault detection and diagnosis, as well as energy benchmarking. These applications effectively facilitate energy efficiency in buildings to meet today's energy needs. However, further research is needed to explore the use of AI in the construction phase to support the development of energy-efficient construction techniques and systems, in addition to scheduling and predictive decision-making.",Not About Sufficiency
Rapid Screening Strategy of 2D Materials for Li-Ion Battery (LIB) Electrode Based on Deep Neural Networks (DNN),"The development of electrode materials is crucial for achieving an optimal performance in secondary ion batteries. Previous research has accumulated a substantial amount of data on electrode materials, creating varied data sets that include information on ion species, voltage, and other relevant characteristics. In this study, we processed the latest data and employed a deep neural network (DNN) machine learning (ML) platform to construct a regression model. The model relies on easily accessible input information, such as the initial structure, and utilizes high-quality data to validate its reliability. The two-dimensional material data set containing only the material structure is taken as the target set to predict the average discharge voltage (U av), according to which more than 2500 potential electrode materials are selected. From this pool, we rigorously selected a subset of anode materials for detailed density functional theory (DFT) calculations. These materials exhibit promising elemental compositions and have not been previously investigated as electrode materials. The results of DFT calculations confirmed the reliability of the ML model's predictions, demonstrating that the combination of ML and DFT calculations can effectively screen data sets lacking expensive DFT-calculated data. This strategy can significantly reduce computational costs by predicting specific performance metrics and conducting preliminary screenings.",Not About Sufficiency
Neighbourhood-scale urban forest ecosystem classification,"Urban forests are now recognized as essential components of sustainable cities, but there remains uncertainty concerning how to stratify and classify urban landscapes into units of ecological significance at spatial scales appropriate for management. Ecosystem classification is an approach that entails quantifying the social and ecological processes that shape ecosystem conditions into logical and relatively homogeneous management units, making the potential for ecosystem-based decision support available to urban planners. The purpose of this study is to develop and propose a framework for urban forest ecosystem classification (UFEC). The multifactor framework integrates 12 ecosystem components that characterize the biophysical landscape, built environment, and human population. This framework is then applied at the neighbourhood scale in Toronto, Canada, using hierarchical cluster analysis. The analysis used 27 spatially-explicit variables to quantify the ecosystem components in Toronto. Twelve ecosystem classes were identified in this UFEC application. Across the ecosystem classes, tree canopy cover was positively related to economic wealth, especially income. However, education levels and homeownership were occasionally inconsistent with the expected positive relationship with canopy cover. Open green space and stocking had variable relationships with economic wealth and were more closely related to population density, building intensity, and land use. The UFEC can provide ecosystem-based information for greening initiatives, tree planting, and the maintenance of the existing canopy. Moreover, its use has the potential to inform the prioritization of limited municipal resources according to ecological conditions and to concerns of social equity in the access to nature and distribution of ecosystem service supply. (C) 2015 Elsevier Ltd. All rights reserved.",Not About Sufficiency
Millimeter-wave radar-based bathroom fall detection using time-frequency features,"Falls in the elderly are a major public health problem. In this paper, an effective fall detection method based on millimeter wave radar system is proposed. Using the time-frequency characteristics of radar echoes, the signal-to-noise ratio is significantly improved by least squares adaptive filtering to eliminate interference, and classification is performed by combining short-time Fourier transform (STFT) and machine learning algorithms KNN (Nearest Neighbor), Support Vector Machine (SVM), Multilayer Perceptron (MLP) and Decision Tree (DT). By collecting data from two idle bathrooms, data from four actions were collected and the effectiveness of the machine learning SVM, KNN and MLP algorithms were verified by the measured data, in which the classification performance of SVM was better than the other three algorithms.",Not About Sufficiency
Critical Review of Data Analytics Techniques used in the Expanded Program on Immunization (EPI),"Background: Immunization is a significant public health intervention to reduce child mortality and morbidity. However, its coverage, in spite of free accessibility, is still very low in developing countries. One of the primary reasons for this low coverage is the lack of analysis and proper utilization of immunization data at various healthcare facilities. Purpose: In this paper, the existing machine learning-based data analytics techniques have been reviewed critically to highlight the gaps where this high potential data could be exploited in a meaningful manner. Results: It has been revealed from our review that the existing approaches use data analytics techniques without considering the complete complexity of Expanded Program on Immunization which includes the maintenance of cold chain systems, proper distribution of vaccine and quality of data captured at various healthcare facilities. Moreover, in developing countries, there is no centralized data repository where all data related to immunisation is being gathered to perform analytics at various levels of granularities. Conclusion: We believe that the existing non-centralized immunization data with the right set of machine learning and Artificial Intelligence-based techniques will not only improve the vaccination coverage but will also help in predicting the future trends and patterns of its coverage in different geographical locations.",Not About Sufficiency
A Comprehensive Study and Performance Based Evaluation on Routing Protocols of WiMAX,"Worldwide Interoperability for Microwave Access (WiMAX) technology is a subdivision of the vast field of wireless communications, which breaks the restrictions of DSL, cable-modem or T1 infrastructure based wire line of our broadband Internet connections [1]. Wireless networking has become an important area of research in academic and industry. The four high-speed intensive applications such as high-speed data, video, voice and streaming media are being served properly by the WiMAX network, which overcomes the term digital divide. It has a number of spearhead applications with at-home and dynamic internet accessibility along with large geographical coverage and relatively low expense to pursuit. Routing overhead, unidirectional link support, delay, throughput, QoS support and multicast are the analysable terms by whom we can define the WiMAX protocols. To attain best use of this technology selection of suitable protocol is very important. In our paper, most imperative protocols of 802.16 WiMAX network called AODV, DSDV and OLSR is ventilated as well as further criticized in accordance with their performance on WiMAX where AODV is a reactive (on demand) protocol and both DSDV and OLSR are proactive.",Not About Sufficiency
"On the trade-off between profitability, complexity and security of forecasting-based optimization in residential energy management systems","With the emergence of affordable access to data sources, machine learning models and computational resources, sophisticated control concepts for residential energy management systems (EMSs) are on the rise. At the heart of those are production and consumption forecasts. Given the wide spectrum of implementation opportunities, selection of appropriate forecasting strategies is challenging. This work systematically evaluates forecasting-based optimization for residential EMSs in terms of trade-offs between economic profitability, computational complexity and security. The foundation of the study is two real prosumer cases equipped with a photovoltaic-battery system. Results demonstrate that, within the considered scenarios, best trade-offs are achieved based on forecasts of a default gradientboosted decision trees model, using a short initial training set, weather forecast inputs and regular retraining. Over 90% of the theoretical maximum economic benefit is achieved in this scenario, at significantly lower computational complexity than others with similar savings, while being applicable to new systems without large data history. In terms of security, this scenario exhibits tolerance against weather input manipulation. However, sensitivity to price tampering may require data integrity checking in residential EMSs.& COPY; 2023 The Author(s). Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).",Not About Sufficiency
Application of the Delphi method to the development of common data elements for social drivers of health: A systematic scoping review,"Collaborative data science requires standardized, harmonized, interoperable, and ethically sourced data. Developing an agreed-upon set of elements requires capturing different perspectives on the importance and feasibility of the data elements through a consensus development approach. This study reports on the systematic scoping review of literature that examined the inclusion of diverse stakeholder groups and sources of social drivers of health variables in consensus-based common data element (CDE) sets. This systematic scoping review included sources from PubMed, Embase, CINAHL, WoS MEDLINE, and PsycINFO databases. Extracted data included the stakeholder groups engaged in the Delphi process, sources of CDE sets, and inclusion of social drivers data across 11 individual and 6 social domains. Of the 384 studies matching the search string, 22 were included in the final review. All studies involved experts with healthcare expertise directly relevant to the developed CDE set, and only six (27%) studies engaged health consumers. Literature reviews and expert input were the most frequent sources of CDE sets. Seven studies (32%) did not report the inclusion of any demographic variables in the CDE sets, and each demographic SDoH domain was included in at least one study with age and sex assigned at birth included in all studies, and social driver domains included only in four studies (18%). The Delphi technique engages diverse expert groups around the development of SDoH data elements. Future studies can benefit by involving health consumers as experts. Having diverse expert stakeholders is an effective strategy to establish common data elements for social drivers of health, which is crucial for driving positive changes in patient outcomes. Collecting and capturing social factors that affect individuals' health is imperative. Social drivers of health data allow researchers to understand health disparities to make healthcare available, accessible, and affordable. However, collecting common health data elements has challenged researchers due to limited resources to facilitate change. Incorporating various stakeholders, such as individuals and patient advocacy groups, can effectively contribute to the research process as community advisors. This article reviews the studies that used the Delphi method and brings together experts to agree on guidelines for collecting common data elements. The article's findings reveal that experts are healthcare professionals and researchers, leaving out the crucial input from patients and caregivers. This article emphasized that developing a standard set of data elements can improve the standardization of social drivers of health. Common data elements provide the opportunity to improve patients' and social circumstances and their efforts toward health outcomes. Graphical Abstract",Not About Sufficiency
Customary land management systems and urban planning in peri-urban informal settlements,"Customary land management systems are informal, community-driven land-use regulation systems that adapt zoning regulations and customary tenure to cooperatively self-regulate land-use management in multi-ethnic peri-urban settlements. The research uses an integrative literature review to critically re-evaluate the various concepts and practices of customary land management, their impact on the unique morphology of peri-urban areas and their relationship with urban planning. The research results indicate that customary land management systems are intrinsically linked to peri-urban settlements due to their polymorphic spatial structure and complex social groupings. It provides a simplified accessible and affordable land management system with multiple avenues for agency and a balance of power between different authorities. This generates a new set of social relations around neo-customary tenure. Customary land management systems are also linked to urban planning within a dual regulatory structure, combining formal policies and informal customs and providing alternatives for exploitative and exclusionary processes in weak and inefficient states.",Not About Sufficiency
Internet of things (IoT) platform competition: Consumer switching versus provider multihoming,"Internet of things (IoT) brings new opportunities and represents a new source of welfare and efficiency. However, the emerging consumer IoT platform competition creates the risk of monopoly power due to network effects. Overall, it is likely that both competition (incentivized through lowering consumer switching costs) and cooperation (achieved through interoperability, which enables data portability and service provider multihoming) are needed to maximize social welfare. This article aims to address how consumer switching costs and provider multihoming affect competition of emerging consumer IoT data platforms under different market conditions and regulatory schemes. It utilizes agent-based modelling that is especially suitable when decision making is distributed at a micro level while some rules are applied in a centralized fashion. The obtained findings emphasize the role of the regulator in guiding the market. It seems that when switching costs diminish at all sides of the platforms, consumers and service providers will favour the platform with a higher number of users. Further, service provider multihoming mitigates market concentration on both sides of a platform when switching costs are low. Thus, there seems to be a minimum level of interoperability needed to promote market competition. Further, although data portability gives more freedom to consumers in choosing a platform provider, it may result in a winner-takes-all situation due to strong indirect network effects.",Not About Sufficiency
Convergent data-driven workflows for open radiation calculations: an exportable methodology to any field,"The fast growth worldwide of linkable scientific datasets supposes significant challenges in their management and reuse. Large experiments, such as the Latin American Giant Observatory, generate volumes of data that can benefit other kinds of studies. In this sense, there is a modular ecosystem of external radiation tools that should harvest and supply datasets without being part of the main pipeline. Workflows for personal dose estimation, muongraphy in volcanology or mining, or aircraft dose calculations are built with different privacy policies and exploitation licenses. Every numerical method has its own requirements and only parts could make use of the Collaboration's resources, which implies the convergence with other computing infrastructures. Our work focuses on developing an agnostic methodology to address these challenges while promoting open science. Leveraging the encapsulation of software in nested containers, where the inner layers accomplish specific standardization slices and calculations, the wrapper compiles metadata and data generated and publishes them. All this allows researchers to build a data-driven computer continuum that complies with the findable, accessible, interoperable, and reusable principles. The approach has been successfully tested in the computer-demanding field of radiation-matter interaction with humans, showing the orchestration with the regular pipeline for diverse applications. Moreover, it has been integrated into public or federated cloud environments as well as into local clusters and personal computers to ensure the portability and scalability of the simulations. We postulate that this successful use case can be customized to any other field.",Not About Sufficiency
A Fast and Effective Machine Learning Approach for Road Cracks Classification,"Road transportation is the backbone of the economic activities of any nation. Millions of commuters use national and state highways and local municipal roads for daily operations. Road maintenance is an indispensable part of the concerned authorities for hassle-free and safe transportation. Poor maintenance of the road and its potholes could cause fatal injuries to the commuters. As the length of these nationwide road networks is overwhelming, the modern-day machine learning-based approach can efficiently detect and classify such a variety of road cracks and send information to the appropriate parties. This paper introduces a straightforward machine learning-guided pipeline that can successfully classify the different types of cracks on the road. It is also compared with state-of-the-art Convolutional Neural Network (CNN) models. Our proposed machine learning approach and different feature selection techniques provide consistent and competitive results in a short training time. It is observed that Orthogonal Matching Pursuit (OMP) outperforms other used feature selectors in terms of selected feature subset size, classification performance, and training time. The final, conclusive result is as high as similar to 97% accuracy obtained from OMP feature selection and Optuna-based tuned Random Forest Classifier.",Not About Sufficiency
Navigating complexity in sustainable conservation: A multi-criteria decision making of architectural heritage in urbanizing China,"The conservation of architectural heritage has received significant attention due to the pressures of rapid urbanization and the complexities of stakeholder dynamics. However, existing studies frequently fail to pinpoint key issues or develop effective response frameworks. This study employs a multi-stage mixed-methods approach, integrating Multi-Criteria Decision Making (MCDM) with a five-dimensional analytical framework that includes architecture, urban planning, social factors, economic considerations, and property-related issues. Data collection involved 22 experts and 573 residents, leading to the identification of 36 critical issues in architectural heritage conservation. The findings underscore five key concerns from residents' perspectives, including challenges in structural reinforcement, pressures from modern transportation, and impacts on their quality of life. To address these challenges, the study suggests innovative solutions, including intelligent reinforcement technologies, smart traffic management systems, and shared community benefit funds. By presenting a systematic, multi-dimensional framework, this research offers practical tools to foster stakeholder collaboration and enhance the sustainability and social benefits of conservation efforts.",Not About Sufficiency
Data Redundancy Elimination and Noise Processing via Large Language Model Prompt Engineering,"With the rapid advancement of artificial intelligence (AI), intelligent systems have been widely applied across various practical domains. During their experimental processes, data collection often occurs in complex environments, leading to challenges in ensuring data quality and integrity. Two significant issues are data redundancy and data noise, which can degrade model performance and impact the generalization ability and prediction accuracy of AI models. Traditional methods for handling these issues are either rule-based, relying heavily on domain knowledge, or machine learning-based, which require large volumes of high-quality training data and significant computational resources. In this paper, we propose a novel approach leveraging Large Language Models (LLMs) combined with the Sequential Chain optimization algorithm to address data redundancy elimination and noise processing. By designing automated prompt templates tailored to experimental data characteristics and utilizing LLMs' extensive knowledge and reasoning capabilities, our approach improves the effectiveness of preprocessing tasks. Additionally, the Sequential Chain technique enhances LLM processing performance, reducing the hallucination phenomenon and ensuring higher accuracy in data preprocessing. Our experimental results demonstrate that LLM-based methods can achieve results comparable to traditional techniques while offering greater scalability and adaptability. Future work could focus on developing more efficient LLMs with lower computational requirements and refining prompt engineering techniques to reduce the time investment needed, making these advanced methods more accessible and practical for a broader range of applications.",Not About Sufficiency
"Risk Narrative of Emergency and Disaster Management, Preparedness, and Planning (EDMPP): The Importance of the 'Social'","Risk perception, literacy, communication, narrative, governance, and education are important aspects of emergency and disaster management, preparedness, and planning (EDMPP) as they for example influence and direct EDMPP policies and actions. A thorough understanding of the 'social aspects of risk is important for EDMPP, especially in relation to marginalized populations who are often overlooked. Technologies are increasingly employed for EDMPP. How these technology applications identify and engage with the 'social' of risk in general and the 'social' of risk experienced by marginalized populations is important for EDMPP. Equity, diversity, and inclusion (EDI) and similar phrases are employed as policy concepts to improve research, education, and participation in the workplace for marginalized groups such as women, Indigenous peoples, visible/racialized minorities, disabled people, and LGBTQ2S including in workplaces engaging with EDMPP which includes universities. The aim of this scoping review was to generate data that allows for a detailed understanding of the risk related discussions within the EDMPP academic literature as these discussions shape EDMPP policies and actions. The objective of this scoping review study was to map out the engagement with risk, specifically the social aspects of risk, in the EDMPP-focused academic literature with a focus on (a) EDMPP in general, (b) COVID-19, (c) EDMPP and marginalized groups, (d) EDMPP and patients, and (e) EDMPP and technologies (artificial intelligence, machine learning, machine reasoning, algorithm design approaches such as Bayesian belief networks, e-coaching, decision support systems, virtual coaching, automated decision support, e-mentoring, automated dialogue and conversational agents). Using the academic databases SCOPUS, Web of Sciences, and databases accessible under Compendex and EBSCO-HOST and performing hit count frequency searches of online and downloaded abstracts and thematic analysis of downloaded abstracts the study reveals a lack of coverage on the social aspects of risk and engagement with risk concepts such as risk perception, risk governance, risk literacy, risk communication, risk education and risk narrative especially in conjunction with marginalized groups and technologies employed in EDMPP decision support. Our findings suggest many opportunities to further the EDMPP academic inquiry by filling the gaps.",Not About Sufficiency
A Biobank of Stem Cells of Human Exfoliated Deciduous Teeth: Overview of Applications and Developments in Brazil,"A biobank is an organized collection of biological human material and its associated information stored for research according to regulations under institutional responsibility, without commercial purposes, being a mandatory and strategical activity for research, regenerative medicine, and innovation. Stem cells have largely been employed in research and frequently stored in biobanks, which have been used as an essential source of biological materials. Stem cells of human exfoliated deciduous teeth (SHED) are stem cells which have a high multipotency and can be easily obtained. Besides, this extremely accessible tissue has advantages with respect to storage, as the SHED obtained in childhood can be used in later life, which implies the necessity for the creation and regulation of biobanks. The proper planning for the creation of a biobank includes knowledge of the material types to be stored, requirements regarding handling and storage conditions, storage time, and room for the number of samples. Thus, this study aimed to establish an overview of the development of a SHED biobank. Ethical and legal standardization, current applications, specific orientations, and challenges for the implementation of a SHED biobank were discussed. Through this overview, we hope to encourage further studies to use SHED biobanks.",Not About Sufficiency
Energy in the European Green Deal: impacts and recommendations for MENA countries,"The European Union (EU) presented its new and ambitious programme at the end of 2019-the European Green Deal (EGD), which should align its economy to reach the goal of climate neutrality in 2050. A vital part of the deal aims for the energy sector within the leitmotif of supplying clean, affordable and secure energy. Because energy-related Greenhouse Gas (GHG) emissions accounted in 2020 for three-quarters of all GHG emissions of the EU, the most important pillar for the EGD to succeed is the energy sector. The deal does not function within a vacuum as it will impact other countries, including those with vast conventional energy resources and close ties to the EU, like nations from the region of the Middle East and North Africa (MENA). This article assesses the deal's impacts on MENA countries and includes recommendations concerning various aspects of low-carbon transition that could be applied. The EGD programme will be used as a reference scenario for those recommendations because of its universal character.",Not About Sufficiency
"Unbundling Stockholm: The networks, planning and social welfare nexus beyond the unitary city","This paper focuses on the extent to which recent infristructure-oriented urban developments in Stockholm concord with various aspects of the 'splintering urbanism' thesis of Graham and Marvin. This contextualisation allows us to extend their work empirically and conceptually. In the first instance, we study a particular case of the decline of a unitary networked city (in an urban context largely absent from their book). In the second instance, we develop their notion of 'unbundling' to capture not just core changes in the organisation of infrastructure provision, but an overarching disjunction of the established nexus between networks, planning and social welfare in the city. This disjunction operates through interlinked transformations concerning, for example, privatisation and Outsourcing in network services, separation of infrastructure planning from broader urban planning, contradictions between the environmental and social mandates of infrastructure, and a (prospective) Curtailment of the redistributive, social role of essential network service provision. We conclude nonetheless that this 'destructive' moment of unbundling has not so far been pursued by more explicitly 'creative' urban fragmentation strategies, due largely to the vestiges of a socio-political consensus based around redistribution and equality. In this respect, the Stockholm case pleads for a conception of 'splintering' as a dynamic and multi-stage process which is not only always ongoing, unstable and incomplete, but also non-linear and open to resistance/regulation. (C) 2008 Elsevier Ltd. All rights reserved.",Not About Sufficiency
The redemption of thalidomide: Standardizing the risk of birth defects,"In this paper we examine how a standardized drug distribution system contributed to a therapeutic and symbolic make-over of thalidomide. In the 1960s, thalidomide was seen as a horror drug that caused severe birth defects among over 10,000 babies who were exposed to it in utero. Currently, thalidomide is viewed as a potentially life-saving drug which is being distributed in the USA. We discuss this transformation from a social worlds perspective, showing how the standardized drug distribution system normalized the risk of foetal birth defects, while preserving the autonomy of health care professionals. The distribution system accomplished this transformation by focusing on the risk associated with female reproductive behaviour, and by providing close reproductive surveillance of female patients. This standardized system solidified social inequalities and professional power relationships, revealing assumptions about trust, responsibility and risk.",Not About Sufficiency
Just food transition: for a gender mainstreaming approach in urban food policies. A review of 20 cities,"Urban areas are increasingly adopting the tool of urban food policies (UFPs) to address food-related challenges, especially for a sustainable transition of their food systems (FS). Indeed, FS were assessed to have a great environmental impact, and cities are recognized as privileged actors to address this challenge. More recently, the problem of justice in such transition was raised. One dimension of vulnerability proven to crosscut all FS stages, and to increase exposure to climate change effects, is gender. However, literature on just food transition and UFPs does not seem to always consider this dimension of potential vulnerability. This paper firstly elaborates on the concept of ""just food transition"" (JFT), then presents relevant literature on gender differences in FS to argue that interventions aimed at JFT should apply gender mainstreaming. Later, it analyzes twenty relevant cases of UFPs to assess whether they consider gender differences, with the aim of demonstrating that, although theory seems to prove its relevance, a gender mainstreaming approach is still widely lacking from policy practice and rather some policies risk to reinforce existing gender stereotypes related to the FS. The paper ends with conclusions and suggestions for further research.",Not About Sufficiency
Kunz and post-communist geographies of urban poverty in Romania,"This article examines the spatial, social, economic, and civic dimensions of poverty in a post-communist Romanian city, focusing on the Kunz, an informal neighborhood in Timisoara. In Romanian, areas marked by concentrated poverty are pejoratively referred to as mahala, and internationally as, akin to the pejorative word, 'slums'. This study sheds light on spatially concentrated poverty in post-communist cities through the lens of urban planning, citizenship, and environmental justice. Drawing on multiple research methods, the research reveals how environmental injustice is perpetuated by the lack of inclusive urban planning strategies, exacerbating existing poverty due to a continuous influx of impoverished populations from various regions. Exploiting legal uncertainties, these newcomers built homes without property deeds, subdividing older land plots. The strong socio-economic cohesion, basic urban infrastructure arrangements, temporary identity cards, and strong family ties within the community mitigate the fear of eviction from substandard housing. Economic crises generate a certain sense of security in the face of eviction as residents in these impoverished areas interpret crises as opportunities for safety. This sentiment prevails as substantial public investments are often delayed, reducing the likelihood of demolition and mass evictions, leaving the population nowhere else to turn.",Not About Sufficiency
A social psychological comparison of the Turkish elderly residing at high or low quality institutions,"This paper aims to provide a comparison of the Turkish elderly residing at high- or low-quality institutions in terms of institutional and life satisfaction, attitudes toward institutional living, feelings of control, nature of relocation, importance attributed to different aspects of institutions, and preferences for different living conditions. In-depth interviews were carried out with 106 Turkish males and females (62-89 years) divided equally between high- and low-quality institutions from Istanbul. Compared to those from low-quality institutions, respondents from high-quality institutions reported feeling more satisfied with their lives and institutional living, having more personal control, having moved to the institution more voluntarily, and were more likely to prefer their current living environments and to attribute importance to facilities, services, physical surroundings and leisure time activities. In spite of such differences in assessments of current living environments, those residing at high-quality institutions did not seem to have significantly more positive attitudes toward institutional living in general, although there was a trend in that direction. In terms of gender differences, females reported more life satisfaction, more positive attitudes toward institutional living, more voluntary relocation, and were more likely to prefer their current living places and to attribute importance to people. Present results from the collectivistic Turkish context are discussed in reference to related findings from the more individualistic societies, as well as those concerning the community-residing elderly. (C) 1999 Academic Press.",Not About Sufficiency
"""Sweeter Than a Swisher': amount and themes of little cigar and cigarillo content on Twitter","Objective Despite recent increases in little cigar and cigarillo (LCC) useparticularly among urban youth, African-Americans and Latinosresearch on targeted strategies for marketing these products is sparse. Little is known about the amount or content of LCC messages users see or share on social media, a popular communication medium among youth and communities of colour. Methods Keyword rules were used to collect tweets related to LCCs from the Twitter Firehose posted in October 2014 and March-April 2015. Tweets were coded for promotional content, brand references, co-use with marijuana and subculture references (eg, rap/hip-hop, celebrity endorsements) and were classified as commercial and organic'/non-commercial using a combination of machine learning methods, keyword algorithms and human coding. Metadata associated with each tweet were used to categorise users as influencers (1000 and more followers) and regular users (under 1000 followers). Results Keyword filters captured over 4372293 LCC tweets. Analyses revealed that 17% of account users posting about LCCs were influencers and 1% of accounts were overtly commercial. Influencers were more likely to mention LCC brands and post promotional messages. Approximately 83% of LCC tweets contained references to marijuana and 29% of tweets were memes. Tweets also contained references to rap/hip-hop lyrics and urban subculture. Conclusions Twitter is a major information-sharing and marketing platform for LCCs. Co-use of tobacco and marijuana is common and normalised on Twitter. The presence and broad reach of LCC messages on social media warrants urgent need for surveillance and serious attention from public health professionals and policymakers. Future tobacco use prevention initiatives should be adapted to ensure that they are inclusive of LCC use.",Not About Sufficiency
MACHINE LEARNING-BASED ECONOMIC DEVELOPMENT MAPPING FROM MULTI-SOURCE OPEN GEOSPATIAL DATA,"Timely and accurate socioeconomic indicators are the prerequisite for smart social governance. For example, the level of economic development and the structure of population are important statistics for regional or national policy-making. However, the collection of these characteristics usually depends on demographic and social surveys, which are time- and labor-intensive. To address these issues, we propose a machine learning-based approach to estimate and map the economic development from multi-source open available geospatial data, including remote sensing imagery and OpenStreetMap road networks. Specifically, we first extract knowledge-based features from different data sources; then the multi-view graphs are constructed through different perspectives of spatial adjacency and feature similarity; and a multi-view graph neural network (MVGNN) model is built on them and trained in a self-supervised learning manner. Then, the handcrafted features and the learned graph representations are combined to estimate the regional economic development indicators via random forest models. Taking China's county-level gross domestic product (GDP) as an example, extensive experiments have been conducted and the results demonstrate the effectiveness of the proposed method, and the combination of the knowledge-based and learning-based features can significantly outperform baseline methods. Our proposed approach can advance the goal of acquiring timely and accurate socioeconomic variables through widely accessible geospatial data, which has the potential to extend to more social indicators and other geographic regions to support smart governance and policy-making in the future.",Not About Sufficiency
The Water Sector Was Born and Raised with Big-Impact Water Data,"For thousands of years, data allowed societies to firstunderstandthe connection between water quality and public health and later torefine water and waste management strategies. In this commentary,we look at the past, present, and future of 'big-impact'data in thewater sector, from ancient water systems to Paris' valorizationof wastes, London's tribulations with cholera, and modern environmentalstandards for the protection of public and environmental health. Wecontinue the journey of water data with emerging applications in understandingour water resources, including blue-green city planning, understandingwater use trends in remote areas, water resource monitoring, and wastewaterepidemiology. We conclude with a foray into the future of the watersector, where proactively managing water resources to ensure equitableand resilient water access can be informed by remote sensing, satellite-basedmonitoring, and machine learning. By understanding how water datarevolutionized our societies in the past, we aim to inspire the useof big water data to open possibilities for managing urban and environmentalwater in our future.",Not About Sufficiency
Atrial Fibrillation Identification With PPG Signals Using a Combination of Time-Frequency Analysis and Deep Learning,"Atrial fibrillation (AF) is the most common persistent arrhythmia and is likely to cause strokes and damage to heart function in patients. Electrocardiogram (ECG) is the gold standard for detecting AF. However, ECGs have short boards with short monitoring cycles and problems with gathering. It is also difficult to detect a burst AF through ECG. In contrast, photoplethysmography (PPG) is easy to perform and suitable for long-term monitoring. In this study, we propose a method that combines time-frequency analysis with deep learning and identifies AF based on PPG. The advantage of the method is that there is no need for the noise filtering and feature extraction of PPG, and it has a high generalization capability. The data for the experiment came from three publicly accessible databases. The first part of the experimental method uses data augmentation to convert the 10 s PPG segment into a time-frequency chromatograph by means of time-frequency analysis. The second part inputs the chromatograph into a hybrid framework that combines a convolutional neural network (CNN) and long short-term memory (LSTM) for AF/nonAF classification. The experimental results show that the method has a high classification accuracy, sensitivity, specificity, and F1 score, which are equal to 98.21%, 98.00%, 98.07% and 98.13%, respectively. The area under the receiver operating characteristic curve (AUC) is 0.9959. The model we propose not only aids doctors in diagnosing AF but also provides a method for identifying AF through portable wearable devices.",Not About Sufficiency
Autonomous navigation and positioning for indoor wireless measurements using mobile robots,"Personal Wireless Communications have experienced a tremendous growth in the last decade. In indoor scenarios, such as underground stations or buildings, the signal received at the mobile propagates in a different way than in outdoor environments. Therefore, measurement techniques used in outdoor cells cannot be used indoors, especially when determining the position of the mobile test equipment. Alternative methods of navigation and positioning are required for specific indoor measurement campaigns. Three mobile robot platforms have been developed to improve the accuracy of characterisation of the indoor propagation channel. The aim of each system is to provide an accurate mechanism for determining relative position within acceptable error bounds, depending on the specific characteristics of the measurement campaign and especially where other methods of positioning are not available. This paper describes each of the robots developed, emphasising the penetration that robotics has had in the wireless mobile communication industry, as a useful and affordable tool to improve propagation measurement techniques.",Not About Sufficiency
Decision Support System for Optimum Lifetime Sustainability-Based Maintenance Planning of Highway Bridges,"Robust decision support for the optimum management of engineering systems must contain a module for structural performance assessment, which may be accomplished with life-cycle risk and sustainability assessment procedures. A performance indicator representative of sustainability effectively integrates indirect (e.g., environmental and social impacts), as well as, direct (e.g., economic impacts) impacts. Within this paper, lifetime functions are employed to model, using closed-form analytical expressions, the time-variant effect of intervention actions on the performance of civil infrastructure systems. Additionally, multi-attribute utility theory is used to incorporate the influence of the decision maker's risk attitude on the relative desirability of lifetime maintenance strategies. Furthermore, optimization procedures may carried out that simultaneously maximize the utilities associated with maintenance cost and sustainability in order to determine optimum maintenance strategies under uncertainty. The capabilities of the proposed decision support system, specifically the aspects related to the sustainability assessment, are illustrated on an existing highway bridge.",Not About Sufficiency
Machine learning-assisted optical nano-sensor arrays in microorganism analysis,"Microbial infection can cause problems for public health, and to realize efficient microorganism detection is of great importance. However, the simultaneous identification of microorganism still faces challenges due to the high similarity of the surface microenvironment. With the assistance of machine learning algorithms, nanomaterials-based optical sensor arrays are emerging as a promising analysis technique for microorganism discrimination with the merits of high sensitivity, time-saving and easy operation. We present here the recent development of machine learning assisted optical sensor arrays for microor-ganism identification. In the first part, five types of optical nano-sensor arrays that include fluorescent sensor arrays, colorimetric sensor arrays, multi-response-based sensor arrays, SERS-based sensor arrays and FTIR-based sensor arrays are discussed. Then, eight commonly used machine learning algorithms in the array-based sensors are introduced. Detailed calculation principles involved in the statistical analysis of array-based sensors are overviewed. It is ended by outlining the current challenges and perspectives.(c) 2023 Elsevier B.V. All rights reserved.",Not About Sufficiency
Electrocardiogram-based artificial intelligence for the diagnosis of heart failure: a systematic review and meta-analysis,"BACKGROUND The electrocardiogram (ECG) is an inexpensive and easily accessible investigation for the diagnosis of cardiovascular diseases including heart failure (HF). The application of artificial intelligence (AI) has contributed to clinical practice in terms of aiding diagnosis, prognosis, risk stratification and guiding clinical management. The aim of this study is to systematically review and perform a meta-analysis of published studies on the application of AI for HF detection based on the ECG. METHODS We searched Embase, PubMed and Web of Science databases to identify literature using AI for HF detection based on ECG data. The quality of included studies was assessed using the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS-2) criteria. Random-effects models were used for calculating the effect estimates and hierarchical receiver operating characteristic curves were plotted. Subgroup analysis was performed. Heterogeneity and the risk of bias were also assessed. RESULTS A total of 11 studies including 104,737 subjects were included. The area under the curve for HF diagnosis was 0.986, with a corresponding pooled sensitivity of 0.95 (95% CI: 0.86-0.98), specificity of 0.98 (95% CI: 0.95-0.99) and diagnostic odds ratio of 831.51 (95% CI: 127.85-5407.74). In the patient selection domain of QUADAS-2, eight studies were designated as high risk. CONCLUSIONS According to the available evidence, the incorporation of AI can aid the diagnosis of HF. However, there is heterogeneity among machine learning algorithms and improvements are required in terms of quality and study design.",Not About Sufficiency
Accurate Prediction of Lysine Methylation Sites Using Evolutionary and Structural-Based Information,"Methylation is considered one of the proteins' most important post-translational modifications (PTM). Plasticity and cellular dynamics are among the many traits that are regulated by methylation. Currently, methylation sites are identified using experimental approaches. However, these methods are time-consuming and expensive. With the use of computer modelling, methylation sites can be identified quickly and accurately, providing valuable information for further trial and investigation. In this study, we propose a new machine-learning model called MeSEP to predict methylation sites that incorporates both evolutionary and structural-based information. To build this model, we first extract evolutionary and structural features from the PSSM and SPD2 profiles, respectively. We then employ Extreme Gradient Boosting (XGBoost) as the classification model to predict methylation sites. To address the issue of imbalanced data and bias towards negative samples, we use the SMOTETomek-based hybrid sampling method. The MeSEP was validated on an independent test set (ITS) and 10-fold cross-validation (TCV) using lysine methylation sites. The method achieved: an accuracy of 82.9% in ITS and 84.6% in TCV; precision of 0.92 in ITS and 0.94 in TCV; area under the curve values of 0.90 in ITS and 0.92 in TCV; F1 score of 0.81 in ITS and 0.83 in TCV; and MCC of 0.67 in ITS and 0.70 in TCV. MeSEP significantly outperformed previous studies found in the literature. MeSEP as a standalone toolkit and all its source codes are publicly available at https://github.com/arafatro/MeSEP.",Not About Sufficiency
Computer-Aided Diagnosis of Melanoma Subtypes Using Reflectance Confocal Images,"Simple Summary Melanoma is a serious public health concern that causes significant illness and death, especially among young adults in Australia and New Zealand. Reflectance confocal microscopy is a non-invasive imaging technique commonly used to differentiate between different types of melanomas, but it requires specialized expertise and equipment. In this study, we used machine learning to develop classifiers for classifying patient image stacks between two types of melanoma. Our approach achieved high accuracy, demonstrating the utility of computer-aided diagnosis to improve expertise and access to reflectance confocal imaging among the dermatology community. Lentigo maligna (LM) is an early form of pre-invasive melanoma that predominantly affects sun-exposed areas such as the face. LM is highly treatable when identified early but has an ill-defined clinical border and a high rate of recurrence. Atypical intraepidermal melanocytic proliferation (AIMP), also known as atypical melanocytic hyperplasia (AMH), is a histological description that indicates melanocytic proliferation with uncertain malignant potential. Clinically and histologically, AIMP can be difficult to distinguish from LM, and indeed AIMP may, in some cases, progress to LM. The early diagnosis and distinction of LM from AIMP are important since LM requires a definitive treatment. Reflectance confocal microscopy (RCM) is an imaging technique often used to investigate these lesions non-invasively, without biopsy. However, RCM equipment is often not readily available, nor is the associated expertise for RCM image interpretation easy to find. Here, we implemented a machine learning classifier using popular convolutional neural network (CNN) architectures and demonstrated that it could correctly classify lesions between LM and AIMP on biopsy-confirmed RCM image stacks. We identified local z-projection (LZP) as a recent fast approach for projecting a 3D image into 2D while preserving information and achieved high-accuracy machine classification with minimal computational requirements.",Not About Sufficiency
New Protagonists in Global Economic Governance: Brazilian Agribusiness at the WTO,"The existing international economic order has been heavily shaped by US power and the US has been a key driver of globalisation and neoliberal economic restructuring, prompting speculation about whether the rise of new developing country powers could rupture the current trajectory of neoliberal globalisation. This paper analyses the case of Brazil at the World Trade Organization (WTO), a core institution in global economic governance. In the last decade, Brazil successfully waged two landmark trade disputes against the US and EU and created a coalition of developing countries - the G20 - which brought an end to the dominance of the US and EU at the WTO and made their trade policies a central target of the Doha Round. Brazil's activism has been widely hailed as a major victory for developing countries. However, I argue that rather than challenging the neoliberal agenda of the WTO, Brazil has emerged as one of the most vocal advocates of free market globalisation and the push to expand and liberalise global markets. I show that Brazil's stance has been driven by the rise of its export-oriented agribusiness sector. This case demonstrates that business actors from the Global South are becoming significant new protagonists in global economic governance; they are taking the tools created by the states and corporations of the Global North - in this case, the WTO and its neoliberal discourse - and turning them against their originators. At the same time, their interests are being wrapped in and advanced through a discourse of development and social justice and a strategic mobilisation of the politics of the North-South divide.",Not About Sufficiency
The improved Moho depth imaging in the Arabia-Eurasia collision zone: A machine learning approach integrating seismic observations and satellite gravity data,"The Arabia-Eurasia convergences created one of the earth's topographic highs on the Central Tethys collisional belt. Despite the area's geological significance, a comprehensive and high-resolution map of Moho depth has been lacking due to the sparse and uneven distribution of seismically constrained Moho depth data. This study addresses this deficiency by compiling an extensive dataset of nearly 2500 seismically measured Moho depth points from 68 seismic local scale studies, resulting in the development of an updated seismically constrained Moho depth model (S-Moho) at a 0.5 degrees x 0.5 degrees spatial resolution. Despite some coverage gaps in the remote areas, the S-Moho model offers a more detailed view than previously available. To further improve the coverage of the S-Moho depth model, an incremental data-driven approach was employed. Initially, a gravity-based regression Moho depth model (SB-Moho) was developed by correlating S-Moho depth points with corresponding Bouguer anomalies. However, its accuracy was constrained by unaccounted isostatic and non-isostatic components. To address this limitation, a sliding window approach was applied to derive a windowed SB-Moho model (WSBMoho). Additionally, a machine learning-based Moho model (ML-Moho) was developed using seismic Moho depth points along with 11 predictive variables. Both WSB-Moho and ML-Moho models demonstrated consistent and smooth Moho depth variations. The Zagros region reveals a prominent NW-SE oriented Moho depression (45-60 km thick), attributed to the underthrusting of the Arabian Plate beneath the Iranian Plateau. The models suggest that crustal thickening extends beyond tectonic boundaries, likely influenced by the dip of suture zones. In contrast, the crustal thickening in eastern Anatolia, northwest of the Zagros, is less pronounced, indicating different geodynamic processes. Strike-slip faulting and magmatic activity in this area contribute to a broader distribution of deformation compared to the more localized crustal thickening in the Zagros. In southeastern Zagros, strike-slip faults in central Iran accommodate much of the northward convergence of the Arabian Plate, thereby limiting the extent of crustal thickening.",Not About Sufficiency
Incorporating artificial intelligence in medical diagnosis: A case for an invisible and (un)disruptive approach,"As big data becomes more publicly accessible, artificial intelligence (AI) is increasingly available and applicable to problems around clinical decision-making. Yet the adoption of AI technology in healthcare lags well behind other industries. The gap between what technology could do, and what technology is actually being used for is rapidly widening. While many solutions are proposed to address this gap, clinician resistance to the adoption of AI remains high. To aid with change, we propose facilitating clinician decisions through technology by seamlessly weaving what we call 'invisible AI' into existing clinician workflows, rather than sequencing new steps into clinical processes. We explore evidence from the change management and human factors literature to conceptualize a new approach to AI implementation in health organizations. We discuss challenges and provide recommendations for organizations to employ this strategy.",Not About Sufficiency
"The utility of EOS data for federal, state and regional government applications: Prospects and challenges","NASA's Earth Observation System (EOS) satellite data are contributing to the development of a variety of policy relevant remote sensing applications within State/Regional governments in the United States. The Synergy program is a NASA-funded activity designed to promote the use of EOS data and related remote sensing technologies for a variety of applications. Examples from the Synergy program indicate that EOS data are suitable for developing policy relevant applications in precision agriculture, management of natural resources, management of water resources, public health, urban planning and disaster management. There are however several barriers that need to be overcome before users outside the research community fully adopt these technologies. Some of the barriers include lack of awareness, experience and training by end users in these technologies, unproven cost benefits, and issues related to data formats, consistence and continuity.",Not About Sufficiency
Public Health Aspects of Climate Change Adaptation in Three Cities: A Qualitative Study,"Climate change presents an unprecedented public health challenge as it has a great impact on population health outcomes across the global population. The key to addressing these health challenges is adaptation carried out in cities through collaboration between institutions, including public health ones. Through semi-structured interviews (n = 16), this study investigated experiences and perceptions of what public health aspects are considered by urban and public health planners and researchers when planning climate change adaptation in the coastal cities of Soderhamn (Sweden), Porto (Portugal) and Navotas (the Philippines). Results of the thematic analysis indicated that participating stakeholders were aware of the main climate risks threatening their cities (rising water levels and flooding, extreme temperatures, and air pollution). In addition, the interviewees talked about collaboration with other sectors, including the public health sector, in implementing climate change adaptation plans. However, the inclusion of the public health sector as a partner in the process was identified in only two cities, Navotas and Porto. Furthermore, the study found that there were few aspects pertaining to public health (water and sanitation, prevention of heat-related and water-borne diseases, and prevention of the consequences associated with heat waves in vulnerable groups such as children and elderly persons) in the latest climate change adaptation plans posted on each city's website. Moreover, participants pointed to different difficulties: insufficient financial resources, limited intersectoral collaboration for climate change adaptation, and lack of involvement of the public health sector in the adaptation processes, especially in one of the cities in which climate change adaptation was solely the responsibility of the urban planners. Studies using larger samples of stakeholders in larger cities are needed to better understand why the public health sector is still almost absent in efforts to adapt to climate change.",Not About Sufficiency
Local temperature impact of urban heat mitigation strategy based on WRF integrating urban canopy parameters and local climate zones,"Urbanization has led to significant alterations in surface properties, contributing to surging urban heatwave events. The detrimental impact of urban heatwaves on public health highlights the urgency for proactive evaluations of diverse heat mitigation strategies. This study assesses the impacts of cool roof (CR), green roof (GR) and rooftop photovoltaic panel (PV) strategies on urban air temperatures within Guangzhou and Foshan agglomeration, utilizing the Weather Research and Forecasting with Urban Canopy Model (WRF-UCM). A comparative analysis of heat exchanges is conducted between two types of urban morphology datasets: local climate zone maps with categorized urban canopy parameters (LCZ-UCPs) and gridded urban canopy parameters from real building databases (gridded-UCPs). The results indicate that the discrepancies in the average temperature between LCZ-UCP and gridded-UCP is almost negligible, whereas increase to 0.39 degrees C and 0.54 degrees C for the daily maximum and minimum temperature, respectively. CR provides the most substantial cooling at an average of 0.44 degrees C, followed by GR and PV. Scenarios with the two types of morphology datasets reveal varying mitigation efficiencies. CRs with LCZ-UCPs show more pronounced temperature reductions, whereas GRs and PVs with gridded-UCPs in central urban regions demonstrate stronger cooling effects. CRs and PVs cool temperature by decreasing sensible heat flux, whereas GR is largely influenced by enhanced evapotranspiration especially with grass plantation. These findings elucidate the differing efficiencies of the three mitigation strategies and highlight the representation discrepancies in climatic simulations brought by the two urban canopy datasets. This emphasizes the importance of accurate urban morphological datasets in evaluating mitigation strategies in WRFUCM, thus providing practicable insights for urban planning and climate policy-making.",Not About Sufficiency
Descriptor selection for predicting interfacial thermal resistance by machine learning methods,"Interfacial thermal resistance (ITR) is a critical property for the performance of nanostructured devices where phonon mean free paths are larger than the characteristic length scales. The affordable, accurate and reliable prediction of ITR is essential for material selection in thermal management. In this work, the state-of-the-art machine learning methods were employed to realize this. Descriptor selection was conducted to build robust models and provide guidelines on determining the most important characteristics for targets. Firstly, decision tree (DT) was adopted to calculate the descriptor importances. And descriptor subsets with topX highest importances were chosen (topX-DT, X=20, 15, 10, 5) to build models. To verify the transferability of the descriptors picked by decision tree, models based on kernel ridge regression, Gaussian process regression and K-nearest neighbors were also evaluated. Afterwards, univariate selection (UV) was utilized to sort descriptors. Finally, the top5 common descriptors selected by DT and UV were used to build concise models. The performance of these refined models is comparable to models using all descriptors, which indicates the high accuracy and reliability of these selection methods. Our strategy results in concise machine learning models for a fast prediction of ITR for thermal management applications.",Not About Sufficiency
Standardizing Community Resilience Planning and Assessment,"Communities are complex socio-technical systems. To understand how communities can better respond to and recover from stressors and shocks such as natural, technical, or human-caused hazards, it is important to understand the social and economic priorities of the community and then to determine how buildings and infrastructure systems should perform in order to meet those priorities. When the anticipated performance of these systems for hazards is determined, gaps between desired performance and anticipated performance present opportunities to improve resilience. The National Institute of Standards and Technology (NIST) Community Resilience Program develops guidance and tools to assist communities in developing plans to improve their resilience. The Community Resilience Planning Guide for Buildings and Infrastructure Systems (Guide), published in 2015, describes a six-step process that guides communities to develop resilience plans or incorporate resilience into existing long-term plans. A companion Community Resilience Economic Decision Guide for Buildings and Infrastructure Systems (EDG) and Economic Decision Guide Software (EDGe$) Tool allows evaluation of alternative approaches to increase community resilience by considering benefits and costs, including losses avoided and co-benefits. A number of communities have begun using the Guide, either following the process as described or as a reference to inform the incorporation of resilience concepts in other plans. NIST has had the opportunity to be involved with some of these implementations and to learn how the Guide is being used, how it relates to other planning processes, and how tools based on the Guide process can make resilience planning and implementation more accessible to a larger number of communities. This experience informs NIST research focused on development of science-based tools to measure community resilience and support decision-making. This paper describes the Guide process, what is being learned from its use, and identifies an opportunity for a standard based on the Guide process.",Not About Sufficiency
Accurate Geometries of Large Molecules by Integration of the Pisa Composite Scheme and the Templating Synthon Approach,"An effective yet reliable computational workflow is proposed, which permits the computation of accurate geometrical structures for large flexible molecules at an affordable cost thanks to the integration of machine learning tools and DFT models together with reduced scaling computations of vibrational averaging effects. After validation of the different components of the overall strategy, a panel of molecules of biological interest have been analyzed. The results confirm that very accurate geometrical parameters can be obtained at reasonable cost for molecules including up to about 50 atoms, which are the largest ones for which comparison with high-resolution rotational spectra is possible. Since the whole computational workflow can be followed employing standard electronic structure codes, accurate results for large-sized molecules can be obtained at DFT cost also by nonspecialists.",Not About Sufficiency
"Take a hike: Spatializing allemannsretten and transportation accessibility for outdoor recreation in the Greater Stavanger Region, Norway","A special connection between people and their environment is legally recognized in Norway as Allemannsretten, the right to enjoy a large part of Norwegian nature. Scholarship on leisure mobility, spatial planning, and transport geography recognizes the intrinsic value of this spatial connection. As people travel for outdoor recreation, equitable access to recreation is a goal for just transport systems and must be achieved as these systems are digitalized and electrified for low-carbon transitions. The aim of the article is to identify the impact of transportation accessibility on outdoor recreation habits in the Greater Stavanger Region, Norway's third-largest metropolitan region. With car-centric development since the 1970s and ambitious automobility-reduction targets, transportation accessibility for outdoor recreation is a key indicator of challenges to overcome in mobility transitions, yet features marginally in public debate. In focusing on popular outdoor day trips and based on multisited interviews with both car owners and non-owners, the authors identify oversights in accessibility, spatializing the issue of local destinations in relation to urban transport transition. They conclude that policymakers must address specific gaps to make transport systems desirable and inclusive, and that a spatial lens can be used to problematize and advance just low-carbon transitions.",Not About Sufficiency
Genetics in primary care: validating a tool to pre-symptomatically assess common disease risk using an Australian questionnaire on family history,"BackgroundA positive family history for diabetes, cardiovascular diseases or various types of cancer increases the relative risk for these diseases by 2 to 5 times compared to people without a positive family history. Taking a family history in daily general practice is useful for early, pre-symptomatic risk assessment, but at the moment no standardized family history questionnaire is available in the Dutch language. In this study we used a 9-item questionnaire, previously developed and applied in an Australian study, to probe family history for 7 specific conditions. The aim of the present qualitative study was to test face and content validity of the Australian family history questionnaire in Dutch general practice and to advance the standardization of intake information at an international level. We conducted 10 cognitive interviews with patients over 4 rounds, using the verbal probing technique. This approach allows the collection of data through a series of probe questions, with the aim of obtaining detailed information. After each interview round we modified the questionnaire based on the answers of the interviewees. We also performed 10 semi-structured interviews with general practitioners (GPs) to get their opinion on the content and usability of the questionnaire in practice.ResultsPatients varied in age and gender, and 4 patients were known to have a genetic disorder. The GPs varied in age, gender, clinical experience, type of practice and location. In the first round, seven problems were identified in the questionnaire in the categories Comprehension (1), Recall (2), Judgement (0), Response process (2) and Completeness, (2); by the fourth and final round no problems remained. The content and usability of the questionnaire were assessed positively.ConclusionsWhen translated for everyday use in Dutch general practice, the Australian family history questionnaire showed a strong face and content validity, and GPs were positive regarding feasibility. Validation of this family history questionnaire could aid in the standardized integration of genetically relevant information in the electronic health record and clinical research. Conspicuous questionnaire information might alert the GP regarding specific conditions and enable detection of disease at an earlier stage. Additional questionnaire requirements needed however are accurate patient information and consistent, accessible locations in the electronic health record with a possibility to be automatically registered. By deriving a Dutch family history questionnaire convenient for GPs, we adapted a template that might also prove useful for other countries and other medical professionals. This development could make the rapid operationalization of readily available genetic knowledge feasible in daily practice and clinical research, leading to improved medical care.",Not About Sufficiency
Machine Learning Fusion Model Approach for the Real-Time Detection of Head Gestures using IMUs,"Modern sensor technology has contributed to the study of human behaviors during social interactions. Inertial movement units (IMUs) have shown great promise in the recognition of communication cues displayed by head gestures, which are important for healthy interactions. However, no gold standard exists to automatically detect head actions from IMUs. This paper presents the design of a real-time head-action detection (HAD) unit based on a new real-time fusion model architecture approach. An analysis of buffer sizes and feature contribution using a decision tree (DT) classifier and a predictor importance fusion is presented. The fusion model is composed of two classification stages wherein the first stage focus on recognizing head position and the second on recognizing head motion. The designed HAD unit uses a data buffer size of 3s, 7 features in total, and a DT classifier. Results show a testing accuracy of 97.91% and an Fl-score of 98.5% The use of the designed HAD unit and its architecture could allow for easy retraining to add recognition of additional head actions by having specialized head action classification models.",Not About Sufficiency
Enhancing Cervical Cancer Prediction: A Comparative Analysis of Machine Learning Algorithms and Development of a Novel Screening Tool,"The early discovery of cervical cancer is crucial for efficient treatment and increased survival rates, making it a severe public health concern [1]. This study uses a consistent dataset to compare various machine-learning methods for cervical cancer prediction. We utilized a variety of machine learning techniques, including Random Forest, Naive Bayes, Support Vector Machine (SVM) with a linear kernel, K-Nearest Neighbors (KNN), Logistic Regression, and Extreme Gradient Boosting (XGBoost), to identify and forecast the risk of cervical cancer. Based on the accuracy, precision, recall, F1-score, and confusion matrices, the effectiveness of these algorithms was assessed [2]. The most appropriate model for this application is XGBoost, which fared better than other models in recall and F1-score, even if more conventional methods, such as Random Forest and KNN, showed excellent overall accuracy. The study results imply that XGBoost has excellent potential for creating an efficient cervical cancer screening tool due to its balance of sensitivity and precision. The model is then integrated into a web-based application and an interactive chatbot designed to facilitate early detection and assessment of cervical cancer risks.",Not About Sufficiency
YADA - Reference Free Deconvolution of RNA Sequencing Data,"Introduction We present YADA, a cellular content deconvolution algorithm for estimating cell type proportions in heterogeneous cell mixtures based on gene expression data. YADA utilizes curated gene signatures of cell type-specific marker genes, either obtained intrinsically from pure cell type expression matrices or provided by the user.Methods YADA implements an accessible and extensible deconvolution framework uniquely capable of handling marker genes alone as inputs. Adoption barriers are lowered significantly by relying solely on literature-supported cell type-specific signatures rather than full transcriptomic profiles from purified isolates. However, flexible inputs do not necessitate sacrificing rigor - predictions match metrics of current methodologies through an integrated optimization scheme balancing multiple inference algorithms. Efficiency optimizations via compiled runtimes enable rapid execution. Packaging as an importable Python toolkit promotes community enhancement while retaining codebase extensibility.Results Validation studies demonstrate that YADA matches or exceeds the performance of current deconvolution methods on benchmark datasets. To demonstrate the utility and enable immediate usage, we provide an online Jupyter Notebook implementation coupled with tutorials.Conclusion YADA provides an accurate, efficient, and extensible Python-based toolkit for cellular deconvolution analysis of heterogeneous gene expression data.",Not About Sufficiency
The Management of IoT-Based Organizational and Industrial Digitalization Using Machine Learning Methods,"Recently, the widespread adoption of the Internet of Things (IoT) model has led to the development of intelligent and sustainable industries that support the economic security of modern societies. These industries can offer their participants a higher standard of living and working services via digitalization. The IoT also includes ubiquitous technology for extracting context information to deliver valuable services to customers. With the growth of connected things, the related designs often suffer from high latency and network overheads, resulting in unresponsiveness. The continuous transmission of enormous amounts of sensor data from IoT nodes is problematic because IoT-based sensor nodes are highly energy-constrained. Recently, the research community in the field of IoT and digitalization has labored to build efficient platforms using machine learning (ML) algorithms. ML models that run directly on edge devices are intensely interesting in the context of IoT applications. The use of intelligence ML algorithms in the IoT can automate training, learning, and problem-solving while enabling decision-making based on past data. Therefore, the primary aim of this research is to provide a systematic procedure to review the state-of-the-art on this scope and offer a roadmap for future studies; thus, a structure is introduced for industry sustainability, based on ML methods. The publications were reviewed using a systematic approach that divided the papers into four categories: reinforcement learning, semi-supervised learning, unsupervised learning, and supervised learning. The results showed that ML models could manage IoT-enabled industries efficiently and provide better results compared to other models, with significant differences in learning time and performance. The study findings are considered from a variety of angles concerning the industrial sector's capacity management of the new elements of Industry 4.0 by combining the industry IoT and ML. Additionally, unique and relevant instructions are provided for the designers of expert intelligent production systems in industrial domains.",Not About Sufficiency
Factors affecting orderly parking of dockless shared bicycles: an exploratory study,"As dockless shared bicycles (DSB) have become more commonplace, disorderly parking behaviour has attracted public concern and infringes upon public spaces in urban areas. The current research explored factors that affect users' decisions regarding where to park their DSB in order to examine the importance of intrinsic and extrinsic factors suggested to influence parking decisions. Following a literature review and focus group study, a survey instrument measuring social demographics, travel behaviour, parking behaviour, planned behaviour and motivation for prosocial behaviour consisting of 20 items was developed and deployed. Using ordered logit model estimation, users' intentions for orderly parking of DSB was measured and influential factors enhancing or limiting intention were identified. The lack of a shared definition of 'orderly parking' is the most noteworthy factor affecting DSB parking. The empirical findings confirm the significant role played by social norms, reciprocity, communication responsibility and institutional environment in regulating and influencing proper DSB parking behaviour and suggest that these factors may represent a more important aspect to be accounted for than financial incentives.",Not About Sufficiency
"Lexibank, a public repository of standardized wordlists with computed phonological and lexical features","The past decades have seen substantial growth in digital data on the world's languages. At the same time, the demand for cross-linguistic datasets has been increasing, as witnessed by numerous studies devoted to diverse questions on human prehistory, cultural evolution, and human cognition. Unfortunately, most published datasets lack standardization which makes their comparison difficult. Here, we present a new approach to increase the comparability of cross-linguistic lexical data. We have designed workflows for the computer-assisted lifting of datasets to Cross-Linguistic Data Formats, a collection of standards that make these datasets more Findable, Accessible, Interoperable, and Reusable (FAIR). We test the Lexibank workflow on 100 lexical datasets from which we derive an aggregated database of wordlists in unified phonetic transcriptions covering more than 2000 language varieties. We illustrate the benefits of our approach by showing how phonological and lexical features can be automatically inferred, complementing and expanding existing cross-linguistic datasets.",Not About Sufficiency
"Constructing and Optimizing the ""Special Planning"" Practical Teaching System of Resources Environment and Rural and Urban Planning Management Professional","The ""special planning"" practice teaching of resources environment and rural and urban planning management professional is the professional practice teaching is an important part of the system. Based on analyses the importance of the professional ""special planning"" practice teaching, the ""special planning"" practice teaching system of resources environment and rural and urban planning management professional was build in the curriculum practice teaching, the fieldtrip practice teaching, the second classroom teaching, the social practice teaching, the integrated design practice teaching and so on, and put forward the optimization method of the professional ""special planning"" practice teaching: deepening education teaching reform, improve experiment, practice conditions, formulate the perfect evaluation system.",Not About Sufficiency
Data-driven Interactive Crowd Management Systems for Metaverse Scenarios,"Understanding human behavior in urban settings has never been more accessible thanks to the growing accessibility of enormous volumes of data produced by people, cars, and other connected objects. We are entering a new era of innovation and creativity with the Metaverse. It will integrate comprehensive technologies such as 3D imaging, eXtended reality (XR), and advanced sensing into next-generation Internet services. Moreover, recent advances in disruptive technology, such as Artificial Intelligence (AI), and Machine Learning (ML), adding to the increasing decentralization of the Internet due to the possibility to deploy complex analytics across Edge-Cloud, are driving the further development of the Metaverse, as a 3D network of experiences and services. It allows users a sense of synchronous and persistent presence, with continuity of data, which highlights Crowd Management (CM) techniques to manage and control data-driven communications. CM techniques are vital in channelizing the data to help design mobile networks, control emergencies, and plan infrastructures. In this research, we will provide an overview of CM, its properties, and its functional breakdown following the integration of AI into CM. We will introduce the concept of Metaverse Crowd Management (MCM) and examine the various scenarios and associated challenges and opportunities.",Not About Sufficiency
A sustainable pathway towards prioritization of multifunctional benefits in urban agriculture contributing to shrinking cities in developing countries - An empirical case of Sri Lanka,"Even though currently at the growth stage, many developing countries are projected to experience urban shrinkage in the future, which demands long-term actions to improve urban sustainability. The study's objective is to investigate the feasibility of using urban agriculture as an open space management strategy under future shrinking and ageing scenarios in developing countries from the urban planning perspective. The study set a future urban shrinking and ageing scenario for the Colombo District, Sri Lanka, for the year 2100. It assessed the perceptions of 93 urban planning professionals about using urban agriculture as a strategy to manage open spaces in the study area in the given future scenarios. These respondents were assumed to be future urban planning professionals living in the year 2100. Results revealed that urban planning professionals highly accept that the multifunctionality of urban agriculture is vital for urban sustainability in future urban shrinkage, and higher preferences were shown for the non-cash benefits, including educational, health, and social benefits. The study also identified the sub-benefits that need to be prioritized under each benefit category and the inter-relationships between them. Finally, it was also found that when these multifunctional benefits are integrated into the urban planning under the future urban shrinkage and ageing scenarios, farmlands that are <1 acre in size, home gardens, community gardens, and privately owned lands are the most suitable land-use characteristics to be considered.",Not About Sufficiency
MACHINE LEARNING TECHNIQUES USING ENVIROMENTAL DATA FROM REMOTE SENSING APPLIED TO MODELING DENGUE RISK IN BRAZIL,"Mosquitoes are the most important vectors of human diseases in tropical countries. In particular, Aedes aegypti is the main vector for Chikungunya, Dengue and Zika in South America and Asia. In recent years, transmission of these diseases has increased predominantly in urban areas and has become a major public health problem. This study presents an algorithm for dengue risk prediction in Brazil. It is based in machine learning techniques and uses environmental and socioeconomic variables to predict the risk of dengue propagation all over Brazil, at municipality level. Data from 2010 to 2013 of monthly dengue episodes distribution have been obtained from the Notifiable Diseases Information System (SINAN), developed by the Brazil ministry of health. In addition, environmental data had been obtained from the distributed products of earth observation satellite missions that acquire globally and periodically data. Three types of machine learning algorithms have been applied and compared. The best results have been obtained when using random forests.",Not About Sufficiency
AI and data-driven media analysis of TV content for optimised digital content marketing,"To optimise digital content marketing for broadcasters, the Horizon 2020 funded ReTV project developed an end-to-end process termed ""Trans-Vector Publishing"" and made it accessible through a Web-based tool termed ""Content Wizard"". This paper presents this tool with a focus on each of the innovations in data and AI-driven media analysis to address each key step in the digital content marketing workflow: topic selection, content search and video summarisation. First, we use predictive analytics over online data to identify topics the target audience will give the most attention to at a future time. Second, we use neural networks and embeddings to find the video asset closest in content to the identified topic. Third, we use a GAN to create an optimally summarised form of that video for publication, e.g. on social networks. The result is a new and innovative digital content marketing workflow which meets the needs of media organisations in this age of interactive online media where content is transient, malleable and ubiquitous.",Not About Sufficiency
Understanding spatial inequalities and stratification in transportation accessibility to social infrastructures in South Korea: multi-dimensional planning insights,"This research investigated spatial inequalities in transportation accessibility to social infrastructures (SIs) in South Korea, using a multi-dimensional methodological approach, including descriptive/bivariate analysis, explanatory factor analysis (EFA), K-Mean cluster analysis, and multinomial logit model (MNL). Our study confirmed pronounced spatial disparities in transportation accessibility to SIs, highlighting significantly lower access in rural and remote regions compared to urban centers and densely populated areas, consistent with existing literature. Building on prior findings, several additional findings were identified. First, we uncovered significant positive correlations among accessibility to different types of SIs in four critical categories: green and recreation spaces, health and aged care facilities, educational institutions, and justice and emergency services, revealing prevalent spatial inequality patterns. Second, we identified three distinct accessibility clusters (High, Middle, and Low) across the critical SI categories. Specifically, residents within the High cluster benefited from the closest average network distances to all SIs, while those in the Low cluster faced significant accessibility burdens (e.g., 22.9 km for welfare facilities, 20.1 km for hospitals, and 19.2 km for elderly care facilities). Third, MNL identified factors such as population density and housing prices as pivotal in spatial stratification of accessibility. Specifically, areas with lower SI accessibility tended to have a higher proportion of elderly residents. Also, decreased accessibility correlated with diminished traffic volumes across all transportation modes, particularly public transportation. This research contributes to enhancing our understanding of spatial inequalities in transportation accessibility to SIs and offers insights crucial for transportation and urban planning.",Not About Sufficiency
Machine learning-assisted check dam planning on the Chinese Loess Plateau,"Check dams, as an effective soil and water conservation measure, have intercepted billions of tons of eroded sediment on the Chinese Loess Plateau (CLP), significantly reducing the Yellow River's sediment load. However, uncertainty regarding the optimal sites and appropriate number of check dams for future planning limits their potential ecological and economic benefits. Here, we employ a machine learning model trained on hydrological, topographic, and economic factors to identify suitable watersheds for check dam construction across 437,630 watersheds on the CLP. Additionally, we use the check dam system planning method to determine the appropriate number of check dams for future construction. Our analysis indicates that 14,280 watersheds are suitable for check dam construction, primarily located in the High-plain Gully Region and Loess Hilly and Gully Region of the CLP. In these watersheds, constructing 4,551 key dams and 24,816 small and medium-sized check dams is feasible. Validation using the receiver operating characteristic curve shows an area under the curve value of 0.972, demonstrating excellent model accuracy. Additionally, the Mean Decrease Gini index indicates that, among the numerous factors we considered, the soil erosion rate is the most influential factor in determining optimal watersheds. These findings will assist decision-makers in developing plans for the largest soil and water conservation projects on the CLP, and provide methodological insights for dam siting studies in other regions.",Not About Sufficiency
Modelling the impact of medical student stress after a public health emergency based on Logistic regression algorithm,"The COVID-19 outbreak, declared a public health emergency by the WHO, has intensified stress levels for medical students who were already under significant pressure. Medical students has been an increase in mental health issues and changes in health behaviors. This study utilized a Logistic regression algorithm to determine factors that cause stress among medical students following public health emergencies in China. Data on demographic characteristics, mental health conditions, and health behaviors post-emergency were collected from 1927 medical students. Chi-square tests and logistic regression were used to analyze the factors that influenced stress levels among Chinese medical students following public health emergencies. Results showed that after public health emergencies, medical students experienced higher stress levels (44.84%). Psychological factors such as anxiety and loneliness, and health behaviors like sleep patterns, social isolation, and changes in daily routines were identified as primary contributors to increased stress levels (p<0.05). In conclusion, psychological factors and health behaviors are primary contributors to increased stress levels, the study highlights the importance of addressing various factors that contribute to stress among medical students, including enhancing stress management abilities and encouraging healthy behaviors to promote the physical and mental well-being of Chinese medical students during public health emergencies.",Not About Sufficiency
Chronic Diseases Prediction using two different pipelines TPOT and Genetic Algorithm based models: A Comparative analysis,"Chronic diseases are a significant burden on public health systems worldwide, and early detection is crucial for their effective management. While automated tools such as TPOT have proven useful for predicting chronic diseases, they still have limitations such as complex pipeline structures, dependence on random search methods for hyperparameter tuning, and lengthy model building times. To address these limitations, this study compared TPOT with self-designed Genetic Algorithm (GA)-based pipelines in predicting several chronic diseases, including COPD, Cancer, Heart Disease, and Diabetes. Three GA-based pipelines were created, each encapsulating distinct machine learning models. The results revealed that GA-based pipelines outperformed TPOT in performance, achieving exceptional accuracies of 99% and 93.4% on the cancer and diabetes datasets, respectively. The findings suggest that GA-based models, designed with domain expertise and tailored to specific datasets, provide a more reliable and effective strategy for predicting chronic diseases. This underscores the notion that GA-based pipelines offer clinicians and practitioners a dependable means of obtaining accurate results expeditiously.",Not About Sufficiency
Planned resettlement to avoid climatic hazards: What prospects for just outcomes in China?,"Planned resettlement is being widely considered as a response to the impacts of climate change. As many millions of people are expected to be displaced in the coming decades, scholars and policymakers are searching for precedence to inform their research and planning, particularly from experiences of Development-Induced Displacement and Resettlement (DIDR). Nowhere in the world is DIDR and other closely related forms of planned resettlement more prevalent than in China: an estimated 78 million people have been displaced by development projects over the last six decades. While planned resettlement has consistently been shown to cause impoverishment, the Chinese state views it as the answer to a multitude of social ills including poverty, environmental damage, low levels of domestic consumption, and most recently, climate change, providing impetus to the normalisation of resettlement as adaptation. This paper examines the prospects for just outcomes in resettlement projects by examining distributive justice at multiple scales in existing resettlement practice in China. It finds that due to the interplay between resettlement and questions of procedural justice, prospects for just outcomes are quite limited, and that in order to achieve fair adaptation, alternatives to planned resettlement should be emphasised.",Not About Sufficiency
Revealing multiscale and nonlinear effects of urban green spaces on heat islands in high-density cities: Insights from MSPA and machine learning,"Urban heat islands (UHI), intensified by global warming and urbanization, threaten urban sustainability and public health. This study examines the spatiotemporal dynamics of urban green spaces (UGS) in Shanghai (2000-2020), focusing on spatial scale effects, morphological patterns, and nonlinear relationships. Using Morphological Spatial Pattern Analysis (MSPA), Pearson correlation, and random forest models, the study explores UGS-UHI interactions. The results reveal a 49.6 % reduction in UGS area (from 2808.48 km2 to 1414.72 km2), accompanied by fragmentation of Core and Bridge patterns, and a rise in land surface temperature (LST) from 42.91 degrees C to 45.22 degrees C. Core areas decreased by 48.8 % during 2000-2011 but rebounded to 30 % of total UGS by 2020. Bridge areas, vital for connectivity, dropped sharply from 25 % in 2011 to 9 % in 2020. At the 300- meter scale, UGS showed strong linear cooling effects, while at the 1200-meter scale, nonlinear relationships emerged, with Core areas >= 0.5 km2 significantly reducing UHI. Fragmented patterns like Islet (rising from 10 % to 23 % during 2005-2011) intensified UHI, while Loop and Bridge patterns mitigated heat by enhancing connectivity. This study advances understanding of UGS-UHI dynamics, emphasizing scale-dependent and morphological thresholds. The findings provide actionable insights for climate-resilient urban planning, supporting SDG 11 and SDG 13.",Not About Sufficiency
Understanding the mechanism of microplastic-associated antibiotic resistance genes in aquatic ecosystems: Insights from metagenomic analyses and machine learning,"The pervasive presence of microplastics (MPs) in aquatic systems facilitates the transmission of antibiotic resistance genes (ARGs), thereby posing risks to ecosystems and human well-being. However, owing to variations in environmental backgrounds and the limited scope of research subjects, studies on ARGs in MPs lack unified conclusions, particularly regarding whether different types of MPs selectively promote ARG enrichment. Analysing large-scale datasets can better encompass broad spatiotemporal scales and diverse samples, facilitating a more extensive exploration of the complex ecological relationships between MPs and ARGs. The present study integrated existing metagenomic datasets to conduct a comprehensive risk assessment and comparative analysis of resistance groups across various MPs. In addition, we endeavoured to elucidate potential associations between ARGs and bacterial taxa, as well as MP structural features, using machine learning (ML) methods. The findings of our research highlight the pivotal role of MP type in shaping plastispheres, accounting for 9.56 % of the biotic variation (Adonis index) and explaining 18.59 % of the ARG variance. Compared to conventional MPs, biodegradable MPs, such as polyhydroxyalkanoates (PHA) and polylactic acid (PLA), exhibit lower species uniformity and diversity but pose a higher risk of ARG occurrence. These ML approaches effectively forecasted ARG abundance by using the bacterial taxa and molecular structure descriptors (MDs) of MPs (average R2 tra = 0.882, R2 test = 0.759). Feature analysis showed that MDs associated with lipophilicity, solubility, toxicity, and surface potential significantly influenced the relative abundance of ARGs in the plastispheres. The interpretable multiple linear regression (MLR) model, particularly notable, elucidated a linear relationship between bacterial genera and ARGs, offering promise for identifying potential ARG hosts. This study offers novel insights into ARG dynamics and ecological risks within aquatic plastispheres, highlighting the importance of comprehensive MP monitoring initiatives.",Not About Sufficiency
Food Freshness Measurements and Product Distinguishing by a Portable Electronic Nose Based on Organic Field-Effect Transistors,"Determination of food freshness, which is the most ancient role of the human sense of smell, is still a challenge for compact and inexpensive electronic nose devices. Fast, sensitive, and reusable sensors are long-awaited in the food industry to replace slow, labor-intensive, and expensive bacteriological methods. In this work, we present microbiological verification of a novel approach to food quality monitoring and spoilage detection using an electronic nose based on organic field-effect transistors (OFETs) and its application for distinguishing products. The compact device presented is able to detect spoilage-related gases as early as at the 4 x 104 CFU g-1 bacteria count level, which is 2 orders of magnitude below the safe consumption threshold. Cross selective sensor array based on OFETs with metalloporphyrin receptors were made on a single substrate using solution processing leading to a low production cost. Moreover, machine learning methods applied to the sensor array response allowed us to compare spoilage profiles and separate them by the type of food: pork, chicken, fish, or milk. The approach presented can be used to monitor food spoilage and distinguish different products with an affordable and portable device.",Not About Sufficiency
Are young adults' choices temporary or permanent? Navigating economic recession with residential location and household expenditure,"This study started from changes in young adults' choices of residential location and travel behavior since the 2010s, which differ from the prior generations. This study aimed to determine whether the observed choices represent permanent shifts in preference changes or temporary responses to specific circumstances. Using longitudinal data and statistical models, this study analyzed the impact of the economic recession on the residential choices of young adults and their household expenditure. The analysis results indicate that young adults' choices are likely temporary responses to economic hardship. Young adults showed different choices in relocation and household expenditure during the recession, compared to before and after the recession, and reduced their housing and transportation expenditures during the recession, whereas the older age group did not. The young adult model indicated that increasing the location distance from cities increased their housing and transportation expenditure, which differed from the older adult model. Lastly, lower-income young adults chose to relocate, and the lowest-income group moved to areas closer to city centers. These findings have implications for urban development and smart growth policies, namely, providing various housing options in cities, including more affordable housing, and improving public transit systems for more young adults to use.",Not About Sufficiency
Quality Measurement Classification for Water Treatment using Neural Network with Reinforcement Programming for Weighting Optimization,"Water Quality is a basic need for human. If the quality level of the water is not appropriate, it will give dangerous impacts to the human life. Therefore, the measurement of the water quality becomes important because it needs specific treatments to make more acceptable for specific uses of the water such as drinkable water, paddy fields, river flow maintenance, water recreation or other environmental preservation purposes. in this paper we present measurement classification of water quality for treatment with involving some parameters of the water such as Biological Oxygen Demand, Chemical Oxygen Demand, Ph, Suspended Solid, etc. We use Neural Network to deal with the classification problems and make classification into 13 types for the water treatment. In this paper we propose our Reinforcement Programming algorithm to optimize weighting mechanism for Neural Network. Reinforcement Programming is an optimization algorithm derived from Reinforcement Learning, a new learning paradigm in machine learning that learns from the interaction with external environments to achieve a goal. Reinforcement Programming improved the Reinforcement Learning by shifting goal-based to function-based approach in order to solve the optimization problems in weighting mechanism of Neural Network. It updated the weights of Neural Network by implementing the exploitation and exploration of Neural Network weights, and then measured the differences of state values from a given state-function to assign a reward or punishment of the state. We applied our proposed Reinforcement Programming to optimize Neural Network weighting mechanism for water quality measurement classification and made series of experimental study with water quality treatment dataset provided by UCI Machine Learning Repository. To scrutinize the applicability of our proposed approach, we made performance comparison with common existing Neural. Neural weight update using Backpropagation. In the experimental results, our proposed Reinforcement Programming outperformed Backpropagation for weighting mechanism in precision and time.",Not About Sufficiency
The role of trade and investment liberalization in the sugar-sweetened carbonated beverages market: a natural experiment contrasting Vietnam and the Philippines,"Background: Trade and investment liberalization may facilitate the spread of sugar-sweetened carbonated beverages (SSCBs), products associated with increased risk factors for obesity, type II diabetes, and cardiovascular diseases (Circulation 121: 1356-1364, 2010). Apart from a limited set of comparative cross-national studies, the majority of analyses linking liberalization and the food environment have drawn on case studies and descriptive accounts. The current failure of many countries to reverse the obesity epidemic calls for investigation into both individual and systemic factors, including trade and investment policies. Methods: Using a natural experimental design we tested whether Vietnam's removal of restrictions on foreign direct investment (FDI) subsequent to its accession to the World Trade Organization in 2007 increased sales of SSCBs compared with a matched country, the Philippines, which acceded in 1995. Difference-in-difference (DID) models were used to test pre/post differences in total SSCB sales and foreign company penetration covering the years 1999-2013. Results: Following Vietnam's removal of restrictions on FDI, the growth rate of SSCB sales increased to 12.1 % per capita per year from a prior growth rate of 3.3 %. SSCB sales per capita rose significantly faster pre-and post-intervention in Vietnam compared with the control country the Philippines (DID: 4.6 L per annum, 95 % CI: 3.8 to 5.4 L, p < 0.008). Vietnam's increase in SSCBs was primarily attributable to products manufactured by foreign companies, whose annual sales growth rates rose from 6.7 to 23.1 %, again unmatched within the Philippines over this period (DID: 12.3 %, 95 % CI: 8.6 to 16.0 %, p < 0.049). Conclusions: Growth of SSCB sales in Vietnam, led by foreign-owned companies, significantly accelerated after trade and investment liberalization.",Not About Sufficiency
Investigating and Validating On-body Temperature Sensors for Personal Heat Exposure Tracking,"Individuals and communities around the world are increasingly exposed to extreme heat as a result of climate change. Urban residents are particularly vulnerable to extreme heat due to the urban heat island efect. However, understanding an individual's heat exposure and risk is difcult to assess due to variations in temperature within an urban environment. In this paper, we examine the potential for wearable temperature sensors to accurately measure personal heat exposure. We synthesize literature from felds spanning urban planning to public health and present the results of a user study validating a set of four commonly used of-the-shelf temperature sensors in two diferent urban settings across five on-body locations. Our investigation found that wearable temperature sensors are less reliable in highly urban areas and when worn in direct sunlight. We discuss important design considerations for wearable temperature sensors and identify actionable ways to improve future studies.",Not About Sufficiency
Patient-clinician brain concordance underlies causal dynamics in nonverbal communication and negative affective expressivity,"Patient-clinician concordance in behavior and brain activity has been proposed as a potential key mediator of mutual empathy and clinical rapport in the therapeutic encounter. However, the specific elements of patient-clinician communication that may support brain-to-brain concordance and therapeutic alliance are unknown. Here, we investigated how pain-related, directional facial communication between patients and clinicians is associated with brain-to-brain concordance. Patient-clinician dyads interacted in a pain-treatment context, during synchronous assessment of brain activity (fMRI hyperscanning) and online video transfer, enabling face-to-face social interaction. In-scanner videos were used for automated individual facial action unit (AU) time-series extraction. First, an interpretable machine-learning classifier of patients' facial expressions, from an independent fMRI experiment, significantly distinguished moderately painful leg pressure from innocuous pressure stimuli. Next, we estimated neural-network causality of patient-to-clinician directional information flow of facial expressions during clinician-initiated treatment of patients' evoked pain. We identified a leader-follower relationship in which patients predominantly led the facial communication while clinicians responded to patients' expressions. Finally, analyses of dynamic brain-to-brain concordance showed that patients' mid/posterior insular concordance with the clinicians' anterior insula cortex, a region identified in previously published data from this study(1), was associated with therapeutic alliance, and self-reported and objective (patient-to-clinician-directed causal influence) markers of negative-affect expressivity. These results suggest a role of patient-clinician concordance of the insula, a social-mirroring and salience-processing brain node, in mediating directional dynamics of pain-directed facial communication during therapeutic encounters.",Not About Sufficiency
Application of environmental ecological strategy in smart city space architecture planning,"A smart city requires the organic integration of a good ecological environment with people's healthy life, so that the city can achieve the purpose of economic construction and ecological environmental protection. In this case, the spatial layout of a smart city must be based on the ecological environment. Under the guidance of the concept of ecological protection, the ecological space of smart cities is not only green space and parks. At the same time, the building communities in the city and the construction land in the city should be green and environmentally friendly. These situations require planning and design in the spatial design layout through networking and informatization. This article mainly studies the space development of smart cities under the integration of information technology and ecological environment from the perspective of urban space architectural planning. A series of problems in our country's cities and the close connection between smart cities and the ecological environment. After comprehensively analyzing the smart city spatial planning of different cities and regions under the influence of the ecological environment, it tries to analyze the guiding role and development direction of the smart city spatial architectural layout planning under the ecological environment. (C) 2021 Elsevier B.V. All rights reserved.",Not About Sufficiency
How can we deliver on the promise of precision medicine in oncology and beyond? A practical roadmap for action,"BackgroundPrecision medicine (PM) is a form of personalized medicine that recognizes that individuals with the same condition may have different underlying factors and uses molecular information to provide tailored treatments. This approach can improve treatment outcomes and transform lives through favorable risk/benefit ratios, avoidance of ineffective interventions, and possible cost savings, as evidenced in the field of lung cancer and other oncology/therapeutic settings, including cardiac disease, diabetes, and rare diseases. However, the potential benefits of PM have yet to be fully realized. DiscussionThere are many barriers to the implementation of PM in clinical practice, including fragmentation of the PM landscape, siloed approaches to address shared challenges, unwarranted variation in availability and access to PM, lack of standardization, and limited understanding of patients' experience and needs throughout the PM pathway. We believe that a diverse, intersectoral multistakeholder collaboration, with three main pillars of activity: generation of data to demonstrate the benefit of PM, education to support informed decision-making, and addressing barriers across the patient pathway, is necessary to reach the shared goal of making PM an accessible and sustainable reality. Besides healthcare providers, researchers, policymakers/regulators/payers, and industry representatives, patients in particular must be equal partners and should be central to the PM approach?from early research through to clinical trials and approval of new treatments?to ensure it represents their entire experience and identifies barriers, solutions, and opportunities at the point of delivery. ConclusionWe propose a practical and iterative roadmap to advance PM and call for all stakeholders across the healthcare system to employ a collaborative, cocreated, patient-centered methodology to close gaps and fully realize the potential of PM.",Not About Sufficiency
HOW TO RECONCILE HEALTH LAW AND ECONOMIC LAW WITH HUMAN RIGHTS? ADMINISTRATION OF JUSTICE IN TOBACCO CONTROL DISPUTES,"Tobacco companies and tobacco exporting WTO members have initiated an increasing number of disputes in national, regional and worldwide jurisdictions and investor-state arbitrations challenging the legal consistency of tobacco control measures such as Australia's ""Tobacco Plain Packaging"" legislation and regulations with international trade, investment and intellectual property law. The defendant countries and non-governmental organizations tend to justify tobacco-control measures by invoking public health provisions in international economic law am, domestic constitutional laws, public health legislation, human rights law and the World Health Organization (WHO) Framework Convention on Tobacco Control (FCTC) ratified by 177 UN member states. This article begins by asking how the fragmented systems of multilevel health, economic and human rights law and governance should be interpreted and coordinated in order to promote their mutual legal coherence. It then explores how multilevel courts should ""administer justice"" in tobacco control disputes with due regard to their diverse national and international jurisdictions, applicable laws and methods of legal interpretation. The article concludes that multilevel judicial administration of justice in tobacco control disputes requires judicial cooperation in applying ""constitutional methodologies"" (e.g. regarding ""balancing"" of competing rights, proportionality of restrictions, reasonable judicial justifications promoting 'Public reason'), mutually ""consistent interpretations"" (e.g. based on the ""integration principle"" limiting legal ""fragmentation') and ""judicial comity"" (e.g. regarding rule of law, respecting ""margins of appreciation"", protecting ""access to justice') so as to avoid incoherent judgments. The main lesson from more than 2500 years of legal and political experiences e.g. since the ancient Constitution of Athens (500 BC) with collective protection of ""Public goods"" (res publica) demanded by citizens remains the need for limiting abuses of power through multilevel ""republican constitutionalism"" providing for legal, judicial and democratic accountability mechanisms.",Not About Sufficiency
A Comprehensive Literature Review on Children's Databases for Machine Learning Applications,"The COVID-19 pandemic can be attributed as a main factor to accelerate the current digital transformation and to encourage innovation and technological adoption. Consequently, the care provided to our children, one of the significant aspects of life, needs to be adapted with the life's changes. Children are our future and our most precious resources. They need our attention in all life domains including health, education, safety and social interaction. Nowadays, technologies have been incorporated with machine learning and it has been proven that they are more powerful, reliable and profitable. Machine learning methods have been applied by many children-related studies to generate predictive models for different applications. The efficacy of the generated models mainly relies on the constructed databases. This article carries out a comprehensive survey on available children's databases constructed for machine-learning-based solutions with their methodologies, characteristics, challenges, and applications. First, it provides an overview of the available studies and classifies them based on their applications. Next, it defines a set of attributes and evaluates them while also shedding light on their pros and cons. The primary concerns related to collection, development and distribution of children's databases are also discussed. This study can be considered as a guideline for researchers in multidisciplinary fields to construct reliable databases and to develop more advanced techniques.",Not About Sufficiency
Machine Learning Prediction of Foodborne Disease Pathogens: Algorithm Development and Validation Study,"Background: Foodborne diseases, as a type of disease with a high global incidence, place a heavy burden on public health and social economy. Foodborne pathogens, as the main factor of foodborne diseases, play an important role in the treatment and prevention of foodborne diseases; however, foodborne diseases caused by different pathogens lack specificity in clinical features, and there is a low proportion of clinically actual pathogen detection in real life. Objective: We aimed to analyze foodborne disease case data, select appropriate features based on analysis results, and use machine learning methods to classify foodborne disease pathogens to predict foodborne disease pathogens that have not been tested. Methods: We extracted features such as space, time, and exposed food from foodborne disease case data and analyzed the relationship between these features and the foodborne disease pathogens using a variety of machine learning methods to classify foodborne disease pathogens. We compared the results of 4 models to obtain the pathogen prediction model with the highest accuracy. Results: The gradient boost decision tree model obtained the highest accuracy, with accuracy approaching 69% in identifying 4 pathogens including Salmonella, Norovirus, Escherichia coli, and Vibrio parahaemolyticus. By evaluating the importance of features such as time of illness, geographical longitude and latitude, and diarrhea frequency, we found that they play important roles in classifying the foodborne disease pathogens. Conclusions: Data analysis can reflect the distribution of some features of foodborne diseases and the relationship among the features. The classification of pathogens based on the analysis results and machine learning methods can provide beneficial support for clinical auxiliary diagnosis and treatment of foodborne diseases.",Not About Sufficiency
GIS based analysis for assessing the accessibility at hierarchical levels of urban green spaces,The accessibility to hierarchy (defined based on function and size) of Urban Green Spaces (UGS) is essential for frequent and optimal use of UGS as it promotes social interaction and physical activity among city population. The issue of accessibility to UGS is one of the crucial aspects of sustainable urban planning and it is linked to growing concern over the wellbeing of urban population particularly children and lower socioeconomic groups. The following study presents use of GIS based network analysis to assess the accessibility of UGS at hierarchical levels by applying different network distance to each hierarchy of UGS in a dense and complex urban setting in a developing region. Results show that there is poor accessibility to UGS at all hierarchical levels particularly at lower hierarchy of UGS which is meant for early age children. The large variability in accessibility at all hierarchical levels is also indicative of highly varying and skewed development patterns in the study area. The studies carried out in these regions and approach applied in this study may provide useful tools to planners to identify the deficient areas for future development of UGS for balanced and sustainable planning. (C) 2016 Elsevier GmbH. All rights reserved.,Not About Sufficiency
French grassroots volunteer organizations and prospective: Challenges and avenues for nonprofit research,"This article explores the importance of prospective for practitioners in the sector of grassroots volunteer organizations (GVOs, as associations). Prospective can be defined as an attitude of studying the future that must lead to action. In this, in times of crisis, prospective seems crucial and the GVOs' point of view can open a research agenda. The French case of GVOs deserves more attention because of their characteristics (e.g., 90% of GVOs have no employees, 75% of them have an annual budget of less than 10,000 euros), their relationship with the public authorities and their strong social impact on the community. The scenarios of evolution of the GVOs sector proposed by La Fonda are analyzed and become filters for the reading of the French literature dedicated to GVOs. They lead to four lines of research (isomorphism; normalization and standardization; evaluation and impact; organizational capacities). Following this, a research agenda dedicated to French GVOs is proposed. This work has intrinsic implications, conducting a literature review and proposing a research agenda compared to international agendas. It highlights the challenges GVOs face and the differences between desirable research in France and international research agendas. It constitutes possible inspirations for GVOs leaders thanks to a synoptic table.",Not About Sufficiency
"Globalization, pharmaceutical pricing, and South African health policy: Managing confrontation with US firms and politicians","Brewing since the advent of South African democracy in 1994 and promises of health sector transformation, an extraordinary drug war between President Nelson Mandela's African National Congress government and U.S. pharmaceutical manufacturers took on global proportions in 1998-1999. Within months of the passage of South African legislation aimed at lowering drug prices, the U.S. government quickly applied powerful pressure points to repeal a clause allowing potential importation of generic substitutes and imposition of compulsory licensing. At stake were not only local interpretations of patent law and World Trade Organization rules on Trade in Intellectual Property, but international power relations between developing countries and the pharmaceutical industry. In reviewing the ongoing debate, this article considers post-apartheid public health policy, U.S. government pressure to change the law, and pharmaceutical industry interests and links to the U.S. government, and evaluates various kinds of resistance to U.S. corporate and government behavior. The case thus raises-not for the first time-concerns about contemporary imperialism (""globalization""), the role of the profit motive as an incentive in vital pharmaceutical products, and indeed the depth of ""democracy"" in a country where high-bidding international drug firms have sufficient clout to embarrass Vice President Al Gore by pining him against the life-and-death interests of millions of consumers of essential drugs in South Africa and other developing countries.",Not About Sufficiency
"COVID-19: Evaluation of Fever Clinic and Fever Sentinel Configuration-A Case Study of Harbin, China","The COVID-19 pandemic has placed the inequalities in health services in countries around the world under severe pressure. As crucial pillars in the prevention and control of COVID-19, fever clinics and fever sentinels are important sites for the screening, diagnosis, and isolation of patients. This study comprehensively evaluated the spatial-layout characteristics, configuration quantity, and service capacity of 42 fever clinics and 418 fever sentinels in Harbin from the perspective of supply by using GIS spatial-analysis methods such as kernel density analysis. From the perspective of demand, we evaluated the accessibility of fever clinics with the modified two-step floating catchment area (2SFCA) method; the OD cost matrix method and Voronoi diagram method were used to evaluate the accessibility and service pressure of fever sentinels. This study found that a monocentric clustering characterizes the spatial layout of fever clinics, and the design of fever clinics in new urban areas and marginal rural areas is relatively lacking. The spatial layout of fever sentinels includes blank areas, and the service pressure in the central city area is relatively high. Combined with the assessment results, the study discussed optimization strategies and implementation paths for improving the public health and epidemic prevention system for COVID-19 in terms of four aspects: the transformation of governance practice, the spatial-planning response, the digital infrastructure response, and guarantees of policies and regulations.",Not About Sufficiency
FHIR-standardized data collection on the clinical rehabilitation pathway of trans-femoral amputation patients,"Lower limb amputation is a medical intervention which causes motor disability and may compromise quality of life. Several factors determine patients' health outcomes, including an appropriate prosthetic provision and an effective rehabilitation program, necessitating a thorough quantitative observation through different data sources. In this context, the role of interoperability becomes essential, facilitating the reuse of real-world data through the provision of structured and easily accessible databases. This study introduces a comprehensive 10-year dataset encompassing clinical features, mobility measurements, and prosthetic knees of 1006 trans-femoral amputees during 1962 hospital stays for rehabilitation. The dataset is made available in both comma-separated values (CSV) format and HL7 Fast Healthcare Interoperability Resources (FHIR)-based representation, ensuring broad utility and compatibility for researchers and healthcare practitioners. This initiative contributes to advancing community understanding of post-amputation rehabilitation and underscores the significance of interoperability in promoting seamless data sharing for meaningful insights into healthcare outcomes.",Not About Sufficiency
A Smart growth evaluation system based on Analytic Hierarchy Process,"With the acceleration of urbanization, urban planning becomes more and more important. We did some in-depth discussion about sustainable city and smart growth initiatives in this paper. Firstly, we define a metric which is a comprehensive index to measure the success of smart growth of a city. On the basis of a comprehensive analysis of the all factors that affect the smart growth of urban, we use the Analytic Hierarchy Process (AHP), which is a combination of quantitative and qualitative methods to define the metric. We select the smart growth index as the target layer, the three E's of sustainability (Economically prosperous, socially Equitable, and Environmentally Sustainable) as the principle layer. Besides, we select seven indicators (e.g. Gini coefficient, Employment growth rate) from a series of factors by using Cluster analysis and Principal component analysis to constitute the solution layer. Hence, we get a three-layer evaluation system. Secondly, we select Pittsburgh as our evaluation object. Using the metric defined and the data collected, we get the evaluation results of success of smart growth. Pittsburgh's current development plan does well in meeting the smart growth principle. Thirdly, according to the problems reflected in the evaluation results, combined with the local specific situation, we make the plans of sustainable development for the it-strengthening infrastructure construction of public transportation preferentially and constructing some distinctive places for residence. The suggestions and measures based on the evaluation results will be helpful for the sustainable development of cities.",Not About Sufficiency
Comprehensive learning particle swarm optimization enabled modeling framework for multi-step-ahead influenza prediction,"Epidemics of influenza are major public health concerns. Since influenza prediction always relies on the weekly clinical or laboratory surveillance data, typically the weekly Influenza-like illness (ILI) rate series, accurate multi-step-ahead influenza predictions using ILI series is of great importance, especially, to the potential coming influenza outbreaks. This study proposes Comprehensive Learning Particle Swarm Optimization based Machine Learning (CLPSO-ML) framework incorporating support vector regression (SVR) and multilayer perceptron (MLP) for multi-step-ahead influenza prediction. A comprehensive examination and comparison of the performance and potential of three commonly used multi-step-ahead prediction modeling strategies, including iterated strategy, direct strategy and multiple-input multiple-output (MIMO) strategy, was conducted using the weekly ILI rate series from both the Southern and Northern China. The results show that: (1) The MIMO strategy achieves the best multi-step-ahead prediction, and is potentially more adaptive for longer horizon; (2) The iterated strategy demonstrates special potentials for deriving the least time difference between the occurrence of the predicted peak value and the true peak value of an influenza outbreak; (3) For ILI in the Northern China, SVR model implemented with MIMO strategy performs best, and SVR with iterated strategy also shows remarkable performance especially during outbreak periods; while for ILI in the Southern China, both SVR and MLP models with MIMO strategy have competitive prediction performance (C) 2021 Elsevier B.V. All rights reserved.",Not About Sufficiency
Is There a Digital Rebound in the Process of Urban Green Development? New Empirical Evidence Using Ensemble Learning Methods,"The convergence of digitization and greening is an unavoidable path of modern economic progress. Nonetheless, the digital economy does not consistently align with the principles of green development, potentially leading to a rebound effect in urban digitalization initiatives. To investigate the correlation between the digital rebound effect and urban green development, this study utilizes panel data from Chinese prefecture-level cities spanning from 2011 to 2019. By examining the dual impact of the digital economy on green development, the paper posits a theoretical hypothesis regarding the nonlinear marginal effect of the digital economy. This research demonstrates an inverted U-shaped correlation between the digital economy and urban green development via empirical analyses employing the random forest algorithm and partial dependency plots. It supports the existence of a moderate digital resiliency effect, which eventually reaches a state of stability rather than greatly diminishing the degree of green development in urban areas. In addition, the heterogeneity analysis reveals that the positive effects of the digital economy are more popular in cities located in the eastern and central regions, as well as in the National Comprehensive Pilot Zone for Big Data. However, these effects do not vary significantly among different ranks of cities. The mechanism test found that the information effect and the capital allocation effect are the mechanisms by which the digital economy affects green development, and there is a ""U-shaped"" relationship between the digital economy and information asymmetry and capital mismatch. According to the study's results, improving the digital economy's governance structure continues to make more sense than merely increasing the number of digital inputs.",Not About Sufficiency
Computing the Shapley Value of Facts in Query Answering,"The Shapley value is a game-theoretic notion for wealth distribution that is nowadays extensively used to explain complex data-intensive computation, for instance, in network analysis or machine learning. Recent theoretical works show that query evaluation over relational databases fits well in this explanation paradigm. Yet, these works fall short of providing practical solutions to the computational challenge inherent to the Shapley computation. We present in this paper two practically effective solutions for computing Shapley values in query answering. We start by establishing a tight theoretical connection to the extensively studied problem of query evaluation over probabilistic databases, which allows us to obtain a polynomial-time algorithm for the class of queries for which probability computation is tractable. We then propose a first practical solution for computing Shapley values that adopts tools from probabilistic query evaluation. In particular, we capture the dependence of query answers on input database facts using Boolean expressions (data provenance), and then transform it, via Knowledge Compilation, into a particular circuit form for which we devise an algorithm for computing the Shapley values. Our second practical solution is a faster yet inexact approach that transforms the provenance to a Conjunctive Normal Form and uses a heuristic to compute the Shapley values. Our experiments on TPC-H and IMDB demonstrate the practical effectiveness of our solutions.",Not About Sufficiency
Association Between Childhood Residential Mobility and Non-medical Use of Prescription Drugs Among American Youth,"Prescription drug abuse is a public health epidemic, resulting in 15,000 deaths annually. Disruption of childhood residence has been shown to increase drug-seeking behavior among adolescents; however, little research has explored its association specifically with non-medical use of prescription drugs (NMUPD). The objective of the study was to measure the association between residential mobility and NMUPD. The 2010 National Survey on Drug Use and Health data were analyzed for 15,745 participants aged 12-17 years. NMUPD was defined as self-report of any non-medical use (i.e., taking a prescription drug that was not prescribed to them or consumption for recreational purposes) of tranquilizers, pain relievers, sedatives, or stimulants. Logistic regression for survey data was used to estimate the association between residential mobility and NMUPD, adjusting for potential confounders. After controlling for demographic, intrapersonal, interpersonal, and community factors, adolescents with low mobility (1-2 moves in the past 5 years) and residential instability (a parts per thousand yen3 moves) were 16 % (OR 1.16, 95 % CI 1.01, 1.33) and 25 % (OR 1.25, 95 % CI 1.00, 1.56) more likely to report NMUPD compared to non-mobile adolescents (0 moves). Low-mobile adolescents were 18 % (OR 1.18, 95 % CI 1.01, 1.38) more likely to abuse pain relievers, specifically. No relationship was found between moving and tranquilizer, stimulant, or sedative use. Increasing childhood residential mobility is associated with NMUPD; therefore, efforts to prevent NMUPD should target mobile adolescents. Further examination of the psychological effects of moving and its association with pain reliever abuse is indicated.",Not About Sufficiency
Symptom Prediction and Mortality Risk Calculation for COVID-19 Using Machine Learning,"Background: Early prediction of symptoms and mortality risks for COVID-19 patients would improve healthcare outcomes, allow for the appropriate distribution of healthcare resources, reduce healthcare costs, aid in vaccine prioritization and self-isolation strategies, and thus reduce the prevalence of the disease. Such publicly accessible prediction models are lacking, however. Methods: Based on a comprehensive evaluation of existing machine learning (ML) methods, we created two models based solely on the age, gender, and medical histories of 23,749 hospital-confirmed COVID-19 patients from February to September 2020: a symptom prediction model (SPM) and a mortality prediction model (MPM). The SPM predicts 12 symptom groups for each patient: respiratory distress, consciousness disorders, chest pain, paresis or paralysis, cough, fever or chill, gastrointestinal symptoms, sore throat, headache, vertigo, loss of smell or taste, and muscular pain or fatigue. The MPM predicts the death of COVID-19-positive individuals. Results: The SPM yielded ROC-AUCs of 0.53-0.78 for symptoms. The most accurate prediction was for consciousness disorders at a sensitivity of 74% and a specificity of 70%. 2,440 deaths were observed in the study population. MPM had a ROC-AUC of 0.79 and could predict mortality with a sensitivity of 75% and a specificity of 70%. About 90% of deaths occurred in the top 21 percentile of risk groups. To allow patients and clinicians to use these models easily, we created a freely accessible online interface at www. aicovid.net. Conclusion: The ML models predict COVID-19-related symptoms and mortality using information that is readily available to patients as well as clinicians. Thus, both can rapidly estimate the severity of the disease, allowing shared and better healthcare decisions with regard to hospitalization, self-isolation strategy, and COVID-19 vaccine prioritization in the coming months.",Not About Sufficiency
My Mind is a Prison: A Boosted Deep Learning approach to detect the rise in depression since COVID-19 using a stacked bi-LSTM CatBoost model,"The COVID-19 pandemic has significantly altered our way of life. Physical, social interactions are being steadily replaced with virtual connections and remote interactions. Social media platforms such as Facebook, Twitter, and Instagram have become the primary medium of communication. However, being relegated to a solely online presence has had a major impact on the mental health of users since the onset of the pandemic. The present study aims to identify depressed Twitter users by analyzing their tweets. We propose a deep learning model which stacks a bidirectional LSTM layer along with a CatBoost Algorithm layer to classify tweets and detect depression. The results show that the proposed model outperforms standard machine learning approaches to classification and that there was a definite rise in depression since the beginning of the pandemic. The study's primary contribution is the novel deep learning model and its ability to detect depression.",Not About Sufficiency
The Health Gym: synthetic health-related datasets for the development of reinforcement learning algorithms,"In recent years, the machine learning research community has benefited tremendously from the availability of openly accessible benchmark datasets. Clinical data are usually not openly available due to their confidential nature. This has hampered the development of reproducible and generalisable machine learning applications in health care. Here we introduce the Health Gym - a growing collection of highly realistic synthetic medical datasets that can be freely accessed to prototype, evaluate, and compare machine learning algorithms, with a specific focus on reinforcement learning. The three synthetic datasets described in this paper present patient cohorts with acute hypotension and sepsis in the intensive care unit, and people with human immunodeficiency virus (HIV) receiving antiretroviral therapy. The datasets were created using a novel generative adversarial network (GAN). The distributions of variables, and correlations between variables and trends in variables over time in the synthetic datasets mirror those in the real datasets. Furthermore, the risk of sensitive information disclosure associated with the public distribution of the synthetic datasets is estimated to be very low.",Not About Sufficiency
"Access to urban equipment and quality of life. Quilpue and Villa Alemana, Chileen copropiedad en Chile","Access to urban equipment and services has gained centrality in the Chilean urban discussion, being a fundamental element of the policy, indicators and the bill on urban social integration. In this way, the Chilean State recognizes that to democratize the quality of life and human development it is not enough to ensure a home, but must promote functionally complex residential environments. The article measures the accessibility to urban equipment and its relationship with the distribution of socioeconomic groups in two communes of the Valparaiso Metropolitan Area: Quilpue and Villa Alemana. The results show an unbalanced urbanization pattern, with high coverage of urban services such as schools and health centers, but deficient in the distribution of opportunities for leisure, sport and culture, which translates into a factor of reproduction of social inequalities, since that deprives medium and vulnerable groups of opportunities for recreation and culture in the vicinity of their place of residence.",Not About Sufficiency
URBAN PLANNING AS A STRATEGY FOR THE SUSTAINABLE DEVELOPMENT,"Objective: Through the concept of strategic functionality, the influence of urban planning on the sustainable development of cities is analyzed, taking as a reference the case of Oaxaca, Mexico, during 2000-2015. Hypothesis: It is proposed that there is a high significant correlation between the strategic functionality and the sustainable development of the cities analyzed. Methodology: Statistical and network analysis is used. Conclusions: The strategic functionality has an incipient influence on the sustainable development and the standard of living of the cities of Oaxaca, where these cities operate with a center-periphery model that generates competition for resources and opportunities in the territory.",Not About Sufficiency
Exploring the Contribution of Grassroots Innovations to Justice: Using the Capability Approach to Normatively Address Bottom-Up Sustainable Transitions Practices,"There is growing interest in the potential of grassroots innovations for the transition towards more just and sustainable societies. Nevertheless, there is lack of clear normative discussion regarding these processes. The paper strives to propose and test a framework that enables an analysis of how and in which sense specific grassroots innovation processes may be contributing to the construction of more just societies. To this end, we connect elements of the multi-level perspective on sociotechnical transitions (frequently used in the analysis of grassroots innovations) with elements of the capability approach, which offers a multi-dimensional perspective to justice. The framework is used to address two purposively selected empirical cases in two key sectors in Spain: an energy cooperative and a food purchasing group. We draw on the information of 25 individual interviews with members of these two cases, on observation, and on secondary sources. Information was processed by means of a qualitative content analysis. We draw on predefined categories from the framework, which was refined during the analysis. The paper illustrates that grassroots innovations may be contributing to justice in several aspects: they expand capabilities in different dimensions, improve public reasoning processes, and create better structural conditions for human flourishing. Nevertheless, these processes are not free of tensions and contradictions.",Not About Sufficiency
The Development and Diffusion of IS Standards in the Canadian Healthcare Industry: An Industry Level Analysis,"There are major steps being taken at national, provincial, regional, and organizational levels to increase the implementation of information systems (IS) in the healthcare system. Developing and implementing nationally accessible electronic health records in the form of panCanadian electronic health records is the ultimate goal for the Canadian government. The solution called for by the healthcare industry involves the continuous development and diffusion of healthcare industry vertical standards to enable seamless electronic integration. Our proposed research project is an exploratory case study which will address the following research question: What are the processes taking place as the Canadian healthcare industry develops and diffuses vertical industry standards to achieve interoperability and integration? By taking a socio-technical perspective to answering this question, we will further research on standards development and diffusion enabling us to assist in the development of relevant guidelines to serve the practitioner community in developing and diffusing vertical industry standards.",Not About Sufficiency
XIS-temperature: A daily spatiotemporal machine-learning model for air temperature in the contiguous United States,"The challenge of reconstructing air temperature for environmental applications is to accurately estimate past exposures even where monitoring is sparse. We present XGBoost-IDW Synthesis for air temperature (XIS-Temperature), a high-resolution machine-learning model for daily minimum, mean, and maximum air temperature, covering the contiguous US from 2003 through 2023. XIS uses remote sensing (land surface temperature and vegetation) along with a parsimonious set of additional predictors to make predictions at arbitrary points, allowing the estimation of address-level exposures. We built XIS with a computationally tractable workflow for extensibility to future years, and we used weighted evaluation to fairly assess performance in sparsely monitored regions. The weighted root mean square error (RMSE) of predictions in site-level cross-validation for 2023 was 1.78 K for the minimum daily temperature, 1.19 K for the mean, and 1.48 K for the maximum. We obtained higher RMSEs in earlier years with fewer ground monitors. Comparing to three leading gridded temperature models in 2021 at thousands of private weather stations not used in model training, XIS had at most 60% of the mean square error for the minimum temperature and 93% for the maximum. In a national application, we report a stronger relationship between summertime minimum temperature and social vulnerability with XIS than with the other models. Thus, XIS-Temperature has potential for reconstructing important environmental exposures, and its predictions have applications in environmental justice and human health.",Not About Sufficiency
Machine learning-assisted design of porous carbons for removing paracetamol from aqueous solutions,"To accelerate the design and production of porous carbons targeting desired performance characteristics, we propose to incorporate machine learning (ML) regression into pore size distribution (PSD) analysis. Here, we implemented a ML algorithm for predicting paracetamol adsorption capacity of porous carbons from two pore structure parameters: total surface area and surface area of supermicropores-mesopores. These structural parameters of porous carbons are accessible from the software provided with automatic volumetric gas adsorption analyzers. It was shown that theoretical paracetamol capacities of porous carbons predicted using the ML algorithm lies within the range of experimental uncertainty. Nanoporous carbon beads with a high surface area of supermicropores (997 m(2)/g) and mesopores (628 m(2)/g) had the highest adsorption capacity of paracetamol (experiment: 480 +/- 24 mg/g, ML predicted: 498 mg/g). The novel strategy for designing of porous carbon adsorbents using ML-PSD approach has a great potential to facilitate production of novel carbon adsorbents optimized for purification of aqueous solutions from non-electrolyte contaminates.",Not About Sufficiency
Determinants of housing prices: Serbian Cities' perspective,"This study investigates the spatial and temporal dynamics of housing prices in Serbia, addressing the critical need to understand the drivers of real estate prices and their implications for economic and social welfare. Employing a panel data analysis approach on a unique dataset covering 24 distinct urban areas in Serbia from 2011 to 2021, we examine the relevance of diverse economic, demographic, and infrastructural indicators, providing novel insights within a developing country context. Our findings reveal that the housing market stock-flow model effectively predicts housing price appreciation trends, explaining over 60 percent of variation in property prices. Notably, disparities in labour income, captured by average wages and registered employment rates, emerge as significant determinants of real estate prices, underlining socio-economic disparities within Serbian cities. Housing prices exhibit a positive response to the population/housing stock ratio, suggesting higher prices in cities experiencing faster population growth relative to housing supply. Intensified construction is associated with elevated housing prices. Additionally, we find positive association between the inflation variable and housing prices, underlining real estate's potential as an inflation hedge. Public service provision and infrastructural amenities also emerge as contributors to higher housing prices in urban areas, emphasizing the importance of comprehensive urban planning strategies. Our study contributes to the literature by providing specific quantitative evidence, advancing the understanding of urban housing market dynamics in developing countries. By offering nuanced insights into determinants of housing prices, our research informs policymakers and urban planners seeking to foster equitable and sustainable urban development strategies.",Not About Sufficiency
Neighborhood sustainability assessment tools: Research trends and forecast for the built environment,"Rapid urbanization, environmental concerns and demand for sustainable cities contributed to the development of Neighbourhood Sustainability Assessment Tools (NSATs) such as LEED-ND and BREEAM-Communities. Whilst their success in increasing sustainability outcomes within the built environment is evident, there are unnoticed gaps in NSAT frameworks that might hinder their future suitability for sustainable urban planning and design. Therefore, to remain pertinent, NSATs must address the constantly evolving sustainability issues. To determine the gaps in NSATs frameworks and identify trends in sustainable communities, this study utilised a bibliometric exploration of NSAT-related publications. A total of 117 research articles over the last decade were reviewed to inform and provide insights on the research and development needs as well as areas of enhancing NSATs efficacy. The results revealed that research methods used to evaluate NSATs in journal articles are predominantly qual-itative. This significantly limits the precision of research outcomes and highlights the need to increase quanti-tative (experiential and experimental) performance-based investigations which provide context and practically relevant outcomes. Furthermore, an upsurge in research themes related to big data and climate change, focused on smartness and resilience, was revealed. This indicates the suitability of NSATs for addressing existing societal concerns.To build on this achievement, tools may need to incorporate more health-based dynamics while considering issues of climate justice in order to remain effective and relevant. Another observation is the low research contribution from developing regions and lack of research from African regions. The observations and recommendations given in this study are pertinent to various stakeholders, including developers, and industry experts and consider the role of researchers in enhancing the performance of NSATs.",Not About Sufficiency
Acoustic design evaluation in educational buildings using artificial intelligence,"Speech intelligibility is a critical aspect of building science, particularly in educational buildings where poor sound quality may have a detrimental impact on students' learning and teachers' health. However, considering the numerous building regulations proposing varying definitions and ranges of acoustic comfort, calculating the necessary acoustic indicators can be challenging for designers. Speech intelligibility is a crucial component of indoor acoustics and acoustic comfort and can be calculated using formulas, simulation software, and data-based web tools. While formulas are fast, they lack details; acoustic simulation software is highly accurate but timeconsuming and expensive. Data-based web tools, including machine learning algorithms, offer both speed and accuracy and are widely accessible. In this study, we present a system utilizing machine learning techniques to predict acoustic indicators, numeric and heatmap, in an educational building. The Pachyderm plugin in the Grasshopper was utilized to conduct extensive simulations in a single educational space, focusing on acoustic indicators in six different frequencies and general modes. Using Catboost and the pix2pix algorithm, the prediction models provide numerical and image indices on the developed dataset. Also, SHAP values were employed to interpret the Catboost model, analyzing the significance of each feature. The results showed remarkable accuracy, (i.e., between 89 % and 99 %) in the numerical portion, and PSNR index ranging from 0.817 to 0.970, and an SSIM index ranging from 15.56 to 31.57 in the image section. By utilizing data-driven methods, the system provides an efficient and accurate approach to calculating acoustic indicators, helping to ensure optimal acoustic environment in educational buildings.",Not About Sufficiency
A Deep Neural Network-Based Permanent Magnet Localization for Tongue Tracking,"Permanent magnet localization (PML) is a nascent method of wireless motion tracking that can estimate the 5D state (3D position and 2D orientation) of a cylindrical magnet from its magnetic field. PML is well suited for applications where a wireless tracking is crucial, and in particular for tongue motion which opens up many new interesting applications. To allow its usage outside of a research lab, our tongue tracking system relies on the PML to avoid impeding tongue's natural motion due to its wireless tracking method and ensure safe use inside the mouth. Our tracking module is embedded in a headset to be portable and simple to use while being affordable by relying on the mass-produced components. The classical implementation of PML has many shortcomings that limit its practicality for real-world applications because it is computationally intensive due to its iterative algorithm, subject to local minimum convergence issues, and sensitive to its initial state and calibration parameters. Additionally, its physical model is an approximation that is only valid in limited tracking conditions. In this paper, we investigated the potential of deep learning to create a PML model for tongue tracking by training a feedforward neural network (3 layers, 100 neurons per layer) on a dataset composed of similar to 17 million states spanning a volume of 10 x 10 x 10 cm(3). Our PML was validated on 337000 states in a 4 x 6 x 7 cm(3) volume and tested on 100000 samples emulating a more natural tongue motion with curves and twists. A fully automated SD positional stage was engineered by our team to collect these large datasets of the 5D magnet states. Our PML prediction, limited to the magnet's 3D positions in this paper, reaches positional errors of 1.4 mm (median) and 1.8 mm (Q3).",Not About Sufficiency
Privacy-Preserving Federated Model Predicting Bipolar Transition in Patients With Depression: Prediction Model Development Study,"Background: Mood disorder has emerged as a serious concern for public health; in particular, bipolar disorder has a less favorable prognosis than depression. Although prompt recognition of depression conversion to bipolar disorder is needed, early prediction is challenging due to overlapping symptoms. Recently, there have been attempts to develop a prediction model by using federated learning. Federated learning in medical fields is a method for training multi-institutional machine learning models without patient-level data sharing. Objective: This study aims to develop and validate a federated, differentially private multi-institutional bipolar transition prediction model. Methods: This retrospective study enrolled patients diagnosed with the first depressive episode at 5 tertiary hospitals in South Korea. We developed models for predicting bipolar transition by using data from 17,631 patients in 4 institutions. Further, we used data from 4541 patients for external validation from 1 institution. We created standardized pipelines to extract large-scale clinical features from the 4 institutions without any code modification. Moreover, we performed feature selection in a federated environment for computational efficiency and applied differential privacy to gradient updates. Finally, we compared the federated and the 4 local models developed with each hospitales data on internal and external validation data sets. Results: In the internal data set, 279 out of 17,631 patients showed bipolar disorder transition. In the external data set, 39 out of 4541 patients showed bipolar disorder transition. The average performance of the federated model in the internal test (area under the curve [AUC] 0.726) and external validation (AUC 0.719) data sets was higher than that of the other locally developed models (AUC 0.642-0.707 and AUC 0.642-0.699, respectively). In the federated model, classifications were driven by several predictors such as the Charlson index (low scores were associated with bipolar transition, which may be due to younger age), severe depression, anxiolytics, young age, and visiting months (the bipolar transition was associated with seasonality, especially during the spring and summer months). Conclusions: We developed and validated a differentially private federated model by using distributed multi-institutional psychiatric data with standardized pipelines in a real-world environment. The federated model performed better than models using local data only.",Not About Sufficiency
Large-scale physical activity data reveal worldwide activity inequality,"To be able to curb the global pandemic of physical inactivity(1-7) and the associated 5.3 million deaths per year(2), we need to understand the basic principles that govern physical activity. However, there is a lack of large-scale measurements of physical activity patterns across free-living populations worldwide(1,6). Here we leverage the wide usage of smartphones with built-in accelerometry to measure physical activity at the global scale. We study a dataset consisting of 68 million days of physical activity for 717,527 people, giving us a window into activity in 111 countries across the globe. We find inequality in how activity is distributed within countries and that this inequality is a better predictor of obesity prevalence in the population than average activity volume. Reduced activity in females contributes to a large portion of the observed activity inequality. Aspects of the built environment, such as the walkability of a city, are associated with a smaller gender gap in activity and lower activity inequality. In more walkable cities, activity is greater throughout the day and throughout the week, across age, gender, and body mass index (BMI) groups, with the greatest increases in activity found for females. Our findings have implications for global public health policy and urban planning and highlight the role of activity inequality and the built environment in improving physical activity and health.",Not About Sufficiency
"'Threshold of a new era': The development of an integrated hospital system in Northeast Scotland, 1900-39","Regional integration of hospital services was widely discussed by policy-makers in interwar Britain, although historians remain divided on the strength of the consensus in favour of planning. The aim of this article is to explore the forces at work in the comparatively early reorganization of services in the Scottish city of Aberdeen. This included the relocation of its medical school and main hospitals, the conversion of its poor law accommodation to a municipal hospital, and the joint agreements made with neighbouring authorities to provide public health services. In part these were expressions of the shared interest of the voluntary hospital leadership, public health officials, and the university, in the efficient harnessing of science to the demands of mass medicine. However, the `Aberdeen experiment' also owed much to the ideas and assertiveness of three successive Medical Officers of Health. These men were closely associated with the Scottish Board of Health (later Department of Health for Scotland) and their work provides an insight into the development of distinctive Scottish health care arrangements prior to the establishment of the National Health Service.",Not About Sufficiency
An Approach to Rapidly Evaluating Rock Mass Quality in Underground Engineering Based on Multi-source Heterogeneous Data,"A fast, accurate and easily accessible rock mass quality evaluation method is important for the advancement of underground engineering. In this study, we first analyze the application of mainstream single-factor and multi-factor evaluation methods through extensive tests conducted on the tunnel construction site. Next, by analyzing the evaluation methods for rock mass quality and the selection of evaluation indicators in various standards, we select the most suitable evaluation indicators for the current project from multi-source heterogeneous data based on the principles of scientificity, sensitivity, accessibility, and easy parameterized characterization. After that, we construct a Gaussian process classification model for underground engineering rock mass quality evaluation based on multi-source heterogeneous data. The results show that the accuracy of the tunnel geological prediction method considering single factors is relatively low-only 42.9% for class-V surrounding rocks. The multi-factor evaluation method features simple operation and high accuracy in practical engineering, but its application is limited by lithology. The evaluation method based on multi-source heterogeneous data screening and Gaussian process is free from the influence of geological conditions, and its evaluation results are completely consistent with the actual situation. It can also help provide probability calculation to characterize the uncertainty of underground engineering rock mass quality. The proposed principle for selecting indicators can be used as a reference for the studies in other fields. The proposed evaluation method features high accuracy, simple operation and good applicability.",Not About Sufficiency
Skin cancer detection using dual optimization based deep learning network,"Skin cancer is the most perilous kind of cancer, which is a most important public health problem. Skin cancer can be prevented and treated more effectively if malignant lesions are identified in its early stages. In artificial intelligence, machine learning and deep learning algorithms are used to classify data with high accuracy based on features in the input images. In this work, a novel dual optimization based deep learning network (DODL net) has been proposed for detecting the skin cancer. Initially, the dermoscopic images are gathered from MNIST HAM10000 dataset. The collected images are pre-processed using adaptive median filter and these noise-free images are processed in U-net for segmenting the particular region of the skin lesion. Further, the dual optimization algorithms which is hybridization of Bacterial Foraging Optimization (BFO) and Particle Swarm Optimization (PSO) are utilized for extracting the features from the segmented images. Finally, the Deep Convolutional neural network (CNN) classifies the seven different classes of skin cancer based on the extracted features. The performance of the DODL net has been evaluated using specific parameters such as precision, recall, F1 score and accuracy. The accuracy attained by the proposed DODL net is 98.76% for MNIST HAM10000 dataset.",Not About Sufficiency
"Neighbourhood demolition, relocation and health. A qualitative longitudinal study of housing-led urban regeneration in Glasgow, UK","We conducted a qualitative longitudinal study to explore how adult residents of disadvantaged urban neighbourhoods (Glasgow, UK) experienced neighbourhood demolition and relocation. Data from 23 households was collected in 2011 and 2012. Some participants described moves to new or improved homes in different neighbourhoods as beneficial to their and their families' wellbeing. Others suggested that longstanding illnesses and problems with the new home and/or neighbourhood led to more negative experiences. Individual-level contextual differences, home and neighbourhood-level factors and variations in intervention implementation influence the experiences of residents involved in relocation programmes. (C) 2015 Elsevier Ltd. All rights reserved.",Not About Sufficiency
"Influenza pandemic and the development of public health infrastructure in Bombay city, 1919-1935","The arrival of the influenza pandemic and the end of the First World War had a definite impact on the policies associated with the development of public health infrastructure in the inter-war period. The influenza pandemic was crucial in generating awareness about the insufficient medical relief infrastructure for a city like Bombay (present day Mumbai) with a growing population. In this paper, I therefore elaborate on the politics surrounding funding of public health and development of hospitals in interwar Bombay. The paper uses health planning in the aftermath of the influenza pandemic as a lens to understand colonial power and transitioning governance in the late colonial period. It evaluates how colonialism operated through health planning and how it catalyzed the transformation of power relations between colonial and local Indian powers. Secondly, I argue that, while the government authorities designed plans, it was the native population that bore the financial responsibility of these projects. It becomes evident that the unwillingness displayed by the colonial state to improve healthcare infrastructure resulted in the native inhabitants taking it upon themselves to cater to the demands of the general population.",Not About Sufficiency
A Fundamental Performance Limitation for Adversarial Classification,"Despite the widespread use of machine learning algorithms to solve problems of technological, economic, and social relevance, provable guarantees on the performance of these data-driven algorithms are critically lacking, especially when the data originates from unreliable sources and is transmitted over unprotected and easily accessible channels. In this letter, we take an important step to bridge this gap and formally show that, in a quest to optimize their accuracy, binary classification algorithms-including those based on machine-learning techniques-inevitably become more sensitive to adversarial manipulation of the data. Further, numerical evidence suggests that the accuracy-sensitivity tradeoff depends solely on the statistics of the data, and cannot be improved by tuning the algorithms or increasing their complexity.",Not About Sufficiency
Architecture as machine; Towards an architectural system for human well-being,"Le Corbusier's well-known phrase 'The house is a machine for living in' suggested a kind of machinic aesthetic that became an important concept behind the functionality, standardization and rational order that together laid the foundation of modern architecture. This paper attempts to expand on Le Corbusier's idea of machine by particularly examining architecture as a machinic system and how it could potentially depict spatial qualities that fulfill their functional purpose for human well-being. The idea of machine became a way to introduce scientific and logical reasoning as the basis of designing architecture through the establishment of standards. There were, however, some criticisms against the idea of machine since it tends to dehumanize, by assuming that human being had the same basic needs that could be standardized. This paper attempts to highlight that the establishment of standard becomes necessary, not in generating standard architectural forms but in defining the performance standard of architecture for human well-being.",Not About Sufficiency
Lessons from the National institutes of health innovation corps program: defining barriers to developing and commercializing novel solutions for persons with opioid use disorder,"BackgroundTranslating innovative research advancements into commercially viable medical interventions presents well-known challenges. However, there is limited understanding of how specific patient, clinical, social, and legal complexities have further complicated and delayed the development of new and effective interventions for Opioid Use Disorder (OUD). We present the following case studies to provide introductory clinical, social, and business insights for researchers, medical professionals, and entrepreneurs who are considering or are currently developing medical.MethodsFour small business recipients of National Institute on Drug Abuse (NIDA) small business grant funding collected a total of 416 customer discovery interviews during the 2021 National Institutes of Health (NIH) Innovation-Corps (I-Corps) program. Each business received funding to advance an OUD-specific innovation: therapeutics (2 companies), medical device (1 company), and Software as a Medical Device (SaMD) (1 company). Interview participants included stakeholders from a variety of disciplines of Substance Use Disorders (SUD) healthcare including clinicians, first responders, policymakers, relevant manufacturers, business partners, advocacy groups, regulatory agencies, and insurance companies.ResultsAgnostic to the type of product (therapeutic, device, or SaMD), several shared barriers were identified: (1) There is a lack of standardization across medical providers for managing patients with OUD, resulting in diverse implementation practices due to a fragmented healthcare policy; (2) Underlying Social Determinants of Health (SDOH) present unique challenges to medical care and contribute to poor outcomes in OUD; (3) Stigma thwarts adoption, implementation, and the development of innovative solutions; (4) Constantly evolving public health trends and legal policies impact development and access to OUD interventions.ConclusionIt is critical for innovators to have early interactions with the full range of OUD stakeholders to identify and quantify true unmet needs and to properly position development programs for commercial success. The NIH I-Corps program provides a framework to educate researchers to support their product design and development plans to increase the probability of a commercially successful outcome to address the ongoing opioid epidemic.",Not About Sufficiency
Explaining the Housing Bubble,"There is little consensus as to the cause of the housing bubble that precipitated the financial crisis of 2008. Numerous explanations exist: misguided monetary policy; a global savings surplus; government policies encouraging affordable homeownership; irrational consumer expectations of rising housing prices; inelastic housing supply. None of these explanations, however, is capable of fully explaining the housing bubble. This Article posits a new explanation for the housing bubble. First, it demonstrates that the bubble was a supply-side phenomenon attributable to an excess of mispriced mortgage finance: mortgage-finance spreads declined and volume increased, even as risk increased-a confluence attributable only to an oversupply of mortgage finance. Second, it explains the mortgage-finance supply glut as resulting from the failure of markets to price risk correctly due to the complexity, opacity, and heterogeneity of the unregulated private-label mortgage-backed securities (PLS) that began to dominate the market in 2004. The rise of PLS exacerbated informational asymmetries between the financial institutions that intermediate mortgage finance and PLS investors. These intermediation agents exploited informational asymmetries to encourage overinvestment in PLS that boosted the financial intermediaries' volume-based profits and enabled borrowers to bid up housing prices. This Article proposes the standardization of PLS as an information-forcing device. Reducing the complexity and heterogeneity of PLS would facilitate accurate risk pricing, which is necessary to rebuild a sustainable, stable housing-finance market.",Not About Sufficiency
Data needs to be a priority,"Findability, Accessibility, Interoperability, and Reusability (FAIR) data are essential to heliophysics and all scientific research. The principles of FAIR data ensure the reusability and findability of data, as well as its long-term care. The goal is that data are accessible for the ongoing discovery and verification process and can be used on their own or with newly generated data in future studies leading to innovations. With the onset in the previous decades of NASA and other agencies requiring mission data to be open to the public, heliophysics has already made great strides toward FAIR data and benefited from these efforts. Continued improvements in our metadata, data archives, and data portals and the addition of DOIs for data citation will ensure data will be FAIR, enabling further scientific discoveries, reproducibility of results, longitudinal studies, and verification and validation of models. Currently, not all the data collected are findable and on open networks or archives, and not all data on archives have DOIs. Within this study, we make recommendations to prioritize resources needed to satisfy FAIR data principles, treating them as a fundamental research infrastructure rather than a simple research product.center dot Data collection, preparation, archiving, and accessibility need to be a priority. center dot Data collection, preparation, archiving, and accessibility need dedicated and sustained funding support.center dot Data need to be accessible through investment in infrastructure: tools to access and read the data and personnel to maintain these data and IT infrastructure.center dot Data need to be collected in sustained ways to enable further science and, specifically, model validation efforts.",Not About Sufficiency
Data sustainability: Data governance in data infrastructures across technological and human generations,"The paper highlights the importance of data sustainability in the data infrastructures aimed at long-term knowledge discoveries. Data sustainability refers to data's capacity to endure across technological and human generations, and it problematizes the data governance literature from a temporal perspective. Existing work has already moved the literature from the organizational setting to more complex interorganizational settings, highlighting discrepancies between normative data governance models and organizational practices. We broaden this literature temporally by examining and outlining research directions for data sustainability from different meta-theoretical perspectives - evolutionary, relational, and durational. Data sustainability across technological and human generations navigates complementary and competing temporal demands: Data need to transition across socio-technical regimes over time, yet be embedded in social and material networks to be meaningful; historical and present data also must remain available and accessible in near and distant futures, for going back in time and seeing new data linkages and combinations. We argue that data sustainability is critical in ensuring progression in social and environmental sustainability. The paper contributes both to data governance and sustainability literatures.",Not About Sufficiency
Detection of Malpractice in E-exams by Head Pose and Gaze Estimation,"Examination malpractice is deliberate wrongdoing contrary to official examination rules designed to place a candidate at an unfair advantage or disadvantage. The proposed system depicts a new use of technology to identify malpractice in e-exams, which is essential due to online education growth. The current solutions for such a problem either require complete manual labor or have various vulnerabilities exploited by an examinee. The proposed application encompasses an end-to-end system that assists an examiner/evaluator in deciding whether a student passes an online exam without any probable attempts of malpractice or cheating in e-exams with the help of visual aids. The system works by categorizing the student's VFOA (visual focus of attention) data by capturing the head pose estimates and eye gaze estimates using state-of-the-art machine learning (ML) techniques. The system only requires the student (test-taker) to have a functioning internet connection and a webcam to transmit the feed. The examiner is alerted when the student wavers in his VFOA from the screen greater than X, a predefined threshold of times. If this threshold X is crossed, the application will save the person's data when his VFOA is off the screen and send it to the examiner to be manually checked and marked whether the student's action was attempted malpractice or just a momentary lapse in concentration. The system uses a hybrid classifier approach where two different classifiers are used. One when gaze values are being read successfully. On failing this due to various reasons like transmission quality or glare from his spectacles, the model falls back to the default classifier, which only reads the head pose values to classify the attention metric. It is later used to map the student's VFOA to check the likelihood of malpractice. The model has achieved an accuracy of 96.04 percent in classifying the attention metric.",Not About Sufficiency
The Evaluation of Social Farming through Social Return on Investment: A Review,"In recent years, there has been a need for a shared methodology for evaluating social farming (SF) practices to verify not only their effectiveness but also their social and economic sustainability. The evaluation of SF has been highlighted using the methodology of the social return on investment (SROI) due to the potential of such approach regarding the quantification of social impact. The main purpose of this study is to provide an overview, through a systematic review, of the application of SROI to SF experiences to check the results comparability, both in terms of outcomes standardization and comparisons between SROI ratios. The results first show some similarities on the construction of outcomes that allow for the initial comparability of the results. Secondly, all the indicators calculated in the articles report a social return value of social farming projects that varies approximately from EUR 2 to EUR 3 per euro invested. Critical issues remain regarding the application of this methodology to SF practices, regarding the number of the applications of SROI to SF, the process of stakeholder engagement and the construction of outcome. There is a need for more studies that apply SROI to SF experiences in order to standardize the process of analysis.",Not About Sufficiency
Spatiotemporal patterns and hot spots of PM2.5 in Bangladesh,"Urban areas in Bangladesh have seen alarming levels of particulate matter for an extended period, posing serious threats to public health and economic stability. Particulate matter with a diameter of 2.5 mu m or smaller, known as PM2.5, can be inhaled by humans and cause serious respiratory and cardiovascular health problems. This study revealed spatial and temporal patterns, seasonal and regional variations, hot spots, and cold spots of PM2.5 in Bangladesh. In addition, the relationship between PM2.5 and meteorological variables was investigated. The results indicate a positive spatial autocorrelation in PM2.5 concentrations, with recent hot spots primarily clustered in the Dhaka, and western parts of the Chittagong divisions. In contrast, cold spots are observed in the Sylhet, Rangpur, and eastern parts of the Chittagong divisions. Seasonal variations revealed notably high PM2.5 concentrations during the winter season. Furthermore, annual average PM2.5 concentrations showed increasing trends for most divisions in Bangladesh, particularly elevated concentrations in Dhaka, Barisal, Khulna, and Chittagong. Overall, this study provides a comprehensive analysis of PM2.5 spatial distributions, clusters, and temporal patterns contributing to understanding the variation and distribution of PM2.5 concentrations across Bangladesh. The findings of this study can be applied to urban planning by prioritizing areas for new air quality monitoring stations, directing efforts to reduce pollution in hot spot areas, and formulating long-term, sourcespecific policies to improve air quality and public health.",Not About Sufficiency
Antibiotic utility and susceptibility changes of multidrug-resistant Escherichia coli and Klebsiella spp: 5-year experience in a tertiary healthcare centre,"Objectives Multiple studies have identified cross-sectional relationships between antibiotic use and bacterial resistance. The aim of this study was to analyse the susceptibility of multidrug-resistant (MDR) and non-MDR (nMDR) isolates of Escherichia coli and Klebsiella spp to cephalosporins: ceftazidime (CTZ), ceftriaxone (CTX), cefepime (CEF) and fluoroquinolones: ciprofloxacin (CIP) and levofloxacin (LEV) in a tertiary healthcare centre from 2014 to 2018. In addition, we aimed to evaluate a correlation between the antibiotic utility and susceptibility of the selected enterobacteria. Methods Antibiotics consumption and antimicrobial resistance were monitored in a tertiary care university hospital from 2014 to 2018. Utilisation of antibiotics in the observed period was expressed as defined daily dose (DDD) per 100 bed/days (DBD). Bacterial susceptibility was reported as the percentage of susceptible results among all tested isolates from all patient samples. In further analysis, bacterial strains were considered as MDR or nMDR species. An MDR bacterial strain was defined as one with acquired non-susceptibility to at least one agent in three or more antimicrobial categories. Results Our results suggest that cephalosporins were the most used antibiotics, followed by fluoroquinolones, during the entire observed period 2014-2018. Our findings show that MDR isolates of E. coli had an increasing trend in susceptibility in relation to CTX (p=0.005), whereas a decreasing trend was observed for MDR isolates of E. coli susceptibility towards CIP and LEV (p<0.001). Klebsiella spp susceptibility for MDR isolates showed a decreasing trend in relation to CEF (p<0.001) and both fluoroquinolones (p<0.001). A significant negative association between CEF consumption and Klebsiella spp MDR isolates susceptibility was observed (p=0.045). Conclusion Implementation of antimicrobial stewardship programmes with early detection and close monitoring of MDR bacterial strains of E. coli and Klebsiella spp may be a crucial step in reducing the menace of antimicrobial resistance, which is now a global problem.",Not About Sufficiency
Tightly Coupling Input Output Economics with Spatio-Temporal Land Use in a Dynamic Planning Support System Framework,"Planning support systems (PSSs) should generally be designed to promote the participation of stakeholders in planning and design processes through the delivery of useful, localized information, an ability to collect feedback, and an ability to model and test various 'what-if' scenarios. This paper focuses on such a PSS tool. The tool integrates the Land-use Evolution and Assessment Model (LEAM) with a Regional Economic Input-Output Model (REIM) in a tightly coupled computational process made accessible to stakeholders through a web-based PSS. The integrated tool allows for users to easily navigate the models and test land use and economic scenarios without expert assistance. It also keeps simulations updated with dynamic inputs and engages users in PSS development and application through responsive feedback to enhance plan-making abilities. In this paper, we demonstrate an application of the LEAM-REIM PSS in Sangamon County, Illinois. The application demonstrates an ability to provide more efficacious and detailed land use estimations through the connection of economic and land-use models, allowing users to easily engage with, navigate, and respond to scenario tests. We discuss the PSS tool, model integration approach, and detailed application to assess its usefulness in urban planning and design. We also propose some opportunities for further research.",Not About Sufficiency
Detection of differential item functioning in large-scale state assessments: A study evaluating a two-stage approach,"In differential item functioning (DIF) studies, examinees from different groups are typically ability matched, and then one or more statistical indices are used to compare performance on a set of test items. Typically, matching is on total test score (a criterion both observable and easily accessible), but it may be limited in value because if DIF is present, it is likely to distort test scores and potentially confound any item performance differences. Thus, some researchers have advocated iterative approaches for DIF detection. In this article, a two-stage methodology for evaluating DIF in large-scale state assessment data was explored. The findings illustrated the merit of iterative approaches for DIF detection. Items being flagged as DIF in the second stage were not necessarily the same items identified as DIF in the first stage and vice versa, and this finding was directly related to the amount of DIF found in the Stage 1 analyses.",Not About Sufficiency
Delineation of an Urban Community Life Circle Based on a Machine-Learning Estimation of Spatiotemporal Behavioral Demand,"Delineating life circles is an essential prerequisite for urban community life circle planning. Recent studies combined the environmental contexts with residents' global positioning system (GPS) data to delineate the life circles. This method, however, is constrained by GPS data, and it can only be applied in the GPS surveyed communities. To address this limitation, this study developed a generalizable delineation method without the constraint of behavioral data. According to previous research, the community life circle consists of the walking-accessible range and internal structure. The core task to develop the generalizable method was to estimate the spatiotemporal behavioral demand for each plot of land to acquire the internal structure of the life circle, as the range can be delineated primarily based on environmental data. Therefore, behavioral demand estimation models were established through logistic regression and machine learning techniques, including decision trees and ensemble learning. The model with the lowest error rate was chosen as the final estimation model for each type of land. Finally, we used a community without GPS data as an example to demonstrate the effectiveness of the estimation models and delineation method. This article extends the existing literature by introducing spatiotemporal behavioral demand estimation models, which learn the relationships between environmental contexts, population composition and the existing delineated results based on GPS data to delineate the internal structure of the community life circle without employing behavioral data. Furthermore, the proposed method and delineation results also contributes to facilities adjustments and location selections in life circle planning, people-oriented transformation in urban planning, and activity space estimation of the population in evaluating and improving the urban policies.",Not About Sufficiency
Treatment of adolescent borderline patients in a psychiatric unit,"In case of borderline-disease in adolescents the authors emphasize a basic need of long-term psychotherapy in specialized inpatient or outpatient centers. At the time these special services for treatment of young borderline patients do not exist in poland. If there is a need for hospitalization, they have to be treated in general psychiatric units for adolescents. The article deals with several dilemmas and peculiarities posed by this necessity. Despite all difficulties the authors draw an optimistic picture, that with special training of the staff and appropriate structuring of the unit the patients can be helped to acquire some elementary insights, which can be the basis for a future long-term outpatient psychotherapy. Great importance is attached to structuring, setting limits and clear boundaries in connection with an holding environment; training and regular staff meetings in order to deal with specific problems of primitive defense mechanisms such as splitting or projective identification, standardization of different therapeutic models, integration of the patient's family into the therapeutic process.",Not About Sufficiency
"The End of Urban Exploitation and the Rise of the Urban Imaginary: Histories and Futures of Detroit, Michigan","Top-down urban development projects often perpetuate sociospatial injustice by focusing renewal in areas occupied by a region's most disenfranchised populations, most commonly urban minorities. Because these populations often lack sufficient political and economic power, they are left with little opportunity to democratically or otherwise oppose the impositions of the dominant urban regime. Legacy forms of urban development through public-private partnership fail to provide an equitable distribution of rights to imagine alternative urban futures even when lead by the most socially radical planners.[1] Such development is conversely complicit in ensuring and facilitating capitalist exploitation of city land. In this paper I will trace the multiple forms of this exploitation before exploring projective strategies for establishing a new urban imaginary as a productive alternatives to the existing paradigm. There exists an extensive body of analytical and critical literature concerning spatial issues of urban injustice and inequality through the lens of Marxist critique, geography, urban planning, architecture, and public policy. This literature focuses on cities as spaces of conflict. This work suggests the potential of interactive media practices to instrumentalized as a means of innovating operative spatial strategies that could enfranchise populations with capacities for self-directed planning that exist outside of both the reactionary activist paradigms of resistance and revolution, whose tools are outmoded. This argument is intended to present one of many potential modes that the paper hopes to inspire. In the first half of the paper, taking Detroit, MI as a paradigm case in a global context, I will trace historic hegemonic regimes of urban development through three phases that dominated Detroit's urban imaginary: (1) federal funded creative destruction (1949-1969), (2) globalization and entrepreneurial governance (1970-1989), (3) postmodern monopoly rents (1990-present). This history will reveal how the imposition of top-down urban imaginaries resulted in the exploitative and spatially unjust environment of Post-World War II Detroit. In the second half of the paper I will begin to theorize the potential of new forms for mediated spatial practices to empower disenfranchised urban populations with the capacity to lay claim to a more equitable future through the production of a radically new urban imaginary.",Not About Sufficiency
"Up around the bend? How transport poverty can lead to social exclusion in a low-income community in Lagos, Nigeria","Well planned and executed urban mobility policies are essential for economic, social, and environmental development. Cities in developing countries, in particular, face serious problems involving infrastructure, traffic congestion, and services accessibility. These factors, made worse by poor planning, lead to social inequality. This paper conducts a multidimensional analysis of how travel times and the related opportunity costs lead to transport-related social exclusion (TRSE) situations in Ajegunle, in Lagos, Nigeria. Our contribution to the literature is to extend and quantify the TRSE framework in the case of a low-income community in a developing country. The methodology applies an interdisciplinary research design that uses open-access remote-sensing and route planning web applications (Google Maps, Google Earth, and Lara.ng) and a critical literature review, embedded in a TRSE framework. Thereby we obtain the difference between the estimated and the ideal travel times that residents of this community take to reach locations offering five different services - education, work, shopping, leisure, and health - distributed in seven locations across the city. Based on the difference between the estimated and ideal travel times, which on average is between half an hour and two hours, we calculate the opportunity cost based on local living wages as benchmark, which results in an average between 7.6% and 27.4% of the daily wages. This concept captures a temporal and spatial valuation, expressed in monetary terms, of the impact of an inadequate public transport system on poorer residents' everyday lives. This analysis using the TRSE framework shows that poor public transport, and consequently high opportunity costs, translates into social exclusion in this low-income community. Finally, we recommend a set of practical transport, urban-planning, and social policies that can remedy this predicament.",Not About Sufficiency
Evaluation of automated anthropometrics produced by smartphone-based machine learning: a comparison with traditional anthropometric assessments,"Automated visual anthropometrics produced by mobile applications are accessible and cost effective with the potential to assess clinically relevant anthropometrics without a trained technician present. Thus, the aim of this study was to evaluate the precision and agreement of smartphone-based automated anthropometrics against reference tape measurements. Waist and hip circumference (WC; HC), waist:hip ratio (WHR) and waist:height ratio (W:HT) were collected from 115 participants (69 F) using a tape measure and two smartphone applications (MeThreeSixty (R), myBVI (R)) across multiple smartphone types. Precision metrics were used to assess test-retest precision of the automated measures. Agreement between the circumferences produced by each mobile application and the reference were assessed using equivalence testing and other validity metrics. All mobile applications across smartphone types produced reliable estimates for each variable with intraclass correlation coefficients >= 0 center dot 93 (all P < 0 center dot 001) and root mean square coefficient of variation between 0 center dot 5 and 2 center dot 5 %. Precision error for WC and HC was between 0 center dot 5 and 1 center dot 9 cm. WC, HC, and W:HT estimates produced by each mobile application demonstrated equivalence with the reference tape measurements using 5 % equivalence regions. Mean differences via paired t-tests were significant for all variables across each mobile application (all P < 0 center dot 050) showing slight underestimation for WC and slight overestimation for HC which resulted in a lack of equivalence for WHR compared with the reference tape measure. Overall, the results of our study support the use of WC and HC estimates produced from automated mobile applications, but also demonstrates the importance of accurate automation for WC and HC estimates given their influence on other anthropometric assessments and clinical health markers.",Not About Sufficiency
"The Impact of Housing Vulnerability on the Relationship Between Social Capital, Residential Satisfaction, and Attitudes Toward Disadvantaged Groups in South Korea","This study examines the relationships among social capital, residential satisfaction, and attitudes toward disadvantaged groups in South Korea, with a focus on the moderating effects of educational and employment vulnerability. Using data from the 2022 Seoul Survey, which included a sample of 39,340 individuals, the analysis employed Hayes' Process Macro to assess both mediation and moderated mediation effects. The findings show that social capital significantly enhances residential satisfaction (beta = 0.557, p < 0.001), which, in turn, positively influences attitudes toward disadvantaged groups (beta = 0.411, p < 0.001). Notably, the impact of residential satisfaction on attitudes was stronger for individuals who were educationally and employment-vulnerable, underscoring the amplified role of housing conditions in shaping social attitudes for these groups. These results highlight the importance of strengthening social capital and implementing targeted housing policies to improve the well-being of vulnerable populations. Policy recommendations include integrating social capital-building initiatives with urban planning strategies and addressing the specific needs of vulnerable groups through tailored housing interventions to foster social cohesion and inclusivity. Future research should explore other dimensions of vulnerability and utilize longitudinal data to assess long-term impacts.",Not About Sufficiency
The Trans-Pacific Partnership: Is It Everything We Feared for Health?,"Background: Negotiations surrounding the Trans-Pacific Partnership (TPP) trade and investment agreement have recently concluded. Although trade and investment agreements, part of a broader shift to global economic integration, have been argued to be vital to improved economic growth, health, and general welfare, these agreements have increasingly come under scrutiny for their direct and indirect health impacts. Methods: We conducted a prospective health impact analysis to identify and assess a selected array of potential health risks of the TPP. We adapted the standard protocol for Health impact assessments (HIAs) (screening, scoping, and appraisal) to our aim of assessing potential health risks of trade and investment policy, and selected a health impact review methodology. This methodology is used to create a summary estimation of the most significant impacts on health of a broad policy or cluster of policies, such as a comprehensive trade and investment agreement. Results: Our analysis shows that there are a number of potentially serious health risks associated with the TPP, and details a range of policy implications for the health sector. Of particular focus are the potential implications of changes to intellectual property rights (IPRs), sanitary and phytosanitary measures (SPS), technical barriers to trade (TBT), investor-state dispute settlement (ISDS), and regulatory coherence provisions on a range of issues, including access to medicines and health services, tobacco and alcohol control, diet-related health, and domestic health policy-making. Conclusion: We provide a list of policy recommendations to mitigate potential health risks associated with the TPP, and suggest that broad public consultations, including on the health risks of trade and investment agreements, should be part of all trade negotiations.",Not About Sufficiency
Global air quality index prediction using integrated spatial observation data and geographics machine learning,"Air pollution can occur in the whole world, with each region having its unique driving factors that contribute to human's health. However, effective mitigation of air pollution is often hindered by the uneven distribution of air quality monitoring stations, which tend to be concentrated in potential hotspots like major cities. This study aims to detect and improve the accuracy of the Global Air Quality Index from Remote Sensing (AQI-RS) by integrating AQI from ground-based stations with driving factors such as meteorological, environmental, sources of air pollution, and air pollution magnitude from satellite observation parameters as independent variables using Geographics Machine Learning (GML). This study utilizes 425 air pollution stations and the driving factors data globally from 2013 to 2024. The GML considers geographical characteristics in the analysis by calculating the optimal bandwidth area in its algorithm. The study employs nine scenarios to identify which parameters significantly contribute to the model and determine the best parameter combinations. In determining the best scenario, this study considers the R2 value, Root Mean Square Error (RMSE), and uncertainty in each of the scenarios. This study produced an AQI-RS model with an average R2, RMSE, and uncertainty in the best scenario of 0.89, 5.58, and 5.69 (AQI unit), respectively. The results indicate that GML significantly improves the accuracy of global AQI-RS over previous studies. By considering geographical characteristics using GML, this research is expected to gain an accurate prediction of AQI globally especially in regions without ground-based air pollution stations for the worldwide mitigation.",Not About Sufficiency
Comparative Assessment of Regression Techniques for Wind Power Forecasting,"Considering the escalating rates of exhaustion of non-renewable energy resources, coupled with the harmful environmental side effects of harnessing them (e.g. damage to public health via air pollution), the need for a near-complete transition to renewable energy production seems inevitable. In recent times, renewable energy production has seen a strong support from investors, governmental initiatives, and industries across the world. Globally installed wind power capacity has seen an increase of 345.24% over the past decade. This increase brings along a need for robust power production management systems having a potential for predicting wind turbine power outputs primarily based on real-time input wind velocities. We propose and compare five optimized robust regression models for forecasting the wind power generated through turbines based on wind velocity vector components, out of which the Extreme Gradient Boosting regression model provided the best results. The forecasted output of our model can be compared with a city's daily average threshold power requirement in order to make informed decisions about either shutting down an appropriate number of turbines to avoid excessive power production and wastage, or to compensate forecasted shortcomings in production on less windy days via alternative energy generation methodologies.",Not About Sufficiency
Impact of COVID-19 on electricity demand of Latin America and the Caribbean countries,"Governments worldwide have adopted different public health measures in order to slow down the spread of COVID-19. As a result, the electricity demand has been impacted by the changes in human activity. Many of the Latin America and the Caribbean (LAC) countries have adopted different approaches to control the COVID-19 pandemic, including severe shutdown of most social and economic activities. This paper analyzes how this pandemic has influenced, from its appearance until the fall of 2020, the demand of ten LAC countries (Peru, Bolivia, Costa Rica, Brazil, Guatemala, Mexico, Dominican Republic, Argentina, Chile and Uruguay). The approach is based on the concepts of size and shape impacts, which have been proposed in order to decompose the problem for a better understanding of the impact. The size impact accounts for the observed variations on the daily demand, whereas the shape impact focuses on the variations observed on the standardized hourly demand profiles for each day. To calculate both impacts, the observed demand is compared to the expected one if the COVID-19 crisis had not happened. To obtain reliable estimations in the scenario without COVID19, machine learning techniques have been used. Peru and Bolivia are the two countries where the pandemic has had the greatest impact during 2020, with a size impact in April 2020 of around -30%. At the opposite extreme would be Chile and Uruguay, with a maximum monthly size impact of -6%. The other considered countries have maximum monthly impacts in the range of -11% to -17%. (c) 2022 Elsevier Ltd. All rights reserved.",Not About Sufficiency
Analysis and enhanced prediction of the Spanish Electricity Network through Big Data and Machine Learning techniques,"Electricity demand is shown to steadily increase in the last few years, and it is one of the key aspects of living standards and quantifying welfare effects. However, the irregularity of electricity demand is one of the main problems in this field. Therefore, it is important to accurately anticipate future expenditures in order to optimize energy generation and to avoid unexpected wastes. As a result, we developed Machine Learning models to predict electricity demand. In particular, our study has been performed using data of the Spanish Electricity Network from 2007 to 2019. To this end, we propose the implementation of a set of Machine Learning techniques using various frameworks. In particular, we implemented six different prediction models: Linear Regression, Regression Trees, Gradient Boosting Regression, Random Forests, Multi-layer Perceptron, and three types of recurrent neural networks. Our experimentation shows promising results in all cases, since our models provides better prediction than the one estimated by the Spanish Electricity Network with an improvement of 12% in the worst case and up to 37% for the best predictor, which turned out to be the Gated Recurrent Unit neural network. (C) 2021 Elsevier Inc. All rights reserved.",Not About Sufficiency
"How Do Urban Environments Affect Young People's Mental Health? A Novel Conceptual Framework to Bridge Public Health, Planning, and Neurourbanism","Childhood and adolescence are crucial periods for mental and social development. Currently, mental illness among young people is a global epidemic, and rates of disorders such as depression and anxiety are rising. Urban living, compared with rural living, is linked with a higher risk of serious mental illness, which is important because the world is urbanizing faster than ever before. Urban environments and their landscapes, designs, and features influence mental health and well-being. However, no conceptual frameworks to date have detailed the effect of urban environments on young people's mental health, and few studies have considered the growing role of digital and social media in this relationship, leading to calls for the development of holistic approaches to describe this relationship. This article synthesizes existing knowledge on urban places (both built and natural environments) and mental health in the public health and urban planning literature and examines the emerging field of neurourbanism (a multidisciplinary study of the effect of urban environments on mental health and brain activity) to enhance current practice and research. We developed 2 novel conceptual frameworks (1 research-oriented, 1 practice-oriented), adapted from Bronfenbrenner's socioecological model, that focus on the relationship between urban environments and young people's mental health. We added a digital and social media contextual level to the socioecological model, and we applied a multilayer concept to highlight potential cross-field interactions and collaborations. The proposed frameworks can help to guide future practice and research in this area.",Not About Sufficiency
A 10 m resolution urban green space map for major Latin American cities from Sentinel-2 remote sensing images and OpenStreetMap,"Mapping is fundamental to studies on urban green space (UGS). Despite a growing archive of land cover maps (where UGS is included) at global and regional scales, mapping efforts dedicated to UGS are still limited. As UGS is often a part of the heterogenous urban landscape, low-resolution land cover maps from remote sensing images tend to confuse UGS with other land covers. Here we produced the first 10 m resolution UGS map for the main urban clusters across 371 major Latin American cities as of 2017. Our approach applied a supervised classification of Sentinel-2 satellite images and UGS samples derived from OpenStreetMap (OSM). The overall accuracy of this UGS map in 11 randomly selected cities was 0.87. We further improved mapping quality through a visual inspection and additional quality control of the samples. The resulting UGS map enables studies to measure area, spatial configuration, and human exposures to UGS, facilitating studies on the relationship between UGS and human exposures to environmental hazards, public health outcomes, urban ecology, and urban planning.",Not About Sufficiency
Emotions and Dog Bites: Could Predatory Attacks Be Triggered by Emotional States?,"Simple Summary Dog bites are a worldwide problem that have severe consequences for both the animal and the victim involved in the incident. Epidemiological studies have analyzed the victim features, the characteristics of biting dogs and the context in which attacks occur. Little is known regarding the role of emotions in predatory attacks toward humans and conspecifics in dogs. This paper aims at proposing the potential involvement of emotions for the expression of predatory motor patterns. It is suggested that the reporting of dog biting episodes needs to consider this crucial factor, which is fundamental for providing a realistic and reliable picture of the dog bite phenomenon.</p> Dog biting events pose severe public health and animal welfare concerns. They result in several consequences for both humans (including physical and psychological trauma) and the dog involved in the biting episode (abandonment, relocation to shelter and euthanasia). Although numerous epidemiological studies have analyzed the different factors influencing the occurrence of such events, to date the role of emotions in the expression of predatory attacks toward humans has been scarcely investigated. This paper focuses on the influence of emotional states on triggering predatory attacks in dogs, particularly in some breeds whose aggression causes severe consequences to human victims. We suggest that a comprehensive analysis of the dog bite phenomenon should consider the emotional state of biting dogs in order to collect reliable and realistic data about bite episodes. </p>",Not About Sufficiency
"Action planning for healthy cities: the role of multi-criteria analysis, developed in Italy and France, for assessing health performances in land-use plans and urban development projects","In the last decades a growing attention has been paid to the relationship between urban planning and public health. The introduction of the social model of health has stressed the importance of the determinants of health such as socioeconomic, cultural, and environmental conditions, in addition to living and working conditions. Starting from the assumption that urban planning plays a crucial role for enhancing healthy lifestyles and environments, the paper describes two different approaches to include health issues into land use plans and urban development projects. Two different evaluation tools, defined according to the Italian and French legal framework, have been compared in order to find out whether they could be considered as an innovative answer to the instance of creating a more effective cross field of work and training among urban planners and public health professionals.",Not About Sufficiency
The role of global reanalyses in climate services for health: Insights from the Lancet Countdown,"As the linkages between extreme weather events, changes in climatic conditions and health impacts in exposed populations become clearer, so does the need for climate-smart decisions aimed at making the public health sector more responsive and resilient. By integrating climate and health information, climate services for health provide robust decision-support tools. The Lancet Countdown monitoring system uses global climate reanalyses products to track annual changes in a set of health-related outcomes. In the monitoring system, multiple variables from reanalysis datasets such as ERA5 and ERA5-Land are retrieved and processed to capture heatwaves, precipitation extremes, wildfires, droughts, warming and ecosystem changes across the globe and over multiple decades. This reanalysis-derived information is then input into a hazard-exposure-vulnerability framework that delivers, as outcomes, indicators tracking the year-by-year impacts of climate-related hazards on human mortality, labour capacity, physical activity, sentiment, infectious disease transmission, and food security and undernutrition. Building on the reanalysis gridded format, the indicators create worldwide 'maps without gaps' of climate-health linkages. Our experience shows that reanalysis datasets allow standardization across the climate information used in the framework, making the system potentially adaptable to multiple geographical scales. An ongoing challenge is to quantify how the inherent bias of global reanalyses influences indicator outcomes. We foresee the health sector as a key user of reanalysis products. Therefore, public health professionals and health impact modellers should be involved in the co-development of future iterations of reanalysis datasets, to reach finer spatial resolutions and provide a wider set of health-relevant climate variables.",Not About Sufficiency
Happy parents' tweets: An exploration of Italian Twitter data using sentiment analysis,"BACKGROUND Demographers are increasingly interested in connecting demographic behaviour and trends with 'soft' measures, i.e., complementary information on attitudes, values, feelings, and intentions. OBJECTIVE The aim of this paper is to demonstrate how computational linguistic techniques can be used to explore opinions and semantic orientations related to parenthood. METHODS In this article we scrutinize about three million filtered Italian tweets from 2014. First, we implement a methodological framework relying on Natural Language Processing techniques for text analysis, which is used to extract sentiments. We then run a supervised machine-learning experiment on the overall dataset, based on the annotated set of tweets from the previous stage. Consequently, we infer to what extent social media users report negative or positive affect on topics relevant to the fertility domain. RESULTS Parents express a generally positive attitude towards being and becoming parents, but they are also fearful, surprised, and sad. They also have quite negative sentiments about their children's future, politics, fertility, and parental behaviour. By exploiting geographical information from tweets we find a significant correlation between the prevalence of positive sentiments about parenthood and macro-regional indicators of both life satisfaction and fertility level. CONTRIBUTION We show how tweets can be used to represent soft measures such as attitudes, values, and feelings, and we establish how they relate to demographic features. Linguistic analysis of social media data provides a middle ground between qualitative studies and more standard quantitative approaches.",Not About Sufficiency
Actionable business intelligence: How to make it available through Service-oriented Architectures,"The paper discusses the main characteristics of Service-oriented Architecture and examines the feasible ways of using business intelligence solutions as Web services in an SOA environment. With the evolution of Web services, organizations are becoming more sophisticated in their goals for and requirements of this technology as it offers faster and more flexible deployment, customization and easy integration of BI solutions. Those organizations that choose a Web services strategy will be best positioned to deliver BI content across and beyond the enterprise, making BI accessible to everyone, wherever they work, at a lower cost and in more innovative ways.",Not About Sufficiency
Development of methods to estimate microcystins removal and water treatment resiliency using mechanistic risk modelling,"Drinking water treatment processes are capable of removing microcystins but consistent operation of processes optimized for cyanobacterial harmful algal bloom (cHAB) conditions is not fiscally feasible. Therefore, utilities must ready themselves and start the cHAB processes as a reactionary response. Predictive analytics and modelling are impactful tools to prepare water systems for cHABs, but are still in early stages of development. Until those prospective models are completed, a method to determine best actions in advance of a bloom event thus improving system resiliency is needed. In this study, an adaptation of the quantitative microbial risk analysis (QMRA) methodology was applied to develop this method. This method and resulting models were developed around the Toledo (Ohio, USA) water crisis of 2014, but being mechanistic, they are easily adaptable to other systems' process operations data. Results from this internally validated model demonstrate how rapid action using both powdered activated carbon and measured increases in chlorine dose can mitigate health risks. Our research also demonstrates the importance of modelling the cellular status of the toxins (toxins either in an intact cell or in the water from a lysed cell). Risks were characterized using hazard quotients (HQ) and at the peak of the crisis ranged from a minimum of 0.00244 to a maximum of 2.84 for adults. In simulations where cHAB-specific treatment was used this decreased to 0.00 057 and 0.236 respectively. We further outline how this methodology can be used to simulate water system resiliency to likely and aberrant microbial hazard events to plan for the best interventions to protect public health. This method can be used for other hazards expected to be variable in the future, where system prepatory planning is critical to continued public health protection. Considering the water quantity and quality fluctuations occurring and likely to intensify under climate change, this type of computationally supported preparedness is vital to maintaining robust water system resiliency. (c) 2020 Elsevier Ltd. All rights reserved.",Not About Sufficiency
Stance and Sentiment Analysis of Health-related Tweets with Data Augmentation,"Common social media platforms like Twitter are important as up-to-date information sources for several monitoring purposes, including instant public health monitoring. In this sense, large volumes of health -related social media posts (such as tweets on the COVID-19 pandemic) have been produced recently, and are ready to be analyzed to facilitate healthrelated decision making. In this paper, joint stance detection and sentiment analysis on tweets about the COVID-19 vaccination was performed, in order to showcase the contribution of different machine learning and deep learning techniques equipped with data augmentation. Training and test tweet datasets are compiled and annotated for both stance and sentiment analysis and next, the training dataset is extended using an automatic data augmentation technique to increase its size. Experiments with different classifiers are performed for automated stance and sentiment analyses, using this extended dataset during training. The data augmentation technique adopted in this study to cope with data scarcity problems in machine learning research leads to better performance rates in this domain of health -related social media analysis. Comparative evaluations are also performed using a publicly -available sentiment analysis tool. The extended dataset and the test dataset, along with the approaches, and evaluation results are significant for health informatics, because, they facilitate joint estimation of instant community stance and sentiment towards COVID-19 vaccination which has been an important public health concern. Therefore, public health decision -makers can extensively and readily benefit from the findings and resources of the current study.",Not About Sufficiency
Parkinson's Disease Diagnosis: Towards Grammar-based Explainable Artificial Intelligence,"Machine Learning (ML) approaches are vastly used for supporting humans in decision-making processes. However, the poor explainability associated to their behavior hampers their application in fields were the impact of the decision is critical, as it is the case for medical application, since physicians cannot simply use the predictions of the model but they must trust the results it provides. This work focuses on the automatic detection of Parkinson's disease (PD), whose impact on both the individual's quality of life and social well-being is constantly increasing with the aging of the population. To this end, we propose an explainable approach based on Genetic Programming, called Grammar Evolution (GE). This technique uses context-free grammar to describe the language of the programs to be generated and evolved. In this case, the generated programs are the explicit classification rules for the diagnosis of the subjects. The results of the experiments obtained on the publicly available HandPD data set show GE's high expressive power and performance comparable to those of several ML models that have been proposed in the literature.",Not About Sufficiency
Harnessing Machine Learning for Predictive Analytics: A Case Study of Lassa Fever Outbreaks in Nigeria,"In the ongoing battle against global pandemics, understanding the key determinants that fuel outbreaks are of paramount importance. With this focus, our study aims to assess and rank the predictive capabilities of a wide range of socio-economic, eco-climatic, and spatiotemporal variables in predicting Lassa Fever (LF) outbreaks, using data from previous Nigerian outbreaks (2012-2019). Employing machine learning methods, particularly XGBoost and Random Forest, our study aims to offer accurate and robust predictions concerning LF incidence rates. As a crucial add-on, we leverage the innovative SHAP (SHapley Additive exPlanations) technique as a postprocessing tool to dissect and better understand the contributions of individual features towards the predictions generated by our machine learning models. This multi-layered approach allowed us to place a pronounced focus on healthcare infrastructure, population demographics, land cover, and other climatic co-variates. Among the models evaluated, XGBoost performed the best; delivering an accuracy of 0.93, and AUC of 0.90, and an F1 score of 0.86 on 2018 data. For 2019 data, it maintained a strong accuracy of 0.90, an AUC of 0.89, and an F1 score of 0.75. Our SHAP analysis further emphasized precipitation seasonality, diagnostic center density, and land cover characteristics as pivotal influencers in predicting LF outbreaks. These findings shed light on the complex interplay between environmental conditions, urbanization, and healthcare infrastructure. Given these promising results, our work sets the stage for the development of an advanced early warning system for Lassa Fever in Nigeria: a system that could efficiently intertwine computational insights with on-ground interventions, ensuring timely and targeted responses to potential outbreaks.",Not About Sufficiency
Joint kinematics alone can distinguish hip or knee osteoarthritis patients from asymptomatic controls with high accuracy,"Osteoarthritis (OA) is one of the leading musculoskeletal disabilities worldwide, and several interventions intend to change the gait pattern in OA patients to more healthy patterns. However, an accessible way to follow up the biomechanical changes in a clinical setting is still missing. Therefore, this study aims to evaluate whether we can use biomechanical data collected from a specific activity of daily living to help distinguish hip OA patients from controls and knee OA patients from controls using features that potentially could be measured in a clinical setting. To achieve this goal, we considered three different classes of statistical models with different levels of data complexity. Class 1 is kinematics based only (clinically applicable), class 2 includes joint kinetics (semi-applicable under the condition of access to a force plate or prediction models), and class 3 uses data from advanced musculoskeletal modeling (not clinically applicable). We used a machine learning pipeline to determine which classification model was best. We found 100% classification accuracy for KneeOA-vs-Asymptomatic and 93.9% for HipOA-vs-Asymptomatic using seven features derived from the lumbar spine and hip kinematics collected during ascending stairs. These results indicate that kinematical data alone can distinguish hip or knee OA patients from asymptomatic controls. However, to enable clinical use, we need to validate if the classifier also works with sensor-based kinematical data and whether the probabilistic outcome of the logistic regression model can be used in the follow-up of patients with OA.",Not About Sufficiency
Holistic Approach of Swiss Fetal Progenitor Cell Banking: Optimizing Safe and Sustainable Substrates for Regenerative Medicine and Biotechnology,"Safety, quality, and regulatory-driven iterative optimization of therapeutic cell source selection has constituted the core developmental bedrock for primary fetal progenitor cell (FPC) therapy in Switzerland throughout three decades. Customized Fetal Transplantation Programs were pragmatically devised as straightforward workflows for tissue procurement, traceability maximization, safety, consistency, and robustness of cultured progeny cellular materials. Whole-cell bioprocessing standardization has provided plethoric insights into the adequate conjugation of modern biotechnological advances with current restraining legislative, ethical, and regulatory frameworks. Pioneer translational advances in cutaneous and musculoskeletal regenerative medicine continuously demonstrate the therapeutic potential of FPCs. Extensive technical and clinical hindsight was gathered by managing pediatric burns and geriatric ulcers in Switzerland. Concomitant industrial transposition of dermal FPC banking, following good manufacturing practices, demonstrated the extensive potential of their therapeutic value. Furthermore, in extenso, exponential revalorization of Swiss FPC technology may be achieved via the renewal of integrative model frameworks. Consideration of both longitudinal and transversal aspects of simultaneous fetal tissue differential processing allows for a better understanding of the quasi-infinite expansion potential within multi-tiered primary FPC banking. Multiple fetal tissues (e.g., skin, cartilage, tendon, muscle, bone, lung) may be simultaneously harvested and processed for adherent cell cultures, establishing a unique model for sustainable therapeutic cellular material supply chains. Here, we integrated fundamental, preclinical, clinical, and industrial developments embodying the scientific advances supported by Swiss FPC banking and we focused on advances made to date for FPCs that may be derived from a single organ donation. A renewed model of single organ donation bioprocessing is proposed, achieving sustained standards and potential production of billions of affordable and efficient therapeutic doses. Thereby, the aim is to validate the core therapeutic value proposition, to increase awareness and use of standardized protocols for translational regenerative medicine, potentially impacting millions of patients suffering from cutaneous and musculoskeletal diseases. Alternative applications of FPC banking include biopharmaceutical therapeutic product manufacturing, thereby indirectly and synergistically enhancing the power of modern therapeutic armamentariums. It is hypothesized that a single qualifying fetal organ donation is sufficient to sustain decades of scientific, medical, and industrial developments, as technological optimization and standardization enable high efficiency.",Not About Sufficiency
Age and COVID-19 mortality in the United States: a comparison of the prison and general population,"Purpose The USA has a rapidly aging prison population that, combined with their poorer health and living conditions, is at extreme risk for COVID-19. The purpose of this paper is to compare COVID-19 mortality trends in the US prison population and the general population to see how mortality risk changed over the course of the pandemic. The authors first provide a national overview of trends in COVID-19 mortality; then, the authors assess COVID-19 deaths among older populations using more detailed data from one US state. Design/methodology/approach The authors used multiple publicly available data sets (e.g. Centers for Disease Control and prevention, COVID Prison Project) and indirect and direct standardization to estimate standardized mortality rates covering the period from April 2020 to June 2021 for the US and for the State of Texas. Findings While 921 COVID-19-related deaths among people in US prisons were expected as of June 5, 2021, 2,664 were observed, corresponding to a standardized mortality ratio of 2.89 (95%CI 2.78, 3.00). The observed number of COVID-19-related deaths exceeded the expected number of COVID-19-related deaths among people in prison for most of the pandemic, with a substantially widening gap leading to a plateau about four weeks after the COVID-19 vaccine was introduced in the USA. In the state population, the older population in prison is dying at younger ages compared with the general population, with the highest percentage of deaths among people aged 50-64 years. Research limitations/implications People who are incarcerated are dying of COVID-19 at a rate that far outpaces the general population and are dying at younger ages. Originality/value This descriptive analysis serves as a first step in understanding the dynamic trends in COVID-19 mortality and the association between age and COVID-19 death in US prisons.",Not About Sufficiency
House relocation: A redevelopment tool for rapidly changing urban environments,"House relocation is a circular reuse approach with potential to address multi-pronged community challenges in rapidly redeveloping cities. Historically used throughout the USA, Austin, Texas serves as the testing ground for this contemporary research on house relocation, exemplifying a urban context experiencing a range of challenges, including rampant demolition, housing shortages, exponential population increase, and unmet zero waste goals. Researchers took a mixed-methods approach, including literature review, interviews, and permit record analysis to assess the use of house relocation as a tool to address these urban challenges, while identifying associated community benefits, barriers, and applications. Written in parallel, the academic research also informed an educational guide for the development community and broader public audience released in 2023.",Not About Sufficiency
The citizens have participated - what now? An action research study of factors impacting the use of participatory citizen knowledge in planning processes,"Citizen participation is embedded into planning practice and policy, yet it remains unclear how the results of participation are shared in planning organisations and utilised to inform planning outcomes. This article examines barriers and enablers for systematic gathering, sharing and utilisation of participatory citizen knowledge and analyses the collaborative development of knowledge-sharing practices in two Finnish municipalities. The article focuses specifically on the collection and dissemination of local, experiential knowledge from two municipality-wide Public Participation GIS surveys. The study adopts an action research approach to deepen the understanding of planners' complex relationships with participatory citizen knowledge: how they gather and share it, how it informs their work and how they develop their everyday practice. The results suggest that a variety of practical, technical and cultural factors influence whether planners access and utilise gathered citizen knowledge. Planners' backgrounds, skills and personalities also influence how they perceive and utilise citizen knowledge. When provided with accessible and representative data about citizens' behaviours and preferences, adequate resources and a sense of agency through supportive organisation culture, officials from different sectors were eager to develop the use of participatory knowledge. These emergent practices can be used to inform wider iterative development to meet practitioners' needs.",Not About Sufficiency
A risk assessment framework for multidrug-resistant Staphylococcus aureus using machine learning and mass spectrometry technology,"The emergence of multidrug-resistant bacteria is a critical global crisis that poses a serious threat to public health, particularly with the rise of multidrug-resistant Staphylococcus aureus. Accurate assessment of drug resistance is essential for appropriate treatment and prevention of transmission of these deadly pathogens. Early detection of drug resistance in patients is critical for providing timely treatment and reducing the spread of multidrug-resistant bacteria. This study aims to develop a novel risk assessment framework for S. aureus that can accurately determine the resistance to multiple antibiotics. The comprehensive 7-year study involved >20 000 isolates with susceptibility testing profiles of six antibiotics. By incorporating mass spectrometry and machine learning, the study was able to predict the susceptibility to four different antibiotics with high accuracy. To validate the accuracy of our models, we externally tested on an independent cohort and achieved impressive results with an area under the receiver operating characteristic curve of 0. 94, 0.90, 0.86 and 0.91, and an area under the precision-recall curve of 0.93, 0.87, 0.87 and 0.81, respectively, for oxacillin, clindamycin, erythromycin and trimethoprim-sulfamethoxazole. In addition, the framework evaluated the level of multidrug resistance of the isolates by using the predicted drug resistance probabilities, interpreting them in the context of a multidrug resistance risk score and analyzing the performance contribution of different sample groups. The results of this study provide an efficient method for early antibiotic decision-making and a better understanding of the multidrug resistance risk of S. aureus.",Not About Sufficiency
OpenKBP: The open-access knowledge-based planning grand challenge and dataset,"Purpose To advance fair and consistent comparisons of dose prediction methods for knowledge-based planning (KBP) in radiation therapy research. Methods We hosted OpenKBP, a 2020 AAPM Grand Challenge, and challenged participants to develop the best method for predicting the dose of contoured computed tomography (CT) images. The models were evaluated according to two separate scores: (a) dose score, which evaluates the full three-dimensional (3D) dose distributions, and (b) dose-volume histogram (DVH) score, which evaluates a set DVH metrics. We used these scores to quantify the quality of the models based on their out-of-sample predictions. To develop and test their models, participants were given the data of 340 patients who were treated for head-and-neck cancer with radiation therapy. The data were partitioned into training (n=200), validation (n=40), and testing (n=100) datasets. All participants performed training and validation with the corresponding datasets during the first (validation) phase of the Challenge. In the second (testing) phase, the participants used their model on the testing data to quantify the out-of-sample performance, which was hidden from participants and used to determine the final competition ranking. Participants also responded to a survey to summarize their models. Results The Challenge attracted 195 participants from 28 countries, and 73 of those participants formed 44 teams in the validation phase, which received a total of 1750 submissions. The testing phase garnered submissions from 28 of those teams, which represents 28 unique prediction methods. On average, over the course of the validation phase, participants improved the dose and DVH scores of their models by a factor of 2.7 and 5.7, respectively. In the testing phase one model achieved the best dose score (2.429) and DVH score (1.478), which were both significantly better than the dose score (2.564) and the DVH score (1.529) that was achieved by the runner-up models. Lastly, many of the top performing teams reported that they used generalizable techniques (e.g., ensembles) to achieve higher performance than their competition. Conclusion OpenKBP is the first competition for knowledge-based planning research. The Challenge helped launch the first platform that enables researchers to compare KBP prediction methods fairly and consistently using a large open-source dataset and standardized metrics. OpenKBP has also democratized KBP research by making it accessible to everyone, which should help accelerate the progress of KBP research. The OpenKBP datasets are available publicly to help benchmark future KBP research.",Not About Sufficiency
"Classifying Medulloblastoma Subgroups Based on Small, Clinically Achievable Gene Sets","As treatment protocols for medulloblastoma (MB) are becoming subgroup-specific, means for reliably distinguishing between its subgroups are a timely need. Currently available methods include immunohistochemical stains, which are subjective and often inconclusive, and molecular techniques-e.g., NanoString, microarrays, or DNA methylation assays-which are time-consuming, expensive and not widely available. Quantitative PCR (qPCR) provides a good alternative for these methods, but the current NanoString panel which includes 22 genes is impractical for qPCR. Here, we applied machine-learning-based classifiers to extract reliable, concise gene sets for distinguishing between the four MB subgroups, and we compared the accuracy of these gene sets to that of the known NanoString 22-gene set. We validated our results using an independent microarray-based dataset of 92 samples of all four subgroups. In addition, we performed a qPCR validation on a cohort of 18 patients diagnosed with SHH, Group 3 and Group 4 MB. We found that the 22-gene set can be reduced to only six genes (IMPG2, NPR3, KHDRBS2, RBM24, WIF1, and EMX2) without compromising accuracy. The identified gene set is sufficiently small to make a qPCR-based MB subgroup classification easily accessible to clinicians, even in developing, poorly equipped countries.",Not About Sufficiency
Machine learning applications to differentiate comorbid functional seizures and epilepsy from pure functional seizures,"Purpose: We have utilized different methods in machine learning (ML) to develop the best algorithm to differentiate comorbid functional seizures (FS) and epilepsy from those who have pure FS. Methods: This was a retrospective study of an electronic database of patients with seizures. All patients with a diagnosis of FS (with or without comorbid epilepsy) were studied at the outpatient epilepsy clinic at Shiraz University of Medical Sciences, Shiraz, Iran, from 2008 until 2021. We arbitrarily selected 14 features that are important in making the diagnosis of patients with seizures and also are easily obtainable during history taking. Pytorch and Scikit-learn packages were used to construct various models including random forest classifier, decision tree classifier, support vector classifier, k-nearest neighbor, and TabNet classifier. Results: Three hundred and two patients had FS (82.5%), while 64 patients had FS and comorbid epilepsy (17.5%). The ""TabNet classifier"" could provide the best sensitivity (90%) and specificity (74%) measures (accuracy of 76%) to help differentiate patients with FS from those with FS and comorbid epilepsy. Conclusion: These satisfactory differentiating measures suggest that the current algorithm could be used in clinical practice to help with the difficult task of distinguishing patients with FS from those with FS and comorbid epilepsy. Based on the results of the current study, we have developed an Application (SeiDx). This App is freely accessible at the following address: https://drive.google.com/file/d/1rAgBXKNPW9bmUCDioaGHHzLBQgzZ-HZ 2/view. This App should be validated in a prospective assessment.",Not About Sufficiency
PLOMaR: An Ontology Framework for Context Modeling and Reasoning on Crowd-Sensing Platform,"Crowd-sensing is a popular way to sense and collect data using smartphones that reveals user behaviors and their correlations with device performance. PhoneLab is one of the largest crowd-sensing platform based on the Android system. Through experimental instrumentations and system modifications, researchers can tap into a sea of insightful information that can be further processed to reveal valuable context information about the device, user and the environment. However, the PhoneLab data is in JSON format. The process of inferring reasons from data in this format is not straightforward. In this paper, we introduce PLOMaR -an ontology framework that uses SPARQL rules to help researchers access information and derive new information without complex data processing. The goals are to (i) make the measurement data more accessible, (ii) increase interoperability and reusability of data gathered from different sources, (iii) develop extensible data representation to support future development of the PhoneLab platform. We describe the models, the JSON to RDF mapping processes, and the SPARQL rules used for deriving new information. We evaluate our framework with three application examples based on the sample dataset provided.",Not About Sufficiency
The role of informal green spaces in reducing inequalities in urban green space availability to children and seniors,"Urban green spaces (UGS) offer a wide range of ecosystem services to city dwellers, contributing to their health and well-being. The resources of formally designated UGS, such as parks and forests, are frequently under-provided, however. This results in unequal access to UGS, which has become an environmental justice issue. We investigated the potential of informal green spaces (IGS) to complement existing formal UGS to reduce distributional inequity in UGS availability. We focused on the most vulnerable groups of citizens, children and elderly residents, for whom the availability of UGS plays a particularly important role. The study was performed in two Eastern-European cities, Warsaw and Lodi, both characterised by a well-developed system of UGS but of different spatial composition and UGS configuration. We focus on unmanaged areas within IGS and link them to the provisioning of recreational ecosystem services, such as enabling direct contact with nature. We identified different categories of formal and informal UGS based on publicly available data, supported by NDVI values, followed by UGS availability analysis of formal and informal UGS in the service area of 300 m for each residential building. We found that informal UGS are equally important as formal ones, and they may contribute to better access to selected urban ecosystem services. Both cities are characterised by unequal distribution of formal UGS. In Lodi, children are least favoured in terms of UGS availability, while in Warsaw, elderly citizens are most excluded. In both cities, it is the green areas associated with transportation routes communication routes, multifamily housing and agricultural lands along with grasslands that have the greatest potential to improve the equal availability of UGS to residents. We propose maintaining various types of unmanaged green areas in cities to help limit disparities to UGS access, and it would also increase their importance in the provisioning of recreational ecosystem services.",Not About Sufficiency
"Infrastructure, programs, and policies to increase bicycling: An international review","Objectives. To assess existing research on the effects of various interventions on levels of bicycling. Interventions include infrastructure (e.g., bike lanes and parking), integration with public transport, education and marketing programs, bicycle access programs, and legal issues. Methods. A comprehensive search of peer-reviewed and non-reviewed research identified 139 studies. Study methodologies varied considerably in type and quality, with few meeting rigorous standards. Secondary data were gathered for 14 case study cities that adopted multiple interventions. Results. Many studies show positive associations between specific interventions and levels of bicycling. The 14 case studies show that almost all cities adopting comprehensive packages of interventions experienced large increases in the number of bicycle trips and share of people bicycling. Conclusions. most of the evidence examined in this review supports the crucial role of public policy in encouraging bicycling. Substantial increases in bicycling require an integrated package of many different, complementary interventions, including infrastructure provision and pro-bicycle programs, supportive land use planning, and restrictions on car use. (C) 2009 Elsevier Inc. All rights reserved.",Not About Sufficiency
"The socialities of everyday urban walking and the ""right to the city'","This paper explores the socialities of everyday urban walking. The paper begins from the starting contention that a wide range of social and cultural theory, urban planning and transport literatures position walking as a practice that unproblematically encourages social mixing', community cohesion' and social interaction'. Through the analysis of in-depth interview and diary data from research on urban walking in London, this paper engages with a series of underexamined questions. What, for example, is the nature of social interactions on foot? Who are they with, what initiates them and how do they unfold? How do these interactions relate to how we understand the relationship between walking and urban space? Attention is drawn to verbal and non-verbal interactions of strangers as they walk and to the significance of the practical accomplishment of walking together. However, an examination of the discursive organisation of diary and interview data extends existing work concerning the practical organisation of everyday pedestrian mobilities by considering the significance of participants' accounts of their walking experiences. This analytic move foregrounds a counterposition to dominant discourses surrounding everyday walking practices that is situated in the context of broader concerns with everyday urban politics and the right to the city'. This approach contributes to a clearer engagement with the socialities of urban walking whilst raising important questions concerning the ways in which particular walking discourses inform urban scholarship. The paper concludes that in the promotion of walking as a form of low-carbon active travel greater account should be taken of pedestrian encounters.",Not About Sufficiency
SLEAP: A deep learning system for multi-animal pose tracking,"The desire to understand how the brain generates and patterns behavior has driven rapid methodological innovation in tools to quantify natural animal behavior. While advances in deep learning and computer vision have enabled markerless pose estimation in individual animals, extending these to multiple animals presents unique challenges for studies of social behaviors or animals in their natural environments. Here we present Social LEAP Estimates Animal Poses (SLEAP), a machine learning system for multi-animal pose tracking. This system enables versatile workflows for data labeling, model training and inference on previously unseen data. SLEAP features an accessible graphical user interface, a standardized data model, a reproducible configuration system, over 30 model architectures, two approaches to part grouping and two approaches to identity tracking. We applied SLEAP to seven datasets across flies, bees, mice and gerbils to systematically evaluate each approach and architecture, and we compare it with other existing approaches. SLEAP achieves greater accuracy and speeds of more than 800 frames per second, with latencies of less than 3.5 ms at full 1,024 x 1,024 image resolution. This makes SLEAP usable for real-time applications, which we demonstrate by controlling the behavior of one animal on the basis of the tracking and detection of social interactions with another animal. SLEAP is a versatile deep learning-based multi-animal pose-tracking tool designed to work on videos of diverse animals, including during social behavior.",Not About Sufficiency
MTBI Identification From Diffusion MR Images Using Bag of Adversarial Visual Features,"In this paper, we propose bag of adversarial features (BAFs) for identifying mild traumatic brain injury (MTBI) patients from their diffusion magnetic resonance images (MRIs) (obtained within one month of injury) by incorporating unsupervised feature learning techniques. MTBI is a growing public health problem with an estimated incidence of over 1.7 million people annually in USA. Diagnosis is based on clinical history and symptoms, and accurate, concrete measures of injury are lacking. Unlike most of the previous works, which use hand-crafted features extracted from different parts of brain for MTBI classification, we employ feature learning algorithms to learn more discriminative representation for this task. A major challenge in this field thus far is the relatively small number of subjects available for training. This makes it difficult to use an end-to-end convolutional neural network to directly classify a subject from MRIs. To overcome this challenge, we first apply an adversarial auto-encoder (with convolutional structure) to learn patch-level features, from overlapping image patches extracted from different brain regions. We then aggregate these features through a bag-of-words approach. We perform an extensive experimental study on a dataset of 227 subjects (including 109 MTBI patients, and 118 age and sex-matched healthy controls) and compare the bag-of-deep-features with several previous approaches. Our experimental results show that the BAF significantly outperforms earlier works relying on the mean values of MR metrics in selected brain regions.",Not About Sufficiency
"SignSpeaker: A Real-time, High-Precision SmartWatch-based Sign Language Translator","Sign language is a natural and fully-formed communication method for deaf or hearing-impaired people. Unfortunately, most of the state-of-the-art sign recognition technologies are limited by either high energy consumption or expensive device costs and have a difficult time providing a real-time service in a daily-life environment. Inspired by previous works on motion detection with wearable devices, we propose SignSpeaker - a real-time, robust, and user-friendly American sign language recognition (ASLR) system with affordable and portable commodity mobile devices. SignSpeaker is deployed on a smartwatch along with a smartphone; the smartwatch collects the sign signals and the smartphone outputs translation through an inbuilt loudspeaker. We implement a prototype system and run a series of experiments that demonstrate the promising performance of our system. For example, the average translation time is approximately 1.1 seconds for a sentence with eleven words. The average detection ratio and reliability of sign recognition are 99.2% and 99.5%, respectively. The average word error rate of continuous sentence recognition is 1.04% on average.",Not About Sufficiency
Statistical Tools and Methodologies for Ultrareliable Low-Latency Communication-A Tutorial,"Ultrareliable low-latency communication (URLLC) constitutes a key service class of the fifth generation (5G) and beyond cellular networks. Notably, designing and supporting URLLC pose a herculean task due to the fundamental need to identify and accurately characterize the underlying statistical models in which the system operates, e.g., interference statistics, channel conditions, and the behavior of protocols. In general, multilayer end-to-end approaches considering all the potential delay and error sources and proper statistical tools and methodologies are inevitably required for providing strong reliability and latency guarantees. This article contributes to the body of knowledge in the latter aspect by providing a tutorial on several statistical tools and methodologies that are useful for designing and analyzing URLLC systems. Specifically, we overview the frameworks related to the following: 1) reliability theory; 2) short packet communications; 3) inequalities, distribution bounds, and tail approximations; 4) rare-events simulation; 5) queuing theory and information freshness; and 6) large-scale tools, such as stochastic geometry, clustering, compressed sensing, and mean-field (MF) games. Moreover, we often refer to prominent data-driven algorithms within the scope of the discussed tools/methodologies. Throughout this article, we briefly review the state-of-the-art works using the addressed tools and methodologies, and their link to URLLC systems. Moreover, we discuss novel application examples focused on physical and medium access control layers. Finally, key research challenges and directions are highlighted to elucidate how URLLC analysis/design research may evolve in the coming years.",Not About Sufficiency
Earthquake management: a decision support system based on natural language processing,"When an earthquake occurs, a huge amount of data is generated by social media users. Social networks play therefore a fundamental role in the development of decision support systems that could help both government and citizens. From user-generated contents, the information about an occurring emergency could be acquired and exploited to understand the critical event and its evolution over time. On the other side, the social interactions among users can be exploited as a dissemination gate to make people informed. In this paper, we present a decision support system for earthquake management based on machine learning and natural language processing to effectively extract and organize knowledge from online social media data. The proposed system, on a real Twitter dataset, has shown significant results for identifying messages related to (real) earthquakes and critical tremors, highlighting those posts provided by spontaneous users and containing any actionable knowledge about damages, magnitude, location and time references.",Not About Sufficiency
Transformations of the Beirut River: Between Temporary and Permanent Liminality,"This article presents the case of the Beirut River corridor in Lebanon, which defines the administrative border between the capital Beirut, its eastern and south-eastern suburbs. The Beirut River has undergone several transformations from being a lotic environment to becoming complex urban infrastructure. This is often unnoticeable due to the scarcity of its running water and its walled existence at the edge of administrative boundaries. The separation from its riverbanks, disconnection from the urban fabric, and continuous pollution have contributed to its liminality, being simultaneously neither present nor absent. To understand this in-betweenness, the river's spatial, temporal, and social liminality are analysed by identifying major events, actors, and key urban planning interventions that impacted the river at the national, city region, and local scales. The article explores the development of the river corridor both in terms of urbanisation and population dynamics; its distinct positionality in different periods that corresponded to major events and decisions made; and the contrasting river experiences and perceptions across generations, which vary between reminiscence and aversion. By examining the various transformative processes, collective practices, perceptions, and diverse actors, the article highlights the contextual implications of this obdurate liminality, but also Beirut River's potential alternative future positionality amidst present and imminent urban challenges.",Not About Sufficiency
Advancing Human Activity Recognition Using Ultra-Wideband Channel Impulse Response Snapshots,"Human Activity Recognition (HAR) has a significant contribution to a wide range of applications, including security surveillance and healthcare. It provides a non-intrusive, cost-effective solution building upon wireless signal technologies. Despite the recent growing availability of Ultra-Wideband (UWB) in the mass market, its potential in HAR is still under-researched compared with other wireless technologies. In this paper, we use UWB's high-resolution Channel Impulse Response (CIR) for human activity classification tasks. A diverse UWB CIR dataset in real-world indoor settings is collected to evaluate the performance in recognition of six independent activities. Our novel approach treats each CIR as a unique snapshot, capturing specific activities and creating a dynamic activity representation through concatenated feature vectors. A thorough grid search is conducted to explore optimal parameters in constructing these vectors. In applying eleven different learning models for classification analysis on the dataset, it is generally observed that deep learning methods yielded enhanced classification accuracy compared to traditional machine learning techniques. Our research demonstrates the potential of UWB in HAR and provides insights into effective feature vector creation and model performance. The code and dataset are publicly accessible at https://bit.ly/422WJEB",Not About Sufficiency
Global knowledge gaps in acute febrile illness etiologic investigations: A scoping review,"Background Acute febrile illness (AFI), a common reason for people seeking medical care globally, represents a spectrum of infectious disease etiologies with important variations geographically and by population. There is no standardized approach to conducting AFI etiologic investigations, limiting interpretation of data in a global context. We conducted a scoping review to characterize current AFI research methodologies, identify global research gaps, and provide methodological research standardization recommendations. Methodology/Findings Using pre-defined terms, we searched Medline, Embase, and Global Health, for publications from January 1, 2005-December 31, 2017. Publications cited in previously published systematic reviews and an online study repository of non-malarial febrile illness etiologies were also included. We screened abstracts for publications reporting on human infectious disease, aimed at determining AFI etiology using laboratory diagnostics. One-hundred ninety publications underwent full-text review, using a standardized tool to collect data on study characteristics, methodology, and laboratory diagnostics. AFI case definitions between publications varied: use of self-reported fever as part of case definitions (28%, 53/190), fever cut-off value (38 center dot 0 degrees C most commonly used: 45%, 85/190), and fever measurement site (axillary most commonly used: 19%, 36/190). Eighty-nine publications (47%) did not include exclusion criteria, and inclusion criteria in 13% (24/190) of publications did not include age group. No publications included study settings in Southern Africa, Micronesia & Polynesia, or Central Asia. We summarized standardized reporting practices, specific to AFI etiologic investigations that would increase inter-study comparability. Conclusions Wider implementation of standardized AFI reporting methods, with multi-pathogen disease detection, could improve comparability of study findings, knowledge of the range of AFI etiologies, and their contributions to the global AFI burden. These steps can guide resource allocation, strengthen outbreak detection and response, target prevention efforts, and improve clinical care, especially in resource-limited settings where disease control often relies on empiric treatment. PROSPERO: CRD42016035666. Author summary Acute febrile illness (AFI) is a common reason for people seeking medical care globally with potentially serious infectious etiologies. However, AFI has no current consensus standardized approach when considered as a syndromic case definition for public health surveillance or research, especially in global settings where AFI treatment is performed with limited diagnostic availability. Therefore, the aim of this review was to describe current methodologies in AFI research, identify gaps in research, and provide recommendations for standardization of AFI research. We screened abstracts and completed full-text reviews on publications found through an extensive search. A total of 190 publications were included in the final review, from which we collected data on study characteristics, methodology, and laboratory diagnostics. These collected data elements allowed us to identify where there were inconsistencies in reporting and investigative methods, which data elements were not regularly collected, and what laboratory testing methods were commonly used. Standardized reporting methods for AFI investigations, along with laboratory testing capacity for multiple pathogens, can improve our knowledge of the causative agents of AFI in certain regions of the world. This can help us determine where resources are needed, how to strengthen outbreak detection and response, and how to improve medical care, especially in regions with limited resources.",Not About Sufficiency
Catching Intrusions: Classifier Performances for Detecting Network-specific Anomalies in Energy Systems,"Future energy infrastructures, as the smart grid, will decentralize and integrate different producers, consumers, and network entities. They will be the most complex and likewise the most exposed network infrastructures. Their protection is crucial for the modern society and calls for appropriate security mechanisms to implement multi-level security. This requires a protocol-specific monitoring in the core of the process control networks as well as in peripheral subsystems to detect intrusions. For an application in a wide range of the integrated subnetworks, the monitoring must be self-adapting to the network traffic of the respective domain. For this purpose, protocol knowledge and machine learning algorithms can compose intelligent and flexible anomaly detectors. This paper presents recent results in developing such machine learning detectors by analyzing six classification methods with real-world traffic traces from two energy control networks. The results for different traffic processing methods are discussed in terms of f-score, precision, and recall. They show the high potential of using classification methods for training detectors to enable an intelligent identification of anomalies in smart energy networks with minimal configuration effort.",Not About Sufficiency
Heat treatment control technology of high-strength steel gears based on support vector machine,"In the actual production process of gears often because of the selection of heat treatment parameters is unreasonable and can not accurately achieve the small deformation, high precision, less grinding machining allowance heat treatment sample requirements, there are uneven distribution of carburized layer, surface hardness, hardness of the heart can not meet the requirements of the indicators. At the present stage, the method of multi-parameter multi-level combination test block trial production is often used, but its production cycle is long, and the waste of human and material resources is serious. In this study, with the help of machine learning, a support vector machine prediction model of gear tissue distribution is constructed based on heat treatment parameters, and the radial basis functions kernel function is selected as the kernel function of the support vector machine to improve the accuracy of model prediction by optimizing the kernel parameters. The root mean square error value of the final model is 3.16%, and the coefficient of determination is 0.993. The results show that the method of this paper can accurately and efficiently predict the heat treatment results of gears, and save the manufacturing cycle and cost. The precise control of hardness, carburization layer distribution pattern and metallographic organization of ultra-high-strength steel gears can be realized in actual production.",Not About Sufficiency
A perspective on education's importance for urban development,"The 21(st) century has been called the age of the city'. The concentration of human activity is what makes cities such an important space of opportunity and challenge. This article views urban development challenges from an education perspective and argues that education must be viewed as an important intermediary for capitalising on the physical, intellectual and social capital available in cities. The distribution of educational opportunity within cities must be monitored to ensure that education plays a role in reducing and not exacerbating urban inequalities. Making sure that the city works for all requires improving how we plan cities and making urban planning processes more inclusive through knowledge-based participation. There needs to be more appreciation of education's role in transformative urban development and stronger advocacy by education stakeholders to gain a seat in the circles that wield the most power in the urban futures debates.",Not About Sufficiency
Assessing Open Archive OAI-PMH implementations,"The number of works made freely accessible on the Web by institutions joined to Open Archive Initiatives such as libraries, research institutions, scientific and cultural archives, is constantly growing. Until now some international agreements among key players of Open Access approach have been established but the low level of standardization and the lack of a digital resource and repository certification authority prevents any quality control on the works and subsequently their trustable dissemination within the cultural and scientific community. In this scenario, the benefit of having freely accessible resources is lost. The study proposed in this paper aims at providing a statistical analysis of the state of art of OA implementations in order to outline weak practices, propose assessment metrics for self analysis and stimulate a general improvement of the quality of OA implementations.",Not About Sufficiency
European Healthy City Network Phase V: patterns emerging for healthy urban planning,"There is a tradition of planning cities and their infrastructure to successfully tackle communicable disease arising from urban development. Non-communicable disease follows a different course. Development brings in its wake a basket of adverse health and health equity outcomes that are proving difficult to tackle. In response, within Phase V of the European Healthy Cities Network, municipalities have implemented a range of policy and physical interventions using a settings approach. Owing to the time lag between physical interventions and health outcomes, this research interrogates city activity itself to develop better understanding. Self-reported city case studies and questionnaire data were analysed to reveal patterns using an inductive approach. Findings indicate that some categories of intervention, such as whole city planning and transport, have a systemic impact across the wider determinants of health. Addressing transferability and stakeholder understanding helped cities create conditions for successful outcomes. Cities had varying urban development approaches for tackling climate change. Improvements to current practice are discussed, including; a distinction between supply side and demand side in healthy urban planning; valuing co-benefits and developing integrative approaches to the evidence-base. This evaluative article is important for cities wanting to learn how to maximize benefits to public health through urban development and for researchers exploring, with a systemic approach, the experiences of European cities acting at the interface of urban development and public health. This article also provides recommendations for future phases of the WHO European Healthy Cities programme, posing questions to better address governance and equity in spatial planning.",Not About Sufficiency
"Craftsmen, Cartography and Empire. The Social Production of a Nautical Instrument in the Iberian World, 1500-1650","Objective/context: Explaining how and under what conditions some of the most emblematic scientific instruments of the modem world were built remains a challenge for historians of science and technology. This principle also affects nautical instruments. Thus, the objective of this article is to consider the ""how"" and the ""under what conditions"", in a broad sense, by examining one of these devices: the nautical charts developed during the European maritime expansion. Originality: The originality of this article lies not only in the reconstruction of this instrument in Portugal and Spain, providing new information about its development process and its manufacturers, but also in shedding light on old historiographical debates that underlie the genesis of modern science, such as the difficult collaboration between practical men and theoretical men; the contribution of artisan communities to the world of knowledge; the role that practical knowledge played in the establishment of new forms of standardization in science; the adoption of an empirical methodology for achieving mastery of the world or the construction of large global empires founded upon manual labor. Methodology: Analyzing these debates requires a methodological approach that addresses the main categories and concepts put into circulation by the most recent history and philosophy of science. In short, unveiling the conditions that enabled the establishment of epistemological agreements (and also disputes) between different knowledge communities by taking a nautical instrument as ""mediator."" Conclusions: The article concludes that examining empirical knowledge cultures and their material achievements are a sine qua non condition to understand the genesis of European scientific modernity and the construction of a global world.",Not About Sufficiency
The Effect of World Happiness Aspects with COVID-19 Infected using Machine Learning Multiple Regression Model,"An outbreak of pneumonia was found in China in December 2019, known to be caused by a virus called COVID-19. Since then, the virus has spread all over the world causing countries to lock their borders to prevent the spread of infections. There is no known medicine for COVID-19 and other corona virus. The objective of this paper is to reveal whether five aspects of world happiness that was obtained from https://worldhappiness.report which are Freedom to make life choices, Social support, Healthy life expectancy, Logged GDP per capita and Ladder score have effect on the number COVID-19 infected cases where the dataset is taken from COVID-19 World Dataset from www.kaggle.com. Using Multiple Regression as the model, the result shows that the predicted and actual infected cases are similar and hence there are no effect between COVID-19 and those five aspects.",Not About Sufficiency
Effects of heatwave features on machine-learning-based heat-related ambulance calls prediction models in Japan,"Researchers agree that there is substantial evidence of an increasing trend in both the frequency and duration of extreme temperature events. Increasing extreme temperature events will place more pressure on public health and emergency medical resources, and societies will need to find effective and reliable solutions to adapt to hotter summers. This study developed an effective method to predict the number of daily heat-related ambulance calls. Both national- and regional-level models were developed to evaluate the performance of machine-learning-based methods on heatrelated ambulance call prediction. The national model showed a high prediction accuracy and can be applied over most regions, while the regional model showed extremely high prediction accuracy in each corresponding region and reliable accuracy in special cases. We found that the introduction of heatwave features, including accumulated heat stress, heat acclimatization, and optimal temperature, significantly improved prediction accuracy. The adjusted coefficient of determination (adjusted R2) of the national model improved from 0.9061 to 0.9659 by including these features, and the adjusted R2 of the regional model also improved from 0.9102 to 0.9860. Furthermore, we used five bias-corrected global climate models (GCMs) to forecast the total number of summer heat-related ambulance calls under three different future climate scenarios nationally and regionally. Our analysis demonstrated that, at the end of the 21st century, the total number of heat-related ambulance calls in Japan will reach approximately 250,000 per year (nearly four times the current amount) under SSP-5.85. Our results suggest that disaster management agencies can use this highly accurate model to forecast potential high emergency medical resource burden caused by extreme heat events, allowing them to raise and improve public awareness and prepare countermeasures in advance. The method proposed in Japan in this paper can be applied to other countries that have relevant data and weather information systems.",Not About Sufficiency
Social media data and housing recovery following extreme natural hazards,"Identifying initiatives that influence the decision-making process of individuals in the aftermath of extreme natural events is a critical task in post-disaster recovery research. Due to the diversity of disaster-induced physical and psychosocial damage, as well as the complexity of human behavior, a comprehensive under standing of contributing factors requires a collective effort. The growth of social media platforms with millions of users provides researchers with an exceptional opportunity to conceptualize spatial patterns and communal behaviors. This longitudinal study proposes a multistep machine learning algorithm to understand such recovery decisions using social media data. Two publicly available databases, New York City tax lot data and 109 million geotagged tweets from the period October 2012-October 2014 were used to explore residents' recovery decisions in the two years following Hurricane Sandy. The results reveal that communities with more tweets about social interactions and fewer tweets related to infrastructure and assets were more likely to rebuild rather than relocate.",Not About Sufficiency
Live Demonstration: An AI driven multiplexed diagnostic platform for improving cancer care,"We present a live demonstration of Neutrocheck (R), an innovative low-cost diagnostic sensing platform designed for personalized and connected cancer care. With advances in artificial intelligence (AI) and multiplexed sensors, the medical field is rapidly moving towards personalized patient healthcare. Neutrocheck embodies this shift by offering a rapid, affordable and digitally linked point-of-care diagnostic solution. It addresses the urgent need for improved triaging of chemotherapy patients at risk of neutropenic sepsis, a potentially life-threatening complication caused by suppressed immune systems. Visitors to the live demonstration will have the opportunity to test the device and mobile phone app, while learning about how this smart sensor technology will be a gamechanger for improving cancer care.",Not About Sufficiency
European Cities in the Knowledge-Based Economy: Observations and Policy Challenges,"This essay discusses a set of related issues concerning the urban transition towards a knowledge-based economy. First, it deals with the uneven distribution of economic growth among various types of cities in the knowledge-based economy; second, it reflects on emerging economic and social divisions within cities. Does the knowledge economy offer opportunities to repair these social and economic divides or does it deepen them instead? Third, we deal with the alleged new role of institutions of higher education (HEIs). Cities increasingly consider them to be engines of growth in the urban knowledge economy, but is this borne out by reality? Our final topic is concerned with urban planning. Many European cities try to create ""knowledge locations"", aiming to promote innovation and creativity in those areas and/or increase the value of the real estate. We add some critical reflections.",Not About Sufficiency
"Health, energy security or people's jobs? Understanding cooking transition narratives and energy justice implications in Tanzania","The achievement of Sustainable Development Goal 7 'affordable and clean energy for all' is incomplete with 1.8 billion people worldwide still dependent on biomass for cooking, with detrimental effects on health, well-being and environment. The situation is especially acute in Sub-Saharan Africa. This study explores cooking transition narratives in the Tanzanian context. The recent policy initiative towards clean cooking from the Tanzanian government provides the opportunity to investigate the main actors and perspectives that set the scene for clean cooking, and justice implications. Drawing on interviews with relevant stakeholders and analysis of key strategic documents we find that the current narratives highlight technical, financial and environmental dimensions, but has little emphasis on the end-users. This provides limited understanding of the practices that underpin cooking, and people's ability to transition to clean cooking technologies. In the future, there is a need to consult the endusers to ensure a successful, just and sustainable transition.",Not About Sufficiency
Automated Triaging Medical Referral for Otorhinolaryngology Using Data Mining and Machine Learning Techniques,"Public hospitals receive and triage a large volume of medical referrals for otorhinolaryngology annually and it can be a challenge to derive knowledge from them as they are written in unstructured text and may be unavailable in electronic formats. Acquiring knowledge and insights from these referrals are important to public health management and policymakers. Triaging of general practitioner (GP) referrals for ear, nose, and throat (ENT) specialists is a manual process performed by experienced clinicians, but it is time-consuming. This paper proposes utilising machine learning and data mining to automate the process of referrals. In this study, an ensemble of machine learning algorithms to perform clinical text mining against the unstructured referral text in order to derive the relationship among the discovered medical terms was proposed and implemented. A set of comprehensive term sets' association rules which describe the entire referral dataset's characteristics was obtained from the association rule mining experiments. The neural network-based text classification model that can classify referrals with high accuracy was developed, tested and reported in this paper.",Not About Sufficiency
Agglomeration under forward-looking expectations: Potentials and global stability,"This paper considers a class of migration dynamics with forward-looking agents in a multi-country solvable variant of the core-periphery model of Krugman [Krugman, P., 1991. Increasing returns and economic geography, journal of Political Economy 99, 483-499]. We find that, under a symmetric externality assumption, our static model admits a potential function, which allows us to identify a stationary state that is uniquely absorbing and globally accessible under the perfect foresight dynamics whenever the degree of friction in relocation decisions is sufficiently small. In particular, when trade barriers are low enough, full agglomeration in the country with the highest barrier is the unique stable state for small frictions. New aspects in trade and tax policy that arise due to forward-looking behavior are discussed. (C) 2009 Elsevier B.V. All rights reserved.",Not About Sufficiency
"Effects of Urbanization on the Dynamics and Equity of Access to Urban Parks from 2000 to 2015 in Beijing, China","Urban parks provide multiple ecosystem services as an important element of the urban space and improve human health and wellbeing. This study used the Gaussian-based 2SFCA method to evaluate the spatiotemporal distribution of and changes in park accessibility within the Sixth Ring Road in Beijing over 15 years. The study also used bivariate correlation analysis to analyze the relationship between urbanization factors and park access. The results showed that the overall park accessibility in both quantity and proximity had increased from 2000 to 2015, but there were still certain areas (percentage) that had limited access to parks. The inequity of distribution in park accessibility had been detected accompanying the rapid increase in park quantity in 2015. Furthermore, the development of urban parks mismatched that of urbanization in terms of urban land increase. The correlation between accessibility changes and population urbanization is not significant. Proper urban green space planning based on the distribution of population density and urban land use is indispensable in avoiding the aggravation of inequity in the process of urban expansion. This study contributes to the assessment of the current park allocation efficiency and helps urban planners and policymakers make prompt adjustments in the rapidly urbanizing process.",Not About Sufficiency
A Novel Feature-Engineered-NGBoost Machine-Learning Framework for Fraud Detection in Electric Power Consumption Data,"This study presents a novel feature-engineered-natural gradient descent ensemble-boosting (NGBoost) machine-learning framework for detecting fraud in power consumption data. The proposed framework was sequentially executed in three stages: data pre-processing, feature engineering, and model evaluation. It utilized the random forest algorithm-based imputation technique initially to impute the missing data entries in the acquired smart meter dataset. In the second phase, the majority weighted minority oversampling technique (MWMOTE) algorithm was used to avoid an unequal distribution of data samples among different classes. The time-series feature-extraction library and whale optimization algorithm were utilized to extract and select the most relevant features from the kWh reading of consumers. Once the most relevant features were acquired, the model training and testing process was initiated by using the NGBoost algorithm to classify the consumers into two distinct categories (""Healthy"" and ""Theft""). Finally, each input feature's impact (positive or negative) in predicting the target variable was recognized with the tree SHAP additive-explanations algorithm. The proposed framework achieved an accuracy of 93%, recall of 91%, and precision of 95%, which was greater than all the competing models, and thus validated its efficacy and significance in the studied field of research.",Not About Sufficiency
Machine learning algorithm approach to complete blood count can be used as early predictor of COVID-19 outcome,"Although the SARS-CoV-2 infection has established risk groups, identifying biomarkers for disease outcomes is still crucial to stratify patient risk and enhance clinical management. Optimal efficacy of COVID-19 antiviral medications relies on early administration within the initial 5 d of symptoms, assisting high-risk patients in avoiding hospitalization and improving survival chances. The complete blood count (CBC) can be an efficient and affordable option to find biomarkers that predict the COVID-19 prognosis due to infection-induced alterations in various blood parameters. This study aimed to associate hematological parameters with different COVID-19 clinical forms and utilizes them as disease outcome predictors. We performed a CBC in blood samples from 297 individuals with COVID-19 from Belo Horizonte, Brazil. Statistical analysis, as well as ROC Curves and machine learning Decision Tree algorithms were used to identify correlations, and their accuracy, between blood parameters and disease severity. In the initial 4 d of infection, traditional hematological COVID-19 alterations, such as lymphopenia, were not yet apparent. However, the monocyte percentage and granulocyte-to-lymphocyte ratio (GLR) proved to be reliable predictors for hospitalization, even in cases where patients exhibited mild symptoms that later progressed to hospitalization. Thus, our findings demonstrate that COVID-19 patients with monocyte percentages lower than 7.7% and a GLR higher than 8.75 are assigned to the hospitalized group with a precision of 86%. This suggests that these variables can serve as important biomarkers in predicting disease outcomes and could be used to differentiate patients at hospital admission for managing therapeutic interventions, including early antiviral administration. Moreover, they are simple parameters that can be useful in minimally equipped health care units.",Not About Sufficiency
Review: Impact of food safety on global trade,"Food safety encompasses the supply and assurance of safe, high-quality food for consumers. It is a crucial aspect of food security, gaining greater global attention due to the increasing number of widespread foodborne incidents. International trade is expanding as countries increasingly rely on each other to secure a sufficient and diverse food supply. Beyond this, concerns about food safety have become more prevalent due to various factors. Therefore, this review aims to investigate the effects of food safety-associated risks on the international trade of food and related products. A total of 37 published studies retrieved using different search engines were included in this review. This review revealed that because of rapid population growth and rising food demand in developing nations, agricultural intensification is growing. It has been found that foodborne illnesses and associated discrepancies can impede the international trade of food commodities. Trade bans due to the fear of foodborne illnesses are growing. The consequences of foodborne diseases are multifaceted and include financial losses from trade restrictions, medical costs for prevention or control, resource depletion and a decline in food production. The overall effects are increased international trade tensions and livelihood vulnerability to poverty, notably for small-scale livestock producers. Potential food contaminants include microbes, pesticides, pharmaceutical residues, heavy metals and fraudulent such as improper food processing, mislabelling, poor packaging, adulteration and substitution. Hence, countries are encouraged to harmonize the rights and duties set by the World Trade Organization under sanitary and phytosanitarys to maximize their advantages in global markets. Based on this evidence, we recommend that each country develop and integrate regulations that would ensure the safety of both domestic and international food production systems. Furthermore, the global community should either revise the current functioning food regulatory and monitoring body or establish a more genuine collaborative network. Food safety has become a pressing global issue due to growing health concerns. Contaminants can infiltrate the food supply chain, from agricultural production to processing and final consumption. Factors, such as improper agricultural inputs, microbial contamination in production and processing and fraudulent practices, are hindering international food trade. In response, extensive food safety regulations have been implemented. Yet, these regulatory measures may paradoxically create barriers to the global food market. image",Not About Sufficiency
Lithofacies classification integrating conventional approaches and machine learning technique,"This study introduces an integrated approach for lithofacies classification utilizing core samples, wireline logs, and a machine learning technique. It is specialized for the Eagle Ford shale and the Austin Chalk where operators often face difficulties to define geological heterogeneity due to relatively consistent and low reservoir quality compared to conventional reservoirs. A set of cored slabs, thin sections, scanning electron microscope (SEM) images were utilized for lithofacies classification. Four lithofacies were defined with depositional texture, fabric, mineralogy, pore type, diagenesis, and biological features: (1) Organic-matter-rich mudstone, (2) Organic-matter-lean calcareous marl, (3) Heterogeneous argillaceous wackestone and marl, (4) Massive marly chalk. These four lithofacies are not easily classifiable through conventional wireline log cross-plotting. A convolutional neural network (CNN) model was trained to classify the lithofacies using five wireline logs (Gamma ray, bulk density, neutron porosity, deep resistivity, and compressional sonic logs) which are commonly accessible at field sites. In order to validate and test the CNN model, k-fold cross-validation and blind well test were conducted. This data-driven approach is expected to provide technical insights to operators seeking practical approaches to identifying prospective drilling locations and optimal completion strategies based on in-depth reservoir characterization.",Not About Sufficiency
"Developing, implementing and governing artificial intelligence in medicine: a step-by-step approach to prevent an artificial intelligence winter","Objective Although the role of artificial intelligence (AI) in medicine is increasingly studied, most patients do not benefit because the majority of AI models remain in the testing and prototyping environment. The development and implementation trajectory of clinical AI models are complex and a structured overview is missing. We therefore propose a step-by-step overview to enhance clinicians' understanding and to promote quality of medical AI research. Methods We summarised key elements (such as current guidelines, challenges, regulatory documents and good practices) that are needed to develop and safely implement AI in medicine. Conclusion This overview complements other frameworks in a way that it is accessible to stakeholders without prior AI knowledge and as such provides a step-by-step approach incorporating all the key elements and current guidelines that are essential for implementation, and can thereby help to move AI from bytes to bedside.",Not About Sufficiency
The cooling impact of urban greening: A systematic review of methodologies and data sources,"Urban greening is a practical adaptation to improve the liveability of a city by providing urban cooling. This paper systematically evaluates the existing literature to understand the methodologies and associated data sources for determining the cooling effect provided by urban greening. A bibliometric analysis, including research focus parallelship network analysis and keyword co-occurrence network analysis using Gephi and BibExcel, was first performed on 156 articles to identify the main resulting research themes in cooling provided by UGAs. Subsequently, 72 of these were selected to extract more information, such as the focus, the measured parameters, the methods, and tools employed to measure cooling parameters, and the studies` scales. The bibliometric analysis identified three emerging themes: (1) on-site measurement to directly collect meteorological data, (2) remote sensing techniques used to calculate cooling indices such as cooling distance and intensity, and (3) estimation and prediction of urban temperature through modelling. The results showed that the field measurement was used in studies that have smaller spatial scale (i.e. those that examined a small number of parks). Remote sensing techniques were used to investigate larger spatial areas in which cooling indices were estimated through measurement of land surface temperature rather than the air temperature. It is also revealed that some green space and meteorological parameters have received more attention. The findings of this research will enable green space designers and urban planners to become aware of the differences between various methods for calculating cooling and the importance of various parameters. However, future research should include a mix of data acquisition, covering all urban and greenery features to quantify urban cooling, and economic and social benefits of urban greening development. A novel measurement design is proposed through which a more accurate estimation of the cooling effect of green spaces can be obtained.",Not About Sufficiency
Integrating machine learning into business and management in the age of artificial intelligence,"Machine learning, with its capacity to leverage computational techniques for experiential learning, has profoundly influenced various disciplines, including business and management. Despite its contributions to the progress of these fields and the advent of artificial intelligence presenting new challenges, there remains ambiguity regarding the specific areas of significant advancement and those with potential for further development. This study addresses three central questions: (1) How is the intellectual landscape of machine learning in business and management research organized and structured? (2) What are the primary applications of machine learning in business administration? And (3) What strategic considerations should companies adopt to effectively leverage machine learning in their business applications? By means of co-occurrence analysis of over 9399 peer-reviewed documents retrieved from Scopus discussing machine learning in business and management, we identified fifteen clusters within the literature. This classification serves as a starting point for firms looking to integrate ML into their routines across fifteen distinct topics. Although some firms have appropriated ML, the upsurge of artificial intelligence presents new challenges, including the digital divide, infrastructure and acquisition dilemmas, security concerns especially with outsourced services, and cost-effectiveness in algorithm selection and practical applications.",Not About Sufficiency
Skin microbiota and allergic symptoms associate with exposure to environmental microbes,"A rural environment and farming lifestyle are known to provide protection against allergic diseases. This protective effect is expected to be mediated via exposure to environmental microbes that are needed to support a normal immune tolerance. However, the triangle of interactions between environmental microbes, host microbiota, and immune system remains poorly understood. Here, we have studied these interactions using a canine model (two breeds, n = 169), providing an intermediate approach between complex human studies and artificial mouse model studies. We show that the skin microbiota reflects both the living environment and the lifestyle of a dog. Remarkably, the prevalence of spontaneous allergies is also associated with residential environment and lifestyle, such that allergies are most common among urban dogs living in single-person families without other animal contacts, and least common among rural dogs having opposite lifestyle features. Thus, we show that living environment and lifestyle concurrently associate with skin microbiota and allergies, suggesting that these factorsmight be causally related. Moreover, microbes commonly found on human skin tend to dominate the urban canine skin microbiota, while environmental microbes are rich in the rural canine skin microbiota. This in turn suggests that skin microbiota is a feasible indicator of exposure to environmental microbes. As short-term exposure to environmental microbes via exercise is not associated with allergies, we conclude that prominent and sustained exposure to environmental microbiotas should be promoted by urban planning and lifestyle changes to support health of urban populations.",Not About Sufficiency
Towards a Comprehensive Solution for a Vision-Based Digitized Neurological Examination,"The ability to use digitally recorded and quantified neurological exam information is important to help healthcare systems deliver better care, in-person and via telehealth, as they compensate for a growing shortage of neurologists. Current neurological digital biomarker pipelines, however, are narrowed down to a specific neurological exam component or applied for assessing specific conditions. In this paper, we propose an accessible vision-based exam and documentation solution called Digitized Neurological Examination (DNE) to expand exam biomarker recording options and clinical applications using a smartphone/tablet. Through our DNE software, healthcare providers in clinical settings and people at home are enabled to video capture an examination while performing instructed neurological tests, including finger tapping, finger to finger, forearm roll, and stand-up and walk. Our modular design of the DNE software supports integrations of additional tests. The DNE extracts from the recorded examinations the 2D/3D human-body pose and quantifies kinematic and spatio-temporal features. The features are clinically relevant and allow clinicians to document and observe the quantified movements and the changes of these metrics over time. A web server and a user interface for recordings viewing and feature visualizations are available. DNE was evaluated on a collected dataset of 21 subjects containing normal and simulated-impaired movements. The overall accuracy of DNE is demonstrated by classifying the recorded movements using various machine learning models. Our tests show an accuracy beyond 90% for upper-limb tests and 80% for the stand-up and walk tests.",Not About Sufficiency
"Modeling impacts of traffic, air pollution, and weather conditions on cardiopulmonary disease mortality","Aims: Cardiopulmonary disease (CPD) is a leading cause of death worldwide. Increasing evidence shows that air pollution and exposure to weather conditions have important contributory roles. Understanding the interaction of these factors is difficult due to the complexity of the relationship between CPD, air pollution, and environmental factors.Methods: This paper uses regression models and machine learning approaches to explore these relationships, and investigate whether meteorological factors and air pollution have a synergistic effect on CPD. We use daily data from 2009-2018 from four cities representing the heterogenous climate conditions in Norway: the far north, the west coast, mid-Norway, and the south-east.Results: We demonstrate the importance of the interaction between weather and air pollution associated with higher CPD mortality, as is exposure to air pollution in the form of NOx and particulate matter. This impact is seasonal. Traffic is also positively related to CPD mortality, which may be caused indirectly through increased pollution. We demonstrate that machine learning outperforms regression models in terms of the accuracy of predicting CPD mortality.Conclusions: The inclusion of rich lagged structures and interactions between environmental factors are both important but can lead to overfitting of traditional models; since these cities are not large cities by international standards, it is surprising that environmental factors have such obvious impacts on CPD mortality. CPD mortality shows a clear negative trend, implying an improvement in the public health situation.",Not About Sufficiency
Non-agricultural Market Access Negotiations in the World Trade Organization,In this article an attempt is made to assess whether Africa is likely to benefit from the World Trade Organization s (WTO) non-agricultural market access (NAMA) negotiations We propose a comparison between the African trade ministers declaration usually called the Arusha Declaration and the Hong Kong WTO ministerial declaration on NAMA negotiations After assessing the modalities proposed towards the Hong Kong WTO Ministerial conference we come to a critical conclusion that Africa may not benefit much from these negotiations African countries could derive benefits from MFN tariff reduction provided that their supply-side constraints and market entry barriers are properly addressed Concrete proposals from an African post-Hong Kong perspective are therefore identified Furthermore this article highlights the limited supply capacity and low industrial diversification in Africa Improving industrial market access is also conditioned on increasing Africa s exports diversification,Not About Sufficiency
Equitable Resilience in Infrastructure Systems: Empirical Assessment of Disparities in Hardship Experiences of Vulnerable Populations during Service Disruptions,"The objective of this study was to examine social inequality in exposure and hardship experienced by various groups due to infrastructure service disruptions in disasters. After more than two decades, the existing literature related to infrastructure resilience mainly focuses on system performance and considers the impacts of service disruptions to be equal for the public. The public, however, is not a monolithic entity, and different subpopulations have distinct needs and expectations of infrastructure systems. Thus, the same duration of service loss will not be experienced equally by the affected residents. Social subpopulations in a community have preexisting differences, or sociodemographic characteristics, which account for differential variations in disaster experience, and often socially vulnerable groups are disproportionally affected. Unfortunately, there is limited empirical information regarding inequity in the societal impacts of infrastructure service disruptions during disasters. This study addresses this knowledge gap by developing an equitable infrastructure resilience approach that integrates both the physical characteristics of the infrastructure systems and the sociodemographic characteristics that contribute to risk disparity experienced by individual households. The risk disparity was assessed by considering both the duration of the service disruptions (exposure) and people's ability to withstand disruptions (zone of tolerance). The study investigated empirical data related to the transportation, power, communication, and water service disruptions caused by Hurricane Harvey in 2017 for Harris County residents. The results concluded that certain socially vulnerable groups reported significant disparity in the hardship people experienced due to infrastructure service disruptions caused by the disaster. The significant experienced hardship was rooted in the group's having a lower zone of tolerance for service disruptions, experiencing a significantly higher duration of service outages, or a coupling effect when there was both greater exposure and lower zone of tolerance. The findings further revealed the following: (1) households with low socioeconomic status reported a coupling effect for communication and water disruptions and reported a lower zone of tolerance for transportation and power disruptions; (2) racial minority groups reported a coupling effect for transportation, communication, and water disruptions and a lower zone of tolerance for power disruption; and (3) households with younger residents reported a coupling effect for communication disruption, greater exposure to transportation and water disruptions, and lower zone of tolerance for power disruption. The findings uncovered existing inequalities in exposure and hardship experienced due to infrastructure service disruptions for various vulnerable subpopulations. Hence, the study establishes the fundamental knowledge and empirical information needed for an equitable resilience approach in infrastructure systems in order to better prioritize investments and therefore effectively reduce the risk disparity of vulnerable populations during service disruptions. (C) 2020 American Society of Civil Engineers.",Not About Sufficiency
Record Linkage for Malaria Deaths Data Recovery and Surveillance in Brazil,"Objective: The objective is to describe the results and the methodological processes of record linkage for matching deaths and malaria cases. Methods: A descriptive cross-sectional study was conducted with probabilistic record linkage of death and malaria cases data in Brazil from 2011 to 2020 using death records from the Mortality Information System (SIM) and epidemiological data from the Notifiable Diseases Information System (Sinan) and Epidemiological Surveillance Information Systems for malaria (Sivep-Malaria). Three matching keys were used: patient's name, date of birth, and mother's name, with an analysis of cosine and Levenshtein dissimilarity measures. Results: A total of 490 malaria deaths were recorded in Brazil between 2011 and 2020. The record linkage resulted in the pairing of 216 deaths (44.0%). Pairings where all three matching keys were identical accounted for 30.1% of the total matched deaths, 39.4% of the matched deaths had two identical variables, and 30.5% had only one of the three key variables identical. The distribution of the variables of the matched deaths (216) was similar to the distribution of all recorded deaths (490). Out of the 216 matched deaths, 80 (37.0%) had poorly specified causes of death in the SIM. Conclusions: The record linkage allowed for the detailing of the data with additional information from other epidemiological systems. Record linkage enables data linkage between information systems that lack interoperability and is an extremely useful tool for refining health situation analyses and improving malaria death surveillance in Brazil.",Not About Sufficiency
Strengths and weaknesses of existing data sources to support research to address the opioids crisis,"Better opioid prescribing practices, promoting effective opioid use disorder treatment, improving naloxone access, and enhancing public health surveillance are strategies central to reducing opioid-related morbidity and mortality. Successfully advancing and evaluating these strategies requires leveraging and linking existing secondary data sources. We conducted a scoping study in Fall 2017 at RAND, including a literature search (updated in December 2018) complemented by semi-structured interviews with policymakers and researchers, to identify data sources and linking strategies commonly used in opioid studies, describe data source strengths and limitations, and highlight opportunities to use data to address high-priority public health research questions. We identified 306 articles, published between 2005 and 2018, that conducted secondary analyses of existing data to examine one or more public health strategies. Multiple secondary data sources, available at national, state, and local levels, support such research, with substantial breadth in data availability, data contents, and the data's ability to support multi-level analyses over time. Interviewees identified opportunities to expand existing capabilities through systematic enhancements, including greater support to states for creating and facilitating data use, as well as key data challenges, such as data availability lags and difficulties matching individual-level data over time or across datasets. Multiple secondary data sources exist that can be used to examine the impact of public health approaches to addressing the opioid crisis. Greater data access, improved usability for research purposes, and data element standardization can enhance their value, as can improved data availability timeliness and better data comparability across jurisdictions.",Not About Sufficiency
Effective feature representation using symbolic approach for classification and clustering of big data,"The tremendous growth in the technology has led to the accumulation of enormous Big Data. Techniques that efficiently analyse this Big Data are in great demand. Tweets from Social media and Sensor data are some of the most common forms of Big Data. Machine learning algorithms pave way for researchers to analyze Big Data. Most Machine learning algorithms depend on efficient feature extraction and feature selection for its success. Here, we explore feature selection methods like entropy and Rough set on the sensor data. Also a symbolic approach of feature extraction is proposed which represents both sensor and twitter data efficiently for further data analysis. Some popular classifiers like Na?ve Bayes, K Nearest Neighbour, Support Vector Machine and Decision Tree are used for validating the efficacy of the features selected. An ensemble classifier technique is also proposed which is compared with various state of the art ensemble classifiers. Symbolic features perform better than both entropy and Rough set features for sensor data and improves the clustering efficiency of twitter data. The proposed ensemble weighted average classifier on Symbolic features outperform all the other ensemble classifiers and independent classifiers. The results obtained from these methods have the potential to aid the public health surveillance.",Not About Sufficiency
A Distributed Sensor Network for Waste Water Management Plant Protection,"Waste water management process has a significant role in guarantee sea and surface water bodies water quality with direct impact on tourism based economy and public health. Protection of this critical infrastructure form illicit discharges is hence paramount for the whole society. Here, We propose a pervasive monitoring centered approach to the protection of wastewater management plant. An hybrid sensor network is actually deployed along the wastewater network including several different transducers. Incepted data are harmonized and processed with an integrated SWMM model and machine learning based approach in order to forecast water qualitative and quantitative aspects, detect and localize anomalies. An advanced WEBGIS-SOS based interface conveys relevant information to the management entity allowing it to take appropriate actions in a timely way, reducing and mitigating the impacts of illicit discharges.",Not About Sufficiency
Code-Switching and Back-Transliteration Using a Bilingual Model,"The challenges of automated transliteration and code-switching-detection in Judeo-Arabic texts are addressed. We introduce two novel machine-learning models, one focused on transliterating Judeo-Arabic into Arabic, and another aimed at identifying non-Arabic words, predominantly Hebrew and Aramaic. Unlike prior work, our models are based on a bilingual Arabic-Hebrew language model, providing a unique advantage in capturing shared linguistic nuances. Evaluation results show that our models outperform prior solutions for the same tasks. As a practical contribution, we present a comprehensive pipeline capable of taking Judeo-Arabic text, identifying non-Arabic words, and then transliterating the Arabic portions into Arabic script. This work not only advances the state of the art but also offers a valuable toolset for making Judeo-Arabic texts more accessible to a broader Arabic-speaking audience and more amenable to modern language tools.",Not About Sufficiency
A Novel Approach to Modeling and Forecasting Cancer Incidence and Mortality Rates through Web Queries and Automated Forecasting Algorithms: Evidence from Romania,"Simple Summary Cancer remains a global burden, currently causing nearly one in six deaths worldwide. Accurate projections of cancer incidence and mortality are needed for effective and efficient policymaking, accurate resource allocation, and to assess the impact of newly introduced policies and measures. However, the COVID-19 pandemic disrupted public health systems and caused a significant number of cancers to remain undiagnosed, thus affecting the quality of official statistics and their usefulness for health studies. This paper addresses this issue by proposing novel cancer incidence/cancer mortality models based on population web-search habits and historical links with official health variables. The models are empirically estimated using data from one of the most vulnerable European Union (EU) members, Romania, a country that consistently reports lower survival rates than the EU average, and are further used to forecast cancer incidence and mortality rates in the country. Research findings have important policy implications, and the novel framework, owing to its generalizability, can be applied to the same task in other countries. Overall, the results indicate a continuation of the increasing trends in cancer incidence and mortality in Romania and thus underline the urgency to change the status quo in the Romanian public-health system. Cancer remains a leading cause of worldwide mortality and is a growing, multifaceted global burden. As a result, cancer prevention and cancer mortality reduction are counted among the most pressing public health issues of the twenty-first century. In turn, accurate projections of cancer incidence and mortality rates are paramount for robust policymaking, aimed at creating efficient and inclusive public health systems and also for establishing a baseline to assess the impact of newly introduced public health measures. Within the European Union (EU), Romania consistently reports higher mortality from all types of cancer than the EU average, caused by an inefficient and underfinanced public health system and lower economic development that in turn have created the phenomenon of ""oncotourism"". This paper aims to develop novel cancer incidence/cancer mortality models based on historical links between incidence and mortality occurrence as reflected in official statistics and population web-search habits. Subsequently, it employs estimates of the web query index to produce forecasts of cancer incidence and mortality rates in Romania. Various statistical and machine-learning models-the autoregressive integrated moving average model (ARIMA), the Exponential Smoothing State Space Model with Box-Cox Transformation, ARMA Errors, Trend, and Seasonal Components (TBATS), and a feed-forward neural network nonlinear autoregression model, or NNAR-are estimated through automated algorithms to assess in-sample fit and out-of-sample forecasting accuracy for web-query volume data. Forecasts are produced with the overperforming model in the out-of-sample context (i.e., NNAR) and fed into the novel incidence/mortality models. Results indicate a continuation of the increasing trends in cancer incidence and mortality in Romania by 2026, with projected levels for the age-standardized total cancer incidence of 313.8 and the age-standardized mortality rate of 233.8 representing an increase of 2%, and, respectively, 3% relative to the 2019 levels. Research findings thus indicate that, under the no-change hypothesis, cancer will remain a significant burden in Romania and highlight the need and urgency to improve the status quo in the Romanian public health system.",Not About Sufficiency
Using Reports of Symptoms and Diagnoses on Social Media to Predict COVID-19 Case Counts in Mainland China: Observational Infoveillance Study,"Background: Coronavirus disease (COVID-19) has affected more than 200 countries and territories worldwide. This disease poses an extraordinary challenge for public health systems because screening and surveillance capacity is often severely limited, especially during the beginning of the outbreak; this can fuel the outbreak, as many patients can unknowingly infect other people. Objective: The aim of this study was to collect and analyze posts related to COVID-19 on Weibo, a popular Twitter-like social media site in China. To our knowledge, this infoveillance study employs the largest, most comprehensive, and most fine-grained social media data to date to predict COVID-19 case counts in mainland China. Methods: We built a Weibo user pool of 250 million people, approximately half the entire monthly active Weibo user population. Using a comprehensive list of 167 keywords, we retrieved and analyzed around 15 million COVID-19-related posts from our user pool from November 1, 2019 to March 31, 2020. We developed a machine learning classifier to identify ""sick posts,"" in which users report their own or other people's symptoms and diagnoses related to COVID-19. Using officially reported case counts as the outcome, we then estimated the Granger causality of sick posts and other COVID-19 posts on daily case counts. For a subset of geotagged posts (3.10% of all retrieved posts), we also ran separate predictive models for Hubei province, the epicenter of the initial outbreak, and the rest of mainland China. Results: We found that reports of symptoms and diagnosis of COVID-19 significantly predicted daily case counts up to 14 days ahead of official statistics, whereas other COVID-19 posts did not have similar predictive power. For the subset of geotagged posts, we found that the predictive pattern held true for both Hubei province and the rest of mainland China regardless of the unequal distribution of health care resources and the outbreak timeline. Conclusions: Public social media data can be usefully harnessed to predict infection cases and inform timely responses. Researchers and disease control agencies should pay close attention to the social media infosphere regarding COVID-19. In addition to monitoring overall search and posting activities, leveraging machine learning approaches and theoretical understanding of information sharing behaviors is a promising approach to identify true disease signals and improve the effectiveness of infoveillance.",Not About Sufficiency
Differentially Private Submodular Maximization over Integer Lattice,"Many machine learning problems, such as medical data summarization and social welfare maximization, can be modeled as the problems of maximizing monotone submodular functions. Differentially private submodular functions under cardinality constraints are first proposed and studied to solve the Combinatorial Public Projects (CPP) problem, in order to protect personal data privacy while processing sensitive data. However, the research of these functions for privacy protection has received little attention so far. In this paper, we propose to study the differentially private submodular maximization problem over the integer lattice. Our main contributions are to present differentially private approximation algorithms for both DR-submodular and integer submodular function maximization problems under cardinality constraints and analyze the sensitivity of our algorithms.",Not About Sufficiency
Experiments in socially guided exploration: lessons learned in building robots that learn with and without human teachers,"We present a learning system, socially guided exploration, in which a social robot learns new tasks through a combination of self-exploration and social interaction. The system's motivational drives, along with social scaffolding from a human partner, bias behaviour to create learning opportunities for a hierarchical reinforcement learning mechanism. The robot is able to learn on its own, but can flexibly take advantage of the guidance of a human teacher. We report the results of an experiment that analyses what the robot learns on its own as compared to being taught by human subjects. We also analyse the video of these interactions to understand human teaching behaviour and the social dynamics of the human-teacher/robot-learner system. With respect to learning performance, human guidance results in a task set that is significantly more focused and efficient at the tasks the human was trying to teach, whereas self-exploration results in a more diverse set. Analysis of human teaching behaviour reveals insights of social coupling between the human teacher and robot learner, different teaching styles, strong consistency in the kinds and frequency of scaffolding acts across teachers and nuances in the communicative intent behind positive and negative feedback.",Not About Sufficiency
An ensemble feature selection algorithm based on PageRank centrality and fuzzy logic,"One of the crucial processes in machine learning algorithms to improve the performance as well as, in some cases, to reduce the computational cost is feature selection. In other words, when there are many features, some are irrelevant; thus, irrelevant features will badly affect the learning process, so selecting the most fitting features can lead to increasing the accuracy of models in high-dimensional datasets. Recent researches benefit from an integration of various feature selection methods to find relevant features more accurately. Despite the fact that applying several feature selection methods can be efficient, the accuracy of current methods and combining the ranks of different feature selection methods are still challenging problems. In this paper, a novel ensemble of feature selection based on weighted PageRank and fuzzy logic, named EFSPF, is proposed to overcome these drawbacks. First, an MxM matrix, called Euclidean Distance Matrix (EDM), is set up, where M is the number of features. Secondly, a fuzzy Type-I is applied individually to tackle uncertainties in ranks. In other words, to specify a reliability rate for each feature, fuzzy Type-I is utilized. Then, EFSPF forms a reliable weighted graph where each feature is considered as a node. Furthermore, the weight between two nodes expresses a combination of their Euclidean Distance in EDM and the calculated reliability rates through fuzzy Type-I. Finally, the rank of each graph node is calculated via the PageRank algorithm. To demonstrate the performance of EFSPF, we have compared it with five ensemble algorithms and eight primary feature selection algorithms. The proposed method is evaluated based on Accuracy, Precision, Recall, and F1 score metrics. The analysis results show that the proposed method outperforms its rivals. The source code of the proposed method is accessible from https://github.com/mehdijoodaki/EFSPF. (c) 2021 Elsevier B.V. All rights reserved.",Not About Sufficiency
"Modal Shift from Cars and Promotion of Walking by Providing Pedometers in Yokohama City, Japan","Mobility management is a transportation policy aiming to change travel behavior from car use to sustainable transportation modes while increasing people's physical activity. Providing pedometers and visualizing step counts, popular interventions in public health practice, may constitute a mobility management program. However, the ease of modal shifts and changeability of walking habits differ across neighborhood environments. Using questionnaire data from 2023 middle-aged and older participants from Yokohama, Japan, in May 2017, this study examined (1) the relationship between the physical and social environments of Yokohama Walking Point Program participants who volunteered to use free pedometers and their modal shifts from cars to walking and public transport, and (2) whether participants' modal shifts were associated with increases in step counts. Multivariate categorical regression analyses identified the frequency of greetings and conversations with neighbors as well as health motivation as important explanatory variables in both analyses. Participants living in neighborhoods far from railway stations and in neighborhoods with a high bus stop density tended to shift to walking and public transport, a modal shift that was highly associated with increased step counts. These results suggest that mobility management should be promoted in collaboration with public health and city planning professionals.",Not About Sufficiency
SPLAL: Similarity-based pseudo-labeling with alignment loss for semi-supervised medical image classification,"Medical image classification presents significant challenges due to limited labeled samples and class imbalance resulting from varying disease prevalence. In many real-world scenarios, a scarcity of labeled images pertaining to specific ailments coexists with an abundance of unlabeled ones. The process of annotating these unlabeled medical images by domain experts is both time-intensive and financially burdensome. Consequently, medical image classification remains an active and evolving research area. Semi-supervised learning (SSL) techniques hold promise in addressing these challenges by effectively leveraging both labeled and unlabeled data. However, the application of SSL to medical image classification necessitates overcoming two primary obstacles: (1) the reliable estimation of pseudo-labels for unlabeled images and (2) the mitigation of biases stemming from class imbalance. In this work, we introduce a novel semi-supervised learning approach, denoted as SPLAL, which adeptly addresses these challenges. SPLAL leverages the concept of class prototypes and employs a weighted combination of classifiers to predict dependable pseudo-labels for a subset of unlabeled medical images. Additionally, we introduce an alignment loss term designed to alleviate model biases, particularly those arising from the overrepresentation of majority classes. This alignment loss mechanism ensures the model's consistency in delivering the same classification output for diverse augmented images of the same disease. To evaluate the effectiveness of our proposed approach, we conducted comprehensive experiments on two publicly accessible benchmark datasets for medical image classification. Our empirical findings unequivocally demonstrate that SPLAL outperforms several state-of-the-art semi-supervised learning methods across various evaluation metrics.",Not About Sufficiency
Automated detection of class diagram smells using self-supervised learning,"Design smells are symptoms of poorly designed solutions that may result in several maintenance issues. While various approaches, including traditional machine learning methods, have been proposed and shown to be effective in detecting design smells, they require extensive manually labeled data, which is expensive and challenging to scale. To leverage the vast amount of data that is now accessible, unsupervised semantic feature learning, or learning without requiring manual annotation labor, is essential. The goal of this paper is to propose a design smell detection method that is based on self-supervised learning. We propose Model Representation with Transformers (MoRT) to learn the UML class diagram features by training Transformers to recognize masked keywords. We empirically show how effective the defined proxy task is at learning semantic and structural properties. We thoroughly assess MoRT using four model smells: the Blob, Functional Decomposition, Spaghetti Code, and Swiss Army Knife. Furthermore, we compare our findings with supervised learning and feature-based methods. Finally, we ran a cross-project experiment to assess the generalizability of our approach. Results show that MoRT is highly effective in detecting design smells.",Not About Sufficiency
Using HIT to deliver integrated care for the frail elderly in the UK: current barriers and future challenges,"In this paper we briefly describe the results of a 3 year project examining the use of Health Information Technologies (e. g., electronic patient record systems) to deliver integrated care. In particular, we focus on one group of patient (the frail elderly) and efforts to design an e-health supported healthcare pathway (the frail elderly pathway - FEP). The aim of FEP is to bring together clinicians and staff from health and social care and allow them to share patient information. Our findings show that progress in delivering a fully-supported and working FEP has been slow, not least because of the difficulties experienced by healthcare staff in using current IT systems. In addition, there are many strategic and technical issues which remain unresolved (e. g., systems interoperability).",Not About Sufficiency
"Contribution of T Cell Receptor Alpha and Beta CDR3, MHC Typing, V and J Genes to Peptide Binding Prediction","Introduction Predicting the binding specificity of T Cell Receptors (TCR) to MHC-peptide complexes (pMHCs) is essential for the development of repertoire-based biomarkers. This affinity may be affected by different components of the TCR, the peptide, and the MHC allele. Historically, the main element used in TCR-peptide binding prediction was the Complementarity Determining Region 3 (CDR3) of the beta chain. However, recently the contribution of other components, such as the alpha chain and the other V gene CDRs has been suggested. We use a highly accurate novel deep learning-based TCR-peptide binding predictor to assess the contribution of each component to the binding. Methods We have previously developed ERGO-I (pEptide tcR matchinG predictiOn), a sequence-based T-cell receptor (TCR)-peptide binding predictor that employs natural language processing (NLP) -based methods. We improved it to create ERGO-II by adding the CDR3 alpha segment, the MHC typing, V and J genes, and T cell type (CD4+ or CD8+) as to the predictor. We then estimate the contribution of each component to the prediction. Results and Discussion ERGO-II provides for the first time high accuracy prediction of TCR-peptide for previously unseen peptides. For most tested peptides and all measures of binding prediction accuracy, the main contribution was from the beta chain CDR3 sequence, followed by the beta chain V and J and the alpha chain, in that order. The MHC allele was the least contributing component. ERGO-II is accessible as a webserver at http://tcr2.cs.biu.ac.il/ and as a standalone code at https://github.com/IdoSpringer/ERGO-II.",Not About Sufficiency
Mapping the capacity of concave green land in mitigating urban pluvial floods and its beneficiaries,"Pluvial floods can exert significant pressure on urban sustainability and human wellbeing during rapid urbanization. Rapid urbanization and exacerbated flooding have stimulated investigations of green land's role in mitigating urban flooding in China. The newly applied ""Sponge City"" plan considers the concave green land (CGL) as an effective tool in mitigating pluvial floods. However, its capacity and beneficiaries still need to be elucidated. This paper fills this research gap by integrating urban flood simulation, scenario analysis, and mitigation assessment in central Shanghai, China. It reveals that CGL could not only mitigate direct runoff and inundation, but also reduce population exposure and enhance community resilience. CGL with a depth of 0.10-0.20 m can mitigate direct runoffs by 23.63-98.35% and inundation extents by 26.09-82.41%, resulting in a slide of the exposed urban population by 0.40-1.04 million persons and the exposed elders by 0.04-0.12 million persons. Moreover, the effectiveness of CGL varies spatially implying for a sound spatial planning of CGLs and a potential combination with other flood mitigation measures. These findings could help cities particularly in developing countries to adapt to rising pluvial flooding through urban planning and nature-based flood mitigation measures.",Not About Sufficiency
Measuring the Sustainable Development Goal (SDG) Transport Target and Accessibility of Nairobi's Matatus,"The urban Sustainable Development Goal (SDG) includes the target to provide ""access to safe, affordable, accessible and sustainable transport systems for all"" by 2030. However, debate exists around the best indicator to measure this target, and few actual measurements exist. This is in part because basic transit data are missing from many of the world's cities, including in Africa where popular or ""informal"" systems dominate. This paper explores how to make progress in measuring indicators for the SDG transport target using Nairobi's minibus system, matatus, as a case study. We partially measure the SDG indicator for the city as currently defined by the UN and then compare the SDG measurement to a location-based accessibility indicator that incorporates income data, travel times, and land-use considerations for Nairobi's highly monocentric spatial urban form. We show that although the SDG analysis suggests generally favorable transit coverage, it also points to underlying transport inequalities for low-income residents. The more fine-grained location-based accessibility analysis reveals rapidly decreasing accessibility to opportunities as distance increases from the city's central business district. This accessibility-based analysis further highlights income-based transport inequalities, identifying opportunities for improving integrated transport for residents living on the city's near and far peripheries. Improving non-motorized transport access for those living in low-income areas with high access potential would also be important to improve access. We recommend that cities start using open-source software and open data to measure a variety of indicators needed for data-driven policy, to meet SDG 11.2 and go further to improve access to opportunities for all residents.",Not About Sufficiency
The PlaceMaker: a flexible and interactive tool to support the sustainable city construction and transformation,"The new structure of the contemporary city is unpredictable and complex, especially given the continuous mix of cultures which brings new elements to the already multiple identity of places, breaking, mixing and recomposing the complexity of urban life. In order to explain such new sites and give new terms, several researchers have tested new methodologies, maps, multimedia images, hypertext, software, able to render this complexity and to permit readability. One matter remains open-ended: the dialogue of these tools with administrators and planners, but also with non-specialists of those sectors, common users of the place, inhabitants, because of the difficulties of finding a single shared model of representation. The method of analysis of the Sensitive Relief identifies the elements of the urban landscape which have value for the identification of the places and are able to influence the cultural and sustainable city construction; those new elements and the complexity of those places are represented in a complex map. Starting from those premises, the aim of this work is to investigate the possibility to create a PlaceMaker, suitable software to connect and communicate the complex information contained in the complex map and give value and significance to those data. The Sensitive Relief assembles, elaborates and reconstructs the data deriving from surveys based on physical reconnaissance, sensory perceptions, graphical elaboration, photographic and video records, and sets this data against that provided by an overview of expectations, an analysis based on traditional cartography and a questionnaire given to local inhabitants. Furthermore, to make the PlaceMaker more accessible to specialist and non-specialists in the field, the aim of this work is to comprehend how to visually represent the data to make them become flexible and interactive work tools and to support the sustainable city construction and transformation.",Not About Sufficiency
Assessing Communicative Effectiveness of Public Health Information in Chinese: Developing Automatic Decision Aids for International Health Professionals,"Effective multilingual communication of authoritative health information plays an important role in helping to reduce health disparities and inequalities in developed and developing countries. Health information communication from the World Health Organization is governed by key principles including health information relevance, credibility, understandability, actionability, accessibility. Multilingual health information developed under these principles provide valuable benchmarks to assess the quality of health resources developed by local health authorities. In this paper, we developed machine learning classifiers for health professionals with or without Chinese proficiency to assess public-oriented health information in Chinese based on the definition of effective health communication by the WHO. We compared our optimized classifier (SVM_F5) with the state-of-art Chinese readability classifier (Chinese Readability Index Explorer CRIE 3.0), and classifiers adapted from established English readability formula, Gunning Fog Index, Automated Readability Index. Our optimized classifier achieved statistically significant higher area under the receiver operator curve (AUC of ROC), accuracy, sensitivity, and specificity than those of SVM using CRIE 3.0 features and SVM using linguistic features of Gunning Fog Index and Automated Readability Index (ARI). The statistically improved performance of our optimized classifier compared to that of SVM classifiers adapted from popular readability formula suggests that evaluation of health communication effectiveness as defined by the principles of the WHO is more complex than information readability assessment. Our SVM classifier validated on health information covering diverse topics (environmental health, infectious diseases, pregnancy, maternity care, non-communicable diseases, tobacco control) can aid effectively in the automatic assessment of original, translated Chinese public health information of whether they satisfy or not the current international standard of effective health communication as set by the WHO.",Not About Sufficiency
Automatic extraction of tents during Hajj from airborne images to support land use optimization,"With the huge number of pilgrims performing Hajj (Islamic Pilgrimage), the use of urban space is a major concern for engineers, urban designers, and urban planners. Pilgrims must stay in the Holy Area (Arafat) for one day as part of Hajj rituals. For this reason, pilgrims are housed in lightweight temporary structures: tents. In Arafat, these tents are constructed before each Hajj season. The arrangement of these tents differs from one year to another and from location to location. For the spatial and temporal constraints of ritual happening in Arafat, space optimization is an important issue. The extensive demand for a rapid, automatic, and high quality algorithm for feature extraction has been the subject of much recent research. In this paper, we present an approach for detecting and extracting tents using airborne images. The approach is used to calculate the areas covered by tents. It utilizes the intensity in digital images in two stages. First, it classifies tents from other features in Arafat's environment. Second, it calculates the number of tents based on image matching subroutines. This can evaluate the design and planning of tents' layout and space optimization. Using this automatic approach, the number of pilgrims in a tested area can also be estimated according to the average capacity of one-meter squares covered by tents. Moreover, services, utilities, and transportation needs can be determined more precisely. An actual sample area in Arafat during the Hajj season is used to test the approach developed in this research. (c) 2006 Elsevier B.V. All rights reserved.",Not About Sufficiency
Enhancing water quality prediction for fluctuating missing data scenarios: A dynamic Bayesian network-based processing system to monitor cyanobacteria proliferation,"Tackling the impact of missing data in water management is crucial to ensure the reliability of scientific research that informs decision-making processes in public health. The goal of this study is to ascertain the root causes associated with cyanobacteria proliferation under major missing data scenarios. For this purpose, a dynamic missing data management methodology is proposed using Bayesian Machine Learning for accurate surface water quality prediction of a river from Limia basin (Spain). The methodology used entails a sequence of analytical steps, starting with data pre-processing, followed by the selection of a reliable dynamic Bayesian missing value prediction system, leading finally to a supervised analysis of the behavioral patterns exhibited by cyanobacteria. For that, a total of 2,118,844 data points were used, with 205,316 (9.69 %) missing values identified. The machine learning testing showed the iterative structural expectation maximization (SEM) as the best performing algorithm, above the dynamic imputation (DI) and entropy-based dynamic imputation methods (EBDI), enhancing in some cases the accuracy of imputations by approximately 50 % in R2, RMSE, NRMSE, and logarithmic loss values. These findings can impact how data on water quality is being processed and studied, thus, opening the door for more reliable water management strategies that better inform public health decisions.",Not About Sufficiency
Quantitation of L-cystine in Food Supplements and Additives Using <SUP>1</SUP>H qNMR: Method Development and Application,"Cystine-enriched food supplements are increasingly popular due to their beneficial health effects. However, the lack of industry standards and market regulations resulted in quality issues with cystine food products, including cases of food adulteration and fraud. This study established a reliable and practical method for determining cystine in food supplements and additives using quantitative NMR (qNMR). With the optimized testing solvent, acquisition time, and relaxation delay, the method exhibited higher sensitivity, precision, and reproducibility than the conventional titrimetric method. Additionally, it was more straightforward and more economical than HPLC and LC-MS. Furthermore, the current qNMR method was applied to investigate different food supplements and additives regarding cystine quantity. As a result, four of eight food supplement samples were found to be inaccurately labeled or even with fake labeling, with the relative actual amount of cystine ranging from 0.3% to 107.2%. In comparison, all three food additive samples exhibited satisfactory quality (the relative actual amount of cystine: 97.0-99.9%). Notably, there was no obvious correlation between the quantifiable properties (price and labeled cystine amount) of the tested food supplement samples and their relative actual amount of cystine. The newly developed qNMR-based approach and the subsequent findings might help standardization and regulation of the cystine supplement market.",Not About Sufficiency
Studying Public Medical Images from the Open Access Literature and Social Networks for Model Training and Knowledge Extraction,"Medical imaging research has long suffered problems getting access to large collections of images due to privacy constraints and to high costs that annotating images by physicians causes. With public scientific challenges and funding agencies fostering data sharing, repositories, particularly on cancer research in the US, are becoming available. Still, data and annotations are most often available on narrow domains and specific tasks. The medical literature (particularly articles contained in MedLine) has been used for research for many years as it contains a large amount of medical knowledge. Most analyses have focused on text, for example creating semi-automated systematic reviews, aggregating content on specific genes and their functions, or allowing for information retrieval to access specific content. The amount of research on images from the medical literature has been more limited, as MedLine abstracts are available publicly but no images are included. With PubMed Central, all the biomedical open access literature has become accessible for analysis, with images and text in structured format. This makes the use of such data easier than extracting it from PDF. This article reviews existing work on analyzing images from the biomedical literature and develops ideas on how such images can become useful and usable for a variety of tasks, including finding visual evidence for rare or unusual cases. These resources offer possibilities to train machine learning tools, increasing the diversity of available data and thus possibly the robustness of the classifiers. Examples with histopathology data available on Twitter already show promising possibilities. This article adds links to other sources that are accessible, for example via the ImageCLEF challenges.",Not About Sufficiency
PharmActa: Personalized pharmaceutical care eHealth platform for patients and pharmacists,"Community pharmacists are critically placed in the patient care chain being an extended frontline within primary healthcare networks across Europe. They are trained to ensure safe and effective medication use, a crucial and responsible role, extending beyond the common misconception limited to just providing timely access to medicines for the population. Technology-wise, eHealth being committed to an effective, networked, patient-centered and accessible healthcare would prove a real asset in this direction by achieving improved therapy adherence with better outcomes and direct contribution to a cost-effective healthcare system. In this work, we present PharmActa, a personalized eHealth platform that addresses key features of pharmaceutical care and enhances communication of pharmacists with patients for optimizing pharmacotherapy. PharmActa empowers patients by providing pharmaceutical care services, such as drug interactions tools, reminders for assisting adhesion and compliance, information regarding adverse drug reactions, as well as pharmacovigilance along with related tools for healthcare management. In addition, it allows the pharmacists to review the medication history in order to provide personalized pharmaceutical care services; thus enhancing their role as healthcare providers. Finally, a mechanism allowing such a system to be interconnected with a developed medical repository following European and International interoperability standards, is also presented. Thus far, the evaluation results presented in this work indicate that PharmActa can be of great benefit to healthcare professionals, especially pharmacists and patients.",Not About Sufficiency
"K-means Clustering Relational Analysis of The Relations Among Vehicle Models, Sizes and Regions","With the enhancement of living standards, Car has become the most out of essential transport tools for us all. But in big cities, one to the rush, the road will be crowded; you can see all kinds of different brands, models, colors and sizes of vehicles. This is not just the status quo in China; most of the countries will have this happen. Then, in different regions, the model of the vehicle, the size will vary it? In this regard, we use K-means clustering algorithm to analysis relations among attributes of vehicles.",Not About Sufficiency
Death in Prison: increasing transparency on next of kin notification and disposition of remains,"BackgroundPolicies for next-of-kin (NOK) notification and disposition of remains surrounding death are unclear across the United States' (US) carceral systems. The goal of this study was to collect data on carceral system policies pertaining to NOK notification and disposition of remains for individuals who are incarcerated. We collected publicly available operational policies for the Federal Bureau of Prisons, Immigration and Customs Enforcement, 50 state prison systems, and the Washington D.C. jail for a total of 53 systems.ResultsApproximately 70% of systems had available policies on NOK notification and disposition of remains. Few systems had information on time constraints for NOK notification, notifying parties or designated contacts person, and ultimate disposition of unclaimed remains. Several systems had no accessible policies.ConclusionsAcross the US, carceral systems vary in policies for notifying NOK after the death of an incarcerated individual and their processes for the disposition of remains. Carceral and health systems should work towards standardization of policies on communication and disposition of remains after death of an individual who is incarcerated to work towards equity.",Not About Sufficiency
Enhancing global health security in Thailand: Strengths and challenges of initiating a One Health approach to avian influenza surveillance,"Infectious disease surveillance systems support early warning, promote preparedness, and inform public health response. Pathogens that have human, animal, and environmental reservoirs should be monitored through systems that incorporate a One Health approach. In 2016, Thailand's federal government piloted an avian influenza (AI) surveillance system that integrates stakeholders from human, animal, and environmental sectors, at the central level and in four provinces to monitor influenza A viruses within human, waterfowl, and poultry populations. This research aims to describe and evaluate Thailand's piloted AI surveillance system to inform strategies for strengthening and building surveillance systems relevant to One Health. We assessed this surveillance system using the United States Centers for Disease Control and Prevention's (U.S. CDC) ""Guidelines for Evaluating Public Health Surveillance Systems"" and added three novel metrics: transparency, interoperability, and security. In-depth key informant interviews were conducted with representatives among six Thai federal agencies and departments, the One Health coordinating unit, a corporate poultry producer, and the Thai Ministry of Public Health-U.S. CDC Collaborating Unit. Thailand's AI surveillance system demonstrated strengths in acceptability, simplicity, representativeness, and flexibility, and exhibited challenges in data quality, stability, security, interoperability, and transparency. System efforts may be strengthened through increasing laboratory integration, improving pathogen detection capabilities, implementing interoperable systems, and incorporating sustainable capacity building mechanisms. This innovative piloted surveillance system provides a strategic framework that can be used to develop, integrate, and bolster One Health surveillance approaches to combat emerging global pathogen threats and enhance global health security.",Not About Sufficiency
Aligning the Western Balkans power sectors with the European Green Deal,"Located in Southern Europe, the Drina River Basin is shared between Bosnia and Herzegovina, Montenegro, and Serbia. The power sectors of the three countries have an exceptionally high dependence on coal for power generation. In this paper, we analyse different development pathways for achieving climate neutrality in these countries and explore the potential of variable renewable energy ( VRE ) and its role in power sector decarbonization. We investigate whether hydro and non- hydro renewables can enable a net-zero transition by 2050 and how VRE might affect the hydropower cascade shared by the three countries. The Open-Source Energy Modelling System ( OSeMOSYS ) was used to develop a model representation of the countries' power sectors. Findings show that the renewable potential of the countries is a significant 94.4 GW. This potential is 68% higher than previous assessments have shown. Under an Emission Limit scenario assuming net zero by 2050, 17% of this VRE potential is utilized to support the decarbonization of the power sectors. Additional fi ndings show a limited impact of VRE technologies on total power generation output from the hydropower cascade. However, increased solar deployment shifts the operation of the cascade to increased short-term balancing, moving from baseload to more responsive power generation patterns. Prolonged use of thermal power plants is observed under scenarios assuming high wholesale electricity prices, leading to increased emissions. Results from scenarios with low cost of electricity trade suggest power sector developments that lead to decreased energy security.",Not About Sufficiency
"Impact of summer heat on urban park visitation, perceived health and ecosystem service appreciation","Urbanization, environmental change and ageing are putting urban health at risk. In many cities, heat stress is projected to increase. Urban green spaces are considered as an important resource to strengthen the resilience of city dwellers. We conducted a questionnaire survey in two structurally distinct parks in Leipzig, Germany, on hot summer days in 2019. We assessed the respondents? activity patterns, satisfaction with the existing infrastructure, heat-related health impairment, changes in park use during heat waves and evaluation of the role of parks in coping with heat stress. We found that the old-grown, tree-rich park was used significantly more frequently for experiencing nature, while the newer, less tree-rich park developed on a former railway-brownfield site was used more often for socializing and having BBQs and picnics. Satisfaction with available drinking fountains and public toilets was generally low and satisfaction with lighting was assessed less satisfactory in the old-grown park. Safety was assessed as satisfactory in general but significantly less satisfactory by female respondents. The heat stress summary score indicating heat-related health impairment was significantly higher for participants in the newer park. A high share of respondents stated that they used parks during heat waves as frequently as usual in the summer (46 %), while some respondents stated that they adapted their park use behaviour (18 %), e.g., by coming later in the evening. Regarding the participants? responses about the role of parks under summer heat conditions, we matched 138 statements to several regulating and cultural ecosystem services, and we found cooling and recreation to be mentioned most often. We concluded that green space planning should diminish usage barriers, such as insufficient lighting and insufficient sanitary infrastructure, to ensure equal park use opportunities for all city dwellers. Specific local environmental and sociocultural conditions, changing environments and climate adaptation must be considered. To maintain ecological processes and functions and to cope with climate change, urban planning should preserve older parks with a large amount of tree coverage while respecting demands for particular built infrastructure.",Not About Sufficiency
Trade-Related Aspects of Intellectual Property Rights Flexibilities and Public Health: Implementation of Compulsory Licensing Provisions into National Patent Legislation,"Context: The protection of intellectual property (IP) rights, given international legal effect through the World Trade Organization (WTO) Trade-Related Aspects of IP Rights (TRIPS) Agreement, has long been a contentious issue. In recent years, the long-standing debate on IP rights as a barrier to the access of affordable medicines has been heightened by the global vaccine inequity evidenced during the COVID-19 pandemic. The TRIPS Agreement contains a number of flexibilities that WTO members can exploit in order to accommodate their policy needs. Among these is the mechanism of compulsory licensing, whereby patent licenses may be granted without consent of the patent holder in certain circumstances. TRIPS Article 31bis created a special mechanism for compulsory licenses specifically for the export of pharmaceutical products to countries with insufficient manufacturing capacity.Methods: We analyzed domestic patent legislation for 195 countries (193 UN members and two observers) and three customs territories. We analyzed patent legislation for provisions on compulsory licenses, including those defined in Article 31bis of the TRIPS Agreement.Findings: We identified 11 countries with no patent legislation. Of the 187 countries with domestic or regional patent laws, 176 (94.1%) had provisions on compulsory licensing and 72 (38.5%) had provisions implementing TRIPS Article 31bis.Conclusions: The results of this study have highlighted the gap in the implementation of TRIPS flexibilities in countries' national patent legislation, especially in least-developed countries. Although it will not fully solve patent barriers to the access of medicines, implementation of compulsory licensing (and specifically those for the import and export of pharmaceutical products) will provide governments with another tool to safeguard their population's public health. Further discussions are needed to determine whether the WTO can provide effective responses to future pandemics or global crises.",Not About Sufficiency
A critical review of model construction and performance for nowcast systems for faecal contamination in recreational beaches,"Faecal contamination is a widespread environmental and public health problem on recreational beaches around the world. The implementation of predictive models has been recommended by the World Health Organization as a complement to traditional monitoring to assist decision-makers and reduce health risks. Despite several advances that have been made in the modeling of faecal coliforms, tools and algorithms from machine learning are still scarcely used in the field and their implementation in nowcast systems is delayed. Here, we perform a literature review on modeling strategies to predict faecal contamination in recreational beaches in the last two decades and the implementation of models in nowcast systems to aid management. Models constructed for surface waters of continental (lakes, rivers and streams), estuarine and marine coastal ecosystems were analyzed and compared based on performance metrics for continuous ( i.e. regression; R-2, Root Mean Square Error: RMSE) and categorical ( i.e. classification; accuracy, sensitivity, specificity) responses. We found 67 articles matching the search criteria and 40 with information allowing to evaluate and compare predictive ability. In early 2000, Multiple Linear Regressions were common, followed by a peak of Artificial Neural Networks (ANNs) from 2010 to 2015, and the rise of Machine learning techniques, such as decision trees (CART and Random Forest) since 2015. ANNs and decision trees presented better accuracy than the remaining models. Rainfall and its lags were important predictor variables followed by water temperature. Specificity was much higher than sensitivity in all modeling strategies, which is typical in data sets where one category ( e.g. closed beach) is far less common than the normal state ( i.e. unbalanced data sets). We registered the implementation of statistical models in early warning systems in 6 countries, mainly by public beach quality management institutions, followed by NGOs in conjunction with universities. We identified critical steps towards improving model construction, evaluation and usage: i) the need to balance the data set previous to model training, ii) the need to separate data set in training, validation and test to perform an honest evaluation of model performance and iii) the transduction of model outputs to plain language to relevant stakeholders. Integrating into a single framework in situ monitoring, model construction and nowcasting systems could help to improve decision making systems to protect users from bathing in contaminated waters. Still the reduction of arrival of faecal coliforms to aquatic ecosystems ( e.g. by improving sewage treatment systems) will be the ultimate factor in reducing health risk.",Not About Sufficiency
Tutorial: Rapidly Identifying Disease-associated Rare Variants using Annotation and Machine Learning at Whole-genome Scale Online,"Accurately identifying disease-associated alleles from large sequencing experiments remains challenging. During this tutorial, participants will learn how to use a new variant annotation and filtering web app called Bystro (https://bystro.io/) to analyze sequencing experiments. Bystro is the first online, cloud-based application that makes variant annotation and filtering accessible to all researchers for even the largest, terabyte-sized whole-genome experiments containing thousands of samples. Using its general-purpose, natural-language filtering engine, attendees will be shown how to perform quality control measures and identify alleles of interest. They will then be guided in exporting those variants, and using them in both a regression context by performing rare-variant association tests in R, as well as classification context by training new machine learning models in Python's scikit-learn library.",Not About Sufficiency
Prednisolone versus antihistamine for allergic rhinitis: No significant difference found in randomized trial,"BackgroundSeasonal allergic rhinitis (AR) impacts public health by affecting work productivity and quality of life. The Swedish tree pollen season starts in February with alder and hazel pollination, followed by birch and ends with oak in May. Systemic corticosteroids are often prescribed when topical treatments fail, despite limited evidence supporting their efficacy.ObjectiveTo compare the effectiveness of prednisolone tablets versus antihistamine tablets in reducing symptoms and medication usage in patients with moderate to severe tree pollen-induced AR.MethodsThis interventional single-center, double-blinded randomized trial included 34 patients. Treatment was initiated, and symptoms were registered during the tree pollen season. The two groups received either prednisolone tablets (20 mg) or ebastine tablets (20 mg) for 7 days. Treatment effects were evaluated by comparing daily symptom scores, use of topical medication, and a combined symptom-medical score between the groups. Quality of life was recorded at the start and after 3 weeks.ResultsBoth interventions demonstrated efficacy in enhancing quality of life metrics. The area under the curve (AUC) for the combined symptom severity and medication usage score averaged 34.0 (SD = 19.1, 95% CI = 24.5-43.4) in the group treated with prednisolone. This was marginally lower than the control group, with an AUC of 32.6 (SD = 13.2, 95% CI = 25.6-39.7). The difference was not statistically significant (p = 0.80). Both groups exhibited only mild adverse events, which were statistically comparable in frequency and severity.ConclusionsPrednisolone tablets did not show superior efficacy over antihistamine tablets in reducing symptoms or medication usage in tree pollen-induced AR. These results suggest that systemic corticosteroids may not provide additional benefits over antihistamines, and clinicians should prioritize individualized treatment based on patient preferences and tolerability.",Not About Sufficiency
Wilding cities for biodiversity and people: a transdisciplinary framework,"Accelerating urbanisation and associated lifestyle changes result in loss of biodiversity and diminished wellbeing of people through fewer direct interactions and experiences with nature. In this review, we propose the notion of urban wilding (the promotion of autonomous ecological processes that are independent of historical land-use conditions, with minimal direct human maintenance and planting interventions) and investigate its propensity to improve biodiversity and people-nature connections in cities. Through a large interdisciplinary synthesis, we explore the ecological mechanisms through which urban wilding can promote biodiversity in cities, investigate the attitudes and relations of city dwellers towards urban wild spaces, and discuss the integration of urban wilding into the fabric of cities and its governance. We show that favouring assembly spontaneity by reducing planting interventions, and functional spontaneity by limiting maintenance practices, can promote plant diversity and provide ecological resources for numerous organisms at habitat and city scales. These processes could reverse biotic homogenisation, but further studies are needed to understand the effects of wilding on invasive species and their consequences. From a socio-ecological perspective, the attitudes of city dwellers towards spontaneous vegetation are modulated by successional stages, with grassland and woodland stages preferred, but dense shrubby vegetation stages disliked. Wild spaces can diversify physical interactions with nature, and enrich multi-sensory, affective and cognitive experiences of nature in cities. However, some aspects of wild spaces can cause anxiety, feeling unsafe, and the perception of abandonment. These negative attitudes could be mitigated by subtle design and maintenance interventions. While nature has long been thought of as ornamental and instrumental in cities, urban wilding could help to develop relational and intrinsic values of nature in the fabric of cities. Wildness and its singular aesthetics should be combined with cultural norms, resident uses and urban functions to plan and design urban spatial configurations promoting human-non-human cohabitation. For urban wilding to be socially just and adapted to the needs of residents, its implementation should be backed by inclusive governance opening up discussion forums to residents and urban workers. Scientists can support these changes by collaborating with urban actors to design and experiment with new wild spaces promoting biodiversity and wellbeing of people in cities.",Not About Sufficiency
Improving route development using convergent retrosynthesis planning,"Retrosynthesis consists of recursively breaking down a target molecule to produce a synthesis route composed of readily accessible building blocks. In recent years, computer-aided synthesis planning methods have allowed a greater exploration of potential synthesis routes, combining state-of-the-art machine-learning methods with chemical knowledge. However, these methods are generally developed to produce individual routes from a singular product to a set of proposed building blocks and are not designed to leverage potential shared paths between targets. These methods do not necessarily encompass real-world use cases in medicinal chemistry, where one seeks to synthesize sets of target compounds in a library mode, looking for maximal convergence into a shared retrosynthetic path going via advanced key intermediate compounds. Using a graph-based processing pipeline, we explore Johnson & Johnson Electronic Laboratory Notebooks (J&J ELN) and publicly available datasets to identify complex routes with multiple target molecules sharing common intermediates, producing convergent synthesis routes. We find that over 70% of all reactions are involved in convergent synthesis, covering over 80% of all projects in the case of J&J ELN data.Scientific contributionWe introduce a novel planning approach to develop convergent synthesis routes, which can search multiple products and intermediates simultaneously guided by state-of-the-art machine learning single-step retrosynthesis models, enhancing the overall efficiency and practical applicability of retrosynthetic planning. We evaluate the multi-step synthesis planning approach using the extracted convergent routes and observe that solvability is generally high across those routes, being able to identify a convergent route for over 80% of the test routes and showing an individual compound solvability of over 90%. We find that by using a convergent search approach, we can synthesize almost 30% more compounds simultaneously for J&J ELN as compared to using an individual search, while providing an increased use of common intermediates.",Not About Sufficiency
The Text Classification for Imbalanced Data Sets,"Imbanlanced data set has caused a significant drawback of the classification performance attainable by most normal machine learning algorithm. However, the samples are often imbalanced. Therefore, how to reduce the effects of uneven distribution of training sets on text classification performance is a great challenge for machine learning on imbalanced data sets. Currently, the study on imbalaced data mainly lies in two aspects: data-level and algorithm-level. The paper focuses on the study of the three solutions: sample set restructuring, enhancement method of feature selection and weight retouch. Experimental results show that these methods are effective in improving classification performance. After comparing and analyzing the effects of these methods based on the experiments, this paper gets expressly some useful conclusions for some key issues, such as which sampling texts should be chosen and how many sampling texts should be decided for sample restructuring, how about defining separate threshold for each category in feature selection and how to adjust the weights in classification algorithm.",Not About Sufficiency
Data-driven cooling tower optimization: A comprehensive analysis of energy savings using microsand filtration,"Effective management of cooling tower systems requires thorough water quality control. While traditional chemical water treatment methods are currently the most prominent strategy, they are costly and may yield limited results when relied upon as the sole approach. Cross-flow microsand filtration systems offer an interesting alternative with the added benefit of potentially increasing evaporative cooling efficiency, thus saving energy. The focus of the study was to evaluate the effect of these filtration systems on cooling tower operation. A comprehensive data-driven analysis over two cooling seasons evaluated the energetic performance of a system equipped with and without an operating filter using continuous monitoring and statistical modeling. For similar environmental conditions, the coefficient of performance was on average 18% higher and was higher 63% of the time when the filter was operating, indicating superior heat transfer efficiency and significant energy savings. It was also 41% higher during periods of high cooling demand. Consequently, the filter and the system work more efficiently at high wet-bulb temperature and thermal load. Machine learning modeling suggested that operating the filter year-round could save between 5% and 13% of the energy bill, primarily during the cooling season. Continuous filter operation is essential as it mitigates biofouling, underscoring its long-term significance, even during periods of lower thermal loads. The results of this study are significant for sustainability, public health and hold broader implications for cooling tower management. Integrating filtration systems into cooling tower management therefore fosters sustainable practices by decreasing energy consumption and biofouling. This study presents a novel approach by demonstrating, for the first time, the significant impact of continuous cross-flow microsand filtration on cooling tower efficiency, both in terms of energy savings and biofouling mitigation.",Not About Sufficiency
Sufficient Citizens: Moderation and the Politics of Sustainable Development in Thailand,"After the 1997 Asian markets crash, the theory of the sufficiency economy altered the discourse of development in Thailand. Emphasizing Buddhist notions of moderation and enough sufficiency recast development as a means to temper socio-economic volatility by reforming consumer affect. Sufficiency theory pinned the nation's economic upheaval on excessive desires, attempting to intervene in these impulses through projects rooted in personal moderation and communality. In this article, I explore the relationship between politics, citizenship, and sufficiency. Through ethnographic analysis of cases from urban squatter settlements taking part in state-driven participatory urban planning policies, I argue that sufficiency has become central to debates over citizenship in contemporary Thailand. Urban planners use sufficiency to attempt to produce what they term personal development and to attempt to defuse political claims by training the poor to moderate their demands by learning enough. I also show how residents of squatter communities use the language and practices of sufficiency to make political claims and to demonstrate their legitimacy as citizens with rights to the city. In doing so, I demonstrate how notions of sustainable development tied to moderation extend and potentially interrupt social inequalities.",Not About Sufficiency
Prospective evaluation of an artificial intelligence-enabled algorithm for automated diabetic retinopathy screening of 30 000 patients,"Background/aims Human grading of digital images from diabetic retinopathy (DR) screening programmes represents a significant challenge, due to the increasing prevalence of diabetes. We evaluate the performance of an automated artificial intelligence (AI) algorithm to triage retinal images from the English Diabetic Eye Screening Programme (DESP) into test-positive/technical failure versus test-negative, using human grading following a standard national protocol as the reference standard. Methods Retinal images from 30 405 consecutive screening episodes from three English DESPs were manually graded following a standard national protocol and by an automated process with machine learning enabled software, EyeArt v2.1. Screening performance (sensitivity, specificity) and diagnostic accuracy (95% CIs) were determined using human grades as the reference standard. Results Sensitivity (95% CIs) of EyeArt was 95.7% (94.8% to 96.5%) for referable retinopathy (human graded ungradable, referable maculopathy, moderate-to-severe non-proliferative or proliferative). This comprises sensitivities of 98.3% (97.3% to 98.9%) for mild-to-moderate non-proliferative retinopathy with referable maculopathy, 100% (98.7%,100%) for moderate-to-severe non-proliferative retinopathy and 100% (97.9%,100%) for proliferative disease. EyeArt agreed with the human grade of no retinopathy (specificity) in 68% (67% to 69%), with a specificity of 54.0% (53.4% to 54.5%) when combined with non-referable retinopathy. Conclusion The algorithm demonstrated safe levels of sensitivity for high-risk retinopathy in a real-world screening service, with specificity that could halve the workload for human graders. AI machine learning and deep learning algorithms such as this can provide clinically equivalent, rapid detection of retinopathy, particularly in settings where a trained workforce is unavailable or where large-scale and rapid results are needed.",Not About Sufficiency
Cardiac Rhythm Device Identification Using Neural Networks,"OBJECTIVES This paper reports the development, validation, and public availability of a new neural network-based system which attempts to identify the manufacturer and even the model group of a pacemaker or defibrillator from a chest radiograph. BACKGROUND Medical staff often need to determine the model of a pacemaker or defibrillator (cardiac rhythm device) quickly and accurately. Current approaches involve comparing a devices radiographic appearance with a manual flow chart. METHODS In this study, radiographic images of 1,676 devices, comprising 45 models from 5 manufacturers were extracted. A convolutional neural network was developed to classify the images, using a training set of 1,451 images. The testing set contained an additional 225 images consisting of 5 examples of each model. The network's ability to identify the manufacturer of a device was compared with that of cardiologists, using a published flowchart. RESULTS The neural network was 99.6% (95% confidence interval [CI]: 97.5% to 100.0%) accurate in identifying the manufacturer of a device from a radiograph and 96.4% (95% CI: 93.1% to 98.5%) accurate in identifying the model group. Among 5 cardiologists who used the flowchart, median identification of manufacturer accuracy was 72.0% (range 62.2% to 88.9%), and model group identification was not possible. The network's ability to identify the manufacturer of the devices was significantly superior to that of all the cardiologists (p < 0.0001 compared with the median human identification; p < 0.0001 compared with the best human identification). CONCLUSIONS A neural network can accurately identify the manufacturer and even model group of a cardiac rhythm device from a radiograph and exceeds human performance. This system may speed up the diagnosis and treatment of patients with cardiac rhythm devices, and it is publicly accessible online. (C) 2019 The Authors. Published by Elsevier on behalf of the American College of Cardiology Foundation.",Not About Sufficiency
Detection of Sleep Apnea from Single-Lead ECG Signal Using a Time Window Artificial Neural Network,"Sleep apnea (SA) is a ubiquitous sleep-related respiratory disease. It can occur hundreds of times at night, and its long-term occurrences can lead to some serious cardiovascular and neurological diseases. Polysomnography (PSG) is a commonly used diagnostic device for SA. But it requires suspected patients to sleep in the lab for one to two nights and records about 16 signals through expert monitoring. The complex processes hinder the widespread implementation of PSG in public health applications. Recently, some researchers have proposed using a single-lead ECG signal for SA detection. These methods are based on the hypothesis that the SA relies only on the current ECG signal segment. However, SA has time dependence; that is, the SA of the ECG segment at the previous moment has an impact on the current SA diagnosis. In this study, we develop a time window artificial neural network that can take advantage of the time dependence between ECG signal segments and does not require any prior assumptions about the distribution of training data. By verifying on a real ECG signal dataset, the performance of our method has been significantly improved compared to traditional non-time window machine learning methods as well as previous works.",Not About Sufficiency
Defining the challenges and opportunities for using patient-derived models in prostate cancer research,"BackgroundThere are relatively few widely used models of prostate cancer compared to other common malignancies. This impedes translational prostate cancer research because the range of models does not reflect the diversity of disease seen in clinical practice. In response to this challenge, research laboratories around the world have been developing new patient-derived models of prostate cancer, including xenografts, organoids, and tumor explants.MethodsIn May 2023, we held a workshop at the Monash University Prato Campus for researchers with expertise in establishing and using a variety of patient-derived models of prostate cancer. This review summarizes our collective ideas on how patient-derived models are currently being used, the common challenges, and future opportunities for maximizing their usefulness in prostate cancer research.ResultsAn increasing number of patient-derived models for prostate cancer are being developed. Despite their individual limitations and varying success rates, these models are valuable resources for exploring new concepts in prostate cancer biology and for preclinical testing of potential treatments. Here we focus on the need for larger collections of models that represent the changing treatment landscape of prostate cancer, robust readouts for preclinical testing, improved in vitro culture conditions, and integration of the tumor microenvironment. Additional priorities include ensuring model reproducibility, standardization, and replication, and streamlining the exchange of models and data sets among research groups.ConclusionsThere are several opportunities to maximize the impact of patient-derived models on prostate cancer research. We must develop large, diverse and accessible cohorts of models and more sophisticated methods for emulating the intricacy of patient tumors. In this way, we can use the samples that are generously donated by patients to advance the outcomes of patients in the future.",Not About Sufficiency
Association of Virulence and Antibiotic Resistance in Salmonella-Statistical and Computational Insights into a Selected Set of Clinical Isolates,"The acquisition of antibiotic resistance (AR) by foodborne pathogens, such as Salmonella enterica, has emerged as a serious public health concern. The relationship between the two key survival mechanisms (i.e., antibiotic resistance and virulence) of bacterial pathogens is complex. However, it is unclear if the presence of certain virulence determinants (i.e., virulence genes) and AR have any association in Salmonella. In this study, we report the prevalence of selected virulence genes and their association with AR in a set of phenotypically tested antibiotic-resistant (n = 117) and antibiotic-susceptible (n = 94) clinical isolates of Salmonella collected from Tennessee, USA. Profiling of virulence genes (i.e., virulotyping) in Salmonella isolates (n = 211) was conducted by targeting 13 known virulence genes and a gene for class 1 integron. The association of the presence/absence of virulence genes in an isolate with their AR phenotypes was determined by the machine learning algorithm Random Forest. The analysis revealed that Salmonella virulotypes with gene clusters consisting of avrA, gipA, sodC1, and sopE1 were strongly associated with any resistant phenotypes. To conclude, the results of this exploratory study shed light on the association of specific virulence genes with drug-resistant phenotypes of Salmonella. The presence of certain virulence genes clusters in resistant isolates may become useful for the risk assessment and management of salmonellosis caused by drug-resistant Salmonella in humans.",Not About Sufficiency
Munsell Color Specification using ARCA (Automatic Recognition of Color for Archaeology),"Munsell Soil Charts are a very common tool used by archaeologists for the color specification task. Charts are usually employed directly on cultural heritage sites to identify color of soils and collected artifacts. However, charts are designed to be used specifying the color through subjective perception of users, by visual mean, in a time-consuming and error-prone procedure. It is likely that two users may estimate different Munsell notations for the same specimen, as colors are not perceived uniformly by different people. Hence, estimation process should be repeated several times and by more than a single expert user to be considered reliable. In this work, we employ our framework, Automatic Recognition of Color for Archaeology (ARCA), specifically designed to provide a method for objective, deterministic, fast, and automatic Munsell estimation. ARCA is a valuable asset for archaeologists as it provides the definition of a smooth pipeline for an affordable Munsell notation estimation: image acquisition of specimens with general purpose digital cameras in an uncontrolled environment, manual sampling of specimen images in the ARCA desktop application, automatic Munsell color specification, and report generation. We further assess our method with improved color tolerance validations and evaluations, introducing a comparison between Delta E-00, Delta E-76, Delta L*, Delta a*, and Delta b* differences. One of the main contributions of this article is the extension of our former dataset ARCA108. We gathered two additional sets of images obtaining a new dataset consisting of pictures of Munsell Soil Charts Editions 2000 and 2009 plus images from a real test case with 16 pottery shards. The new dataset counts 56,160 samples and 328 images, so it has been called ARCA328. Experimental results are reported to investigate which could be the best configuration to be used in the acquisition phase.",Not About Sufficiency
CELCOM Project: Engineering Practice via Community Networks in Amazon,"Concurrent technological, political, and economic events have led to increasing globalization of the world economy. Industry has responded to this transformation by redefining business strategies and new expectations for the set of skills required from college graduates, especially in science, technology, engineering and mathematics (STEM) areas. As a new generation of engineering students get closer to graduate, they are challenged with important issues at local, regional, and global levels. The access to information and communication technologies (ICTs) as a tool for education, economic development, and social well-being is one of these important issues. In this paper, we share a Problem Based Learning (PBL) project focused on ICTs to teach telecommunication technologies to engineering students. This project also has the goal to train and educate students in Humanitarian Engineering to improve the well-being of communities in the Amazon region of Brazil. The results demonstrate that students participating in this PBL project develop a more solid technical background, research and design skills, critical soft skills, better perception of the telecommunication industry and a better understanding of the standardization bodies.",Not About Sufficiency
Tools to Accelerate Population Science and Disease Control Research,"Population science and disease control researchers can benefit from a more proactive approach to applying bioinformatics tools for clinical and public health research. Bioinformatics utilizes principles of information sciences and technologies to transform vast, diverse, and complex life sciences data into a more coherent format for wider application. Bioinformatics provides the means to collect and process data, enhance data standardization and harmonization for scientific discovery, and merge disparate data sources. Achieving interoperability (i.e. the development of an informatics system that provides access to and use of data from different systems) will facilitate scientific explorations and careers and opportunities for interventions in population health. The National Cancer Institute's (NCI's) interoperable Cancer Biomedical Informatics Grid (caBIG') is one of a number of illustrative tools in this report that are being mined by population scientists. Tools are not all that is needed for progress. Challenges persist, including a lack of common data standards, proprietary barriers to data access, and difficulties pooling data from studies. Population scientists and informaticists are developing promising and innovative solutions to these barriers. The purpose of this paper is to describe how the application of bioinformatics systems can accelerate population health research across the continuum from prevention to detection, diagnosis, treatment, and outcome. (Am J Prey Med 2010;38(6):646-651) (C) 2010 American Journal of Preventive Medicine",Not About Sufficiency
Multiple Losses of Social Resources Following Collective Trauma: The Case of the Forced Relocation From Gush Katif,"Collective trauma may lead to a pervasive loss of personal and social resources. The current study used a mixed method design to explore losses of social connections and affiliations following the collective trauma of forced relocation. A sample of 269 relocated residents from Gush Katif completed open-ended questionnaires regarding their ability to cope following the relocation, as well as questionnaires regarding their sense of belonging to the country, their sense of alienation from government institutions, post traumatic symptoms, and well-being. Three themes emerged in the qualitative stage of the study as the primary losses experienced by participants: loss of physical place and landscape, loss of a sense of belonging to Israeli society, and loss of trust and alienation from the country's institutions. The quantitative stage revealed a complementary picture, with lower place commitment and higher alienation contributing directly both to post traumatic symptoms and to a reduced sense of well-being. In addition, a sense of alienation from the institutions of the country mediated the associations between the sense of belonging to the country and post traumatic symptoms and well-being. These findings are discussed in relation to the concept of social capital as a key factor in explaining one's ability to cope with collective trauma.",Not About Sufficiency
Digital Architectural Archives - Aesthetic Reading,"My aim in this paper is to examine the impact of digital technology on the perception and practice of architecture. Online activities leave discernible traces, challenging the traditional ontological view of the subject's independence from the material world. I will offer an exploration of the Internet's role as the primary communication medium transforms architectural practice, emphasizing digital archives and distribution. Platforms like Pinterest, ArchDaily, and Instagram have transformed how architectural practices are shared and consumed, cultivating a new culture of multimodal communication. Within this context, I will discuss how digital technologies have profoundly altered perceptions of materiality, space, and information. I will explore how architects and artists now operate within an interconnected network of social, economic, and technological forces, moving away from traditional media to embrace digital fragmentation. This shift impacts how architectural objects are perceived, from complete forms to fragmented digital representations that are accessible to a global audience",Not About Sufficiency
Prospective and External Evaluation of a Machine Learning Model to Predict In-Hospital Mortality of Adults at Time of Admission,"Importance The ability to accurately predict in-hospital mortality for patients at the time of admission could improve clinical and operational decision-making and outcomes. Few of the machine learning models that have been developed to predict in-hospital death are both broadly applicable to all adult patients across a health system and readily implementable. Similarly, few have been implemented, and none have been evaluated prospectively and externally validated. Objectives To prospectively and externally validate a machine learning model that predicts in-hospital mortality for all adult patients at the time of hospital admission and to design the model using commonly available electronic health record data and accessible computational methods. Design, Setting, and Participants In this prognostic study, electronic health record data from a total of 43180 hospitalizations representing 31003 unique adult patients admitted to a quaternary academic hospital (hospital A) from October 1, 2014, to December 31, 2015, formed a training and validation cohort. The model was further validated in additional cohorts spanning from March 1, 2018, to August 31, 2018, using 16122 hospitalizations representing 13094 unique adult patients admitted to hospital A, 6586 hospitalizations representing 5613 unique adult patients admitted to hospital B, and 4086 hospitalizations representing 3428 unique adult patients admitted to hospital C. The model was integrated into the production electronic health record system and prospectively validated on a cohort of 5273 hospitalizations representing 4525 unique adult patients admitted to hospital A between February 14, 2019, and April 15, 2019. Main Outcomes and Measures The main outcome was in-hospital mortality. Model performance was quantified using the area under the receiver operating characteristic curve and area under the precision recall curve. Results A total of 75247 hospital admissions (median [interquartile range] patient age, 59.5 [29.0] years; 45.9% involving male patients) were included in the study. The in-hospital mortality rates for the training validation; retrospective validations at hospitals A, B, and C; and prospective validation cohorts were 3.0%, 2.7%, 1.8%, 2.1%, and 1.6%, respectively. The area under the receiver operating characteristic curves were 0.87 (95% CI, 0.83-0.89), 0.85 (95% CI, 0.83-0.87), 0.89 (95% CI, 0.86-0.92), 0.84 (95% CI, 0.80-0.89), and 0.86 (95% CI, 0.83-0.90), respectively. The area under the precision recall curves were 0.29 (95% CI, 0.25-0.37), 0.17 (95% CI, 0.13-0.22), 0.22 (95% CI, 0.14-0.31), 0.13 (95% CI, 0.08-0.21), and 0.14 (95% CI, 0.09-0.21), respectively. Conclusions and Relevance Prospective and multisite retrospective evaluations of a machine learning model demonstrated good discrimination of in-hospital mortality for adult patients at the time of admission. The data elements, methods, and patient selection make the model implementable at a system level. This prognostic study evaluates the discrimination of a machine learning model to predict in-hospital mortality for all adult patients at the time of hospital admission. Question How accurately can a machine learning model predict risk of in-hospital mortality for adult patients when evaluated prospectively and externally? Findings In this prognostic study that included 75247 hospitalizations, prospective and multisite retrospective evaluations of a machine learning model demonstrated good discrimination in predicting in-hospital mortality for patients at the time of admission. Area under the receiver operating characteristic curve ranged from 0.84 to 0.89, and prospective and multisite retrospective results were similar. Meaning A machine learning model, designed to be implementable at a system level, demonstrated good discrimination in identifying patients at high risk of in-hospital mortality and may be used to improve clinical and operational decision-making.",Not About Sufficiency
Prevalence of electronic screening for sepsis in National Health Service acute hospitals in England,"Sepsis is a worldwide public health problem. Rapid identification is associated with improved patient outcomes-if followed by timely appropriate treatment.ObjectivesDescribe digital sepsis alerts (DSAs) in use in English National Health Service (NHS) acute hospitals.MethodsA Freedom of Information request surveyed acute NHS Trusts on their adoption of electronic patient records (EPRs) and DSAs.ResultsOf the 99 Trusts that responded, 84 had an EPR. Over 20 different EPR system providers were identified as operational in England. The most common providers were Cerner (21%). System C, Dedalus and Allscripts Sunrise were also relatively common (13%, 10% and 7%, respectively). 70% of NHS Trusts with an EPR responded that they had a DSA; most of these use the National Early Warning Score (NEWS2). There was evidence that the EPR provider was related to the DSA algorithm. We found no evidence that Trusts were using EPRs to introduce data driven algorithms or DSAs able to include, for example, pre-existing conditions that may be known to increase risk.Not all Trusts were willing or able to provide details of their EPR or the underlying algorithm.DiscussionThe majority of NHS Trusts use an EPR of some kind; many use a NEWS2-based DSA in keeping with national guidelines.ConclusionMany English NHS Trusts use DSAs; even those using similar triggers vary and many recreate paper systems. Despite the proliferation of machine learning algorithms being developed to support early detection of sepsis, there is little evidence that these are being used to improve personalised sepsis detection.",Not About Sufficiency
A machine learning-based universal outbreak risk prediction tool,"In order to prevent and control the increasing number of serious epidemics, the ability to predict the risk caused by emerging outbreaks is essential. However, most current risk prediction tools, except EPIRISK, are limited by being designed for targeting only one specific disease and one country. Differences between countries and diseases (e.g., different economic conditions, different modes of transmission, etc.) pose challenges for building models with cross-country and cross-disease prediction capabilities. The limitation of universality affects domestic and international efforts to control and prevent pandemic outbreaks. To address this problem, we used outbreak data from 43 diseases in 206 countries to develop a universal risk prediction system that can be used across countries and diseases. This system used five machine learning models (including Neural Network XGBoost, Logistic Boost, Random Forest and Kernel SVM) to predict and vote together to make ensemble predictions. It can make predictions with around 80%-90 % accuracy from economic, cultural, social, and epidemiological factors. Three different datasets were designed to test the performance of ML models under different realistic situations. This prediction system has strong predictive ability, adaptability, and generality. It can give universal outbreak risk assessment that are not limited by border or disease type, facilitate rapid response to pandemic outbreaks, government decision-making and international cooperation.",Not About Sufficiency
Effects of the COVID-19 pandemic on the use and perceptions of urban green space: An international exploratory study,"Urban green space (UGS) is an essential element in the urban environment, providing multiple ecosystem services as well as beneficial effects on physical and mental health. In a time of societal crisis these effects may be amplified, but ensuring that they are maintained requires effective planning and management - which is a complex challenge given the rapid changes in modern society and the need for continual adaptation. This study aims to identify the drivers that normally attract visitors to UGS, and to assess the effects of social isolation on the usage and perception of UGS during the COVID-19 pandemic. We conducted an online survey during the period in which restrictive measures were imposed in response to the pandemic (March-May 2020), in Croatia, Israel, Italy, Lithuania, Slovenia and Spain. Results showed that urban residents normally have a need for accessible UGS, mainly for physical exercise, relaxing and observing nature. The reduction in UGS visitation during the containment period was related to distinct changes in the motivations of those who did visit, with a relative increase in ""necessary activities"" such as taking the dog out, and a reduction in activities that could be considered non-essential or high-risk such as meeting people or observing nature. Behavioral changes related to proximity were also observed, with an increase in people walking to small urban gardens nearby (e.g. in Italy) or tree-lined streets (e.g. in Spain, Israel), and people traveling by car to green areas outside the city (e.g. in Lithuania). What the respondents missed the most about UGS during the pandemic was ""spending time outdoors"" and ""meeting other people"" - highlighting that during the COVID-19 isolation, UGS was important for providing places of solace and respite, and for allowing exercise and relaxation. Respondents expressed the need for urban greenery even when legally mandated access was limited - and many proposed concrete suggestions for improved urban planning that integrates green spaces of different sizes within the fabric of cities and neighborhoods, so that all residents have access to UGS.",Not About Sufficiency
Deployment of TinyML-Based Stress Classification Using Computational Constrained Health Wearable,"Stress has become a common mental health issue in modern society, causing individuals to experience acute behavioral changes. Exposure to prolonged stress without proper prevention and treatment may cause severe damage to one's physiological and psychological health. Researchers around the world have been working to find and create solutions for early stress detection using machine learning (ML). This paper investigates the possibility of utilizing Tiny Machine Learning (TinyML) in developing a wearable device, comparable to a smartwatch, that is equipped with both physiological and psychological data detection system to enable edge computing and give immediate feedback for stress prediction. The main challenge of this study was to fit a trained ML model into the microcontroller's limited memory without compromising the model's accuracy. A TinyML-based framework using a Raspberry Pi Pico RP2040 on a customized board equipped with several health sensors was proposed to predict stress levels by utilizing accelerations, body temperature, heart rate, and electrodermal activity from a public health dataset. Moreover, a few selected machine learning models underwent hyperparameter tuning before a porting library was used to translate them from Python to C/C++ for deployment. This approach led to an optimized XGBoost model with 86.0% accuracy and only 1.12 MB in size, hence perfectly fitting into the 2 MB constraint of RP2040. The prediction of stress on the edge device was then tested and validated using a separate sub-dataset. This trained model on TinyML can also be used to obtain an immediate reading from the calibrated health sensors for real-time stress predictions.",Not About Sufficiency
A qualitative study of the impact of coronavirus disease (COVID-19) on psychological and financial wellbeing and engagement in care among men who have sex with men living with HIV in Thailand,"Objectives The coronavirus disease (COVID-19) pandemic is an unprecedented event with massive global health and socio-economic impacts on vulnerable populations, especially people living with HIV. The epidemic has severely affected Thailand's economy and potentially impacted the financial and psychological wellbeing of Thai HIV-positive men who have sex with men (MSM). Methods Between 15 June and 10 December 2020, we conducted qualitative interviews with 26 MSM living with HIV in Thailand who participate in an Adam's Love We Care Study. We intentionally recruited individuals who may have experienced a greater impact of COVID-19. Interviews explored worry, stigma and stress surrounding COVID-19, and multiple domains of potential COVID-19 impact: financial/employment, HIV service delivery and antiretroviral (ART) adherence during the first 10 months of the COVID-19 pandemic. Results Participants perceived themselves as immunocompromised and susceptible, and feared contracting COVID-19. Participants worried that contracting COVID-19 would lead to HIV status disclosure and stigmatization. Participants had considerable worry about job loss as a result of the economic downturn, and some shared challenges associated with relocation and re-engaging with HIV care. Financial stress and lack of basic necessities caused by job losses were commonly reported. Participants reported optimal ART adherence as a consequence of local HIV service delivery responses, convenient ART refills and Adam's Love online support interventions. Conclusions Our study highlights that the COVID-19 pandemic produced high levels of anxiety and concerns about additional stigma among MSM living with HIV. It had a significant negative effect on the daily lives of our participants. These findings indicate a need for the provision of confidential COVID-19 diagnosis and care, relief programmes, vaccination roll-out equity, and addressing employment needs of vulnerable populations.",Not About Sufficiency
SBAS DFMC Analysis with a Software Prototype,"The GNSS environment for civil aviation is evolving towards a Dual-Frequency Multi-Constellation ( DFMC) scheme thanks to the deployment and modernization of multiple GNSS constellations ( modernized GPS, Galileo, GLONASS, Beidou) broadcasting dual-frequency signals in the Aeronautical Radio Navigation Service bands. This new GNSS scheme will support in the coming years aeronautical navigation services with improved positioning performances and robustness. The definition and analysis of the GNSS augmentation systems evolution to operate in this new dual-frequency multi-constellation ( DFMC) environment is on-going. In the case of the SBAS evolution, the SBAS Interoperability Working Group ( IWG) has worked on the development of an SBAS DFMC L5 Interface Control Document ( ICD) in order to initiate the standardization of the future SBAS DFMC system and services. Other working groups such as the EUROCAE WG-62 and the RTCA SC 159 have also started the development of standards to support the introduction of future SBAS DFMC user receivers. The SBAS DFMC Service Volume software Prototype ( SBAS DSVP) is a tool compliant with the latest SBAS DFMC L5 ICD produced by the IWG and developed to support SBAS DFMC standardization activities. The prototype generates the sequence of broadcast SBAS DFMC messages as well as the broadcast parameters required to analyze the availability and continuity of APV and CAT I operations in the SBAS service area. The prototype's user module is able to compute Horizontal and Vertical Protection Levels ( HPL/VPL) from the SBAS L5 messages elaborated by the ground module. The behavior and performance of SBAS DFMC systems as defined in the latest IWG documentation have been analysed with the SBAS DSVP for different system configurations. The performance analysis is based on different indicators such as the service availability and continuity, the bandwidth occupation by the different Message Types or the time to first valid position fix ( TTFF). Different system configurations have been analyzed, including different augmented constellations and ground station networks. The analysis is focused on the impact of the following elements: a) the strategy selected to broadcast integrity information to users, b) a potential single-frequency L5 SBAS back-up service, and c) the presence of equatorial ionospheric scintillation.",Not About Sufficiency
On-line water quality inspection system: the role of the wireless sensory network,"There is an increasing dependence on freshwater sources for various human activities because of population growth and rising industrialization across the globe. Meanwhile, the safety of available freshwater is threatened by the massive generation of waste from increasing domestic and industrial activities. The need for continuous assessment of the quality of the environmental water available has become a crucial research concern. The conventional techniques commonly used are not sufficient to meet the expanding demand for real-time, rapid, low-cost, reliable, and sensitive water quality monitoring (WQM). The use of wireless sensor networks (WSN) has been proposed by various researchers as a sustainable substitute for the traditional processes of monitoring water quality. In this work, an array of the literature on the practical applications of the networks in the assessment of vital water quality parameters such as pH, turbidity, temperature, dissolved oxygen (DO), chlorine content, etc., were surveyed and analyzed. Various technologies such as machine learning, blockchain, internet of things (IoT), deep reconstruction model, etc., were incorporated with WSN for real-time monitoring of water quality, data acquisition, and reporting for a broad range of water bodies. The survey shows that the networks are comparatively affordable and allow remote, real-time, and sensitive measurement of these parameters with minimal human involvement. The use of a low-power wide area network (LPWAN) was also introduced to solve a major problem of power supply often associated with the use of WSN. Recent developments also showed the capacity of WSN to assess simultaneously multiple water quality parameters from several locations using unmanned aerial vehicles (UAV). However, the networks rely on established parameters to indicate a compromise in water quality, but in most cases, fail to identify which pollutant species are responsible.",Not About Sufficiency
Peak car and increasing rebound: A closer look at car travel trends in Great Britain,"This paper uses econometric analysis of aggregate time-series data to explore how different factors have influenced the demand for car travel in Great Britain since 1970 and how the rebound effect has changed over that time. Our results suggest that changes in income, the fuel cost of driving and the level of urbanisation largely explain travel trends over this period - with recent reductions in car travel (peak car) being driven by a combination of the rising fuel cost of driving, increased urbanisation and the economic difficulties created by the 2008 financial crisis. We find some evidence that the proportion of licensed drivers has influenced aggregate travel trends, but no evidence that growing income inequality and the diffusion of ICT technology have played a role. Our results also suggest that the rebound effect from improved fuel efficiency has averaged 26% over this period and that the magnitude of this effect has increased over time. However, methodological and data limitations constrain the level of confidence that we can have in these results. (C) 2017 The Authors. Published by Elsevier Ltd.",Not About Sufficiency
A Simulation-Based Method for Optimizing Remote Park-and-Ride Schemes,"Urbanization places greater demand on the link between downtown areas and suburbs, due to commuters' long-distance and diverse trips. As an emerging form of park-and-ride (PNR) services, remote PNR (RPR) facilities have proved to be more economical and environmentally friendly, allowing travelers to park in a suburban area and travel to a rail station via bus. In this regard, a generalized simulation-based bilevel model for optimizing the locations and capacities of RPR facilities is developed in this article. A hybrid algorithm integrating Bayesian optimization, branch and bound, and trust region sequential quadratic programming is proposed to achieve an optimal solution. The proposed integrated method balances the desired efficiency and accuracy through the combination of machine learning-based technology and mathematical optimization methodology. The validity of the proposed model is tested on a large-scale real-world transportation network in Halle, Germany. Modeling and analyzing RPR schemes using the proposed framework may provide new insights into improving social welfare.",Not About Sufficiency
MACHINE LEARNING BASE METHODS FOR BREAST CANCER DIAGNOSE,"Cancer is a serious threat to people's health, and its heterogeneous nature and its ability to divide and proliferate make it difficult to cure. For women around the world, breast cancer has been affecting their health and even the risk of life. Therefore, earlier and more accurate diagnosis can save patient's lives. As research into machine learning has become more advanced, different algorithms have been applied to various datasets, including medical data. In this paper, mainly introduce three algorithms that are commonly used and superior in cancer diagnosis, K-Nearest Neighbor algorithm, Naive Bayesian algorithm based on Bayes' theorem and Support Vector Machine. An experimental case is used to illustrate the F1 score, accuracy and recall rate of these two algorithms on the same data set.",Not About Sufficiency
CHALLENGES AND DRIVERS OF GREEN AND SUSTAINABLE SPATIAL DEVELOPMENT: A CASE STUDY OF LOWER SILESIA AND LATVIA,"The article discusses sustainable spatial development in Lower Silesia and Latvia (NUTS 2 level for territorial units), paying attention to the integration of Sustainable Development Goals (SDGs) and strategies for future growth. In Poland, spatial development guidelines emphasise economic viability, competitiveness, innovation, and sustainability, focusing on cooperation between local governments. Legislative reforms prioritise sustainable development principles in spatial planning to address regional challenges comprehensively. Lower Silesia's diverse topography and urban centres are focal points for understanding local development strategies and climate change mitigation efforts. The sustainable development strategy in Latvia aims to enhance societal well-being, economic growth, environmental quality, and international recognition through long-term development objectives. Collaboration between urban and rural areas is crucial for Latvia's sustainable development, emphasising the optimal utilisation of resources and tourism promotion. The text stresses the importance of aligning economic activities with environmental preservation and promoting social engagement for sustainable development. The research highlights the significance of integrating environmental considerations, spatial planning, and societal engagement to achieve sustainable development and address regional disparities for further exploration and application of sustainable practices.",Not About Sufficiency
Node2VecFuseClassifier: Bridging Perspectives in Modeling Transplantation Attitudes Among Dialysis Patients,"The study of patient attitudes toward transplantation is crucial for dialysis clinic decision-makers. Understanding the factors influencing patient attitudes toward transplantation helps improving transplant access. While many studies attempt to predict patient attitudes toward transplantation from a patient-centric point of view by focusing on patient information, there is a limited number of studies that explore the influence of social interaction among dialysis patients on predicting their attitudes toward transplantation. In this study, we utilize machine learning-based models and social networks to classify hemodialysis patients' attitudes toward transplantation taking into account the influence of social interaction among dialysis patients. We conduct a questionnaire among 110 participants from two hemodialysis centers. We investigate the performance of machine learning algorithms, including linear, non-linear model, and graph-based machine learning models. We use developed models to classify hemodialysis clinic patients into positive and negative attitudes toward transplantation. We introduce a graph-based model named Node2VecFuseClassifier that integrates both patient interactions and patient features. To emphasize the significance of social interaction, we compare the benefits of using a sociodemographic patient-centric versus a social-centric problem representation that considers patient-to-patient and patient-to-staff interactions. We believe that including the social aspect, patient-to-patient, and patient-to-staff network features enhances all machine learning models' performance as compared to relying on patient-centric features alone. By combining patient experiences with staff expertise, the multilevel analysis enhances predictive capabilities by incorporating diverse roles like patients and staff. Thus, we combine patient and staff networks and observe that such a multi-network approach boosts the F1-score in predicting patient attitude toward transplantation by at least 5% compared to using only the patient network or staff network. The proposed Node2VecFuseClassifier that combines Node2Vec embeddings and features improves the accuracy of transplantation attitudes prediction by 4% - 6%. Overall, our study shows that integrating the interaction between patients and staff provides beneficial insights to accurately predicting transplantation attitudes for dialysis patients.",Not About Sufficiency
Turning Gaming EEG Peripherals into Trainable Brain Computer Interfaces,"Companies such as NeuroSky and Emotiv Systems are selling non-medical EEG devices for human computer interaction. These devices are significantly more affordable than their medical counterparts, and are mainly used to measure levels of engagement, focus, relaxation and stress. This information is sought after for marketing research and games. However, these EEG devices have the potential to enable users to interact with their surrounding environment using thoughts only, without activating any muscles. In this paper, we present preliminary results that demonstrate that despite reduced voltage and time sensitivity compared to medical-grade EEG systems, the quality of the signals of the Emotiv EPOC neuroheadset is sufficiently good in allowing discrimination between imaging events. We collected streams of EEG raw data and trained different types of classifiers to discriminate between three states (rest and two imaging events). We achieved a generalisation error of less than 2% for two types of non-linear classifiers.",Not About Sufficiency
Increased Security on Software Defined Network SDN to mitigate attack in Fog Environment Based on Using Artificial Intelligence,"Fog computing is a decentralized cloud infrastructure provides an accurate solution to the trouble of data processing. The most important security requirement of fog technology is service and data availability. One of the most devastating cyber-attack is the Distributed Denial of Service attacks which are the most common and dangerous cyber-attacks, this type of security attack affecting the information integrity and service availability. This paper propose and focus on the development of fog cloud by conducting a contribution study on two different security mechanisms that is SDN with ANNs network algorithm as anomaly-based IDS via merged with a signature -based IDS detection to provide the maximum-security requirements and provide the required machine learning that well supports the fog to define and detect all upcoming and new issue cybersecurity. Moreover, provides the systems with the ability of automatic learning and improves the data and information experience process without being programmed.",Not About Sufficiency
Brazilian Mangrove Status: Three Decades of Satellite Data Analysis,"Since the 1980s, mangrove cover mapping has become a common scientific task. However, the systematic and continuous identification of vegetation cover, whether on a global or regional scale, demands large storage and processing capacities. This manuscript presents a Google Earth Engine (GEE)-managed pipeline to compute the annual status of Brazilian mangroves from 1985 to 2018, along with a new spectral index, the Modular Mangrove Recognition Index (MMRI), which has been specifically designed to better discriminate mangrove forests from the surrounding vegetation. If compared separately, the periods from 1985 to 1998 and 1999 to 2018 show distinct mangrove area trends. The first period, from 1985 to 1998, shows an upward trend, which seems to be related more to the uneven distribution of Landsat data than to a regeneration of Brazilian mangroves. In the second period, from 1999 to 2018, a trend of mangrove area loss was registered, reaching up to 2% of the mangrove forest. On a regional scale, similar to 85% of Brazil's mangrove cover is in the states of MaranhAo, Para, Amapa and Bahia. In terms of persistence, similar to 75% of the Brazilian mangroves remained unchanged for two decades or more.",Not About Sufficiency
Run-time Mechanisms for Fine-Grained Parallelism on Network Processors: the TILEPro64 Experience,"The efficient parallelization of very fine-grained computations is an old problem still challenging also on modern shared memory architectures. Scalable parallelizations are possible if the base mechanisms provided by the run-time support (for inter-thread/inter-process synchronization/communication) are carefully designed and developed on top of parallel architectures. This requires a deep knowledge of the hardware behavior and the interaction patterns used by the parallelism paradigms. In this paper we present our experience in developing efficient inter-thread interaction mechanisms on the Tilera TILEPr064 network processor. Although it is a domain-specific parallel architecture, the TILEPr064 represents a notable example of how advanced architectural structures, such as user-accessible on-chip interconnection networks and configurable cache coherence protocols, are of great importance to design lightweight cooperation mechanisms enabling efficient parallel implementations of fine-grained problems. The paper presents our ideas and an experimental evaluation that compares our proposals with other existing run-time supports.",Not About Sufficiency
Understanding policy and technology responses in mitigating urban heat islands: A literature review and directions for future research,"Policy and technology responses to increased temperatures in urban heat islands (UHIs) are discussed in a variety of research; however, their interaction is overlooked and understudied. This is an important oversight because policy and technology are often developed in isolation of each other and not in conjunction. Therefore, they have limited synergistic effects when aimed at solving global issues. To examine this aspect, we conducted a systematic literature review and synthesised 97 articles to create a conceptual structuring of the topic. We identified the following categories: (a) evidence base for policymaking including timescale analysis, effective policymaking instruments as well as decision support and scenario planning; (b) policy responses including landscape and urban form, green and blue area ratio, albedo enhancement policies, transport modal split as well as public health and participation; (c) passive technologies including green building envelopes and development of cool surfaces; and (d) active technologies including sustainable transport as well as energy consumption, heating, ventilation and air conditioning, and waste heat. Based on the findings, we present a framework to guide future research in analysing UHI policy and technology responses more effectively in conjunction with each other.",Not About Sufficiency
"Identifying Anti-Vaccination Tweets in Arabic Language Utilizing NLP, ML, and DL Approaches","Twitter has had a prominent role during the Covid-19 pandemic in discussing public healthcare-related events, including vaccine efficacy. In this context, the sentiments of individuals about the vaccine have varied between supporters of vaccination programs, and opponents who advocate others not to vaccinate. Being able to identify anti-vaxxers through their tweets could enable public health organizations to make wise decisions to enhance vaccine acceptance, by allocating customized awareness campaigns to individuals to reduce antivaccination sentiments among different groups. This study aims to provide a smart classification model that can identify the sentiments of individuals in the Middle East region toward Covid-19 vaccinations through the Arabic tweets they publish on their own Twitter accounts, exploiting Natural Language Processing, Machine Learning, and Deep Learning-based models. Nine well-known Machine Learning algorithms and four Deep Learning algorithms were investigated with various feature extraction techniques (TF, TF-IDF, and Glove). The results show that the performance of the TF model outperforms other models in the Multinomial Naive Bayes with an accuracy of 82.14% On the other hand, the performance of the TF-IDF model outperforms other models in the Support Vector Machine with an accuracy of 81.81% Finally, the Glove word embedding model outperforms other models in the LSTM with an accuracy of 81.67%.",Not About Sufficiency
The triglyceride-glucose index and its obesity-related derivatives as predictors of all-cause and cardiovascular mortality in hypertensive patients: insights from NHANES data with machine learning analysis,"BackgroundHypertension (HTN) is a global public health concern and a major risk factor for cardiovascular disease (CVD) and mortality. Insulin resistance (IR) plays a crucial role in HTN-related metabolic dysfunction, but its assessment remains challenging. The triglyceride-glucose (TyG) index and its derivatives (TyG-BMI, TyG-WC, and TyG-WHtR) have emerged as reliable IR markers. In this study, we evaluated their associations with all-cause and cardiovascular mortality in hypertensive patients using machine learning techniques.MethodsData from 9432 hypertensive participants in the National Health and Nutrition Examination Survey (NHANES) 1999-2018 were analysed. Cox proportional hazards models and restricted cubic splines were employed to explore mortality risk and potential nonlinear relationships. Machine learning models were utilized to assess the predictive value of the TyG index and its derivatives for mortality outcomes.ResultsThe TyG index and its derivatives were independent predictors of both all-cause and cardiovascular mortality in hypertensive patients. The TyG-WHtR exhibited the strongest association, with each 1-unit increase linked to a 41.7% and 48.1% higher risk of all-cause and cardiovascular mortality, respectively. L-shaped relationships were observed between TyG-related indices and mortality. The incorporation of the TyG index or its derivatives into predictive models modestly improved the prediction performance for mortality outcomes.Conclusions The TyG index and its derivatives are significant predictors of mortality in hypertensive patients. Their inclusion in predictive models enhances risk stratification and may aid in the early identification of high-risk individuals in this population. Further studies are needed to validate these findings in external hypertensive cohorts.",Not About Sufficiency
Validation of Fourier Transform Infrared Spectroscopy for Serotyping of Streptococcus pneumoniae,"Fourier transform infrared (FT-IR) spectroscopy (IR Biotyper; Bruker) allows highly discriminatory fingerprinting of closely related bacterial strains. In this study, FT-IR spectroscopy-based capsular typing of Streptococcus pneumoniae was validated as a rapid, cost-effective, and medium-throughput alternative to the classical phenotypic techniques. A training set of 233 strains was defined, comprising 34 different serotypes and including all 24 vaccine types (VTs) and 10 non-vaccine types (NVTs). The acquired spectra were used to (i) create a dendrogram where strains clustered together according to their serotypes and (ii) train an artificial neural network (ANN) model to predict unknown pneumococcal serotypes. During validation using 153 additional strains, we reached 98.0% accuracy for determining serotypes represented in the training set. Next, the performance of the IR Biotyper was assessed using 124 strains representing 59 non-training set serotypes. In this setting, 42 of 59 serotypes (71.1%) could be accurately categorized as being non-training set serotypes. Furthermore, it was observed that comparability of spectra was affected by the source of the Columbia medium used to grow the pneumococci and that this complicated the robustness and standardization potential of FT-IR spectroscopy. A rigorous laboratory workflow in combination with specific ANN models that account for environmental noise parameters can be applied to overcome this issue in the near future. The IR Biotyper has the potential to be used as a fast, cost-effective, and accurate phenotypic serotyping tool for S. pneumoniae.",Not About Sufficiency
Does Physical Activity Predict Obesity-A Machine Learning and Statistical Method-Based Analysis,"Background: Obesity prevalence has become one of the most prominent issues in global public health. Physical activity has been recognized as a key player in the obesity epidemic. Objectives: The objectives of this study are to (1) examine the relationship between physical activity and weight status and (2) assess the performance and predictive power of a set of popular machine learning and traditional statistical methods. Methods: National Health and Nutrition Examination Survey (NHANES, 2003 to 2006) data were used. A total of 7162 participants met our inclusion criteria (3682 males and 3480 females), with average age ranging from 48.6 (normal weight) to 52.1 years old (overweight). Eleven classifying algorithms-including logistic regression, naive Bayes, Radial Basis Function (RBF), local k-nearest neighbors (k-NN), classification via regression (CVR), random subspace, decision table, multiobjective evolutionary fuzzy classifier, random tree, J48, and multilayer perceptron-were implemented and evaluated, and they were compared with traditional logistic regression model estimates. Results: With physical activity and basic demographic status, of all methods analyzed, the random subspace classifier algorithm achieved the highest overall accuracy and area under the receiver operating characteristic (ROC) curve (AUC). The duration of vigorous-intensity activity in one week and the duration of moderate-intensity activity in one week were important attributes. In general, most algorithms showed similar performance. Logistic regression was middle-ranking in terms of overall accuracy, sensitivity, specificity, and AUC among all methods. Conclusions: Physical activity was an important factor in predicting weight status, with gender, age, and race/ethnicity being less but still essential factors associated with weight outcomes. Tailored intervention policies and programs should target the differences rooted in these demographic factors to curb the increase in the prevalence of obesity and reduce disparities among sub-demographic populations.",Not About Sufficiency
Machine Learning to the Rescue: ML-Assisted Framework for Equity-Driven Education,"Data Science has recently experienced a significant surge in producing undergraduate degree/certificate and course enrollments. This growth has resulted in straining program resources at many institutions and causing concern about how to most effectively respond to the rapidly growing demand and provide equal opportunities for all students to pursue their degree. The primary goal of this work is to provide an intelligent solution to automatically identify biases related to race and gender in the content of Data Science related courses such as Machine Learning (ML), Natural Language Processing (NLP), Data Mining, Data Analytics, and Knowledge Discovery just to name a few. While machine learning allows to build powerful predicting tools, it hasn't been used sufficiently to detect biases rooted in gender and race in educational material. Hence, using effective machine learning techniques we aim to enhance the success rate of undergraduate students in Data Science education and programs while promoting engagement through identifying and mitigating potential gender and racial biases. To this end, we propose a novel bias detection approach which uses a combination of gender identification and sentiment classification for characterization of gender and race in educational contents. Our proposed framework can help educators and course developers to mitigate race and gender biases in their course materials and create an equitable learning experience for students of minority and under-represented groups.",Not About Sufficiency
The social region - Beyond the territorial dynamics of the learning economy,"The purpose of this paper is to launch a debate on a broader meaning of the term 'innovation' and its significance for local and regional development. Innovation and related economic and social categories have been at the centre of policy discussions on the future of the European economy and society. Reflections on the innovative and learning region (Territorial Innovation Models; TIMs) have underpinned regional and local development policies. Yet dissatisfaction with the technologist and market-competition-led development concept of the TIMs is growing and today its shortcomings are well known. But to formulate an alternative based on a different ontology requires a multidimensional reflection on the pillars of territorial development. The first section briefly refers to the critical evaluations of the literature on regional innovation and the so-called Territorial Innovation Models. The second section returns to basic questions about the meaning of regional economic development and innovation. It puts forward community development based on social innovation as an alternative to market-led territorial development. The third section examines the consequences of the community ontology for the definition of a number of basic concepts. Categories such as capital, knowledge, learning, evolution, culture and so on receive a different meaning in a model where the economic is only one dimension of the overall dynamics of community development. The fourth section integrates the role of power relations and the articulation between various spatial scales and institutional settings into the community-development approach. The final section dwells on the consequences of this community-oriented territorial approach for contemporary research agendas on local and regional development policies and strategies.",Not About Sufficiency
Exploring Voice Acoustic Features Associated with Cognitive Status in Korean Speakers: A Preliminary Machine Learning Study,"Objective: To develop a non-invasive cognitive impairment detection system using speech data analysis, addressing the growing global dementia crisis and enabling accessible early screening through daily health monitoring. Methods: Speech data from 223 Korean patients were collected across eight tasks. Patients were classified based on Korean Mini-Mental State Examination scores. Four machine learning models were tested for three binary classification tasks. Voice acoustic features were extracted and analyzed. Results: The Deep Neural Network model performed best in two classification tasks, with Precision-Recall Area Under the Curve scores of 0.737 for severe vs. no impairment and 0.726 for mild vs. no impairment, while Random Forest achieved 0.715 for severe + mild vs. no impairment. Several acoustic features emerged as potentially important indicators, with DDA shimmer from the /i/ task and stdevF0 from the /puh-tuh-kuh/ task showing consistent patterns across classification tasks. Conclusions: This preliminary study suggests that certain acoustic features may be associated with cognitive status, though demographic factors significantly influence these relationships. Further research with demographically matched populations is needed to validate these findings.",Not About Sufficiency
A Stable and Scalable Digital Composite Neurocognitive Test for Early Dementia Screening Based on Machine Learning: Model Development and Validation Study,"Background: Dementia has become a major public health concern due to its heavy disease burden. Mild cognitive impairment (MCI) is a transitional stage between healthy aging and dementia. Early identification of MCI is an essential step in dementia prevention. Objective: Based on machine learning (ML) methods, this study aimed to develop and validate a stable and scalable panel of cognitive tests for the early detection of MCI and dementia based on the Chinese Neuropsychological Consensus Battery (CNCB) in the Chinese Neuropsychological Normative Project (CN-NORM) cohort. Methods: CN-NORM was a nationwide, multicenter study conducted in China with 871 participants, including an MCI group (n=327, 37.5%), a dementia group (n=186, 21.4%), and a cognitively normal (CN) group (n=358, 41.1%). We used the following 4 algorithms to select candidate variables: the F-score according to the SelectKBest method, the area under the curve (AUC) from logistic regression (LR), P values from the logit method, and backward stepwise elimination. Different models were constructed after considering the administration duration and complexity of combinations of various tests. Receiver operating characteristic curve and AUC metrics were used to evaluate the discriminative ability of the models via stratified sampling cross-validation and LR and support vector classification (SVC) algorithms. This model was further validated in the Alzheimer'sDisease Neuroimaging Initiative phase 3 (ADNI-3) cohort (N=743), which included 416 (56%) CN subjects, 237 (31.9%) patients with MCI, and 90 (12.1%) patients with dementia. Results: Except for social cognition, all other domains in the CNCB differed between the MCI and CN groups (P<.008). In feature selection results regarding discrimination between the MCI and CN groups, the Hopkins Verbal Learning Test-5 minutes Recall had the best performance, with the highest mean AUC of up to 0.80 (SD 0.02) and an F-score of up to 258.70. The scalability of model 5 (Hopkins Verbal Learning Test-5 minutes Recall and Trail Making Test-B) was the lowest. Model 5 achieved a higher level of discrimination than the Hong Kong Brief Cognitive test score in distinguishing between the MCI and CN groups (P<.05). Model 5 also provided the highest sensitivity of up to 0.82 (range 0.72-0.92) and 0.83 (range 0.75-0.91) according to LR and SVC, respectively. This model yielded a similar robust discriminative performance in the ADNI-3 cohort regarding differentiation between the MCI and CN groups, with a mean AUC of up to 0.81 (SD 0) according to both LR and SVC algorithms. Conclusions: We developed a stable and scalable composite neurocognitive test based on ML that could differentiate not only between patients with MCI and controls but also between patients with different stages of cognitive impairment. This composite neurocognitive test is a feasible and practical digital biomarker that can potentially be used in large-scale cognitive screening and intervention studies.",Not About Sufficiency
"Social Space Ratio: Calculating the Rate of Public Space Activities That Enhance Social Interaction on a Pedestrian Street in Karlstad, Sweden","William H. Whyte took on the challenge of assessing the amount of public space in a city based on its carrying capacity, pointing out that popular public spaces offer more room for social activities. However, the absence of qualitative characteristics makes this assessment even more challenging to implement. This study aims to find a method to gauge the carrying capacity of urban public spaces by calculating the social space ratio for pedestrian-only streets in Karlstad, Sweden, and quantifying this relationship. The social space ratio represents the proportion of public spaces that foster social interaction throughout their entire area. The method began by selecting the most relevant conceptual framework for social public spaces and then sought theory-based characteristics to assign to seven social activities on Karlstad's pedestrian-only streets. The authors performed a comprehensive search of the literature utilizing the PRISMA approach, gathering information from credible references, placemaking toolkits, transportation toolkits, and academic sources. This was performed to determine the weighting factors and effective social areas by evaluating these activities in terms of nine categories of the chosen framework: accessibility, traffic, social infrastructure, security, places to meet, senses and experience, architecture and aesthetics, development and maintenance, and control and programming. We devised a method to calculate the carrying capacity and social space ratio of Karlstad's pedestrian-only streets, resulting in a ratio of 0.38. The research led to the development of eight quality-control tools to analyze the seven social activities in public places. This innovative approach helps researchers and municipal planners evaluate the benefits and drawbacks of these spaces, contributing significantly to Swedish urban planning and enabling future studies to create a social area factor.",Not About Sufficiency
Machine Learning Approach for Automated Detection of Irregular Walking Surfaces for Walkability Assessment with Wearable Sensor,"The walkability of a neighborhood impacts public health and leads to economic and environmental benefits. The condition of sidewalks is a significant indicator of a walkable neighborhood as it supports and encourages pedestrian travel and physical activity. However, common sidewalk assessment practices are subjective, inefficient, and ineffective. Current alternate methods for objective and automated assessment of sidewalk surfaces do not consider pedestrians' physiological responses. We developed a novel classification framework for the detection of irregular walking surfaces that uses a machine learning approach to analyze gait parameters extracted from a single wearable accelerometer. We also identified the most suitable location for sensor placement. Experiments were conducted on 12 subjects walking on good and irregular walking surfaces with sensors attached at three different locations: right ankle, lower back, and back of the head. The most suitable location for sensor placement was at the ankle. Among the five classifiers trained with gait features from the ankle sensor, Support Vector Machine (SVM) was found to be the most effective model since it was the most robust to subject differences. The model's performance was improved with post-processing. This demonstrates that the SVM model trained with accelerometer-based gait features can be used as an objective tool for the assessment of sidewalk walking surface conditions.",Not About Sufficiency
Social Community Evolution Analysis and Visualization in Open Source Software Projects,"The importance of social communities around open-source software projects has been recognized. Despite that a lot of relevant research focusing on this topic, understanding the structures and dynamics of communities around open-source software projects remains a tedious and challenging task. As a result, an easily accessible and useful application that enables project developers to gain awareness of the status and development of the project communities is desirable. In this paper, we present MyCommunity, a web-based online application system to automatically extract communication-based community structures from social coding platforms such as GitHub. Based on the detected community structures, the system analyzes and visualizes the community evolution history of a project with a set of semantic-rich events, and quantify the strength of community evolution with respect to different events with a series of indexes. Built-in support to quantitative analysis and machine learning tasks based on the quantitative evolutionary events are provided. We demonstrate the usefulness of the system by presenting its ability in predicting project success or failure with the community evolution features. The results suggest the system achieves a prediction accuracy of 88.5% with commonly available machine learning models.",Not About Sufficiency
Shipping alliances under an unexpected shock: Effects on market volatility and social welfare,"This paper analyzes the impact of container shipping alliances on the market stability of prices and volumes in the event of an unexpected shock. Given two complementary or substitutable ports in which oligopolistic liners are operating, an alliance between complementary liners, referred to as ""global alliance"", enhances their strategic complementarity, which makes shocks more easily propagating among ports and increases volatility of the price and volumes of shipping services. Furthermore, due to incomplete adjustment of production facilities of shippers facing unexpected shocks, the shipping demand in the short term is less elastic than that in the long term. It is shown that the increased market volatility of global alliance causes a greater decline in shippers' benefit under a shock through i) shippers' inefficient investment and ii) an excessive price increase in transportation services, while the liners' profit may even increase owing to the second effect.",Not About Sufficiency
MRI for the diagnosis of limb girdle muscular dystrophies,"Purpose of reviewIn the last 30 years, there have many publications describing the pattern of muscle involvement of different neuromuscular diseases leading to an increase in the information available for diagnosis. A high degree of expertise is needed to remember all the patterns described. Some attempts to use artificial intelligence or analysing muscle MRIs have been developed. We review the main patterns of involvement in limb girdle muscular dystrophies (LGMDs) and summarize the strategies for using artificial intelligence tools in this field.Recent findingsThe most frequent LGMDs have a widely described pattern of muscle involvement; however, for those rarer diseases, there is still not too much information available. patients. Most of the articles still include only pelvic and lower limbs muscles, which provide an incomplete picture of the diseases. AI tools have efficiently demonstrated to predict diagnosis of a limited number of disease with high accuracy.SummaryMuscle MRI continues being a useful tool supporting the diagnosis of patients with LGMD and other neuromuscular diseases. However, the huge variety of patterns described makes their use in clinics a complicated task. Artificial intelligence tools are helping in that regard and there are already some accessible machine learning algorithms that can be used by the global medical community.",Not About Sufficiency
Application of Improved HSV Color Model for Early Gingivitis Detection using Image Processing and Machine Learning,"Early gingivitis detection is crucial for preventing severe oral health issues. This study pioneers an innovative approach that integrates image processing techniques with machine learning to enhance early detection capabilities. Focusing on the Hue (H) component of the HSV color space, a dataset comprising 417 intraoral images is meticulously curated. These images undergo targeted processing to extract gingivitis features, employing techniques such as HSV color detection and contour extraction. Various machine learning models are tested, with Convolutional Neural Networks (CNN) emerging as the most effective, achieving an accuracy of 81.68 percent. Leveraging these findings, a user-friendly mobile application is developed, seamlessly integrating the CNN model. This app enables individuals to capture gingival images and receive real-time predictions of gingivitis risk, empowering proactive oral health management. While underscoring the importance of professional diagnosis, this technology offers accessible and proactive oral healthcare solutions.",Not About Sufficiency
Autonomous Navigation of a Center-Articulated and Hydrostatic Transmission Rover using a Modified Pure Pursuit Algorithm in a Cotton Field,"This study proposes an algorithm that controls an autonomous, multi-purpose, center-articulated hydrostatic transmission rover to navigate along crop rows. This multi-purpose rover (MPR) is being developed to harvest undefoliated cotton to expand the harvest window to up to 50 days. The rover would harvest cotton in teams by performing several passes as the bolls become ready to harvest. We propose that a small robot could make cotton production more profitable for farmers and more accessible to owners of smaller plots of land who cannot afford large tractors and harvesting equipment. The rover was localized with a low-cost Real-Time Kinematic Global Navigation Satellite System (RTK-GNSS), encoders, and Inertial Measurement Unit (IMU)s for heading. Robot Operating System (ROS)-based software was developed to harness the sensor information, localize the rover, and execute path following controls. To test the localization and modified pure-pursuit path-following controls, first, GNSS waypoints were obtained by manually steering the rover over the rows followed by the rover autonomously driving over the rows. The results showed that the robot achieved a mean absolute error (MAE) of 0.04 m, 0.06 m, and 0.09 m for the first, second and third passes of the experiment, respectively. The robot achieved an MAE of 0.06 m. When turning at the end of the row, the MAE from the RTK-GNSS-generated path was 0.24 m. The turning errors were acceptable for the open field at the end of the row. Errors while driving down the row did damage the plants by moving close to the plants' stems, and these errors likely would not impede operations designed for the MPR. Therefore, the designed rover and control algorithms are good and can be used for cotton harvesting operations.",Not About Sufficiency
Paradigm or paradox? The 'cumbersome impasse' of the participatory turn in Brazilian urban planning,"The Brazilian urban reform movement expanded citizen participation in decision-making processes through a policy environment motivated by a right to the city (RTC), a collective development strategy for political transformation. Yet recent events evidence that social exclusion and spatial segregation remain dominant features of the Brazilian city. These contradictions have led planning scholars and practitioners to grapple with misalignment between the reform movement's paradigmatic goals and its paradoxical failures. We build upon this genre of thinking to assess critical areas of paradigm and paradox in Brazilian planning - insurgent urbanism, informality and knowledge - each of which is rooted in the lesser-understood concept of autogestao for improving the equity of land division through urban planning.(1) Although not all inclusive of the issues faced by Brazilian cities, these three categories were selected for best representing how Brazil's participatory turn established a range of paradigmatic and paradoxical conditions that can help us to understand cities in Brazil and beyond and might better leverage autogestao in the future.",Not About Sufficiency
Do Large Language Models Understand Us?,"Large language models (LLMs) represent a major advance in artificial intelligence and, in particular, toward the goal of human-like artificial general intelligence. It is sometimes claimed, though, that machine learning is ""just statistics,"" hence that, in this grander ambition, progress in AI is illusory. Here I take the contrary view that LLMs have a great deal to teach us about the nature of language, understanding, intelligence, sociality, and personhood. Specifically: statistics do amount to understanding, in any falsifiable sense. Furthermore, much of what we consider intelligence is inherently dialogic, hence social; it requires a theory of mind. Complex sequence learning and social interaction may be a sufficient basis for general intelligence, including theory of mind and consciousness. Since the interior state of another being can only be understood through interaction, no objective answer is possible to the question of when an ""it"" becomes a ""who,"" but for many people, neural nets running on computers are likely to cross this threshold in the very near future.",Not About Sufficiency
Expanding Tools for Investigating Neighborhood Indicators of Drug Use and Violence: Validation of the NIfETy for Virtual Street Observation,"A growing body of evidence suggests that characteristics of the neighborhood environment in urban areas significantly impact risk for drug use behavior and exposure to violent crime. Identifying areas of community need, prioritizing planning projects, and developing strategies for community improvement require inexpensive, easy to use, evidence-based tools to assess neighborhood disorder that can be used for a variety of research, urban planning, and community needs with an environmental justice frame. This study describes validation of the Neighborhood Inventory for Environmental Typology (NIfETy), a neighborhood environmental observational assessment tool designed to assess characteristics of the neighborhood environment related to violence, alcohol, and other drugs, for use with Google Street View (GSV). GSV data collection took place on a random sample of 350 blocks located throughout Baltimore City, Maryland, which had previously been assessed through in-person data collection. Inter-rater reliability metrics were strong for the majority of items (ICC >= 0.7), and items were highly correlated with in-person observations (r >= 0.6). Exploratory factor analysis and constrained factor analysis resulted in one, 14-item disorder scale with high internal consistency (alpha = 0.825) and acceptable fit indices (CFI = 0.982; RMSEA = 0.051). We further validated this disorder scale against locations of violent crimes, and we found that disorder score was significantly and positively associated with neighborhood crime (IRR = 1.221, 95% CI = (1.157, 1.288), p < 0.001). The NIfETy provides a valid, economical, and efficient tool for assessing modifiable neighborhood risk factors for drug use and violence prevention that can be employed for a variety of research, urban planning, and community needs.",Not About Sufficiency
iNAP: A Hybrid Approach for NonInvasive Anemia-Polycythemia Detection in the IoMT,"The paper presents a novel, self-sufficient, Internet of Medical Things-based model called iNAP to address the shortcomings of anemia and polycythemia detection. The proposed model captures eye and fingernail images using a smartphone camera and automatically extracts the conjunctiva and fingernails as the regions of interest. A novel algorithm extracts the dominant color by analyzing color spectroscopy of the extracted portions and accurately predicts blood hemoglobin level. A less than 11.5 gdL-1 value is categorized as anemia while a greater than 16.5 gdL-1 value as polycythemia. The model incorporates machine learning and image processing techniques allowing easy smartphone implementation. The model predicts blood hemoglobin to an accuracy of +/- 0.33 gdL-1, a bias of 0.2 gdL-1, and a sensitivity of 90% compared to clinically tested results on 99 participants. Furthermore, a novel brightness adjustment algorithm is developed, allowing robustness to a wide illumination range and the type of device used. The proposed IoMT framework allows virtual consultations between physicians and patients, as well as provides overall public health information. The model thereby establishes itself as an authentic and acceptable replacement for invasive and clinically-based hemoglobin tests by leveraging the feature of self-anemia and polycythemia diagnosis.",Not About Sufficiency
Smart and collaborative industrial IoT: A federated learning and data space approach,"Industry 4.0 has become a reality by fusing the Industrial Internet of Things (IIoT) and Artificial Intelligence (AI), providing huge opportunities in the way manufacturing companies operate. However, the adoption of this paradigm shift, particularly in the field of smart factories and production, is still in its infancy, suffering from various issues, such as the lack of high-quality data, data with high-class imbalance, or poor diversity leading to inaccurate AI models. However, data is severely fragmented across different silos owned by several parties for a range of reasons, such as compliance and legal concerns, preventing discovery and insight-driven IIoT innovation. Notably, valuable and even vital information often remains unutilized as the rise and adoption of AI and IoT in parallel with the concerns and challenges associated with privacy and security. This adversely influences interand intra-organization collaborative use of IIoT data. To tackle these challenges, this article leverages emerging multi-party technologies, privacy-enhancing techniques (e.g., Federated Learning), and AI approaches to present a holistic, decentralized architecture to form a foundation and cradle for a cross-company collaboration platform and a federated data space to tackle the creeping fragmented data landscape. Moreover, to evaluate the efficiency of the proposed reference model, a collaborative predictive diagnostics and maintenance case study is mapped to an edge-enabled IIoT architecture. Experimental results show the potential advantages of using the proposed approach for multi-party applications accelerating sovereign data sharing through Findable, Accessible, Interoperable, and Reusable (FAIR) principles.",Not About Sufficiency
Understanding cycling distance according to the prediction of the XGBoost and the interpretation of SHAP: A non-linear and interaction effect analysis,"Cycling benefits both the individual and society in terms of public health promotion, traffic congestion relief and vehicle emissions reduction. To better understand cycling behaviors, we analyze non-linear relationships and interaction effects between the built environment and cycling distance. Few studies explore the interaction ef-fects on cycling distance in which road network patterns interact with the demographic, trip, and other built environment characteristics to produce complex effects. Previous research has not examined the effect size or relative contribution of various variables have on cycling distance. Thus, this study adopts eXtreme Gradient Boosting (XGBoost) to examine the non-linear relationships among road network patterns, demographic, trip, and bike lane infrastructure, and other built environment characteristics and cycling distance, and employs SHapley Additive exPlanations (SHAP) to discover complex interaction effects on cycling distance, based on a bike travel survey in Xi'an, China. The results show that road network patterns have the greatest contribution; bike lane infrastructure is also quite important and has larger collective contributions than land use patterns and socioeconomics. Average geodesic distance and network betweenness centrality, two topological indices to describe road network structure, interact with bike lane infrastructure, land use and demographic characteristics to produce interaction effects on explaining cycling behaviors. When the average geodesic distance is smaller than 2.8 and the network betweenness centrality is smaller than 50%; as average geodesic distance increases, the network betweenness centrality has a positive effect on cycling distance. A network with a lower average geodesic distance, and with a higher intersection density makes cyclists ride a longer distance. A network with a higher value of average geodesic distance discourages cyclists to detour.",Not About Sufficiency
Spatiotemporal patterns of PM10 concentrations over China during 2005-2016: A satellite-based estimation using the random forests approach,"Background: Few studies have estimated historical exposures to PM10 at a national scale in China using satellite-based aerosol optical depth (ADD). Also, long-term trends have not been investigated. Objectives: In this study, daily concentrations of PM10 over China during the past 12 years were estimated with the most recent ground monitoring data, AOD, land use information, weather data and a machine learning approach. Methods: Daily measurements of PM10 during 2014-2016 were collected from 1479 sites in China. Two types of Moderate Resolution Imaging Spectroradiometer (MODIS) AOD data, land use information, and weather data were downloaded and merged. A random forests model (non-parametric machine learning algorithms) and two traditional regression models were developed and their predictive abilities were compared. The best model was applied to estimate daily concentrations of PM10 across China during 2005-2016 at 0.1 degrees (approximate to 10 km). Results: Cross-validation showed our random forests model explained 78% of daily variability of PM10 [root mean squared prediction error (RMSE) = 31.5 mu g/m(3)]. When aggregated into monthly and annual averages, the models captured 82% (RMSE = 19.3 mu g/m(3)) and 81% (RMSE = 14.4 mu g/m(3)) of the variability. The random forests model showed much higher predictive ability and lower bias than the other two regression models. Based on the predictions of random forests model, around one-third of China experienced with PM10 pollution exceeding Grade II National Ambient Air Quality Standard (>70 mu g/m(3)) in China during the past 12 years. The highest levels of estimated PM10 were present in the Taklamakan Desert of Xinjiang and Beijing-Tianjin metropolitan region, while the lowest were observed in Tibet, Yunnan and Hainan. Overall, the PM10 level in China peaked in 2006 and 2007, and declined since 2008. Conclusions: This is the first study to estimate historical PM10 pollution using satellite-based AOD data in China with random forests model. The results can be applied to investigate the long-term health effects of PM10 in China. (C) 2018 Elsevier Ltd. All rights reserved.",Not About Sufficiency
Toward Evidence-Based Urban Planning Integrating Quality Assessments in Literature Reviews,"Literature reviews can play a pivotal role in designing urban policies. Here we introduce two tools used by public health specialists to assess the quality of studies and quantify the evidence derived from them: the Risk of Bias Assessment (RoB) and Evaluation of Certainty of Evidence (ECE). The RoB scores articles on several domains (e.g., selection bias, study design, etc.) to provide an appraisal of how rigorous the study is, whereas the ECE tool provides a framework to clearly state how much certainty there is in the outcomes under study. Both tools can be used to enhance literature review articles in urban planning to better inform practitioners on how to best develop policies using a rigorous approach.",Not About Sufficiency
Assessing accessibility and crowding in urban green spaces: A comparative study of approaches,"Urban green spaces (UGS) are pivotal elements of the structure of urbanised areas, important for the well-being of the city inhabitants. Therefore, it is necessary to provide tools for determining the accessibility and crowdedness of the UGS. To this end, we assess how much space there is for potential UGS users in individual green spaces. It is pilot quantitative study limited to an area of one city, showing the crowdedness of UGS in two approaches. In both approaches, we assume an extreme event observed in the time of pandemic that all people in the accessible distance visit a UGS at the same time. In the approaches, we have combined parameters from literature and the idea that analysing UGS accessibility could be size sensitive to come up with methods for assessing residents' accessibility to green spaces with spatial analysis. Our study shows the variability of UGS accessibility throughout the city. The results indicate that to identify areas in cities with insufficient UGS, analyses using the commonly referenced 300-meter accessibility measure may be sufficient. However, for a more comprehensive assessment of UGS accessibility, it is necessary to conduct studies that are sensitive to UGS size and factor in the estimated population within it. The study tackles the UGS accessibility problems in a novel way of comparing two popular approaches and providing practical insights. The approaches may be useful for spatial planning practices to show the differences in local UGS accessibility and delimit areas with lower UGS accessibility. The findings may support the municipality in the practical task of monitoring the crowding of UGS in the city and facilitate decision making in the new UGS site selection process.",Not About Sufficiency
Building a Tool that Draws from the Collective Wisdom of the Internet to Help Users Respond Effectively to Anxiety-Related Questions,"Online anxiety support communities offer a valuable and accessible source of informational and emotional support for people around the world. However, effectively responding to posters' anxiety-related questions can be challenging for many users. We present ourwork in developing aweb-based tool that draws from previous question-response interactions and trusted online informational resources to help users rapidly produce high-quality responses to anxiety-related questions. We describe our efforts in four parts: 1) Creating a machine learning classifier to predict response quality, 2) developing and evaluating a computational question-answering system that learns from previous questions and responses on support forums, 3) developing and evaluating a system to suggest online resources for anxiety-related questions, and 4) interviewing support community moderators to inform further system design. We discuss how this tool might be integrated into online anxiety support communities and consider challenges with the tool's functionality and implementation. We also provide the dataset we used to train the system to provide opportunities for other researchers to build on this work.",Not About Sufficiency
World trade and investment agreements: Implications for public health,"The Canadian Public Health Association this year adopted a resolution calling for inclusion of social clauses in world trade and investment agreements. Social clauses refers to international declarations on such issues as human and labour rights or environmental protection. World trade and investment agreements are those monitored and enforced by the World Trade Organization, and a new Multilateral Agreement on Investments.",Not About Sufficiency
Systematic Literature Review of Various Neural Network Techniques for Sea Surface Temperature Prediction Using Remote Sensing Data,"The popularity of using various neural network models and deep learning-based models to predict environmental temperament is increasing due to their ability to comprehend and address complex systems. When examining oceans and marine systems, Sea Surface Temperature (SST) is a critical factor to consider in terms of its impact on species, water availability, and natural events such as droughts and floods. This evaluation supplements a detailed analysis of Machine Learning and Deep Learning models that have been employed for several decades to predict SST. The study highlights familiar data, data sources, performance metrics, and a range of models for SSTP (Sea Surface Temperature Prediction), including artificial neural networks, convolutional neural networks, recurrent neural networks, graph neural networks, and ensemble neural networks. The research also examines the latest trends in this field and suggests possible future research directions. The primary focus of this survey is to showcase the significant advancements made by numerous researchers, especially in the areas of DL techniques and ensemble methods for SSTP. It also provides an in-depth analysis of the most commonly used SST datasets along with their data generation source, record period, accessible resolution criteria, strengths, and weaknesses. The ultimate goal of this investigation is to provide a theoretical framework and ontology to support SSTP.",Not About Sufficiency
Soil parent material spatial modeling at high resolution from proximal sensing and machine learning: A pilot study,"Although parent material (PM) is one of the five soil formation factors providing key information on soil variability, the complexity of PM distributions and the difficulty of reaching PM in deep soils prevent its detailed assessment. Proximal sensors such as portable X-ray fluorescence (pXRF) spectrometer and magnetic susceptibility (MS) may be helpful in predicting soil PM in a more practical and accessible way. This pilot study aimed to create spatial PM predictive models for three distinct PMs (charnockite, mudstone, and alluvial sediments) of an experimental farm (Brazil) through random forest (RF) algorithm based on soil samples analyzed via pXRF and MS. Soils were sampled in A and B horizons following a regular-grid design covering the whole study area. The RF algorithm was calibrated to predict PMs using samples from the B horizon of soils with known PM. The prediction model was applied to the area for mapping PM across the whole farm. For validation, PM was identified at 15 different sites and compared with the predicted PM shown on the maps via overall accuracy, Kappa coefficient, producer's and user's accuracies. Al, Fe, Si, Ti, and MS proximal sensor data discriminated well among soils derived from charnockite, mudstone, and alluvial sediments. The map built based on B horizon data showed greater accuracy (overall accuracy = 0.93, Kappa coefficient = 0.85, user's accuracy = 0.92, and producer's accuracy = 0.97) than the map built from the model using A horizon samples (0.73, 0.48, 0.48, and 0.58). These results could represent alternative methods for reducing costs and accelerating the assessment of soil PM spatial variability, supporting soil mapping, and optimized agronomic and environmental decision-making.",Not About Sufficiency
Spatial characteristics and influential mechanism of the coupling coordination degree of urban accessibility and human development index in China,"To optimize the accessibility algorithm and quantify the potential relationship between human development index and traffic comprehensive accessibility system, this paper analyzed the spatial distribution pattern of urban accessibility and human development index of highway, railway, and aviation transportation systems using data on highway, railway, and aviation schedules based on GIS spatial analysis method. Furthermore, the coupling degree between human development index and accessibility and its influence mechanism on the city level in China were explored based on the super-efficiency data envelopment analysis model. Results showed as follows: (1) Spatial distribution of human development index from high to low was gradually changing from east to west. The spatial distribution of urban accessibility of the three traffic systems had an evident ""Hu Huanyong Line"" effect. (2) The coordination degree of urban accessibility and human development index spread in a ladder mode. High coordination of cities in North and East China and the central Yangtze River region forms the spatial distribution of urban development circles connecting an urban development belt. (3) Railway connectivity and control value were highly sensitive to human development index in cities where serious imbalance between accessibility and human development index. Highway connectivity and control value were highly sensitive to human development index in cities where high coordination between accessibility and human development index. The results would provide scientific references for spatial planning of transportation, economic, and social development of cities and coordinated development of urban agglomeration in China.",Not About Sufficiency
On Predicting Sociodemographic Traits and Emotions from Communications in Social Networks and Their Implications to Online Self-Disclosure,"Social media services such as Twitter and Facebook are virtual environments where people express their thoughts, emotions, and opinions and where they reveal themselves to their peers. We analyze a sample of 123,000 Twitter users and 25 million of their tweets to investigate the relation between the opinions and emotions that users express and their predicted psychodemographic traits. We show that the emotions that we express on online social networks reveal deep insights about ourselves. Our methodology is based on building machine learning models for inferring coarse-grained emotions and psychodemographic profiles from user-generated content. We examine several user attributes, including gender, income, political views, age, education, optimism, and life satisfaction. We correlate these predicted demographics with the emotional profiles emanating from user tweets, as captured by Ekman's emotion classification. We find that some users tend to express significantly more joy and significantly less sadness in their tweets, such as those predicted to be in a relationship, with children, or with a higher than average annual income or educational level. Users predicted to be women tend to be more opinionated, whereas those predicted to be men tend to be more neutral. Finally, users predicted to be younger and liberal tend to project more negative opinions and emotions. We discuss the implications of our findings to online privacy concerns and self-disclosure behavior.",Not About Sufficiency
Global Vaccine Hesitancy Segmentation: A Cross-European Approach,"Vaccine-preventable diseases are global mainly in a globalized world that is characterized by a continuous movement of people and goods across countries. Vaccine hesitancy, the reluctance or refusal to vaccinate despite the availability of vaccines, is rising worldwide. What if the problem of vaccine hesitancy could be most effectively managed when treated globally rather than on a national or regional basis? What if a global vaccine-hesitant segment exists and the differences among countries are not so significant? Based on the Global Marketing Strategy paradigm, this paper shows that seven different cross-European segments exist based on the beliefs, attitudes, and behaviors collected in 28 European countries. These pan-European segments are differentiable (people in those segments have similar characteristics that are visibly dissimilar from the ones in other segments) and actionable (organizations would be able to propose interventions to the hesitant segments based on their profiles). With segmentation being the starting point of many public health intervention strategies for avoiding vaccine-hesitancy, the results recommend moderating the full-adaptation strategy that follows the ""context matters"" principle suggested by several political and public health international organizations. Embracing a more standardized strategy will allow the development of better services and strategies that support and enable desirable vaccination behaviors.",Not About Sufficiency
Knowledge Portal Construction and Resources Integration for a Large Scale Hydropower Dam,"Based. on the research project entitled 'Data Collection and Decision Support System for Hydropower Dam Construction' sponsored by the National Development and Reform Commission of China, this paper puts forward a multilayer architecture and a support system for the knowledge portal utilizing web services and J2EE while offering a method for integrating existing resources. The purpose for this system is to help the China Gezhouba Group Corporation (CGGC) manage the construction of a large-scale hydropower dam. Through the interoperability in distributed environment, software integration technology and heterogeneous system integration, the interoperability of heterogeneous systems is realized by the enterprise application integration (EAI). In this project, an open and unified integration environment is provided by the analytic system (AS), making it possible for all kinds of software resources to be made readily accessible. Copyright (c) 2009 John Wiley & Sons, Ltd.",Not About Sufficiency
Artistic Stories: An Intersubjective Path for the Narrative and Bodily Construction of Emancipatory Identities in Compulsory Secondary Education in Catalonia,"This paper starts from the idea that the competence system in the Catalan educational system is based on a new episteme to reproduce identities in coherence with power. Based on this framework, we will aim to propose a pedagogical design that allows to develop emancipatory identities from memory and narration and corporeality through the performing arts during Compulsory Secondary Education (ESO, for its initials in Spanish). As for the methodology, our proposal will start from the potentialities of memory, which we will free from a veritative horizon to focus on how identity guides the exercise of memory, being aware that power derives from the fragility of memory to the fragility of identity. Thus, we will focus on how to build, through evaluation, spaces of intersubjectivity that can be beacons of resistance to the homogenizing impulses of collective memory. In order to make effective the inter-subjective spaces, we will use the artistic action research as a creative and reflexive process and, in addition, we will expose two thematic axes that allow us to question our identity. On the one hand, we will have the backdrop of the Second Republic, the Civil War, and Francoism. On the other hand, we will introduce a fictitious story that will lead us through the processes of standardization of the bodies, where the work The Plague by Robert Gerhard will appear as a constant element. Our contribution raises a new educational horizon by guiding us through the scenic arts in the construction of possible intersubjective plots where we can develop unexpected corporal and narrative identities",Not About Sufficiency
Anomaly Detection Models for SARS-CoV-2 Surveillance Based on Genome k-mers,"Since COVID-19 has brought great challenges to global public health governance, developing methods that track the evolution of the virus over the course of an epidemic or pandemic is useful for public health. This paper uses anomaly detection models to analyze SARS-CoV-2 virus genome k-mers to predict possible new critical variants in the collected samples. We used the sample data from Argentina, China and Portugal obtained from the Global Initiative on Sharing All Influenza Data (GISAID) to conduct multiple rounds of evaluation on several anomaly detection models, to verify the feasibility of this virus early warning and surveillance idea and find appropriate anomaly detection models for actual epidemic surveillance. Through multiple rounds of model testing, we found that the LUNAR (learnable unified neighborhood-based anomaly ranking) and LUNAR+LUNAR stacking model performed well in new critical variants detection. The results of simulated dynamic detection validate the feasibility of this approach, which can help efficiently monitor samples in local areas.",Not About Sufficiency
Impact of Features Reduction on Machine Learning Based Intrusion Detection Systems,"INTRODUCTION: As the use of the internet is increasing rapidly, cyber-attacks over user's personal data and network resources are on the rise. Due to the easily accessible cyber-attack tools, attacks on cyber resources are becoming common including Distributed Denial-of-Service (DDoS) attacks. Intruders are using enhanced techniques for executing DDoS attacks. OBJECTIVES: Machine Learning (ML) based classification modules integrated with Intrusion Detection System (IDS) has the potential to detect cyber-attacks. This research aims to study the performance of several machine learning algorithms, namely Naive Bayes, Decision Tree, Random Forest, and Support Vector Machine in classifying DDoS attacks from normal traffic. METHODS: The paper focuses on DDoS attacks identification for which multiclass dataset is being used including Smurf, SIDDoS, HTTP-Flood and UDP-Flood. balanced datasets are used for both training and testing purposes in order to obtain biased free results. four experimental scenarios are conducted in which each experiment contains a different set of reduced features. RESULTS: Result of each experiment is computed individually and the best algorithm among the four is highlighted by mean of its accuracy, detection rates and processing time required to build and test the classifiers. CONCLUSION: Based on all experimental results, it is found that Decision Tree algorithm has shown promising cumulative performances in terms of the metrics investigated.",Not About Sufficiency
Quantum machine learning with indefinite causal order,"In a conventional circuit for quantum machine learning, the quantum gates used to encode the input parameters and the variational parameters are constructed with a fixed order. The resulting output function, which can be expressed in the form of a restricted Fourier series, has limited flexibility in the distributions of its Fourier coefficients. This indicates that a fixed order of quantum gates can limit the performance of quantum machine learning. Building on this key insight (also elaborated with examples), we introduce indefinite causal order to quantum machine learning. Because the indefinite causal order of quantum gates allows for the superposition of different orders, the performance of quantum machine learning can be significantly enhanced. Considering that the current accessible quantum platforms only allow one to simulate a learning structure with a fixed order of quantum gates, we reform the existing simulation protocol to implement indefinite causal order and further demonstrate the positive impact of indefinite causal order on specific learning tasks. Our results offer useful insights into possible quantum effects in quantum machine learning.",Not About Sufficiency
Rapid enrichment and SERS differentiation of various bacteria in skin interstitial fluid by 4-MPBA-AuNPs-functionalized hydrogel microneedles,"Bacterial infection is a major threat to global public health, and can cause serious diseases such as bacterial skin infection and foodborne diseases. It is essential to develop a new method to rapidly diagnose clinical multiple bacterial infections and monitor food microbial contamination in production sites in real-time. In this work, we developed a 4-mercaptophenylboronic acid gold nanoparticles (4MPBA-AuNPs)-functionalized hydrogel microneedle (MPBA-H-MN) for bacteria detection in skin interstitial fluid. MPBA-H-MN could conveniently capture and enrich a variety of bacteria within 5 min. Surface enhanced Raman spectroscopy (SERS) detection was then performed and combined with machine learning technology to distinguish and identify a variety of bacteria. Overall, the capture efficiency of this method exceeded 50%. In the concentration range of 1 x 107 to 1 x 1010 colony-forming units/mL (CFU/mL), the corresponding SERS intensity showed a certain linear relationship with the bacterial concentration. Using random forest (RF)-based machine learning, bacteria were effectively distinguished with an accuracy of 97.87%. In addition, the harmless disposal of used MNs by photothermal ablation was convenient, environmentally friendly, and inexpensive. This technique provided a potential method for rapid and real-time diagnosis of multiple clinical bacterial infections and for monitoring microbial contamination of food in production sites. (c) 2024 The Author(s). Published by Elsevier B.V. on behalf of Xi'an Jiaotong University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",Not About Sufficiency
Prediction of developmental toxic effects of fine particulate matter (PM2.5) water-soluble components via machine learning through observation of PM2.5 from diverse urban areas,"The global health implications of fine particulate matter (PM2.5) underscore the imperative need for research into its toxicity and chemical composition. In this study, zebrafish embryos exposed to the water-soluble components of PM2.5 from two cities (Harbin and Hangzhou) with differences in air quality, underwent microscopic examination to identify primary target organs. The Harbin PM2.5 induced dose-dependent organ malformation in zebrafish, indicating a higher level of toxicity than that of the Hangzhou sample. Harbin PM2.5 led to severe deformities such as pericardial edema and a high mortality rate, while the Hangzhou sample exhibited hepatotoxicity, causing delayed yolk sac absorption. The experimental determination of PM2.5 constituents was followed by the application of four algorithms for predictive toxicological assessment. The random forest algorithm correctly predicted each of the effect classes and showed the best performance, suggesting that zebrafish malformation rates were strongly correlated with water-soluble components of PM2.5. Feature selection identified the water-soluble ions F- and Cl- and metallic elements Al, K, Mn, and Be as potential key components affecting zebrafish development. This study provides new insights into the developmental toxicity of PM2.5 and offers a new approach for predicting and exploring the health effects of PM2.5.",Not About Sufficiency
"Time, the other dimension of urban form: Measuring the relationship between urban density and accessibility to grocery shops in the 10-minute city","Compact settlements take advantage of economies of scale by sustaining a system of high-quality socio-economic services at close proximities. Urban density with a balanced mix of uses also benefits walking and cycling as mobility modes that provide sufficient access to urban amenities, especially when combined with effective public transport. Indeed, walking and cycling can decrease the use of cars for short-distance trips. From this perspective, urban density can help to reduce pollution, optimise energy consumption and decrease infrastructural expenditures while contributing to more attractive urban environments. These ideas have induced a new wave of time geography planning concepts, such as the '10-minute city', to enhance urban sustainability. For these concepts to move beyond visionary narratives, they must be expressed in specific empirical frameworks. Thus, the current research focuses on accessibility to grocery shops, as an essential urban service, in the Stavanger metropolitan area (Norway) using 10 minutes isochrones for walking and cycling. The study integrates open data, GIS network analyses, statistical regressions and bivariate representations of the results. The research estimates the level of serviceability by quantifying the number of shops that are accessible for each location and interrelates this estimation with spatial and population densities. The paper also presents a method to detect spatial inequalities by visualising over/under-serviced areas. This visualisation can become a tool to support strategies to rebalance such imbalances. Moreover, this study offers a practical approach towards the '10-minute city' concept, as it can be adjusted to different isochrones at different spatial scales. In general, this approach can serve both to analyse existing contexts and to model strategies to support sustainability policies, such as urban densification and the promotion of environmental-friendly transport.",About Sufficiency
Resilient and sustainable semiconductor supply chain network design under trade credit and uncertainty of supply and demand,"Supply Chain (SC) performance appraisal extends beyond economic evaluation to encompass environmental and social impacts, as well as resilience to supply and demand disruptions. Thus, SC network design decisions should simultaneously optimize all these performance metrics as a primary objective. However, existing studies optimized these objectives independently or in a limited scope, failing to comprehensively address the interconnected nature of these performance metrics. Therefore, this study proposes a mixed integer linear programming (MILP) model with four objectives to address a multi-tier resilient and sustainable semiconductor SC network design problem under trade credit, supply, and demand uncertainties. The model aims to determine the optimal number and location of various facilities, the amount to order from suppliers, economic production quantities, quantity allocation between SC entities, and ideal supplier selections. The proposed supply chain network contributes to the global effort to reduce carbon emissions and enhance the circular economy by considering carbon trading, recycling, and the proper disposal of end-of life products. Uncertainties in input parameters are addressed using a fuzzy programming approach, while lexicographic optimization and the epsilon-constraint method are used to solve the proposed model. This paper presents a numerical example to illustrate the applicability of the proposed model and provide managerial insights. The numerical analysis results show that provided an appropriate confidence level for the uncertain parameters, the sets of Pareto optimal solutions can be generated. And it also shows that the total SC cost increases with decreasing carbon emissions level and increasing job opportunities created. This research advances sustainable and resilient supply chain management practices while offering practical guidance for decision-makers in real-world contexts.",About Sufficiency
Sustainable marketing mix and supply chain integration: A systematic review and research agenda,"This study systematically reviews and synthesizes the extant academic literature on sustainable marketing mix through the lens of sustainable supply chain management. Following stringent inclusion/exclusion criteria for journal selection, this study reviews 972 research papers published through a qualitative systematic review and quantitative bibliometric analysis. It presents the top publication trends and overarching themes derived through bibliographic coupling. The analysis indicated that a sustainable supply chain is integral to the sustainable marketing mix. It is crucial to manage the various facets of the supply chain, such as enablers, barriers, and logistical practices, to contribute largely towards sustainable development goals. This is the pioneer study to uncover the components of a sustainable marketing mix and sustainable supply chain and integrate them into a conceptual framework to strengthen their theoretical roots. As emerging from the various thematic structures, this review provides insightful avenues for future research.",About Sufficiency
Enhancing urban sustainability through industrial synergy: A multidisciplinary framework for integrating sustainable industrial practices within urban settings - The case of Hamadan industrial city,"This study conducts an in-depth analysis of the interplay between sustainable industrial growth and integrated industrial urban environments, proposing a novel paradigm for urban production. The aim of this study is to combine sustainable industrial growth with its integration into urban environments, to establish a new and novel way to seamlessly integrate industrial processes within urban surroundings. This research utilizes a thorough approach, incorporating several disciplines, to examine Hamadan industrial city. It includes an extensive survey of existing literature, a comparative analysis based on empirical evidence, and a detailed evaluation of a specific example. This technique aims to address a significant research gap by providing a comprehensive framework that promotes sustainable industrial practices in urban environments. The scholarly contribution of this work is to manifest in its formulation of a pragmatic framework designed to provide urban planners and policymakers with strategies to harmonize industrial growth with urban sustainability imperatives. This article tackles the considerable challenges posed by escalating urbanization and industrialization. To conceive a framework for urban planning and industrial operations that emphasize environmental stewardship, resource efficiency, and social welfare is the primary purpose of this project. The study shows how industrial cities may revitalize economies, innovate industries, and solve urban problems including housing shortages and congestion. The importance of creative, collaborative, and policy-driven initiatives to build sustainable and resilient industrial-urban ecosystems in global industrial sustainability efforts is highlighted. The findings show that synergistic urban-industrial integration is needed for economic growth, environmental protection, and social welfare.",About Sufficiency
Impact of demand forecast information sharing on the decision of a green supply chain with government subsidy,"This paper investigates a green supply chain (GSC) consisting of one manufacturer and one retailer who possesses private demand forecast information. To promote green consumption, the government may provide subsidies to consumers. Within a dynamic game where the manufacturer serves as the leader and the retailer acts as the follower, three cases are examined: centralized decision, decentralized decision with and without demand forecast information sharing between the retailer and the manufacturer. We mainly examine the value of information sharing on the decisions of a GSC in the context of government subsidies for consumers. We find that: (i) demand forecast information sharing benefits the manufacturer but damages the retailer; (ii) if the predicted value is higher than the determinate part of the demand, the manufacturer is willing to choose a higher green degree of products in the case with information sharing compared with that without information sharing; otherwise, the manufacturer is willing to choose a lower green degree of products; (iii) a two-part tariff contract is appropriate to coordinate the GSC and it is effective in increasing the green degree of products; (iv) information sharing benefits the GSC if the green production efficiency is high enough; (v) the ex-ante social welfare always increases with information accuracy. Finally, numerical analyses are conducted to verify the above findings.",About Sufficiency
Is the 15-minute city within reach? Evaluating walking and cycling accessibility to grocery stores in Vancouver,"Leaders around the world have embraced the idea of a ""15-minute city"". This urban planning concept proposes a city where residents can meet their essential needs within a short walking or cycling trip from their home. Local access to grocery stores is a necessary component for cities to achieve the 15-minute city. This study aims to evaluate local accessibility to grocery stores by walking and cycling in the City of Vancouver. We used a cumulative opportunity measure to count the number of grocery stores available within a 15-minute walk and cycle from people's homes. To evaluate accessibility from the perspective of younger and older travellers, we considered different travel speeds. Our results show there is good accessibility to grocery stores when cycling, with less than 1% of the city's population not having a grocery store within a 15-minute cycle. When assuming a walking speed of an older pedestrian, around one-fifth of the population did not have access to a grocery store in their local area. The neighbourhoods that did not have a store within a 15-minute walk had higher proportions of children, older adults, and visible minorities, and lower rates of employment and post-secondary education attainment. In seeking to improve accessibility via walking and cycling, cities should prioritize grocery store locations and investments in pedestrian and cycling infrastructure to underserved neighbourhoods and populations.",About Sufficiency
Sustainable practices and firm competitiveness: an empirical analysis of the Saudi Arabian energy sector,"In pursuit of reducing dependence on oil, the Kingdom of Saudi Arabia has made significant efforts to advance sustainability under the strategic framework known as Saudi Vision. This initiative encompasses a range of activities aimed at promoting sustainability within the energy sector. This paper aims to analyze the existing sustainable practices of companies, employees, and the broader community in the supply chain. The objective is to evaluate current progress toward the sustainability goals of the Kingdom and assess the competitiveness of firms in the energy sector based on the effectiveness of sustainability management in their supply chain. Based on data collected from Saudi energy companies, structural equation modeling (SEM) techniques are utilized to investigate sustainable supply chain management (SSCM) practices and their influence on CP in Saudi Arabian energy companies. This study examines the influence of several practices on competitiveness (CP), including environmental practices (EMPs), social practices for employees (SPEs), social practices for the community (SPCs), operational practices (OPs), and supply chain integration practices (SCIs). The study incorporates five distinct independent variables, namely, EMP, SPE, SPC, OP, and SCI, where the dependent variable under investigation is denoted as CP. The findings from the SEM analysis show that three variables-SPE, SPC, and OP-demonstrate statistical significance, while EMP and SCI do not. These findings enhance the continuing academic discussion about balancing economic growth and environmental preservation through sustainable practices in the energy sector.",About Sufficiency
The future of sustainable supply chains: a novel tertiary-systematic methodology,"Purpose In recent years, economic, environmental and social sustainability has become one of the fastest-growing research fields. The number of primary and secondary papers addressing the triple bottom line is growing significantly, and the supply chain (SC) management discipline is in the same wave. Therefore, this paper aims to propose a novel tertiary systematic methodology to explore, aggregate, categorise and analyse the findings provided by secondary studies. Design/methodology/approach A novel tertiary systematic literature review approach, including 94 secondary studies, is proposed and used to analyse sustainable SC literature. The papers have been analysed using a research protocol, including descriptive and content analysis criteria. Findings This tertiary study does not only provide an overview of the literature on the topic of sustainability in SCs but also goes further, drawing up a categorisation of main research areas and research perspectives adopted by previous researchers. The paper also presents a rank of research gaps and an updated and a prioritised agenda. Originality/value This paper provides a novel interpretation of the research topics addressed by the secondary studies and presents a new classification of the literature gaps and their evolution. Finally, a dynamic research compass for both academicians and practitioners is presented.",About Sufficiency
A systematic review of aggregate production planning literature with an outlook for sustainability and circularity,"Aggregate production planning (APP) is the process of determining production, inventory, and labor levels to meet demand requirements over a planning window up to 1 year. As an emerging field, sustainable APP deals with the accommodation of environmental, economic as well as social sustainability criteria into the planning period which in turn can be achieved by making use of Circular Economy principles in real-world production activities. Different types of models and solution methods have been proposed by many researchers to study the APP problem with applications to a broad spectrum of industries. Yet, there are few studies in the literature that performs a comprehensive review of existing approaches. To the best of our knowledge, this is the first study that offers a systematic review of APP research output spanning the last 50 years. We review about 200 APP papers and systematically classify them with respect to some basic properties (year, type, publisher, publication name and citations) as well as more technical aspects such as model type/structure, solution method, handling of uncertainty and extra features. The main purpose is to find out the current research landscape to link APP with Sustainable Development using digital technologies. Limitations of existing research as well as recommendations for overcoming these shortcomings in the Industry 4.0 era are also discussed. Finally, more insight is given into the APP literature focusing on the sustainability and circularity concepts as well as an outlook for future research directions.",About Sufficiency
A smart inventory management system with medication demand dependencies in a hospital supply chain: A multi-agent reinforcement learning approach,"In light of the intense need for quality health and well-being, the healthcare industry must improve its operations by strengthening its service delivery and reducing overall costs. A smart inventory management system for managing medicines in a hospital supply chain (HSC) is one of the best solutions as its advantages ensure availability of affordable medicines as well as inventory cost reduction. Significant effort is made on inventory management from literature; however, smart inventory systems for HSC considering the various complexities and uniqueness of healthcare systems are limited. This paper attempts to fill this gap by developing a stochastic semi-Markov decision process model which is solved by a multi-agent reinforcement learning method. Data collected from a multispecialty hospital in India's eastern region serves to validate the proposed model. The results present the optimal order quantities and optimal inventory control policy for the HSC considering the uncertain medication demand at the hospital pharmacy and multiple point-of-care units, and demand dependencies among medicines prescribed to the patients admitted in the hospital. The study also has significant managerial implications for enhancing HSC's inventory performance, improving the healthcare service delivery, reducing healthcare cost, thereby ensuring the affordable, accessible, and quality healthcare to the society.",About Sufficiency
Reverse supply chain for end- of- life vehicles treatment: An in- depth content review,"Reverse supply chain (RSC) implementation is integral to waste management. Owing to economic, environmental and social benefits, RSC has been adopted to many industries and automotive is no exception. Multiple components and materials found in End-of-life vehicles (ELVs) have potential recycling value but also cause environmental damages if improperly treated. The number of ELVs is accelerating worldwide, especially in developing countries. Research on RSCs for ELVs treatment has been increasing. However, the literature still lacks a comprehensive understanding of this topic. The authors thus conducted an in-depth content review of publications on RSC of ELVs. The methodology applied PRISMA 2020 guideline combined with the model of Mayring (2001). Extensive search strings were utilized to retrieve articles from SCOPUS and Web of Science. After a rigid selection process and an intensive content assessment on 10,140 publications as raw materials, a total of 151 peer-reviewed papers was selected and carefully analyzed. The content categories were developed deductively and inductively. Major categories included research methodology overarching others: research themes, RSC/ELV types, stages in RSC, country specific and stakeholders. Modeling is the most commonly used among the papers, especially in RSC network design and planning. Mixed-integer linear programming prevails over other modeling approaches while AHP is predominant decision-making tool. Studies on legislations, network performance evaluation, and forecasting remain negligible. Categorization and significant achievements of articles are introduced. Future research is suggested upon identified research gaps. The review benefits academicians and practitioners in better comprehension of the field and promising research directions.",About Sufficiency
Barriers to green supply chain management: An emerging economy context,"Green supply chain management is attracting increasing attention as a way to decrease the adverse environmental effects of industries worldwide. However, considering the context of an emerging economy like Bangladesh, green supply chain management is still in its inception and has not been widely embraced in the textile industry, and therefore barriers hindering its adoption in emerging economy context demand a comprehensive investigation. This research reviews the viewpoints and hurdles in adopting green supply chain management practices in the context of the Bangladeshi textile industry. A questionnaire survey of Bangladeshi textile practitioners of operations and supply chain management division, having a sample size of thirty, was undertaken to identify the barriers, and a hierarchical cluster analysis technique was used in the detailed analysis of this data. Opinions were sought from experts on the significance of the resulting clusters, considering the relative importance of the barriers. Fifteen barriers to the adoption of green supply chain management were identified in the review of the literature, with these barriers then analyzed by using the data collected from Bangladeshi textile industry practitioners. The research indicates that the most important barrier is that there is low demand from customers and financial constraint resulting from short term little financial benefit to businesses, with lack of government regulations also a commonly faced barrier in adopting green supply chain initiatives. This study will provide valuables insights to practitioners and relevant policy makers about the barriers prevailing in the emerging economies towards the adoption of green supply chain management practices, which, in turn, can guide to undertake appropriate steps for alleviating those barriers. (C) 2019 Elsevier Ltd. All rights reserved.",About Sufficiency
Steering model identification and control design of autonomous ship: a complete experimental study,"Steering ship models are important for the study of autonomous ship manoeuverability and design of ship motion control system. It is always a difficult task to find the mathematical model by first principle as it needs prior knowledge of hydrodynamic derivatives. The input-output-based system identification theory can be used to establish system mathematical models. A solution is offered by developing a Wi-Fi-based self-propelled, autonomous system for a ship model with Internet of Things (IoT) capabilities to perform manoeuvering and seakeeping tests in indoor environment without any complex mechanical structure, viz. following bridge. The developed autonomous on-board system equipped with main computer, suitable electronics, sensors, data acquisition system and Wi-Fi-based communication system. The developed system offers a cost effective, modular and portable solution to perform hydrodynamic studies of different hull form without incorporating major changes in the system. The use of IoT makes the data accessible to a naval architecture in real-time to analyse the motion response of the ship in different wave conditions and enables to implement the digital twin to simulate the real field scenario. Input-output-based model identification experiments such as turning circle and zig-zag tests are conducted to estimate the first-order steering model parameters and is further extended to design and implementation of a classical proportional-derivative-based steering control. The design is described in this paper with details of implementation on a demonstration oceanographic coastal research vessel. It illustrates the excellent communication between shore station computer and the on-board system on a wire-free model with robust control and exhibiting all the motion behaviour and dynamic effects. Experiments performed in wave basin in different wave conditions validate the efficacy of the proffered method.",About Sufficiency
Investigation of rental business model for collaborative consumption-workwear garment renting in business-to-business scenario,"Increasing population and improving living standards have continued to amplify the consumption of textiles and apparel, which has created challenges in resource management and sustainable development. In this context, renting-based collaborative consumption business models (CCBMs) have received considerable attention in recent times. Consumption through renting allows the replacement of 'ownership' of a product to 'usership' which creates new opportunities for the companies as well as for the customers. For the development of renting based CCBMs or decision making in rental activities by the companies, one of the fundamental requirements is the baseline characterization of consumers' renting pattern - which includes general rent duration, renting frequency, inventory management, rental product movement, etc. - that acts as input from the operational aspect of a rental business. In this context, the paper aims at investigating the general renting characteristics for workwear garments through data analysis of about 5.4 million garment-rents to multiple business customers over 6 years. Further, the implications of the renting characteristics are investigated through discrete event simulations under various scenarios of inventory management by the customers. The findings from the study are anticipated to help the rental companies in operational and strategic decision-making.",About Sufficiency
Studies on the impact of road freight transport and alternative modes in Australia: a literature study,"The freight sector in Australia has been growing at an ever-increasing rate due to domestic and international demand for goods, commodities, and resources. Increased volume of traffic comes with increased greenhouse gas. Greenhouse gas impacts climate change and air pollution, increasing the risk of public health and safety. The European Union used Marco Polo to shift transit freight from road to sea, rail, and inland waterways to reduce the number of trucks on the road to lessen congestion, less pollution, and more reliable and efficient transport of goods. Fuel Tax Credit was similarly introduced in Australia to address some of these issues. It is now time to analyse the impact of these schemes. This paper is a systematic literature review using the Mixed Method Appraisal Tool and Critical Appraisal Skills Programme. Findings include using alternative modes of transport for long distances reduces carbon dioxide and the likelihood of using renewable fuels like electric and hydrogen fuel for trucks. However, research was limited on renewable fuels.",About Sufficiency
Designing a resilient and sustainable closed-loop supply chain network in copper industry,"Due to industrialization, copper demand has increased over the last decades. Recycling rate of copper is high and its scrap requires less energy than primary production, so sustainable closed-loop supply chain network design is considered a primary decision. Besides, the uneven distribution of copper has exaggerated the destructive effects of natural disasters such as earthquakes on mines. To the best of the authors' knowledge, there is no research about copper supply chain network design. In this paper, a copper network is designed and backup suppliers are used as a resilience strategy to reduce the effects of earthquakes on mining operations. Without backup model and with backup model are presented as multi-objective and are compared with each other. In each model, the economic objective is to maximize the supply chain profit; the environmental objective is to minimize water consumption and air pollutants; and the social objective is to maximize social desirability by considering security and unemployment rates. The models are formulated using mixed-integer linear programming and they are solved by epsilon-constraint and weighted sum methods. Results show that, with backup model increases the supply chain responsiveness. Also, the model is able to improve the economic and social performances of the supply chain. But in environmental aspect, it performs worse than without backup model. This is because the backup suppliers are added to the supply chain and their exploitation will create negative environmental effects. In addition, using copper scraps saves costs, energy and the consumption of this non-renewable metal. [GRAPHICS] .",About Sufficiency
Sensing Urban Manufacturing: From Conspicuous to Sensible Production,"Environmental destruction, social inequalities, geopolitical vulnerability-the limits of the long-time praised paradigm of post-industrial cities and globalised value chains are becoming evident, while calls for (re)localising production in cities are getting increasingly vocal. However, the material implications-i.e., where and in which form manufacturing should concretely take place in cities and the consequences on urban space and relations-are rarely addressed in debates on (re)industrialisation. In this article, we engage with the concept of conspicuous production by combining research on mixed-use zones with sensory methodologies. We focus on the multisensory dimension of urban manufacturing to interrogate the spatial possibilities for production in a small town in Switzerland. Together with a group of graduate students, we apply sensory methods to explore how production shapes urban sensescapes and how these sensescapes affect our relation to production. Our exploratory endeavour provides ideas of how sensory methods can be integrated into urban planning research and practice: we suggest that these methods, which necessarily emphasise subjective experience, can constitute powerful tools if they take into attentive consideration the local political and economic context, including the norms and power relations that shape individual perception. Our study sparks critical questions about conspicuous production and mixed-use zoning and tentatively advances the concept of sensible production: a production that not only is perceptible and can actively be engaged with, but that also shows good sense, makes sense, and focuses on what we need rather than on appearance.",About Sufficiency
The role of 3D printing and open design on adoption of socially sustainable supply chain innovation,"Social sustainability is a growing concern for supply chain management, but questionable practices endure due to insufficient stakeholder pressure on the market leading firms. Meanwhile small, socially oriented firms may have the will but lack the means to change dominant practices when entering a market. In this context 3D printing may offer a solution, by leveraging the voluntary effort of individuals through open design and distributed production. A system dynamics approach is applied to the case of a socially oriented mobile phone producer, whose fair supply chain practices may initially appeal only to a niche market. We examine how open design of 3D printed mobile phone accessories helps overcome size-related resource constraints, facilitate market growth and ultimately generate sufficient consumer demand to alter the market leaders' supply chain practices, in favour of social sustainability. Our findings demonstrate the interaction between availability of 3D printers, consumer attitudes to social sustainability and the market entry. We discuss the implications for technology management, namely that 3D printing can help overcome resource constraints to support the diffusion of socially sustainable supply chain innovation.",About Sufficiency
Fuzzy criteria programming approach for optimising the TBL performance of closed loop supply chain network design problem,"Immense concern for sustainability and increasing stakeholders' involvement has sparked tremendous interest towards designing optimal supply chain networks with significant economic, environmental, and social influence. Central to the idea, this study aims to design a closed loop supply chain (CLSC) network for an Indian laptop manufacturer. The network configuration, which involves a manufacturer, suppliers, third party logistics providers (forward and reverse), retailers, customers and a non-government organisation (NGO), is modelled as a mixed integer linear programming problem with fuzzy goals of minimising environmental impact and maximising net profit and social impact, subject to fuzzy demand and capacity constraints. Profit is generated from the sale of primary and secondary laptops, earned tax credits, and revenue sharing with reverse logistics providers. The environmental implications are investigated by measuring the carbon emitted due to activities of manufacturing, assembling, dismantling, fabrication, and transportation. The social dimension is quantified in terms of the number of jobs created, training hours, community service hours, and donations to NGO. The novelty of the model rests on its quantification of the three triple bottom line (TBL) indicators and on its use of AHP-TOPSIS for modelling the multi-criteria perspectives of the stakeholders. Numerical weights for the triple lines of sustainability are utilized. Further, a fuzzy multi-objective programming approach that integrates fuzzy set theory with goal programming techniques is utilised to yield properly efficient solutions to the multi-objective problem and to provide a trade-off set for conflicting objectives. The significance of the CLSC model is empirically established as a decision support tool for improving the TBL performance of a particular Indian laptop manufacturer. Practical and theoretical implications are derived from the result analysis, and a generalised quantitative closed-loop model can be effectively adapted by other electronic manufacturers to increase their competitiveness, profitability, and to improve their TBL.",About Sufficiency
A Pythagorean fuzzy AHP approach and its application to evaluate the enablers of sustainable supply chain innovation,"Sustainable supply chain innovation (SSCI) simultaneously creates value for social, environmental, and economic dimensions. It is a fact that innovation is very significant in achieving long-term sustainability in the supply chain. Literature analysis reveals that an in-depth investigation of SSCI implementation framework in developing nations is still limited. Hence, this research work investigates the sustainable supply chain innovation enablers (SSCIEs) to achieve sustainability in the supply chain using the Pythagorean fuzzy analytic hierarchy process (AHP) framework. The proposed framework is applied to an Indian manufacturing industry for the adoption of SSCIEs in its supply chain. This research identifies a total of thirty-three SSCIEs through extensive literature review and consultation with the decision-making panel. The identified enablers are then ranked based on their relative importance weight. The results reveal that 'collaboration and engagement of a diverse group of stakeholders (suppliers, local people, customers, environmental specialists, NGOs)' is the most critical SSCIE to achieve sustainability in the supply chain, followed by 'develop R&D labs in relationships with marketing, production, universities, and government institutes'. Sensitivity analysis is also performed in this study to check the robustness in the ranking of SSCIEs. This research helps the managers/policy-makers of the Indian manufacturing industry to identify and understand the most significant SSCIEs to achieve sustainability in the supply chain. The ranking of SSCIEs assists the managers to systematically focus on the most significant SSCIE and work towards its successful adoption. This study implies that the successful adoption of SSCIEs enables simultaneous improvement in social, environmental, and economic dimensions for Indian manufacturing industries. (C) 2020 Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved.",About Sufficiency
Antecedents and consequences of green supply chain management practices in Ghana's manufacturing sector,"PurposeThe study explores manufacturers' supply chain social capital (SCSC) (structural social capital and relational social capital) and supply chain performance, respectively, as drivers and outcome of green supply chain management practices (GSCMPs). Additionally, the study explores the direct relationship between SCSC and supply chain performance of manufacturers.Design/methodology/approachThe author develops and tests a research model grounded in the resource-based view and the natural resource-based view theory using survey data from 100 manufacturing firms operating in Ghana. The measurement model and hypothesized paths were examined using partial least squares structural equation modeling.FindingsThe findings revealed that relational social capital of manufacturers has a positive and significant relationship with supply chain performance, but structural social capital does not. Additionally, manufacturers' structural social capital and relational social capital were found to have a positive and significant relationship with GSCMPs. Lastly, GSCMPs were found to have a positive and significant relationship with supply chain performance.Originality/valueThe study contributes to the limited literature demonstrating the contribution of intangible relational assets, specifically SCSC, toward GSCMPs implementation.",About Sufficiency
Fifteen Years of Community Exposure to Heavy-Duty Emissions: Capturing Disparities over Space and Time,"Disparities in exposure to traffic-related air pollution have been widely reported. However, little work has been done to simultaneously assess the impact of various vehicle types on populations of different socioeconomic/ethnic backgrounds. In this study, we employed an extreme gradient-boosting approach to spatially distribute light-duty vehicle (LDV) and heavy-duty truck emissions across the city of Toronto from 2006 to 2020. We examined associations between these emissions and different marginalization indices across this time span. Despite a large decrease in traffic emissions, disparities in exposure to traffic-related air pollution persisted over time. Populations with high residential instability, high ethnic concentration, and high material deprivation were found to reside in regions with significantly higher truck and LDV emissions. In fact, the gap in exposure to traffic emissions between the most residentially unstable populations and the least residentially unstable populations worsened over time, with trucks being the larger contributor to these disparities. Our data also indicate that the number of trucks and truck emissions increased substantially between 2019 and 2020 whilst LDVs decreased. Our results suggest that improvements in vehicle emission technologies are not sufficient to tackle disparities in exposure to traffic-related air pollution.",About Sufficiency
Blockchain-Driven Food Supply Chains: A Systematic Review for Unexplored Opportunities,"This systematic review critically examines the diverse applications of Blockchain technology in the food supply chain and identifies areas where its potential remains underutilized. By analysing 60 Blockchain-based frameworks, the study highlights the most frequently employed drivers such as transparency, traceability, and security within food supply chains. Additionally, underexplored applications such as food donation and redistribution, supply chain financing, animal welfare, food waste management, and data analysis are identified, revealing opportunities for further innovation. The research employed NVivo 14 to analyze the extent of Blockchain's implementation in various food supply chain drivers, and the findings informed the development of a more diverse framework for Blockchain integration. Key insights demonstrate Blockchain's transformative potential, particularly in enhancing data integrity, trust, and operational efficiency through its immutable ledger and smart contracts, which streamline transactions, cut administrative costs, and reduce fraud. In terms of sustainability and safety, Blockchain improves traceability, accelerates safety responses, promotes environmental sustainability by tracking resource usage, and enhances humanitarian efforts with transparent, efficient resource distribution. Additionally, Blockchain facilitates food waste reduction by optimizing inventory and distribution, while ensuring surplus food reaches those in need. The study concludes by offering a roadmap for future research, pointing toward untapped dimensions of Blockchain's application in food traceability, sustainable supply chain management, and environmental & social impact. While the review provides a comprehensive understanding of Blockchain's current usage in food supply chains, the scope is limited by the systematic review process and specific inclusion criteria. This study serves as a foundation for exploring Blockchain's broader potential in shaping the future of food supply chains.",About Sufficiency
How Effective Is Reverse Cross-Docking and Carbon Policies in Controlling Carbon Emission from the Fashion Industry?,"The present consumer behavior is manipulated by ""fast fashion"", where purchasing new, trendy, affordable clothes is preferred over recycling old ones. This changing mannerism has escalated the GHG emissions from the fashion industry. Energy-intensive raw material production, preparation, and processing contribute to considerable emissions. The management of the returned goods from the primary market and further processing through the secondary outlets indulge in reverse logistics. In this paper, efforts are made to minimize the total cost and the carbon emission amount during the process of managing the return articles from the primary market to the reverse distribution center, further processing of the articles at the secondary outlet, and the return of the unsold or excess articles from the secondary outlet. Reverse cross-docking has been implemented in managing the return articles, while environmental concerns over GHG emissions have been addressed by investing in green technology under a strict carbon cap policy. In this research, return articles from the primary and secondary markets, rework of the returned articles, and disposal of the impaired returned articles have been considered. The carbon emission cost at all stages of transportation, rework, or disposal has also been incorporated into this model. A constrained mixed integer linear programming model is proposed and solved considering green investment. A numerical example has been formulated to investigate the effect of green technology on the total cost. The results portray that, though the total cost increases by nearly 2% due to investment in green technology, it ensures a considerable drop of 23% in the carbon emission amount. Also, the result is successful in establishing that reverse cross-docking is a better option than traditional warehousing in terms of minimizing the cost.",About Sufficiency
Building Trust in Supply Chains: The Blockchain-QR Code Advantage,"Supply chain management is the heart of the global economy, constantly evolving as new technologies emerge. This complexity requires detailed security and traceability, essential to track every stage from production to consumption, and to ensure that data is both secure and authentic. Errors in this process, particularly in the case of sensitive products such as foodstuffs or medicines, can have serious consequences. In this context, much research has explored the use of blockchain technology to improve supply chain security and traceability. However, these solutions have proved incomplete, leaving the way open to new challenges. Our paper sits at the frontier of this emerging research, proposing a novel approach that combines blockchain and QR codes to address the complex needs of supply chain management. We present an architecture that offers unrivalled traceability thanks to the use of unique QR codes for each product, immutably recorded on the blockchain. Each scanned QR code automatically triggers the recording of information on the blockchain via smart contracts, ensuring a precise history that will be accessible to all stakeholders in the chain. Our solution also offers exceptional granularity, enabling each product to be tracked individually, which previous approaches did not allow. This is crucial for ensuring product safety and compliance, boosting consumer confidence and reducing the risk of counterfeiting. Although three similar initiatives have been identified in the literature, our architecture stands out for its level of detail and explicitness, providing the bases. However, widespread adoption will require continued collaboration to standardise protocols and ensure global interoperability. Our future research will focus on optimising operational costs and developing advanced fraud detection and prevention mechanisms, thereby strengthening supply chain security on a global scale.",About Sufficiency
Multi-objective inexact optimization of the biomass supply chain from an energy-land-carbon nexus perspective,"It is attractive and advantageous to utilize marginal land to support regional biomass energy development and improve energy security. In this study, an integrated and comprehensive decision-making framework is proposed to support the strategic planning and tactical management of regional biomass supply networks from an energy- land-carbon nexus perspective. It combines a multi-objective fuzzy chance-constraint programming model with spatial analysis of marginal land and multi-criteria assessment of biorefinery sites. The model is verified through a case study of a major agricultural region, Shandong Province in China. Local agricultural residues remain a key feedstock for bioethanol production. The results highlight the importance of considering the multi-objective tradeoffs and the intricate resource and environmental nexus for stakeholders to achieve sustainability in real practice. A cost-minimization objective drives the construction of large-scale biomass plants to enhance efficiency. An emissions-minimization goal favors smaller, decentralized plants to reduce transport distances and improve local land use. Maximizing social welfare promotes marginal land development, creating more employment opportunities. Decision-makers' management goals, risk preferences, and external fluctuations significantly influence the spatial planning of bioethanol supply chains, marginal land utilization, and operational strategies. Overall, the proposed methodology offers decision-makers an effective tool for achieving optimal decisions while accounting for complex system interdependencies, conflicting objectives, and uncertainties.",About Sufficiency
"Sustainable international business model innovations for a globalizing circular economy: a review and synthesis, integrative framework, and opportunities for future research","The global imperative has increased in recent years for international firms to respond to major threats such as unintended environmental, social, and economic problems arising from ecological destruction, population growth, and economic activity. To respond to this confluence that has created an emerging existential crisis, we identify that a globalizing circular economy (CE) is required and subsequently define a new construct: sustainable international business model innovations. In doing so, we introduce circular inputs, sharing platforms, product as a service, product use extension, and resource recovery as business models that contain the potential to reply to these grand challenges. Based on CE principles, the innovations and designs introduced are contrasted with the traditional linear economic model and are presented as actionable standardization/adaptation alternatives for companies responding to differing informal and formal international institutions. Based on the theoretical underpinnings of the resource-based, dynamic capabilities, and international business model innovation perspectives, we introduce an integrative framework that is accompanied by a series of detailed research questions to provide future research opportunities for the domain. This conceptual approach holds that international resource design influences marketing capabilities adaptation which, in turn, impacts international performance and offers a foundation from which to build the literature.",About Sufficiency
Assessing the market niche of Eurasian rail freight in the belt and road era,"Purpose This paper presents an overview of the recent development of Eurasian rail freight in the Belt and Road era and further evaluates its service quality in terms of transit times and transport costs compared to other transport modes in containerised supply chains between Europe and China. Design/methodology/approach A trade-off model of transit time and transport costs based on quantitative data from primary and secondary sources is developed to demonstrate the market niche for Eurasian rail freight vis-a-vis the more established modes of transport of sea, air and sea/air. In a scenario analysis, further cargo attributes influencing modal choice are employed to show for which cargo type Eurasian rail freight service is favourable from a shipper's point of view. Findings At present, Eurasian rail freight is about 80% less expensive than air freight with only half of the transit time of conventional sea freight. Our scenario analysis further suggests that for shipping time-sensitive goods with lower cargo value ranging from $US1.23/kg to $US10.89/kg as well as goods with lower time sensitivity and higher value in a range of $US2.46/kg to $US21.78/kg, total logistics costs of Eurasian rail freight service rail is cheaper than all other modes of transport. Practical implications As an emerging competitive solution, Eurasian rail freight demonstrates to be an option beneficial in terms of transport cost, transit time, reliability and service availability, which offers a cost-efficient option enabling shippers to build up agile and more sustainable supply chains between China and Europe. Originality/value Our study firstly provides a comprehensive assessment of present Eurasian rail freight including a thorough comparison with alternative modes of transport from a shipper's point of view.",About Sufficiency
The role of social community in influencing purchase intention in live-streaming E-commerce: a social learning theory perspective,"PurposeLive-streaming platforms emphasize dynamic social interaction and fan engagement. Users integrate into the live-streaming community through continuous social learning activities, such as sending bullet comments, reviewing comments and interacting with celebrities. However, comprehensive research on the transactional intricacies of live-streaming e-commerce from the perspectives of community and learning is still lacking.Design/methodology/approachFocusing on the behavior characteristics of the reference group represented by online celebrities and fans in the live-streaming environment, this study utilized social learning as the theoretical basis to examine how reference groups affect consumer purchase intention through a series of intermediary effects. An empirical investigation and machine learning algorithms were utilized to explore and verify the hypothesized model.FindingsThe results show that: (1) reference groups' behavior positively stimulated social presence and enhanced consumer purchase intention through the chain-mediating effect of social presence and trust in online celebrities; (2) celebrity characteristics (professionalism, attractiveness and interactivity) positively impacted consumer trust; (3) in addition, machine learning algorithms substantiated that reference groups' behavior, social presence, trust and celebrity characteristics had a remarkably robust predictive effect on purchase intention.Originality/valueThese findings hold theoretical implications for understanding how the social community affects consumers' purchase intention in the live-streaming context and practical significance for marketing strategies toward live-streaming e-commerce.",About Sufficiency
"Discovering stop and parking behaviors of last mile delivery vehicles for urban areas based on not well conditioned GPS traces, expert knowledge and machine learning","Nowadays urban traffic is one of the most serious challenges for local authorities around the world. This challenge implies issues regarding health, energy, safety, environment and quality of life. Achieving a fair traffic outcome is a key priority for city traffic managers while maintaining city's attractiveness for citizens and travelers and those carrying out commercial activities. As a part of this challenge, delivery and similar companies are relevant stakeholders in optimizing their routes, deliveries, and operational services. It is in this respect that urban traffic and its regulations play a key role.The conditions in which urban traffic takes place thus have to be known and one way to deal with this is based on available and accessible data. In the IoT age, there could be huge amounts of available data generated by countless sensors and systems involved in our lives, cities, infrastructures, etc.... Taking advantage of such data, when the accessibility and quality is good enough, can help us to achieve the desired goals concerning the urban traffic and its consequences. However, in practice, the availability and access to such data is currently a very serious challenge.Following on with this, data generated from the ever-increasing number of sensors on board vehicles could be very useful, not only for checking the vehicle condition, but also to gain a better of the ""real-time""traffic situation or to discover traffic behaviors/patterns from the said data. In this work, a real case based study has been carried out gathering basic real GPS information regarding delivery vehicles in a city environment and OpenData to ""discover""where, when and how long time delivery vehicles use the regulated parking zones for loading/unloading in the city center. Based on Expert Rules, a stop detection criteria is defined and formulated to be applied to real cases in urban areas, focusing on city centers and Machine Learning techniques to discover stop and park behaviors on last mile deliveries in a real urban area.All this is used to plan traffic strategies and facilities which can permit better and more fluent services. On the other hand, the results provide invaluable knowledge support for the expert knowledge of mobility managers, while also supplying new ""findings""about the daily challenges, showing that machine learning techniques and other linked technologies are powerful tools for this challenge.",About Sufficiency
Optimal ship fleet size in an inland river corridor with seasonal change in waterway depth,"Inland river corridor provides a significant connection between inland cities and export seaports. Different from the sea waterway, the navigable ship size in an inland river corridor is restricted by the waterway depths in the plentiful and drought periods. This paper deals with the design issue of ship fleet size in an inland river corridor with seasonal change in waterway depth using a vertical structure model. The interactions among consignors, competitive carriers and a public port operator over plentiful and drought periods are considered in the proposed model, together with the impacts of seasonal waterway depth on the ship fleet size. The consignors aim to minimize transport cost by choosing transfer ports in each period. The competitive carriers are intended to maximize their own net profit by determining the shipping freight rates in each period and the ship fleet size (ship frequencies and composition of ship size) over a given time horizon. A public port operator, as an agent of the government, decides the port service charges in each period to maximize social welfare over the given time horizon. The effects of the ratio of drought period duration to plentiful period duration (duration ratio), the ratio of small ship size to big ship size (ship size ratio), and the actual cargo load of big ship in the drought period on the stakeholders and the system are investigated. The results show that: (i) the government always tends to subsidize the carrier at each port in the plentiful period for a large duration ratio, but to subsidize the carrier at its competitive port in the drought period for a small duration ratio and a low cargo load of a big ship at each port; and (ii) a single big (or small) ship type strategy will underestimate (or overestimate) the ship fleet size for a low cargo load of a big ship in the drought period. The method proposed in this paper can serve as a useful tool for the fleet size plan for the carriers and for port regulation for the government so as to achieve a ""win-win-win"" outcome among consignors, carriers and port operator.",About Sufficiency
GREEN LOGISTICS AS A PART OF CORPORATE SOCIAL RESPONSIBILITY IN THE PROCESS OF GLOBALIZATION,"To maintain a competitive advantage, companies' ability to adopt methods and techniques in the field of energy efficiency enhancement enhances customer loyalty and builds an image and also helps to demonstrate the energy and environmental responsibility of the business. The use of energy in individual areas of the enterprise is influenced by the different technologies, processes and products used energy sources and prices, political, economic and business situation. When we extend the environment to society as such, we can talk about sustainable logistics. By integrating energy use and sustainable green logistics, we can contribute to energy efficiency in production and green logistics. The article deals with the application of green logistics as a part of Corporate Social Responsibility in the ongoing process of globalization. The key to understanding the CSR concept is the organization's goal. A socially responsible company is not only aimed at maximizing profits, but its goals are based on the needs of the internal and external environment and include the social and environmental aspects of its activities. The aim of this article is to focus on the most important application areas of corporate social responsibility associated with green marketing in the globalization process, where social interactions are being interconnected, linking different sites in such a way that events occurring in geographically distant areas affect events occurring at home.",About Sufficiency
Deep learning-assisted real-time defect detection and closed-loop adjustment for additive manufacturing of continuous fiber-reinforced polymer composites,"Real-time defect detection and closed-loop adjustment of additive manufacturing (AM) are essential to ensure the quality of as-fabricated products, especially for carbon fiber reinforced polymer (CFRP) composites via AM. Machine learning is typically limited to the application of online monitoring of AM systems due to a lack of accurate and accessible databases. In this work, a system is developed for real-time identification of defective regions, and closed-loop adjustment of process parameters for robot-based CFRP AM is validated. The main novelty is the development of a deep learning model for defect detection, classification, and evaluation in realtime with high accuracy. The proposed method is able to identify two types of CFRP defects (i.e., misalignment and abrasion). The combined deep learning with geometric analysis of the level of misalignment is applied to quantify the severity of individual defects. A deep learning approach is successfully developed for the online detection of defects, and the defects are effectively controlled by closed-loop adjustment of process parameters, which is never achievable in any conventional methods of composite fabrication.",About Sufficiency
Implementation of circular economy principles during pre-construction stage: the case of Sri Lanka,"Purpose Despite the positive impacts of the construction sector on enhancing economic growth and ensuring societal well-being, its negative impacts on the environment from unsustainable resource consumption levels, emission of greenhouse gases (GHGs) and waste generation is monumental. Circular economy (CE) concept is identified globally as an avenue to address problems regarding adverse impacts of construction on the environment. This paper presents the principles of CE as an avenue for enhancing environmental sustainability during the pre-construction stage within Sri Lankan construction projects. Design/methodology/approach This research was approached through a qualitative research method. Data were collected through semi-structured interviews with subject matter experts. The number of experts were limited due to lack of experts with knowledge on the subject area in Sri Lanka. Data were analysed using content analysis. Findings Findings revealed a range of activities under each R principle of CE, that is, reduce, reuse, recycle, redesign, reclassification and renewability that could be implemented during the pre-construction stage, thereby providing a guide for construction professionals in implementing CE at the pre-construction stage. The need to expand knowledge on CE concepts within the Sri Lankan construction sector was recognized. Originality/value This study provides a qualitative in-depth perspective on how 6R principles of CE could be integrated to a construction project during the pre-construction stage. By adopting the proposed activities under CE principles, construction professionals can enhance the environmental sustainability of construction projects.",About Sufficiency
Industrial location in developing countries,"Despite a diminishing role in industrial countries, the manufacturing sector continues to be an engine of economic growth in most developing countries. This article surveys the evidence on the determinants of industry location in developing countries. It also employs micro data for India and Indonesia to illustrate recent spatial dynamics of manufacturing relocation within urban agglomerations. Both theory and empirical evidence suggest that agglomeration benefits, market access, and infrastructure endowments in large cities outweigh the costs of congestion, higher wages, and land prices. Despite this evidence, many countries have tried to encourage industrial firms to locate in secondary cities or other lagging areas. Cross-country evidence suggests that fiscal incentives to do so rarely succeed. They appear to influence business location decisions among comparable locations, but the result may be a negative-sum game between regions and inefficiently low tax rates, which prevent public goods from being funded at sufficiently high levels. Relocation tends to be within and between agglomerations rather than from large cities to smaller cities or lagging regions. Rather than provide subsidies and tax breaks, policymakers should focus on streamlining laws and regulations to make the business environment more attractive.",About Sufficiency
Investigating the impact of ICT on transport-based CO2 emissions: empirical evidence from a quantile cointegration regression analysis,"In the global initiative to leverage information and communication technologies (ICT) for reducing emissions, sub-Saharan Africa (SSA), a region of unique significance, has exhibited a delay in adopting ICT. This study aims to investigate the intricate relationship between ICT and carbon dioxide (CO2) emissions from transport in SSA. Employing the panel quantile autoregressive distributed lag (PQARDL) technique, the study analyzes panel data from 24 SSA nations spanning from 2000 to 2021. The results indicate that internet usage and fixed telephone subscriptions have a mitigating effect on CO2 emissions from transport across all quantiles in both the short and long run. However, mobile phone subscriptions contribute to CO2 emissions from transport across all quantiles. Additionally, the middle-income groups demonstrate negative relationships between ICT variables and emissions from transport, while the low-income group exhibits significant positive associations. These findings imply that ICT plays a pivotal role in mitigating transport-based emissions and reveal pronounced disparities in ICT adoption across various income groups within SSA, highlighting overarching underdevelopment in ICT infrastructure. Robustness checks employing a two-step system generalized method of moment (GMM) model reinforce our findings. The study provides policy recommendations, including the promotion of ICT infrastructure development, implementation of smart transportation solutions, and fostering public-private partnerships to address these challenges, shedding light on the path toward a greener and more sustainable transport ecosystem in SSA.",About Sufficiency
Consumers' Willingness to Pay for Agri-Food Products Delivered with Electric Vehicles in the Short Supply Chains,"This study aims at investigating the consumers' willingness to pay (WTP) for agri-food products delivered in the farmers markets with electric vehicles (EVs). The empirical analysis has been carried out by administering an online questionnaire to 273 consumers within a hypothetical context. A multiple price list has been provided in order to measure the price premium for 1 kg of oranges transported by EVs comparing with the ones transported by traditional vehicles. A Tobit regression model has been used for the data analysis in order to identify the most influential factors affecting the consumers' WTP. Results suggest that consumers' WTP is mainly influenced by environmental attitudes, knowledge and possibility of engaging in responsible purchasing, and the approval of friends and relatives (social norms). This study delivers insights into support both farmers' choice to invest in EVs and policymakers to develop policies promoting this transition process. The main limitations of this study are related to the use of a random sample of respondents and to the social desirability bias linked to self-reported questions.",About Sufficiency
A Matching Policy to Address ESG and Non-ESG Risks Impacted by a Relocation Policy in China's Chemical Industry,"China's chemical industry has faced severe environmental, social, and governance (ESG) issues, such as high safety and environmental accidents and risks. To address these issues and promote industrial upgrading, China's central government has issued a national relocation and improvement policy targeting its chemical industry. However, its countrywide policy implementation may also lead to other ESG risks during the relocation of chemical enterprises, namely industrial transfer. The typical ESG risks that appear to occur in developed eastern region provinces include a one-size-fits-all solution and unemployment, while less developed central and western region provinces may encounter pollution transfer, carbon leakage, environmental injustice, and health disparities. These ESG risks might overlap with other economic and financial (non-ESG) risks, like stranded assets, industry hollowing-out, and debt sustainability issue. These ESG and non-ESG risks could result from potential mismatches between chemical enterprises and chemical parks, categorized as mismatching errors explained by social-ecological systems, behavioral economics, and information economics. To better manage these risks, we propose an ESG matching policy comprising a national standardized ESG scoring and ranking system, a deferred acceptance mechanism, and a score announcement instrument. Such a policy innovation aims at achieving fair and efficient chemical enterprise-chemical park pairs, which would help manage both ESG and non-ESG risks and provide a just transition toolkit for China and other developing countries.",About Sufficiency
A Deep-Learning Approach to Detect and Classify Heavy-Duty Trucks in Satellite Images,"Heavy-duty trucks serve as the backbone of the supply chain and have a tremendous effect on the economy. However, they severely impact the environment and public health. This study presents a novel truck detection framework by combining satellite imagery with Geographic Information System (GIS)-based OpenStreetMap data to capture the distribution of heavy-duty trucks and shipping containers in both on-road and off-road locations with extensive spatial coverage. The framework involves modifying the CenterNet detection algorithm to detect randomly oriented trucks in satellite images and enhancing the model through ensembling with Mask RCNN, a segmentation-based algorithm. GIS information refines and improves the model's prediction results. Applied to part of Southern California, including the Port of Los Angeles and Long Beach, the framework helps assess the environmental impact of heavy-duty trucks in port-adjacent communities and understand truck density patterns along major freight corridors. This research has implications for policy, practice, and future research.",About Sufficiency
A comprehensive socio-economic assessment of EU climate policy pathways,"The European Green Deal aims to put the EU on track towards climate neutrality by 2050. One of the key elements is a more stringent greenhouse gas emission reduction target of 55% below 1990 levels by 2030. We analyse the socio-economic consequences of alternative policy pathways to reach that target, either relying more on regulatory standards, on carbon pricing, or a mix of both. We develop a modelling framework that captures macro-economic and sectoral impacts and closely aligns economic and energy system modelling. We further decompose aggregate labour market outcomes into skill and occupation types, and downscale representative household results to micro-level household data to evaluate the distributional effects across income groups with heterogeneous expenditure patterns. By combining models and datasets, our framework enables incorporating a high degree of technological detail while revealing socio-economic aspects of the transition that may go unnoticed in coarse, aggregate assessments. In particular, our results highlight the heterogeneous impacts of climate policy across sectors, worker skill types and income groups, which may require particular attention in climate policy design and corresponding complementary measures to ensure a fair transition to a low-carbon economy.",About Sufficiency
Impact of CR Express and intermodal freight transport competition on China-Europe Route: Emission and welfare implications,"China Railway Express (CR Express) has been fast developed, offering an alternative to transportation of the containerized cargo between China and Europe. This paper examines CO2 emissions and social welfare implications of the intermodal competition between the emerging CR Express and incumbent maritime shipping. An analytical model is developed, which shows that the implications depend in general on the relative emission intensities of the two transport modes on a per-trip basis. The entry of CR Express and the resultant intermodal competition, while likely improving welfare, will increase emissions unless CR Express is sufficiently emission efficient. Such impacts on emissions and welfare are enhanced when there are more CR Express operators, or when there are fewer shipping carriers. The competition among Chinese local governments in supporting their own CR Express services also strengthens the impacts. The analytical model is further calibrated with real market and operational data on the route between China's Yangtze River Delta and Central Europe. Our numerical simulations show that intermodal competition leads to more emissions unless CR Express can achieve an approximately 90% reduction in emission intensity and that overall welfare is nonetheless improved. Policy and managerial implications are discussed.",About Sufficiency
Interactive Simulation To Explore Urban Distribution Schemes,"The goal of building a shared vision about city logistics is often obstructed by the diversity of its actors (authorities, carriers, shippers, etc.). As an example, urban deliveries are sometimes not taken into account in the urban planning of cities or are simply misunderstood. This reality highlights the need for accessible and straightforward decision support tools in this field. In this work an interactive simulation is proposed as a playful tool to disseminate theoretical results from academia and lessons learned towards non-experts, who can easily interact with a model. The proposed interactive simulator aims at helping actors to broadly share and enlarge their perspective about a specific problem. That is, identifying the different objectives and constraints of other actors and converging towards a collective solution to the problem at hand. A trial was performed with 28 participants that explored and compared two different distribution schemes using interactive simulation. Positive feedback from the participants shows promising perspectives for this type of tool. (C) 2019 The Authors. Published by Elsevier Ltd.",About Sufficiency
Can blockchain technologies enhance environmental sustainable development goals performance in manufacturing firms? Potential mediation of green supply chain management practices,"The nexus between manufacturing firms and the green environment is unavoidable. It is merely due to the huge waste generation and greenhouse gas emissions resulting from large-scale production and complex supply chain management processes involved in manufacturing firms. Continuation of the traditional business practices during Industry 4.0 made it impossible for the manufacturing firm to commit to green environmental practices and comply with environmental SDGs. At odd, it requires novel business strategies to ensure compliance of manufacturing firms with environmental sustainable development goals (SDGs). Blockchain is an ideal technology for delivering green environmental-related information because it offers real-time, shareable, and entirely transparent data kept on an immutable ledger that is accessible to members of a network. These green environmental-related information received in real-time through blockchain technology help in the transformation of the traditional supply chain management practices into a more green supply chain management practice. This green transformation ensures business compliance with environmental SDGs and that compliance is shared with multiple stakeholders through blockchain technology in real-time, thus approving a positive mediating role of green supply chain management practices between blockchain technology and environmental SDGs performance. The proposed strategic framework through various propositions development will mainly offer policy insights toward (a) identifying novel antecedents of blockchain technology, (b) offering insights toward upgrading the traditional supply chain management framework into a green supply chain management framework, and (c) illuminating novel mediating role of green supply chain management practices between blockchain technology and environmental SDGs performance.",About Sufficiency
MIROR: A middleware software tool for interfacing mobile industrial robots with optimization routing algorithms,"Mobile robots are extensively used in flexible manufacturing systems and intra-logistics on a 24/7 basis to address demand fluctuations, increase productivity and promote environmental, economic and social sustainability. In this regard, routing algorithms are used to navigate mobile industrial robots for optimizing shop-floor logistics operations. However, open-source tools and software libraries implementing routing algorithms provide limited real-world relevance via indicatively neglecting the physical operating environment's characteristics and the kinematics of mobile robots. To this effect, this research designs, analyzes and develops MIROR, a middleware software tool that interfaces mobile industrial robots with route optimization algorithms to deliver a sophisticated testing environment that: (i) applies routing algorithms for the optimal logistics of mobile robots in customized industrial facility layouts; (ii) allows the comparison of the resulting algorithmic outcomes to inform about effective and efficient routing options; and (iii) creates optimal routing plans for informing the logistics operations of equivalent real-world mobile robots in industrial manufacturing settings. Specifically, MIROR integrates open-source Operations Research tools and the Robot Operating System to simulate and explore alternative logistics scenarios for mobile robots in 3D recreated industrial manufacturing facilities. To a greater extent, MIROR allows the transfer of the optimal routing code to equivalent real-world vehicles. Specifically, the implemented middleware tool handles variations of the Traveling Salesman Problem and generates feasible routes in complex facility layouts considering a variety of mobile robots. Academics and practitioners could benefit from the user-friendly interface of MIROR to generate realistic scheduling and routing options for the optimal planning of the logistics operations of mobile industrial robots in manufacturing environments. (C) 2021 The Authors. Published by Elsevier B.V.",About Sufficiency
Developing sustainable supply chain management: The interplay of institutional pressures and sustainability capabilities,"With growing concern about social impacts and environmental protection, sustainable supply chain management has become a focus of interest for business practitioners and academics. This study examines the driving forces of sustainable supply chain management practices. Drowning on institutional theory and the resource-based view, this study empirically examines the driving effects of institutional pressures (in terms of governance pressure, customer pressure, and competitive pressure) and internal sustainability capabilities (in terms of top management leadership and technical capability) and their interactions on sustainable supply chain management practices. Based on empirical analysis of data from 172 Chinese firms, the findings reveal that institutional pressures and sustainability capabilities simultaneously and jointly motivate the implementation of sustainable supply chain management practices. Interestingly, the interactions of technical capabilities with governance pressures and customer pressures significantly and negatively influence sustainable supply chain management practice adoption, while the interactions of top management leadership with competitive pressures positively affect the pursuit of sustainable supply chain management practices. (C) 2021 Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved.",About Sufficiency
Barriers to the adoption of integrated sustainable-green-lean-six sigma-agile manufacturing system (ISGLSAMS): a literature review,"Purpose Scarcity of resources, ecological imbalance, global warming, rising energy prices and the ever-changing need for variety have attracted the government and manufacturers for sustainable development of the industries. The integrated sustainable-green-lean-six sigma-agile manufacturing system (ISGLSAMS) provides a solid platform for meeting both the customers' variety needs and business sustainability requirements. Many organizations opted for ISGLSAMS, but still due to various barriers organizations are not able to fully implement ISGLSAMS. The purpose of this paper is to identify the barriers to the ISGLSAMS, so that a more sustainable industrial manufacturing system and industrial symbiosis can be developed. Design/methodology/approach A literature review, from the Web of Science and Google Scholar database, has been carried out to identify the various barriers to the implementation of ISGLSAMS in the entire value chain. A total of 168 research papers have been reviewed for identifying the ISGLSAMS barriers. Findings This paper elaborates the concept of the ISGLSAMS, its attributes and various barriers and contributes to a better understanding and successful implementation of ISGLSAMS to meet business' sustainability and market performance goals in the entire value chain. The paper also projects the future research framework and directions for the ISGLSAMS, integrated sustainable-green-lean-six sigma-agile (ISGLSA) product and ISGLSA supply and value chain. Practical implications The study contributes to a better understanding of ISGLSAMS' barriers. The government, stakeholders and policymakers may plan the policy, road map and strategies to overcome the ISGLSAMS' barriers. In-depth knowledge of subclauses of ISGLSAMS' barriers will help the practitioners to overcome the ISGLSAMS' barriers strategically. By overcoming the ISGLSAMS barriers, a more sustainable 7 Rs based market focused manufacturing system can be designed. This will also increase the opportunities to enhance the industrial ecology, industrial symbiosis and better recovery of the product, process and supply chain residual value. This will reduce the waste to the ecosystem. Originality/value This work has been carried out in search of a more sustainable manufacturing system, i.e. ISGLSAMS (which is 7 Rs based, i.e. 6 Rs of sustainability with 7th R, reconfiguration) to meet the customer variety needs along with sustainability in the ever-changing customer market. This study adds value to the practitioners to identify and prioritize the ISGLSAMS' industry-specific barriers and design the solution for the more sustainable development of (1) industries, (2) the industrial symbiosis system and (3) the ISGLSA product, process, system and supply value chain with minimum resource consumption and environmental impact. The research also contributes to the (a) ISGLSAMS (b) ISGLSA supply chain (c) reconfigurable, sustainable and modular products and (d) redesign, recovery and refurbishing of the product to increase the product life cycle.",About Sufficiency
A multi-objective optimization approach for designing a sustainable supply chain considering carbon emissions,"Greenhouse gases (GHGs) emission has been growing for decades, that has resulted in serious environmental problems. Many researches have focused on production planning in supply chains that are primarily attributed to objectives of cost minimization and profit maximization. There has been limited research concentrated on emission of GHGs (e.g. CO2\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$_{2}$$\end{document}, N2\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$_{2}$$\end{document}O, CH4\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$_{4}$$\end{document}) from industrial activities. The social concern of air pollution is the motivation towards the proposed study. In this study, we propose an optimization model for designing a sustainable supply chain management (SSCM) network that attempts to reduce carbon emission, improve social benefits and eventually improve customer experience. The optimization model is a multi objective mixed integer linear programming (MILP) formulation. The proposed model is a multi raw-material, multi-product, multi-period, multi-sites model that focuses on improving three dimensions of sustainability: viz. economic, environmental and social. Model performance is demonstrated through numerical experiments. The authors are currently working on solution algorithm for solving larger instances of the proposed model.",About Sufficiency
Does implementing trade-in and green technology together benefit the environment?,"In modern manufacturing operations, green technologies are becoming increasingly popular. Meanwhile, trade-in programs are widely implemented to boost sales and enhance product recycling, which would benefit the environment. It is widely observed that many companies implement the green technology (GT) and trade-in program together. However, whether this act is always beneficial to the environment is unclear. We hence build analytical models to address this issue. To conduct a comprehensive study, we follow the real-world practices and examine both the retailer collects (R-collect) and manufacturer collects (M-collect) scenarios in a supply chain. Our results show that the ""R-collect scheme with GT"" leads to the highest levels of supply chain profit and social welfare but more emissions may be generated. Besides, implementing GT does not always benefit the environment in both R-collect and M-collect schemes. Considering from the environment perspective, we interestingly show that governments should advocate the ""M-collect with GT"" and ""R-collect without GT"" schemes. Correspondingly, to motivate both the supply chain and consumers to accept the advocated strategies, we characterize the carbon tax and subsidy based ""carrot-and-stick"" policy. We further show in the extended analyses that our main results hold under competition, when the emission abatement cost takes different analytical forms, and when consumers are environmentally conscious. (c) 2021 Elsevier B.V. All rights reserved.",About Sufficiency
An innovative optimization model for sustainable hazardous waste reverse logistics network considering co-processing in cement kilns technology,"As the rapid growth in hazardous waste (HW) is threatening both the environment and public health, more efficient disposal methods are needed. This paper proposes an innovative HW reverse logistics network (RLN) considering advanced co-processing in cement kiln technology, establishing three disposal lines to reduce HW volumes, develop renewable products, reduce greenhouse gas emissions, and conserve resources. To optimise the HW disposal process, a multi-objective optimisation model comprehensively considering social, economic, and environmental dimensions is proposed. Then a numerical example is used to demonstrated the validity of the proposed model and a real-world case is conducted to demonstrate the practicality and efficiency of the proposed model in determining the most suitable hazardous waste disposal facilities, transportation routes and transportation quantities. Scenario analyses under different decision-maker attitudes towards the objectives are conducted while also considering the influence of governmental disposal subsidies. Finally, practical policy suggestions are provided to guide scientific and sustainable HW disposal. This study concludes that 1) different decision maker and disposal subsidy levels affect the optimal system decisions, 2) advanced co-processing in cement kiln technology increases the effectiveness of the HW-RLN in terms of economic and environmental perspectives, and 3) policy subsidy support could encourage the application of co-processing in cement kiln technology; a unit disposal subsidy of 160 CNY is considered suitable for ""technologically pessimistic"" decision-makers, while 200-220 CNY is considered suitable for ""technologically optimistic"" decision-makers. (C) 2022 Institution of Chemical Engineers. Published by Elsevier Ltd. All rights reserved.",About Sufficiency
Sustainability of transportation systems: air pollution scenarios,"Sustainability is a planning perspective that accounts for economic, social and environmental aims. The transportation of people and goods represents a special and unique challenge for sustainable development. Although transportation provides many economic and social benefits, the movement of people and goods can have significant environmental consequences that can, in turn, have social and economic repercussions. Sustainable transportation calls for ensuring that the environment is considered, along with economic and social considerations, in transportation decision making. In accordance with sustainable transportation, this paper presents an overview on air-pollution problems in a complex valley, located in the middle of Italy, and then discusses current and on-going new deals for reducing transport-related air pollution. This incorporates both ""how to control transport"" and ""how to improve air quality"". For this purpose a detailed evaluation of the changes in air pollution, following new road projects, is also provided. The analysis includes a comparison of the impacts in a scenario of planned new road network and in a scenario of an actual road network. The issues to be taken into account for an individual road network planning shall include air-quality improvement, economic development and safety improvement. The dispersion of road-traffic emissions has been modelled using the Gaussian Dispersion Model AERMOD-PRIME. AERMOD is a steady-state plume-dispersion model for assessment of pollutant concentrations from a variety of sources. For this assessment, AERMOD-PRIME has been used to calculate concentrations of CO (mg/m(3)) under prevailing weather conditions.",About Sufficiency
"The trilemma of innovation, logistics performance, and environmental quality in 25 topmost logistics countries: A quantile regression evidence","While the deployment of technological innovation was able to avert a devastating global supply chain fallout arising from the impact of ravaging COronaVIrus Disease 19 (COVID-19) pandemic, little is known about potential environmental cost of such achievement. The aim of this paper is to identify the determinants of logistics performance and investigate its empirical linkages with economic and environmental indicators. We built a macro-level dataset for the top 25 ranked logistics countries from 2007 to 2018, conducting a set of panel data tests on cross-sectional dependence, stationarity and cointegration, to provide preliminary insights. Empirical estimates from Fully Modified Ordinary Least Squares (FMOLS), Generalized Method of Moments (GMM), and Quantile Regression (QR) model suggest that technological innovation, Human Development Index (HDI), urbanization, and trade openness significantly boost logistic performance, whereas employment and Gross Fixed Capital Formation (GFCF) fail to respond in such a desirable path. In turn, an increase in the Logistic Performance Index (LPI) is found to worsen economic growth. Finally, LPI exhibits a large positive effect on carbon emissions, which is congruent with a strand of the literature highlighting that the modern supply chain is far from being decarbonized. Thus, this evidence further suggest that more global efforts should be geared to attain a sustainable logistics.",About Sufficiency
Issues and analysis of critical success factors for the sustainable initiatives in the supply chain during COVID-19 pandemic outbreak in India: A case study,"The coronavirus (COVID-19) pandemic has severely affected the supply chain all over. A major challenge for the supply chain (SC) is to address this disruption risk and bring sustainability to SC. The objective of this paper is to identify the stakeholders' requirements and critical success factors (CSFs) for the sustainability initiative in SC during this pandemic situation. Three potential stakeholders' requirements and a total of 16 critical success factors have been identified by taking inputs from experts and decision-makers. Further, these critical factors are analyzed and ranked based on a hybrid quality function deployment (QFD)-best-worst methodology (BWM). The QFD method has been used to identify the stakeholder' requirements. And, the BWM has been adopted to prioritize the CSFs. The scientific value of the study is the contribution of the framework model for the sustainable initiatives in the SC during the COVID-19 pandemic outbreak, identification of stakeholders' requirements and CSFs, and prioritizes these CSFs. The top three most critical success factors are found to be social distancing, emergency logistics systems, and emergency backup facilities. The proposed framework provides a madmap to operation and supply chain managers to come up with good solutions for sustainability initiatives in the supply chain during and after the pandemic outbreak.",About Sufficiency
"NEXUS AMONG SOCIAL SUSTAINABLE PERFORMANCE, SOCIAL SUSTAINABLE SUPPLY CHAIN PRACTICES AND OPERATION PERFORMANCE: DOES THE LONG-TERM ORIENTATION MATTER?","This study's prime objective is to explore the nexus among social sustainable performance, socially sustainable supply chain practices, and operational performance. Additionally, the study has also examined the moderating role of long-term orientation between social sustainability performance and socially sustainable supply chain practices. The results of the study are different from the previous researches in different ways. Operational performance cannot be improved through the implementation of basic social sustainable supply chain practices. Innovation can be suppressed by basic practices in this way. The operational outcomes can improve through the use of advanced social sustainable supply chain practices. Moreover, it influences the operational performance positively. The results are consistent with the previous research, which supports that change in product design and the process can result in improved learning for the organization. In this research, the specific practices and their influence created on the operational performance have been defined, which makes it different from the previous researches. The use of advanced SSSC practices needs coordination and improved strategic planning in the supply chain. The operational outcomes of the firm can improve as well. The study has contributed to long term orientation and social sustainable performance theory regarding the second research question. The use of advanced practices enables the positive influence of sustainability orientation on the firm's operational performance. It has been found that the use of advanced practices results in operational benefits but there is a need for further analysis from the aspect of basic practices. It is important to invest in advanced practices that may require more resources but result in improved operational performance. Disciplinary: Management Sciences (Sustainable Management). (C) 2020 INT TRANS J ENG MANAG SCI TECH.",About Sufficiency
Minnesota Intelligent Transportation Systems Laboratory [ITS Research Lab],"The Minnesota Intelligent Transportation Systems Laboratory (MnITSLab) was established in 1994 as part of the Center for Distributed Robotics (CDR) and the Department of Computer Science and Engineering in the College of Science and Engineering (Figure 1). The MnITSLab research, through collaboration with local state agencies, outside companies, and engineering faculty, focuses on applying sensing, computing, and automation that will improve driver, pedestrian, and worker safety as well as human well-being. The Center and its members conduct a broad array of research activities, spanning intelligent transportation systems, distributed robotics, machine learning, and computer vision. Currently, the CDR supports three full-time staff engineers, one research faculty member, two staff support personnel, eight graduate students, and five undergraduates, and it has more than US$2 million in annual funding.",About Sufficiency
Who Is Planning for Environmental Justice-and How?,"Problem, research strategy, and findingsEnvironmental justice (EJ) seeks to correct legacies of disproportionately burdening low-income and Black, Indigenous, and people of color (BIPOC) communities with environmental hazards that contribute to health inequalities. Federal and state policies increasingly require plans to assess and incorporate EJ principles. The current lack of accessible data and plan evaluation on EJ has been a barrier to policy setting and benchmarking. We created a framework for analyzing content across a large corpus of plans by using quantitative text analysis on 461 California city general plans, also known as comprehensive plans. To verify results and identify specific policies, we conducted content analysis on a subset of seven plans. Demonstrating the broad applicability of EJ principles in planning, policies spanned all required elements of general plans: housing, circulation, land use, health, safety, open space, air quality, and noise. We found that the most headway in EJ planning has been made in cities with a majority population of color and well before the 2018 California state mandate to address EJ. Policies were primarily focused on preventing adverse exposures as opposed to correcting for legacies of inequality. Further, although all policies address vulnerable populations and places, very few specifically address race or racism. Thus, EJ has been largely operationalized as health equity.Takeaway for practiceWe identified 628 EJ policies focused on vulnerable populations across the seven city plans included in content analysis. The smorgasbord of policy approaches provides fodder for cities across the United States to incorporate an EJ approach to planning. Gaps in focus areas reveal room for policy innovation (e.g., emphasis on language justice, formerly incarcerated individuals, and noise ordinance policing). We invite planners and community advocates to search across California's plans for EJ policy inspiration and to use the appendix of EJ policies cataloged in this research as a benchmark of city-level innovation.",About Sufficiency
Investigating the partnership approach in the EU Urban Agenda from the perspective of soft planning,"At the European level, several strategic documents concerned with spatial and urban development have been published during the last decades. While these documents are essential to communicate European ideas and objectives, they are often regarded least influential in practice due to their abstract nature, legally non-binding status and lack of allocated resources. Though these limitations apply to the EU Urban Agenda, this recently published policy paper introduces partnerships as a new implementation tool. The partnerships can be regarded as innovative in two respects: On the one hand, they involve new actors, most importantly cities, in European policy debates. On the other hand, they ensure the anchorage of the Urban Agenda with a broad range of actors at various spatial scales without challenging its legally non-binding status. The Urban Agenda can thus be understood as another example of the move towards soft European spatial planning and urban development. This article investigates the notion of partnership as a soft planning and governance tool within the Urban Agenda. Moreover, based on expert interviews, it presents early opinions and expectations of actors involved in the development of the Urban Agenda and the partnerships on affordable housing.",About Sufficiency
Air quality analysis and PM2.5 modelling using machine learning techniques: A study of Hyderabad city in India,"The rapid urbanization and industrialization in many parts of the world have made air pollution a global public health problem. A study conducted by the Swiss organization IQAir indicated that 22 of the top 30 most polluted cities in the world are in India. This creates the problem of air pollution, which is very relevant to India as well. Exposure to air pollutants has both acute (short-term) and chronic (long-term) impacts on health. Among the major air pollutants, particulate matter 2.5 (PM2.5) is the most harmful, and its long-term exposure can impair lung functions. Pollutant concentrations vary temporally and are dependent on the local meteorology and emissions at a given geographic location. PM2.5 forecasting models have the potential to develop strategies for evaluating and alerting the public regarding expected hazardous levels of air pollution. Accurate measurement and forecasting of pollutant concentrations are critical for assessing air quality and making informed strategic decisions. Recently, data-driven machine learning algorithms for PM2.5 forecasting have received a lot of attention. In this work, a spatio-temporal analysis of air quality was first performed for Hyderabad, indicating that average PM2.5 concentrations during the winter were 68% higher than those during the summer. Following that, PM2.5 modelling was done using three different techniques: multilinear regression, K-nearest neighbours (KNN), and histogram-based gradient boost (HGBoost). Among these, the HGBoost regression model, which used both pollution and meteorological data as inputs, outperformed the other two techniques. During testing, the model acquired an amazing R-2 value of 0.859, suggesting a significant connection with the actual data. Additionally, the model exhibited a minimum Mean Absolute Error (MAE) of 5.717 & mu;g/m(3) and a Root Mean Square Error (RMSE) of 7.647 & mu;g/m(3), further confirming its accuracy in predicting PM2.5 concentrations. In our investigation, we discovered that the HGBoost3 model beat other PM2.5 modelling models by having the lowest error and the highest R-2 value. This study made a substantial addition by incorporating the spatiotemporal relationship between air pollutants and meteorological variables in predicting air quality. This method has the potential to improve the creation of more precise air pollution forecast models.",About Sufficiency
Towards accelerating the adoption of zero emissions cargo handling technologies in California ports: Lessons learned from the case of the Ports of Los Angeles and Long Beach,"There is a growing global trend towards achieving net-zero emissions of greenhouse gases from port operations as consumers and governments become increasingly aware of the impacts of global climate change and ongoing environmental justice issues. However, the existing literature on the topic of zero-emissions ports is broad-based and focuses mainly on technology feasibility. Specifically, it provides insufficient guidance on how ports can best transition to zero emissions. To address this research gap, this review article examines the ongoing zero emissions planning documents and demonstration projects at the Ports of Los Angeles and Long Beach and presents the key lessons learned from those efforts to accelerate the adoption of zero-emissions cargo handling equipment in California. Increasingly, the Ports of Los Angeles and Long Beach are emerging as leaders with a sustainable circular strategy by implementing zero-emissions cargo handling equipment projects at a higher rate than any other ports in California, making these two ports an ideal case study to evaluate their endeavors to transition their operations to zero emissions. The findings of this review study suggest that transitioning to zero emissions cargo handling equipment across all California ports by 2035 is best achievable with: 1) a stronger collaboration between all key stakeholders, 2) the development of statewide regulations, 3) accelerated technology commercialization through increased demonstration projects and infrastructure standardization, 4) improved funding processes, 5) enhanced workforce training, and 6) increased resiliency planning. Although this evaluation and recommendations are largely California-specific, this article can be useful to policy-and decision makers around the world for transitioning to sustainable, net-zero emissions ports.",About Sufficiency
Geographic approaches to resolving environmental problems in search of the path to sustainability: The case of polluting plant relocation in China,"This paper applies a spatial perspective to environmental problems in search for the paths to sustainability, using polluting plant relocation in China as a case study. It examines how environmental improvement in one place may lead to environmental degradation in another place, how geographic concepts such as location, distance, spread and backwash effects, and land use models can help understand such phenomenon, and what the implications are for the environmental Kuznets curve (EKC) and development policies. Field research was conducted from 2006 to 2012 in Chinese cities of Beijing, Dalian, Shanghai, Guangzhou, Wuxi, Hangzhou, and Ningbo. It involved intensive site observations and in-depth interviews with government officials of environmental protection, economic development, and business recruitment, and grassroots environmentalists. The results indicate that environmental improvement in all these cities has led to environmental degradation in their suburbs and rural areas due to relocation of polluting plants. Environmental spread and backwash effects may help explain the severe intra-regional environmental and economic disparities and environmental injustice. The powerful and wealthy may achieve rapid economic growth and environmental recovery at the expenses of the powerless and poor, leading to environmental poverty and sustainability disparities. (C) 2013 Elsevier Ltd. All rights reserved.",About Sufficiency
Time-Of-Use pricing in an energy sustainable supply chain with government interventions: A game theory approach,"In this study, the game theory approach has been used to perform Time-Of-Use (TOU) pricing for renewable and conventional energy supply chains with government intervention to achieve sustainable development goals. Also, a Demand Response Program (DRP) based on TOU pricing has been implemented to improve the profits of power producers and the energy consumption pattern of end customers. Decision variables included the price of conventional and renewable energy during low- and high-load periods, tax rate, and subsidies. These variables are determined in three scenarios with the goals of maximizing government revenue, maximizing social welfare, minimizing environmental impacts, and under two-game structures of cooperative and Nash between the producers. The equilibrium solutions of each game for the three scenarios were obtained by backward induction. The results showed that decisions related to energy prices and tariffs play a major role in achieving the goals of sustainable development, profits of supply chain members, and success in meeting consumer demand. For all three scenarios, the government revenue function and the social welfare functions earn higher values in the Nash game than in the cooperative game, but the environmental impacts and the producers' profit function earn respectively lower and higher values in the cooperative game. (c) 2022 Published by Elsevier Ltd.",About Sufficiency
OPTIMIZATION OF ENGINE EFFICIENCY FOR DIESEL ENGINE EQUIPPED WITH EGR-VGT AND AFTERTREATMENT SYSTEMS,"Engine efficiency improvement is very critical for medium to heavy-duty vehicles to reduce Diesel fuel consumption and enhance U.S. energy security. The tradeoff between engine efficiency and NO, emissions is an intrinsic property that prevents modern Diesel engines, which are generally equipped with exhaust gas recirculation (EGR) and variable geometry turbocharger (VGT), from achieving the optimal engine efficiency while meeting the stringent NO, emission standards. The addition of urea-based selective catalytic reduction (SCR) systems to modern Diesel engine aftertreatment systems alleviate the burden of NOx emission control on Diesel engines, which in return creates extra freedom for optimizing Diesel engine efficiency. This paper proposes two model-based approaches to locate the optimal operating point of EGR and VGT in the air-path loop to maximize the indicated efficiency of turbocharged diesel engine. Simulation results demonstrated that the engine brake specific fuel consumption (BSFC) can be reduced by up to 1.6% through optimization of EGR and VGT, compared to a baseline EGR-VGT control which considers both NO, emissions and engine efficiency on engine side. The overall equivalent BSFCs are 1.8% higher with optimized EGR and VGT control than with the baseline control. In addition, the influence of reducing EGR valve opening on the non-minimum phase behavior of the air path loop is also analyzed Simulation results showed slightly stronger non-minimum phase behaviors when EGR is fully closed.",About Sufficiency
Social sustainability management in the apparel supply chains,"The apparel supply chain is an example of a complex global supply chain where sustainability issues are a concern and where no satisfactory answers have been achieved yet, especially in social aspects. The growth in importance of social sustainability represents a strategic change in the sector with the necessary involvement of different tiers and external stakeholders to mitigate the negative social impacts. In this paper, a qualitative analysis through the application of content analysis using NVivo software is carried out, first to identify the structure and the main entities in this supply chain, and then to understand the main drivers towards social sustainability management. Six global companies were analysed, considering their sustainability reports from 2014 to 2018. The findings showed that social sustainability is a part of strategic goals as policies and commitments, and several actions have been developed along the supply chain to promote human rights, labour conditions, social development, and product responsibility, with external stakeholders collaboration. Finally, this article contributes to understanding how social sustainability should be managed in the apparel sector in a global supply chain context. Furthermore, in order to enrich the knowledge on this field, this paper provides some insight throughout the definition of a roadmap for future research in the area. (C) 2020 Elsevier Ltd. All rights reserved.",About Sufficiency
Towards sustainable ships: Advancing energy efficiency of HVAC systems onboard through digital twin,"Digital twin technology has proven effective for optimizing the energy performance of various ship systems, particularly in power and energy management. However, its application has not extended to HVAC systems. This study introduces a novel framework based on a dynamic simulation tool that accounts for the transient thermal conditions of the ship. It enables the assessment of thermal loads, energy needs, and HVAC system design, along with evaluating of the energy performance across different configurations. The tool can function as a digital twin onboard ship equipped with IoT sensors, providing real-time energy performance monitoring and optimization. Beyond offering swift energy, economic, and environmental analyses, it also allows for real-time adjustments to enhance energy efficiency. The framework integrates a 3D geometrical model of the ship into an energy performance dynamic simulation model. It incorporates actual operating conditions, including real ship load profiles, hourly weather parameters, and the ship's location and orientation. A proof-of-concept demonstration of this framework is provided for a medium-sized ship, with a detailed geometrical model that includes 1595 thermal zones and accommodates 1750 passengers and 880 crew members. The reference scenario explores a typical HVAC system, while five different interventions and six different combinations are explored. The results highlight potential fuel savings ranging from 0.17% (-45.95 t/y) for setpoint temperature attenuation to 1.50% (-0.411 kt/y) for the combination of setpoint temperature reconfiguration with variable external fresh air based on usage-based criteria.",About Sufficiency
Opportunities and Strategies to Incorporate Ecosystem Services Knowledge and Decision Support Tools into Planning and Decision Making in Hawai'i,"Incorporating ecosystem services into management decisions is a promising means to link conservation and human well-being. Nonetheless, planning and management in Hawai'i, a state with highly valued natural capital, has yet to broadly utilize an ecosystem service approach. We conducted a stakeholder assessment, based on semi-structured interviews, with terrestrial (n = 26) and marine (n = 27) natural resource managers across the State of Hawai'i to understand the current use of ecosystem services (ES) knowledge and decision support tools and whether, how, and under what contexts, further development would potentially be useful. We found that ES knowledge and tools customized to Hawai'i could be useful for communication and outreach, justifying management decisions, and spatial planning. Greater incorporation of this approach is clearly desired and has a strong potential to contribute to more sustainable decision making and planning in Hawai'i and other oceanic island systems. However, the unique biophysical, socio-economic, and cultural context of Hawai'i, and other island systems, will require substantial adaptation of existing ES tools. Based on our findings, we identified four key opportunities for the use of ES knowledge and tools in Hawai'i: (1) linking native forest protection to watershed health; (2) supporting sustainable agriculture; (3) facilitating ridge-to-reef management; and (4) supporting statewide terrestrial and marine spatial planning. Given the interest expressed by natural resource managers, we envision broad adoption of ES knowledge and decision support tools if knowledge and tools are tailored to the Hawaiian context and coupled with adequate outreach and training.",About Sufficiency
Municipal solid waste management with recyclable potential in developing countries: Current scenario and future perspectives,"Municipal solid waste (MSW) is a complex challenge to be solved worldwide, intensified in developing countries since in addition to economic and environmental aspects there is also the social aspect represented by the collector individual. From an integrative bibliographic review this study longitudinally analyses the socio-productive inclusion of collectors in the municipal solid waste management (MSWM) at an international level. Aiming: (a) to identify relevant articles describing/studies on MSWM with social inclusion of scavengers around the world; (b) to analyse these articles in order to present its main characteristics; (c) to describe municipal solid waste management with recyclable potential (MSWRP) with socio-productive inclusion of collectors; (d) the study provides a more detailed summary of MSWRP management in some developing countries: Brazil, China, India, Indonesia, Mexico, Nigeria and Pakistan; and (e) to provide a framework indicating the future practices and perspectives of MSWRP. As result, this study presents the paradigms and future questions for an effective improvement of WSWM, such as: Professional Training of Scavengers, Cultural Issues, Empowerment of Scavenger, Recycling Cooperatives and Cooperativism, Policy Issues and Recycling Chain, Management Systems and Process Improvement, Quality of Services Provided, Circular Economy and Informal Sector, Health Quality and Safety at Work, Barriers and Solutions for the Inclusion of the Informal Sector, Environmental, Economic and Social Benefits.",About Sufficiency
An imperative to focus the plastic pollution problem on place-based solutions,"There is an increased focus on plastic pollution and the resultant harms in our oceans and on our shores at local, regional, and global scales. New technologies are being developed and trialed, multilateral agreements are coming into play, and the role of a circular economy is increasingly touted as the key to help solve the plastic pollution crisis. Simultaneously, we are witnessing the disruption of global supply chains from the COVID-19 pandemic, increased fuel prices and increased scope and scale of natural disasters. Individual countries are setting national targets and are developing national plans of action to combat plastic pollution. In this paper, we focus on Australia's National Plastics Plan as a case study of a national approach to addressing this transboundary issue. We discuss the Plan in relation to supply chains, the role of standards and best practices, and principles for a successful circular plastic economy. We explicitly consider the role of reverse logistics and regional approaches that could be developed and implemented within island nations. Overall, we argue for culturally appropriate, economically and environmentally place-based solutions as a necessary approach to help reduce plastic losses to the environment, acknowledging that plastics leakage to the environment is a social equity issue.",About Sufficiency
Net-zero emission vehicles shift and equitable ownership in low-income households and communities: why responsible and circularity business models are essential,"The UK Plan for a Green Industrial Revolution aims to ban petrol and diesel cars by 2030 and transition to electric vehicles (EVs). Current business models for EV ownership and the transition to net-net zero emissions are not working for households in the lowest income brackets. However, low-income communities bear the brunt of environmental and health illnesses from transport air pollution caused by those living in relatively more affluent areas. Importantly, achieving equitable EV ownership amongst low-and middle-income households and driving policy goals towards environmental injustice of air pollution and net-zero emissions would require responsible and circular business models. Such consumer-focused business models address an EV subscription via low-income household tax rebates, an EV battery value-chain circularity, locally-driven new battery technological development, including EV manufacturing tax rebates and socially innovative mechanisms. This brief communication emphasises that consumer-led business models following net-zero emission vehicles shift and decisions must ensure positive-sum outcomes. And must focus not only on profits and competitiveness but also on people, planet, prosperity and partnership co-benefits.",About Sufficiency
Inertial Navigation Meets Deep Learning: A Survey of Current Trends and Future Directions,"Inertial sensing is employed in a wide range of applications and platforms, from everyday devices such as smartphones to complex systems like autonomous vehicles. In recent years, the development of machine learning and deep learning techniques has significantly advanced the field of inertial sensing and sensor fusion, driven by the availability of efficient computing hardware and publicly accessible sensor data. These data-driven approaches primarily aim to enhance model-based inertial sensing algorithms. To foster further research on integrating deep learning with inertial navigation and sensor fusion, and to leverage their potential, this paper presents an indepth review of deep learning methods in the context of inertial sensing and sensor fusion. We explore learning techniques for calibration and denoising, as well as strategies for improving pure inertial navigation and sensor fusion by learning some of the fusion filter parameters. The reviewed approaches are categorized based on the operational environments of the vehicles-land, air, and sea. Additionally, we examine emerging trends and future directions in deep learning-based navigation, providing statistical insights into commonly used approaches.",About Sufficiency
Fuzzy Multi-Agent approach for monitoring SMEs sustainable SC under uncertainty,"In a context of Small and Medium Enterprises (SMEs), the concept of sustainability has become increasingly important within the decision making strategies. The competitiveness and uncertain environment has also a key role for decision makers to find best practices to achieve sustainability which is concerned with integrating environmental, social and economic goals across a supply chain process. This paper proposes an intelligent dynamic system driven by agents that integrates Fuzzy Logic (FL) and Multicriteria/multi-objective decision making (MC/MODM) methods to highlight the assessment process of the triple bottom line performance from a small scale ( SMEs) to a large scale supply chain under uncertainty. (C) 2019 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of the CENTERIS -International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies.",About Sufficiency
A network of circular economy villages: design guidelines for 21st century Garden Cities,"Purpose The purpose is to open the possibility for a research institute, perhaps in partnership with a local council and a major developer, to bring together skills necessary to prototype the CEV development model. Design/methodology/approach This paper advances the development of a hypothetical, systems-based approach to the design and development of smart rural villages - a network of circular economy villages (CEVs). The method is to assimilate visionary ideas from 20th century town planning literature related to decentralisation and the development of new towns in rural areas, identifying key design principles. The present trajectory of infrastructure design and emerging development models are then analysed to modernise the design principles for implementation in the 21st century. Findings The availability of localised, renewable energy micro-grids potentially makes CEVs feasible and affordable. The shift to remote work and movement of people to regional areas suggests that this may be a desirable development form. This can only be confirmed through the development of a pilot project as proof of concept. Originality/value The proposed CEV development model applies circular economy strategies to every aspect of the smart rural village development including financing, ownership, spatial planning, design and material selection.",About Sufficiency
Inland waterway efficiency through skipper collaboration and joint speed optimization,"We address the problem of minimizing the aggregated fuel consumption by the vessels in an inland waterway, e.g., a river, with a single lock. The fuel consumption of a vessel depends on its velocity and the slower it moves, the less fuel it consumes. Given entry times of the vessels into the waterway and the deadlines before which they need to leave the waterway, we start from the optimal velocities of the vessels that minimize their private fuel consumption, where we assume selfish behavior of the skippers. Presence of the lock and possible congestion on the waterway make the problem computationally challenging. First, we prove that in general, a Nash equilibrium might not exist, i.e., if there is no supervision on the vessels' velocities, there might not exist a strategy profile from which no vessel can unilaterally deviate to decrease its private fuel consumption. Next, we introduce simple supervision methods to guarantee the existence of a Nash equilibrium. Unfortunately, though a Nash equilibrium can be computed, the aggregated fuel consumption of such a stable solution can be high compared to the social optimum, where the total fuel consumption is minimized. Therefore, we propose a mechanism involving payments between vessels, guaranteeing a Nash equilibrium while minimizing the fuel consumption. This mechanism is studied for both the offline setting, where all information is known beforehand, and online setting, where we only know the entry time and deadline of a vessel when it enters the waterway. (C) 2020 Elsevier B.V. All rights reserved.",About Sufficiency
Multi-objective fuzzy robust optimization approach to sustainable closed-loop supply chain network design,"Nowadays, the importance of sustainable development has persuaded the supply chain managers to shift their network to sustainable supply. This research develops a multi-objective mathematical model to configure a Sustainable Closed-Loop Supply Chain (SCLSC) network for a water tank considering sustainability measures. The goals of the proposed model are optimizing financial, environmental and social impacts of the SCLSC. Generally, the uncertainty exists in configuring the SCLSC network problem according to the changes in the business environment (like transportation costs and demand). As a result, a Fuzzy Robust Optimization (FRO) is applied to cope with uncertainty in this study. Then, the proposed model is solved using the goal programming approach. The numerical results showed some insightful observations regarding planning and strategic decisions for the SCLSC network design. Finally, several sensitivity analyses are carried out on some important parameters, the changes in objective functions are investigated and the robustness of the proposed model is examined.",About Sufficiency
Assessment tool addresses implementation challenges of ecosystem-based management principles in marine spatial planning processes,"Ecosystem-based marine spatial planning is an approach to managing maritime activities while ensuring human well-being and biodiversity conservation as key pillars for sustainable development. Here, we use a comprehensive literature review and a co-development process with experts to build an assessment framework and tool that integrates the fundamental principles of an ecosystem approach to management and translates them into specific actions to be undertaken during planning processes. We illustrate the potential of this tool through the evaluation of two national marine spatial plans (Spain and France), in consultation with the representatives involved in their development and implementation. To ensure more coherent future planning, socio-ecological system evolution in a climate change scenario and the future marine space needs of maritime sectors should be considered, as well as improving the governance structure and knowledge of ecosystem processes. This framework provides a consistent and transparent assessment method for practitioners and competent authorities.",About Sufficiency
"Mapping Dynamic Urban Land Use Patterns with Crowdsourced Geo-Tagged Social Media (Sina-Weibo) and Commercial Points of Interest Collections in Beijing, China","In fast-growing cities, especially large cities in developing countries, land use types are changing rapidly, and different types of land use are mixed together. It is difficult to assess the land use types in these fast-growing cities in a timely and accurate way. To address this problem, this paper presents a multi-source data mining approach to study dynamic urban land use patterns. Spatiotemporal social media data reveal human activity patterns in different areas, social media text data reflects the topics discussed in different areas, and Points of Interest (POI) reflect the distribution of urban facilities in different regions. Human activity patterns, topics of discussion on social media, and the distribution of urban facilities in different regions were combined and analyzed to infer urban land use patterns. We collected 9.5 million geo-tagged Chinese social media (Sina-Weibo) messages from January 2014 to July 2014 in the urban core areas of Beijing and compared them with 385,792 commercial Points of Interest (POI) from Datatang (a Chinese digital data content provider). To estimate urban land use types and patterns in Beijing, a regular grid of 400 m x 400 m was created to divide the urban core areas into 18,492 cells. By analyzing the temporal frequency trends of social media messages within each cell using K-means clustering algorithm, we identified seven types of land use clusters in Beijing: residential areas, university dormitories, commercial areas, work areas, transportation hubs, and two types of mixed land use areas. Text mining, word clouds, and the distribution analysis of POI were used to verify the estimated land use types successfully. This study can help urban planners create up-to-date land use patterns in an economic way and help us better understand dynamic human activity patterns in a city.",About Sufficiency
Development of an Assessment Model for Industry 4.0: Industry 4.0-MM,"The application of new technologies in the manufacturing environment is ushering a new era referred to as the 4th industrial revolution, and this digital transformation appeals to companies due to various competitive advantages it provides. Accordingly, there is a fundamental need for assisting companies in the transition to Industry 4.0 technologies/practices, and guiding them for improving their capabilities in a standardized, objective, and repeatable way. Maturity Models (MMs) aim to assist organizations by providing comprehensive guidance. Therefore, the literature is reviewed systematically with the aim of identifying existing studies related to MMs proposed in the context of Industry 4.0. Seven identified MMs are analyzed by comparing their characteristics of scope, purpose, completeness, clearness, and objectivity. It is concluded that none of them satisfies all expected criteria. In order to satisfy the need for a structured Industry 4.0 assessment/maturity model, SPICE-based Industry 4.0-MM is proposed in this study. Industry 4.0-MM has a holistic approach consisting of the assessment of process transformation, application management, data governance, asset management, and organizational alignment areas. The aim is to create a common base for performing an assessment of the establishment of Industry 4.0 technologies, and to guide companies towards achieving a higher maturity stage in order to maximize the economic benefits of Industry 4.0. Hence, Industry 4.0-MM provides standardization in continuous benchmarking and improvement of businesses in the manufacturing industry.",About Sufficiency
Contextual relationships in Juran's quality principles for business sustainable growth undercircular economy perspective: a decision support system approach,"Circular economy and sustainable growth are closely linked as they both aim to reconcile economic development with environmental considerations. The circular economy provides a framework and set of principles for achieving sustainable growth. By adopting circular practices, such as resource efficiency, recycling, and product life extension, economic activities can become more sustainable and contribute to long-term growth. In today's world, consumers have high expectations for companies to be accountable for the environmental and social impact of their products and services. The implementation of Total Quality Management (TQM) in manufacturing can contribute to sustainable growth by enhancing quality, reducing waste, and increasing efficiency. This study proposes a novel methodological framework to establish a comprehensive association between Juran's ten quality principles using a mixed-method sequential approach with an integrated Machine Learning Group Decision-Making (MLGDM) and Interpretive Structural Modeling (ISM)-DEMATEL approach. The framework involves using the MLGDM approach to select the optimal number of experts to develop contextual relationships among the principles. This framework is designed to address the challenge of determining the appropriate number of experts to involve in the decision-making process. Involving too few experts can limit the generalizability of the results, while having too many experts can lead to a high degree of inconsistency and make it challenging to reach a consensus. The MLGDM portion of our framework provides a systematic approach to overcome this challenge and helps supply chain managers and academicians implement quality practices in their organizations. Moreover, although several studies have explored the implementation of TQM practices, there is still a lack of a systematic framework that can fully incorporate Juran's quality principles. To fill this gap, the ISM-DEMATEL approach was then used to explore the causal relationships between these principles. Practitioners from the industry were asked to identify contextual associations among variables, which facilitated a better understanding of these principles. Our results suggest that ""Build awareness,"" ""Set goals for improvement,"" and ""Provide training"" are strategic requirements for successful TQM implementation, while ""Carry out projects to solve problems,"" ""Organize to reach the goals,"" and ""Keep score"" are tactical requirements. Furthermore, ""Communicate results,"" ""Report progress,"" ""Give recognition,"" and ""Maintain momentum"" are operational necessities for TQM implementation. The present study represents a significant step forward in giving a new direction to Juran's ten quality principles and providing a holistic picture to decision-makers.",About Sufficiency
"A Multi-objective Multi-Supplier Sustainable Supply Chain with Deteriorating Products, Case of Cut Flowers","There is a growing concern, in the last decade, about the environment and social footprint of business operations. The subject of supply chain sustainability, simultaneously considering economic, environment and/or social values, has gained attention in the academia and from the industry. Particularly for deteriorating and seasonal products, such as fresh produce, the issues of timely supply and disposal of the deteriorated products are of high concerns. This research develops a new replenishment policy in a centralized sustainable supply chain for the deteriorating items. The model considers inventory and transportation costs, as well as the environmental and social impacts, with several transportation vehicles producing various pollution and greenhouse gas levels. Some variables are uncertain as the end-customer demand, partial backordered ratio and deterioration rate. We also consider backorders, quantity discount prices, non-linear holding costs, multiple transportation-route options, and uncertain demand. We consider the deterioration of in-stock inventory. and the deterioration during transportation. The best transportation routes and vehicles, and inventory policy are determined by finding a balance between financial, environmental and social criteria. We develop a linear multi-objective mathematical model and present a numerical example to demonstrate its applicability and effectiveness. (C) 2016, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.",About Sufficiency
Public-Private Partnership and the Development of Transport Infrastructure in Poland: The Analysis of Critical Success Factors,"Efficient transport is an important determinant of the integration of the EU countries. TEN-T has been created by the European Commission to ensure the mobility of citizens, as well as fast and safe trading. However, the European transport infrastructure is characterized by large variations on the level of development that occurs in the East and West of the EU. In Poland, the infrastructure is technically unadjusted to the current transport needs and preferences. Modernization and development of infrastructure requires large capital expenditures. At the same time the public finance sector is overloaded, and the budget deficit is common. European countries and governments around the world are more likely to engage the private sector to finance infrastructure. The usual form of involvement of private investors is the public-private partnership (PPP). However, not all projects are successful. Achieving tangible benefits of PPP requires the creation of appropriate conditions conducive to its implementation. The aim of this study is to identify and analyze critical success factors for PPP transport infrastructure projects in Poland. The research literature proves that there are many factors and they relate to various aspects and stages of the PPP implementation. The factors are classified in four groups: financial and economic, political and legal, technical, and social. The ones that have significant impact on transport infrastructure include: stable macroeconomic situation of the country, political support and commitment of the government, legal system, well-organized public consulting agencies, financial market availability, credible and experienced private investors. In Poland, the most important factors that are necessary for success in the implementation of PPP projects are: support of the government and governmental institutions, legal framework and regulations and the existence of adequate public advisory bodies. For the past 20 years, these conditions have not been fully met and formed a barrier to the development of the partnership. The consequence of the lack of public advisory body is limited knowledge on PPP in public administration as well as in banks, which in the case of the latter, limits the availability of the financial market for private partners.",About Sufficiency
Digital Twin and Reinforcement Learning-Based Resilient Production Control for Micro Smart Factory,"To achieve efficient personalized production at an affordable cost, a modular manufacturing system (MMS) can be utilized. MMS enables restructuring of its configuration to accommodate product changes and is thus an efficient solution to reduce the costs involved in personalized production. A micro smart factory (MSF) is an MMS with heterogeneous production processes to enable personalized production. Similar to MMS, MSF also enables the restructuring of production configuration; additionally, it comprises cyber-physical production systems (CPPSs) that help achieve resilience. However, MSFs need to overcome performance hurdles with respect to production control. Therefore, this paper proposes a digital twin (DT) and reinforcement learning (RL)-based production control method. This method replaces the existing dispatching rule in the type and instance phases of the MSF. In this method, the RL policy network is learned and evaluated by coordination between DT and RL. The DT provides virtual event logs that include states, actions, and rewards to support learning. These virtual event logs are returned based on vertical integration with the MSF. As a result, the proposed method provides a resilient solution to the CPPS architectural framework and achieves appropriate actions to the dynamic situation of MSF. Additionally, applying DT with RL helps decide what-next/where-next in the production cycle. Moreover, the proposed concept can be extended to various manufacturing domains because the priority rule concept is frequently applied.",About Sufficiency
Evaluation of Third Party Logistics Providers Considering Social Sustainability,"In this paper, we investigated a third-party logistics (3PL) provider evaluation problem considering social sustainability as one of the important evaluation criteria. Since the 3PL service is mainly dependent on both transport vehicles and employees, managing them from the viewpoint of social sustainability has become a critical issue. Thus, 3PL providers need to be concerned about not only the service price or quality but also issues related to social sustainability. In line with this trend, we defined the social sustainability of 3PL providers and related evaluation criteria. In addition, a fuzzy analytic hierarchy process (AHP) was used as a main evaluation framework to help decision-makers determine the relative importance of each criteria or alternative using linguistic terms. To show the feasibility of the proposed criteria and evaluation framework, we presented the illustrative example based on a real-world case. The results showed that the proposed approach could be a good alternative to conduct evaluations, and the related sensitivity analysis, considering social sustainability.",About Sufficiency
Transportation of electric vehicle lithium-ion batteries at end-of-life: A literature review,"The market for electric vehicles (EVs) has grown exponentially over the past decade, largely driven by ambitious sales targets in regions around the world. At end-of-life (EoL), these batteries must be managed properly to maximize reuse and recycling, which requires an efficient and safe collection and transportation system; however, the logistics of transporting EoL batteries are rarely examined in depth in scholarly research. In this paper, we conduct a critical review of the peer-reviewed literature on EV traction battery reuse and recycling to assess how transportation is represented. We find that among 60 studies identified, 70% mentioned collection and transportation as a challenge to battery reuse or recycling, and 63% identified a need for policy or further research related to collection and transportation. Among 17 papers that focus on cost, estimates for transportation costs vary widely among studies, from more than five dollars per kg to less than 30 cents, representing, on average, 41% of the total cost of recycling. Studies that examined the environmental impact of EoL transportation suggest it contributes 1-3.5% of life cycle GHG emissions for a recycled battery. In response to the limited and highly variable treatment of battery EoL transportation, the literature review is followed by contextual information about the United States, including the regulatory framework and existing network for EoL batteries. Recommendations for future study include place-specific research on optimal facility siting that considers both existing and projected infrastructure, and which reflects costs and environmental and social impacts at local scales.",About Sufficiency
"Product green degree, service free-riding, strategic price difference in a dual-channel supply chain based on dynamic game","In this paper, we study a green product dual-channel supply chain with a manufacturer and a retailer. The service free-riding is considered, and the channel price difference is also introduced as a strategic tool to adjust the channel conflict. In centralized and decentralized models, we give the optimal decisions of the supply chain members and analyse the effects of the service free-riding and the price difference on the related decisions. In order to coordinate the dual-channel green supply chain, we also design a two-part tariff contract coordination model, which allows both the manufacturer and the retailer to obtain a win-win result. From the analytical and numerical results, we find that the proper service free-riding degree and price difference are beneficial to the decision makers and the greening of supply chain. And in the centralized model, when the degree of service free-riding is small, the increase of the channel price difference can stimulate the manufacturer to provide the lower price product with lower service and green degree, otherwise, it is the opposite. Besides, the result also shows that, except for consumers, the centralized model always performs better than the decentralized model in the profit and social welfare aspects.",About Sufficiency
Assessing the effects of sustainable supply chain management practices on operational performance: the role of business regulatory compliance and corporate sustainability culture,"PurposeThis study examines the mediating and moderating effects of business regulatory compliance (BRC) on the association between sustainable supply chain management practices (SSCMP) and operational performance (PERFOP), and how corporate sustainability culture (CSC) serves as a boundary condition to BRC.Design/methodology/approachThis research draws data from 245 firms operating in multiple industries in Ghana. Ordinary Least Square (OLS) was employed to test the direct effects, while Hayes Process Macros was employed to test the indirect and conditional effects among the study variables using a structural equation modelling approach.FindingsThe results showed that SSCMP has a direct positive effect on PERFOP. The study further revealed that BRC mediates the relationship between SSCMP and PERFOP. This study found that BRC negatively moderates the association between SSCMP and PERFOP, suggesting that high levels of BRC generate unintended adverse effect on the SSCMP- PERFOP link. However, the results revealed that CSC serves as a boundary condition to BRC.Originality/valueTo the best of our knowledge, this is the first study that emphasizes how the resource-based view and regulatory focus theory interact to explain how different degrees of CSC and BRC impact SSCMP performance outcomes. This study advances research in the sustainability literature, in response to calls for further research in this domain. This study draws decision-makers attention on the need to make sustainability practices an integral part of corporate culture in order to set a business tone that stimulates easy compliance to sustainability requirements.",About Sufficiency
Data-Driven Methodology to Support Long-Lasting Logistics and Decision Making for Urban Last-Mile Operations,"Last-mile operations in forward and reverse logistics are responsible for a large part of the costs, emissions, and times in supply chains. These operations have increased due to the growth of electronic commerce and direct-to-consumer strategies. We propose a novel data- and model-driven framework to support decision making for urban distribution. The methodology is composed of diverse, hybrid, and complementary techniques integrated by a decision support system. This approach focuses on key elements of megacities such as socio-demographic diversity, portfolio mix, logistics fragmentation, high congestion factors, and dense commercial areas. The methodological framework will allow decision makers to create early warning systems and, with the implementation of optimization, machine learning, and simulation models together, make the best utilization of resources. The advantages of the system include flexibility in decision making, social welfare, increased productivity, and reductions in cost and environmental impacts. A real-world illustrative example is presented under conditions in one of the most congested cities: the megacity of Bogota, Colombia. Data come from a retail organization operating in the city. A network of stakeholders is analyzed to understand the complex urban distribution. The execution of the methodology was capable of solving a complex problem reducing the number of vehicles utilized, increasing the resource capacity utilization, and reducing the cost of operations of the fleet, meeting all constraints. These constraints included the window of operations and accomplishing the total number of deliveries. Furthermore, the methodology could accomplish the learning function using deep reinforcement learning in reasonable computational times. This preliminary analysis shows the potential benefits, especially in understudied metropolitan areas from emerging markets, supporting a more effective delivery process, and encouraging proactive, dynamic decision making during the execution stage.",About Sufficiency
Improving Sustainable Safe Transport via Automated Vehicle Control with Closed-Loop Matching,"The concept of vehicle automation is a promising approach to achieve sustainable transport systems, especially in an urban context. Automation requires the integration of learning-based approaches and methods in control theory. Through the integration, a high amount of information in automation can be incorporated. Thus, a sustainable operation, i.e., energy-efficient and safe motion with automated vehicles, can be achieved. Despite the advantages of integration with learning-based approaches, enhanced vehicle automation poses crucial safety challenges. In this paper, a novel closed-loop matching method for control-oriented purposes in the context of vehicle control systems is presented. The goal of the method is to match the nonlinear vehicle dynamics to the dynamics of a linear system in a predefined structure; thus, a control-oriented model is obtained. The matching is achieved by an additional control input from a neural network, which is designed based on the input-output signals of the nonlinear vehicle system. In this paper, the process of closed-loop matching, i.e., the dataset generation, the training, and the evaluation of the neural network, is proposed. The evaluation process of the neural network through data-driven reachability analysis and statistical performance analysis methods is carried out. The proposed method is applied to achieve the path following functionality, in which the nonlinearities of the lateral vehicle dynamics are handled. The effectiveness of the closed-loop matching and the designed control functionality through high fidelity CarMaker simulations is illustrated.",About Sufficiency
"The Development of a CO2 Emission Coefficient for Medium- and Heavy-Duty Vehicles with Different Road Slope Conditions Using Multiple Linear Regression, and Considering the Health Effects","The current studies on carbon dioxide (CO2) emissions and the impacts on public health focus mainly on evaluating CO(2)emissions from two types of emission sources. The first is a fixed source type from industrial plants, which can be controlled or easily evaluated. The second is a mobile source type from the transport sector, especially from medium- and heavy-duty vehicles (MHDVs), which produce high emissions. However, the common methods of evaluation of the average value of CO(2)emissions per kilometer of a vehicle use a general Intergovernmental Panel on Climate Change (IPCC) model that does not consider the topography or road conditions. This affects the accuracy of CO(2)emission assessments and, in turn, affects the accuracy of any analysis needed to establish health policies and the management of public health within the affected area. In this paper, therefore, we present the development of emission coefficient calculations with varying topography conditions for MHDVs with consideration of the health effects on the populace. The study area was the Asian highway network in Thailand that passes through all regions and is geographically diverse. Data were collected from the Department of Highway's records, global positioning system (GPS) and electronic control unit (ECU) with data consisting of road data, slope, distance, traffic level and vehicle weight, as well as fuel consumption along the transportation route. To analyze and map out the correlation of the CO(2)emission coefficients for each road slope, we determined the coefficient of the CO(2)emissions using multiple linear regression analysis and validated this using Huber-White robust standard errors for heteroscedasticity. This method was more cost-efficient and time-efficient compared to the conventional approaches. We also created CO(2)emission maps with risk points for health effects, and we propose policy designs and plans to manage the traffic level in each of these areas prone to higher levels of such emissions.",About Sufficiency
"Teasing out the detail: How our understanding of marine AIS data can better inform industries, developments, and planning","Automatic identification system (AIS) is becoming increasingly popular with marine vessels providing accessible, up-to-date information on vessel activity in the marine environment. Although AIS has been utilised in several different fields to address specific questions, no published work has outlined the potential of AIS as a tool for a wide range of industries and users of the marine environment such as spatial planning, developments, and local marine industries (e.g. fisheries). This work demonstrates a procedure for processing, analysing, and visualisation of AIS data with example outputs and their potential uses. Over 730 000 data points of AIS information for 2013 from around Shetland were processed, analysed, and mapped. Tools used included density mapping, vessel tracks, interpolations of vessel dimensions, and ship type analysis. The dataset was broken down by sector into meaningful and usable data packets which could also be analysed over time. Density mapping, derived from both point and vessel track data, proved highly informative but were unable to address all aspects of the data. Vessel tracks showed variation in vessel routes, especially around island groups. Additional uses of AIS data were addressed and included risk mapping for invasive non-native species, fisheries, and general statistics. Temporal variation of vessel activity was also discussed. (C) 2015 Elsevier Ltd. All rights reserved.",About Sufficiency
Linking urban transport and land use in developing countries,"The mobility challenges of the developing world are considerably different than those in wealthier, advanced countries, and so are the challenges of coordinating transportation and land use. Rapid population growth, poverty and income disparities, overcrowded urban cores, poorly designed road networks, spatial mismatches between housing and jobs, deteriorating environmental conditions, and economic losses from extreme traffic by congestion are among the more vexing challenges faced by developing cities that could be assuaged through improved coordination of transportation and urban development.. is is underscored by examples reviewed in this paper from South Asia, Southeast Asia, China, India, Africa, and South America. It is concluded that whatever is done to improve transportation and land-use integration must be pro-poor. The cardinal features of integrated and sustainable transport and urbanism everywhere-accessible urban activities and safe, attractive walking and cycling environs-are particularly vital to the welfare and prosperity of urbanites in the world's poorest countries.",About Sufficiency
Pricing and Collecting Decision of a Closed-Loop Supply Chain Under Market Segmentation With Reward-Penalty Mechanism,"This paper examines the impact of reward-penalty mechanism (RPM) on the decision-making of a closed-loop supply chain (CLSC) under the framework of market segmentation. Decentralized dynamic game models are developed to obtain and compare the pricing and collecting decision of the CLSC with and without RPM. We find that: (i) RPM improves the actual collection rate and the profit of the recycler, while it decreases the prices of new and remanufactured products in market segments in response to higher consumer preferences; (ii) a higher buyback price guarantees that the manufacturer becomes more profitable when the government imposes low or high intensity of reward-penalty. Otherwise, the manufacturer should set a lower buyback price when the government imposes moderate intensity of reward-penalty; (iii) higher intensity of reward-penalty can not only effectively improve the environmental sustainability of CLSC, but also obtain higher social welfare; (iv) the mechanism that reward equals penalty is optimal, in which case the same intensity maximizes the actual collection rate as well as the profits of the manufacturer and the recycler. Our analysis discusses the parameters which have significant impacts on the pricing and collecting decision of the closed-loop supply chain and gains managerial insights that are both environmentally and economically beneficial.",About Sufficiency
Analysis of Cooperation Game to Reverse Logistics System for Regional Economy,"It can help enterprises to decrease cost, add income and then improve their competitiveness that the enterprises within the regional economic jointly build the recovery processing center. A cooperation game model is put forward to enterprises with the regional economic, and then analyses the effect factors which include the scales and quality of recovery products of enterprise itself. At last, this paper advances the income distribution proportion interval which the enterprise pleasure to co-construct the recovery processing center.",About Sufficiency
Sustainable Fashion Supply Chain: Lessons from H&M,"Sustainability is significantly important for fashion business due to consumers' increasing awareness of environment. When a fashion company aims to promote sustainability, the main linkage is to develop a sustainable supply chain. This paper contributes to current knowledge of sustainable supply chain in the textile and clothing industry. We first depict the structure of sustainable fashion supply chain including eco-material preparation, sustainable manufacturing, green distribution, green retailing, and ethical consumers based on the extant literature. We study the case of the Swedish fast fashion company, H&M, which has constructed its sustainable supply chain in developing eco-materials, providing safety training, monitoring sustainable manufacturing, reducing carbon emission in distribution, and promoting eco-fashion. Moreover, based on the secondary data and analysis, we learn the lessons of H&M's sustainable fashion supply chain from the country perspective: (1) the H&M's sourcing managers may be more likely to select suppliers in the countries with lower degrees of human wellbeing; (2) the H&M's supply chain manager may set a higher level of inventory in a country with a higher human wellbeing; and (3) the H&M CEO may consider the degrees of human wellbeing and economic wellbeing, instead of environmental wellbeing when launching the online shopping channel in a specific country.",About Sufficiency
Selection of mode of transport for freight movement,"Purpose The purpose of this research paper is to understand the major factors considered before choosing the mode of transportation for the freight movement in India by different stakeholders and look into the future prospects on each of these sectors, i.e. railways, roadways and inland waterways. Design/methodology/approach This paper collected the primary data from the various stakeholders in the transportation sector and the secondary data through websites and various ministries of each of the sectors. The various factors are then determined by thoroughly analysing the responses by performing factor analysis in SPSS. Findings Earlier railways were the preferred medium of transportation, but the dynamics shifted during the 90's to roadways, and now, it is responsible for nearly 60% of the freight traffic with waterways slowing increasing its share of the pie. Also, there are a lot of factors which stakeholders consider, but the major factors that came out are cost, sustainability, timing, government initiatives, visibility and performance. Practical implications The result of this study implies that sectors should create a robust network for easy reach of the customers and try working in conjunction to create an efficient, affordable and highly connected network. This study will also help in taking vital decisions regarding the future planning of transportation sector. Originality/value The findings help in improving the transportation network and help in better decision-making by various stakeholders while choosing the mode of transportation.",About Sufficiency
Installation of road heavy duty (truck) engines on board of small inland self-propelled barges for an environmentally friendly European inland waterway transport of goods,"Freight transport is an essential issue to the competitiveness of territories but the negative impact that it induces, like polluting emissions and their physical consequences on public health, have received growing attention in recent years. Policy makers set goals for sustainable development transportation. Since 1 January 2014, all new trucks have to be powered with a Euro 6 certified engine. Inland waterways are used for transportation of goods and are recognized for their contribution towards sustainable development therefore, engine's barges are old and emission of air pollutants are significant. Currently, the emission limits for inland waterway barges are defined in the Directive 97/86/CE on non-road mobile machinery. That emission limits are similar to Euro 2. The Euro 6 regulations reduced significantly the emission limits. The European Commission has recently proposed a revision of the Directive 97/86/CE which will significantly reduce emission limits. Development of new engines for the small market of small barges in France and also in Europe will be very expensive. Small barges owners should not be able to afford investments in new engines. The range of road engine power is similar to the small barges one. Using Euro 6 heavy duty engines will permit to reach low emissions of air pollutants and should be less expensive for small barges owners. A technical feasibility study had been carried out. It was based on an approach of all parameters of the re-power existing barges. Stakeholder's interviews permitted to deepen same specific topics. The technical feasibility study, added with expected environmental effect, has shown that no technical impossibility had been identified, but some key technical issues have to be considered. It has also confirmed re-powering existing barges (same propulsion power value, no change in naval transmission, etc.) with Euro 6 truck engines is interesting to limit significantly emission air pollutants. The on-board diagnostic controls monitor the engine's exhaust treatment and, it's important to adapt this monitoring for engines used in barges. (C) 2016 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license.",About Sufficiency
Super apps and the mobility transition,"The mobility sector is undergoing a transformation towards a multimodal, sustainable, and usercentric system, driven by technological advancements and evolving business models. Super apps, which have already matured in Eastern markets, are now entering Western markets through the mobility sector, where they are poised to play a pivotal role in the mobility transition. We argue that the expected uptake of super apps will drive a transition towards a multi-service system, promoting the adoption of more sustainable transport modes while creating synergies with nontransport services. However, there remains a significant gap in research that explores the role of super apps within the broader context of mobility transitions. This perspective seeks to address this gap by describing how super apps could influence existing transport systems, while identifying future research areas such as regulatory frameworks, governance models, transition pathways and incumbent business model adaptation, and the socio-economic impacts of accelerating platformization.",About Sufficiency
How Does Innovativeness Foster Sustainable Supply Chain Management?,"Innovativeness refers to an organization's willingness/ability to change through architectural/radical innovation of processes, products and management systems. Existing literature generally supports the importance of innovativeness for environmental and social sustainability. Open questions, however, still remain: (i) which mechanisms (i.e., tools and routines) do innovative firms leverage that better assist the process through which stakeholder pressure is turned into sustainability strategy? (ii) Which mechanisms do innovative firms leverage that better assist the process through which strategy is turned into a successful adoption of sustainable supply chain management (SSCM) practices? To advance our understanding of how innovativeness fosters SSCM, we leveraged a continuous process of theory generation and data collection through case studies. A set of propositions and a conceptual model were built that complement prior research and can provide guidance to firms that struggle with how to deliver new environmental and social standards in their supply chains.",About Sufficiency
Sustainable supplier selection in a construction company: a new MCDM method based on dominance-based rough set analysis,"PurposeThis study aims to focus on sustainable supplier selection in a construction company considering a new multi-criteria decision-making (MCDM) method based on dominance-based rough set analysis. The inclusion of sustainability concept in industrial supply chains has started gaining momentum due to increased environmental protection awareness and social obligations. The selection of sustainable suppliers marks the first step toward accomplishing this objective. The problem of selecting the right suppliers fulfilling the sustainable requirements is a major MCDM problem since various conflicting factors are underplay in the selection process. The decision-makers are often confronted with inconsistent situations forcing them to make imprecise and vague decisions. Design/methodology/approachThis paper presents a new method based on dominance-based rough sets for the selection of right suppliers based on sustainable performance criteria relying on the triple bottom line approach. The method applied has its distinct advantages by providing more transparency in dealing with the preference information provided by the decision-makers and is thus found to be more intuitive and appealing as a performance measurement tool. FindingsThe technique is easy to apply using ""jrank"" software package and devises results in the form of decision rules and ranking that further assist the decision-makers in making an informed decision that increases credibility in the decision-making process. Originality/valueThe novelty of this study of its kind is that uses the dominance-based rough set approach for a sustainable supplier selection process.",About Sufficiency
A Social-network-enabled Green Transportation System,"We focus on the new models of urban transportation system based on green-energy vehicles. The main goal is to design and demonstrate a prototype of social network enabled transportation system which enables communication between electrical vehicles, monitoring, information gathering, assistant driving and traffic flow control. This service-oriented system targets on significant reduction in energy consumption, pollution impact, traffic congestion, and provides solutions with affordable costs from perspective of both individual travelers and transportation agencies.",About Sufficiency
System dynamics modeling for sustainable supply chain management: A literature review and systems thinking approach,"Globalized economic systems involve complex supply chains (SCs) where environmental and social impacts are to be managed in alignment with diverse stakeholder expectations and to mitigate sustainability-related risks. Quantitative modeling approaches for sustainable supply chain management (SSCM) have gained increasing attention. Compared to analytical models and mathematical programming, simulation methods in SSCM are underrepresented, although system dynamics (SD) modeling is suitable to simulate and examine complex and dynamic systems and to support long-term, strategic decision-making. This paper provides a review of SSCM-related SD models, systematizes SSCM from a systems thinking perspective in a conceptual framework, and proposes guidelines for SD modeling in SSCM research. A content analysis of related literature examines SD models for forward, reverse, and closed-loop SCs that include environmental or social aspects of sustainability. It is found that a majority of models deals with macroscopic levels of analysis while models for intra- and inter-organizational SCs are less prominent. SSCM-related SD models integrate environmental and social sustainability metrics, governmental pressures and incentives or customer expectations, but uncertainties and risks are seldom modeled. Inferences for future SD modeling in SSCM are derived from this review to address strengths and shortcomings of extant SD modeling approaches for SSCM. SSCM-related constructs, model validation and the need of hybrid models, which integrate different simulation, optimization, or multi criteria decision-making models, are addressed. A framework for a systems thinking perspective on SSCM is conceptualized that may serve as a basis for future research. (C) 2018 Elsevier Ltd. All rights reserved.",About Sufficiency
Sourcing green makes green: Evidence from the BRICs,"Facing increasing environmental concerns, many developing countries-especially Brazil, Russia, India, and China (the BRICs), which serve as the world's major business-to-business (B2B) servicing hubs offering manufacturing and sourcing services-seek solutions to reduce damage caused to the environment. We draw on Systems Theory to investigate the manner in which sourcing options of input materials affect the environmental performance of servicing firms in the BRICs. We hypothesize that servicing firms in developing countries can lower pollution intensity by (i) substituting dirty inputs with clean inputs and (ii) substituting domestically sourced inputs with inputs imported from developed countries. Based on the industry-level input-output matrix for the BRICs over the period 1995-2009, our empirical findings suggest that firms significantly improved their environmental performance by using cleaner inputs and more inputs imported from developed countries. We demonstrate service innovation in the form of green sourcing by using clean materials accessible through trade liberalisation to achieve environmental benefits. We advance knowledge of green supply chain management and green sourcing strategies of servicing firms in developing countries.",About Sufficiency
Fifteen Years of Community Exposure to Heavy-Duty Emissions: Capturing Disparities over Space and Time,"Disparities in exposure to traffic-related air pollution have been widely reported. However, little work has been done to simultaneously assess the impact of various vehicle types on populations of different socioeconomic/ethnic backgrounds. In this study, we employed an extreme gradient-boosting approach to spatially distribute light-duty vehicle (LDV) and heavy-duty truck emissions across the city of Toronto from 2006 to 2020. We examined associations between these emissions and different marginalization indices across this time span. Despite a large decrease in traffic emissions, disparities in exposure to traffic-related air pollution persisted over time. Populations with high residential instability, high ethnic concentration, and high material deprivation were found to reside in regions with significantly higher truck and LDV emissions. In fact, the gap in exposure to traffic emissions between the most residentially unstable populations and the least residentially unstable populations worsened over time, with trucks being the larger contributor to these disparities. Our data also indicate that the number of trucks and truck emissions increased substantially between 2019 and 2020 whilst LDVs decreased. Our results suggest that improvements in vehicle emission technologies are not sufficient to tackle disparities in exposure to traffic-related air pollution.",About Sufficiency
Environmental Economic Equilibrium-Based Reverse Logistics Network for Kitchen Waste Disposal,"Increased stress on environmental pollution, resource consumption, and public health resulting from the rapid growth of kitchen waste (KW) have necessitated an effectively harmless disposal way. In this paper, combined with the advanced anaerobic digestion technology and co-processing in cement kilns technology, an innovative reverse logistics network (RLN) considering three disposal lines is established to reduce KW volumes, make renewable products, reduce resource waste and GHG emissions. To optimize the KW disposal process, a mathematical multi-objective programming model is developed to achieve a balance between CO2 emission reduction and cost controlling comprehensively. A practical case is introduced to demonstrate the effectiveness and practicality of the proposed methodology in selecting disposal facilities and determining transportation routes and disposal volume. Scenario analyses for the decision-makers' attitude towards the environment objective are conducted. This paper concluded that the proposed innovative KW RLN has been successfully applied and achieve economic and environmental goals; in order to achieve CO2 emission reduction, different types of decision-makers can input different investment level, that is, setting investment level at 1,300,000x10(3) CNY is suitable for ""environment pessimistic"" decision-makers, setting investment level at 1,450,000x10(3) CNY is suitable for ""environment neutral"" decision-makers, and setting investment level at 1,550,000x10(3) CNY is suitable for ""environment optimistic"" decision-makers.",About Sufficiency
An Intelligently Controlled Charging Model for Battery Electric Trucks in Drayage Operations,"California has set a goal for all drayage trucks operating in the state to be zero-emitting by 2035. In order to achieve this goal, drayage operators would need to transition 100% of their fleets to zero-emission vehicles such as battery electric trucks (BETs). This article presents an intelligently controlled charging model for BETs that minimizes charging costs while optimizing subsequent tour completion. To develop this model, real-world activity data from a drayage truck fleet operating in Southern California was combined with a two-stage clustering technique to identify trip and tour patterns. The energy consumption for each trip and tour was then simulated for BETs with a battery capacity of 565 kWh using a 150 kW charging power level. Home base charging load profiles were generated using the proposed charging model, subject to constraints of the energy needed to complete the next subsequent tour and Time-of-Use energy cost rates. A sensitivity analysis evaluated three scenarios: a passive scenario with a 5% state-of-charge (SOC) constraint after completing the subsequent tour, an average scenario with a 50% SOC constraint, and an aggressive scenario with an 80% SOC constraint. Results indicated that the 80% SOC constraint scenario achieved the lowest charging cost. However, it also yielded the lowest tour completion rate (51%). In contrast, the 5% SOC constraint scenario registered the highest tour completion rate. These results revealed that 96% of the tours could be successfully completed using the intelligently controlled charging model. The remaining tours were infeasible, indicating that the available time at the home base was inadequate for charging the necessary energy for the next tour. In terms of total costs, the scenario with a 5% SOC constraint resulted in an annual cost of approximately $40,000, whereas the 80% SOC scenario nearly doubled that amount.",About Sufficiency
"Impact of Government Subsidies, Competition, and Blockchain on Green Supply Chain Decisions","At present, environmental and competitive pressures urge enterprises to engage in research and development (R&D) of green products, and a green supply chain has become the main trend in the sustainable development of enterprises. This study analyzes the optimal operation decisions of a green supply chain for two manufacturers under different competitive and cooperative relationships, considering factors such as government subsidies, consumer green preferences, and the impact of the green information trust. The results show that government subsidies can lead to higher social welfare when manufacturers have a cooperative relationship, but the optimal choice of subsidies (for R&D costs or product production costs) depends on the level of competition and the difficulty of R&D. For the manufacturers, the optimal choice of R&D strategy (individual or joint) and the use of blockchain technology also depends on the level of difficulty of R&D and the type of government subsidies. Overall, this study highlights the importance of considering various factors when making decisions in a green supply chain to achieve the best outcomes for all parties involved.",About Sufficiency
Providing an Integrated Multi-Objective Model for Closed-Loop Supply Chain under Fuzzy Conditions with Upgral Approach,"This study was conducted aimed at providing a sustainable multi-objective model of supply chain location, inventory, routing under uncertainty with a passive defense approach, in which the Upgral model was first introduced to the world. The Upgral Paradigm is an integrated model for the location-inventory-routing problem in a four-level supply chain where parameters such as demand, facility cost and inventory costs are taken into account uncertain as triangular fuzzy numbers (TFNs). In this study, the characteristics and capabilities of passive defense in the supply chain, such as ""logistical flow rate"", ""backup path security"", ""the possibility of resource and equipment deployment"", and the dispersion principle were considered for location to increase the resilience of supply chain. The model was solved using Whale Optimization Algorithm (WOA) and NSGA-II meta-heuristic algorithm. First, a four-objective mathematical model was proposed for the problem the objectives of which were: 1) minimizing supply chain costs; 2) maximizing social responsibility or social benefits; 3) minimizing environmental impacts; and 4) minimizing risk. Moreover, the experimental sample problems were solved in three small, medium, and large groups using the WOA Algorithm. The results of solving this algorithm were compared with the results of NSGA-II one according to the indices including quality, dispersion, uniformity and solution time indices to prove the efficiency of the algorithm. Based on the results, the WOA algorithm had a higher ability to achieve higher quality and near-optimal solutions than NSGA-II algorithm in all cases. The dispersion index values indicated that the WOA Algorithm performed better in exploring and extracting the feasible region. In addition, with respect to the results of the uniformity and the solution time indices, it was found that the NSGA-II Algorithm had a lower solution time than the WOA Algorithm and the answer space more uniformly.",About Sufficiency
Bottlenecks in Nigeria's fresh food supply chain: What is the way forward?,"Background: Nigeria is one of the leading producers of fresh produce, most of which is grown by smallholder farmers. Over 40% of this fresh produce is lost after harvest, resulting in more than 30% loss in smallholders' income.Scope and approach: We identify the major bottlenecks in Nigeria's fresh food supply chain responsible for the high food losses. We propose possible solutions to mitigate them.Key findings and conclusions: A key bottleneck is that most fresh food produced in the northern region is largely consumed in the south, with the produce traveling long distances through a poor road network. The lack of a continuous cold chain is another key causative factor for food loss. Less than 10% of the fresh food produced passes through a cold chain despite the long distances it needs to travel. There is also limited access to supply chain data and expert market intelligence to assist stakeholders in the decision-making process and process optimizations. To mitigate food losses, smallholder farmers and retailers need better access to clean cooling solutions at affordable prices. This will help them extend produce shelf life and sell produce at attractive market prices by being able to cool their produce. Simple and small-scale passive cooling solutions may also help pre-serve food for these stakeholders, but are currently rarely used. Furthermore, the available open data needs to be made easily accessible to stakeholders along the fresh supply chain so they can make informed decisions on how best to reduce food losses and maximize profit. Smallholder farmers would also significantly benefit from easier access to expert intelligence on postharvest storage and market information. Providing such expertise via mobile apps can be a powerful tool for remote farmers in a country like Nigeria. Otherwise, it could be difficult to reach them. Nigerian governmental agencies can create the largest gains in the cold chain by implementing policies that support stakeholders financially and improve public-private sector partnerships for rural development.",About Sufficiency
Effect of Optimal Subsidy Rate and Strategic Behaviour of Supply Chain Members under Competition on Green Product Retailing,"This paper investigates the impact of the subsidy and horizontal strategic cooperation on a green supply chain where two competing manufacturers distribute substitutable green products through exclusive retailers. Models are formulated in three-stage game structures in five different scenarios, where the government organization determines optimal subsidy by pursuing social welfare maximization. Both manufacturers invest in improving green quality levels of products. The study aims to explore the advantage of vertical integration and strategic collusion from the perspective of green supply chain practice in the presence of subsidy. The key contributions from the present study indicate that under competition, members of both supply chains are able to receive higher profits through horizontal collusion, but green quality levels of the product remain suboptimal. If upstream manufacturers cooperate, government subsidy does not necessarily improve product quality level, and the amount of government expenditure increased substantially. By comparing outcomes where members are vertically integrated with scenarios where members make strategic collusion, we found that the former might outperform by later. Cross-price sensitivity appears as a significant parameter affecting supply chain members' performance and the amount of government expenditure. Cooperation between members at the horizontal level is a more robust strategic measure than vertical integration if consumers are highly price-sensitive.",About Sufficiency
"Food environment and access to fruits and vegetables: ""A metropolis into perspective""","This study aimed to explore the food environment, according to the access to fruits and vegetables (FV), from the perspective of a great Brazilian metropolis, which is an international reference for public policies on food and nutritional security (SAN). We reviewed the literature and the official website that showed SAN and urban planning policies of Belo Horizonte-MG; we also built maps. In this city, SAN programs serve more than 1 million people a day, including initiatives to encourage food production, protection and promotion of food consumption, subsidized marketing of foods and meals, and food training. We found a concentration of shops selling FV - including those with public subsidies (open air markets and public farmers' markets) - in the richest region of the city, limiting the access of those living in the outskirts of the city. We also found territories with health promotion services (Health Academy Program). However, the increase in public and private farmers' markets, open air markets and supermarkets would be able to promote the access to healthy foods in the disadvantaged territories. These data show that civil society and government representatives must watch public policies, aiming to reduce inequities, and contribute to improve the quality of life and social well-being of the population, in addition to collaborate for sustainable food systems. We suggest that SAN public policies should dialogue with other policies, such as those ofurban planning, social welfare, and agriculture, to promote fairness and intersectorally meet the needs of more vulnerable communities.",About Sufficiency
Relocation and investment in R&D by firms,"The literature on foreign direct investment has analyzed corporate location decisions when firms invest in R&D to reduce production costs. Such firms may set up new plants in other developed countries while maintaining their domestic plants. In contrast, we here consider firms that close down their domestic operations and relocate to countries where wage costs are lower. Thus, we assume that firms may reduce their production costs by investing in R&D and likewise by moving their plants abroad. We show that these two mechanisms are complementary. When a firm relocates it invests more in R&D than when it does not change its location and, therefore, its production cost is lower in the first case. As a result, investment in R&D encourages firms to relocate.",About Sufficiency
The vehicle routing problem with backhauls towards a sustainability perspective: a review,"The vehicle routing problem with backhauls (VRPB) allows to integrate inbound and outbound routes, which is an efficient strategy to reduce routing costs and also to reduce the environmental and social impacts of transportation. In this paper, we analyze the VRPB literature with a sustainability perspective, which covers environmental and social objectives, collaborative networks and reverse logistics. First, to better understand and analyze the VRPB literature, all related works are characterized according to a common taxonomy provided for routing problems. This taxonomy is extended to differentiate between economic, environmental and social objectives. After identification of all VRPB papers that include sustainability issues, these are analyzed and discussed in more detail. The analysis reveals that research on VRPBs with sustainability concerns is recent and relatively scarce and the most popular aspects investigated are the minimization of fuel consumption andCO2 emissions. Future research lines driven by sustainability concerns are suggested for the VRPB as a promoter of green logistics.",About Sufficiency
Dynamic decision-making and coordination of low-carbon closed-loop supply chain considering different power structures and government double subsidy,"Low-carbon closed-loop supply chain (LC-CLSC) plays an important role in realizing a low-carbon circular economy. In order to facilitate governments to make emission reduction subsidy and recycling subsidy decisions and LC-CLSC members to formulate pricing, emission reduction investment and recycling investment decisions, this paper proposes multiple three-level differential game models of a LC-CLSC involving the manufacturer, retailer and government considering the dynamic characteristics of product goodwill and recycling rate. Under the four scenarios of three different power structures: manufacturer-led, retailer-led and non-led, and centralized decision-making, some critical equilibrium results are first solved and discussed, including government's optimal emission reduction subsidy and recycling subsidy rates, the manufacturer's wholesale price and emission reduction investment, the retailer's retail price and recycling investment, product goodwill and waste product recycling rate, profits of the manufacturer, retailer and government, etc. To further achieve the LC-CLSC coordination, the contracts under three different power structures are designed, and the conditions that the coordination parameters satisfy are given. Through mathematical derivation of equilibrium results and sensitivity analysis with the help of numerical examples, this paper finds that the government subsidy rates are dependent on the power status between manufacturers and retailers, and the weaker party will get higher subsidy rate. The government subsidy mechanism can significantly reduce the gaps between the manufacturer-led and retailer-led cases, such as manufacturer's emission reduction investment, the retailer's recycling investment, steady-state retail price, and product goodwill and recycling rate. Under the effect of the government subsidy mechanism, the non-led case is more conducive to the recycling of waste products and the improvement of social welfare than the unilateral domination cases. The findings can help manufacturers and retailers in the LC-CLSC formulate optimal strategies like pricing, emission reduction and recycling, and develop coordination contracts to further improve the overall performance of the supply chain according to their different power structures. More importantly, they can also help governments make optimal emission reduction and recycling subsidy decisions according to member companies' different power structures so as to improve subsidy efficiency. [GRAPHICS] .",About Sufficiency
THE NEW RAIL SYSTEM,"The objective of this study was to define the elements of a rail system capable of achieving a 1000-fold increase in freight throughput, accommodate passenger travel, be capable of a phased implementation, be affordable and have a payback to an adopting railroad of less than seven years. This paper describes a novel New Rail System, ""NRS"" that achieves the stated objectives. The NRS utilizes existing rail rights-of-way and tracks. It has the capability to accommodate both freight and passenger movement. In its initial implementation it supports speeds of 85 mph. Although no invention is required, three modifications to the railroad infrastructure are required, (1) elimination of at-grade crossings, (2) electrification, and (3) transition from dieselelectric locomotive powered trains to individual electrical powered pallets to allow flexible routing and sorting in the network. The four elements defining NRS are; (1) self-powered 70-foot pallets, (2) freight and passenger payload modules (loadable and unloadable from the pallets), (3) pallet yards, accumulators and transfer stations, and (4) a pallet control system. An initial cost-benefits analysis shows that if BNSF implemented the NRS on its Southern Transcon route between Los Angeles and Chicago a payback of the initial $21.2 billion investment can be achieved in under four-years.",About Sufficiency
Challenges in Determining the Scope of Rail Megaprojects: Responding to Ever-Increasing Infrastructure Demand,"While megaprojects can be defined as highly complex, time-consuming, and cost-intensive endeavors, for rail infrastructures they are even more problematic. As a starting point, for rail megaprojects, their scope may, at times, alter due to some risks and uncertainties. As many such projects exceed many years in development, their scope and formation will not be a linear trajectory. It is, therefore, the aim of this paper to evaluate the difficulties in determining the scope of rail megaprojects. This paper first introduces the theoretical framework via adaptive decision-making and policy setting when dealing with mega rail projects. Through sustainable development, carefully applied research is undertaken to highlight some of the key shortfalls of current practices when dealing with mega rail projects. This includes categorizing sustainability into four dimensions: social, economic, environmental, and engineering for rail infrastructure. To address the existing gap in the literature, including the appropriate alignment of policy planning and design, this paper will carefully review the complex science of rail megaprojects. This can be seen as a conceptual framework, which combines complex theory and practice to develop a theoretical perspective to initiate, plan, execute, and commission mega rail projects. Particularly with an international focus, this paper will review global development, targeting rail infrastructures. For rail megaprojects, strategically integrated objectives are traditionally key functions within the regional land transport network along with the national network and are necessary to (i) improve connectivity, both nationally and inter-regionally for people, communities, regions, and industry via effectively linking the existing broad-based transport network; (ii) enhance logistical systems and trade; (iii) provide a consistent framework for continuous sustainable development; and (iv) provide a consistent framework for long-term economic and social benefits.",About Sufficiency
Supply chain resilience strategies and their impact on sustainability: an investigation from the automobile sector,"PurposeThe purpose of this study is to propose a framework comprising supply chain (SC) resilience strategies to handle low-frequency high impact disruptive events. This study also evaluates the impact of SC resilience strategies' implementation on the triple bottom line of SC sustainability. Design/methodology/approachA hybrid three-phased method is proposed to meet the research objectives of this study. In the first phase, this study uses the Delphi technique to select SC resilience strategies and SC sustainability dimensions. In the second phase, the best-worst method is used to assess the relative weights of resilience strategies. Finally, in the third stage, summative Likert scoring is used to understand the impact of SC resilience strategies on the SC sustainability triple bottom line. FindingsThe outcomes reveal that firms give due importance to inter-organizational relationships and supplier nearness for supply continuity. In the sustainability context, the obtained scores proved that resilience strategies have the maximum impact on economic sustainability, followed by environmental sustainability. Research limitations/implicationsTo the best of the authors' knowledge, this is the first study that examines aspects of SC resilience strategies and quantifies their impact on the triple bottom line of SC sustainability. This study is specific to the automobile sector; sectoral diversity may expose similarities and dissimilarities in the approach. Practical implicationsThe outcome establishes that supplier-manufacturer relationships need to be strengthened further to tackle any future uncertainties. Besides, supplier location decisions may also be revisited. The strategies proposed will aid SC managers to make informed decisions to prepare for uncertain events. Originality/valueIn the face of uncertain events, often SCs trade-off sustainability in pursuit of resilience. It manifests that resilience is a prerequisite for SC sustainability. While planning SCs, organizations often choose either sustainability or resilience. Thus, this study acknowledges the need to develop effective SC resilience strategies that are in harmony with the sustainability agenda.",About Sufficiency
Modeling expected air quality impacts of Oregon?s proposed expanded clean fuels program,"The public health burden of traffic-related air pollution falls most heavily on the population living closest to major transportation corridors, which leads to exposure disparities between different socio-economic groups across the United States. The state of Oregon has adopted climate policies to reduce transportation fuel carbon intensity (CI). These climate policies have the potential co-benefit of reducing air pollution exposure disparities for different socio-economic groups. Here we analyze the emissions and air quality outcomes of three future year (2035) scenarios for transportation fuels in Oregon: (i) a Business as usual (BAU) scenario, (ii) a Clean Fuels Program (CFP) scenario that represents adoption and successful achievement of a proposed 25% reduction in carbon intensity in 2035; and (iii) a maximum ambition scenario (CFP MAX) that builds on the CFP scenario to achieve a 37% CI reduction by adopting low carbon fuels more aggressively, especially for heavy duty vehicles. Transportation emissions under all scenarios were estimated using the MOVES model for every county in Oregon. Detailed emissions with 4 km spatial resolution were then developed for each scenario by scaling the National Emissions Inventory (NEI) for the year 2017 based on the emissions derived from the MOVES analysis. Air quality in 2035 was simulated using the UCD/CIT chemical transport model that enables a detailed analysis of PM2.5 and PM0.1 components and sources. Exposure fields were analyzed using the BenMAP model to predict public health outcomes. Environmental justice analysis was conducted by race/ethnicity categories and income categories obtained from the American Community Survey (ACS). Results suggest that adoption of low-carbon transportation fuels will improve air quality in Oregon, yielding public health benefits equivalent to approximately $80M/yr. Adoption of low carbon transportation fuels will also reduce disparities in exposure to transportation-related air pollution between residents in different race/ethnicity categories by -14% in Portland and -20% in Salem. Adoption of low-carbon fuels reduces PM2.5 mass disparity by 10% in Salem, but does not have a significant effect in Portland, because on-road mobile sources contribute to less than 3% of the total PM2.5 mass disparity in this city. The analysis reveals that the spatial distribution of each race/ethnicity group in each city is the primary factor that determines the impact of low carbon fuel adoption on exposure disparity.",About Sufficiency
Green Supply Chain Management Based on System Dynamics,"Since the new century, china's various industries and the process of industrialization: have been developed rapidly which greatly improved the people's material standard of living. But at the same time, the performance of the linear trend of economic development of the traditional industrialization has also brought negative effects, resulting in environmental burdens, resources in unsustainable situation, which are bound to pose a serious threat to society. System dynamics method is applied to the management of green supply chain in this article to make an analysis of cause and effect diagram of Green Supply Chain (GSC), and establish a green supply's chain system flow diagram and system dynamics model using Vensim PLE to achieve the system's simulation analysis. Simulation results of the analysis have a certain reference value for corporate: recognizing what the market demands for, the constraints of supply chain's capacity, and the strategic decision-making for corporates.",About Sufficiency
Data Analytics for Air Travel Data: A Survey and New Perspectives,"From the start, the airline industry has remarkably connected countries all over the world through rapid long-distance transportation, helping people overcome geographic barriers. Consequently, this has ushered in substantial economic growth, both nationally and internationally. The airline industry produces vast amounts of data, capturing a diverse set of information about their operations, including data related to passengers, freight, flights, and much more. Analyzing air travel data can advance the understanding of airline market dynamics, allowing companies to provide customized, efficient, and safe transportation services. Due to big data challenges in such a complex environment, the benefits of drawing insights from the air travel data in the airline industry have not yet been fully explored. This article aims to survey various components and corresponding proposed data analysis methodologies that have been identified as essential to the inner workings of the airline industry. We introduce existing data sources commonly used in the papers surveyed and summarize their availability. Finally, we discuss several potential research directions to better harness airline data in the future. We anticipate this study to be used as a comprehensive reference for both members of the airline industry and academic scholars with an interest in airline research.",About Sufficiency
"Maritime Spatial Planning as ""Key Enabler"" of Blue Growth through ""Just Transformations"" in the European Union","In the world of the pressures associated with the climate change, population growth and societal challenges, maritime/marine spatial planning underpinned by the ecosystem-based approach is an ocean management governance tool and the new buzzword of the maritime sustainability debates and marine governance system. This paper provides an insight into the interrelationship between the notions of ecosystem-based maritime spatial planning and Blue Economy or Blue Growth. Purpose of the paper is to explore how maritime spatial planning can operate as the key enabler of the Blue Growth in the European Union. In this regard, firstly, sustainability assessment perspective is used. Secondly, it is supplemented by the concept of just transformations. The analysis of chosen perspectives reveals the possibilities to improve the awareness and interpretation of the maritime spatial planning establishment to address social justice concerns. (C) 2020 Leila Neimane.",About Sufficiency
Mediating Role of Green Supply Chain Management Between Lean Manufacturing Practices and Sustainable Performance,"Manufacturing companies in today's industrial world are seeking to use the new manufacturing process methods. The primary goal of corporations is to achieve optimum production while deploying minimal capital. The fundamental purpose of this study is to examine the influence of various lean manufacturing practices on the sustainability performance of companies and the mediating role of green supply chain management (GSCM). The data was gathered using questionnaires from 250 Pakistani manufacturing firms and analyzed using AMOS 25. Results demonstrate that process and equipment, product design, supplier relationships, and customer relationships significantly affect sustainable performance. It is also recognized that Green Supply Chain Management mediates the interaction between HR processes, product design, supplier relationship, customer relationship, and environmental performance. The findings of this study will enable managers and decision-makers of manufacturing companies to increase sustainable efficiency and reduce waste through the use of lean manufacturing and GSCM implementation.",About Sufficiency
A Holistic Didactical Approach for Industrial Logistics Engineering Education in the LOGILAB at the Montanuniversitaet Leoben,"The successful implementation of Industry 4.0 initiatives can only be established by focusing on a multitude of complex and partially interdepending requirements and enabling factors. Thereby, the education and further training of employees can be seen as one of the major prerequisites to understand, handle, and further develop existing Industry 4.0 concepts and technologies. This paper reviews the current state of the art regarding the usage of learning labs, as modern teaching and learning methods, for industrial logistics engineering education from a holistic perspective. Moreover, the authors discuss learning within the framework of action-based learning approaches as a reflected form of meaningful social interaction and introduce an exemplary system-theoretical and constructivist approach based on the LOGILAB at the Montanuniversitaet Leoben. (C) 2020 The Authors. Published by Elsevier Ltd.",About Sufficiency
Challenges for Effective and Efficient Supply Chain Management for Fast Moving Electrical Goods (FMEG): A Case Study,"Fast Moving Electrical Goods (FMEG) industries are among the most dynamic industry these days. Short product life cycle, intricate supply chain, unpredictable demand, tight profit margins are major challenges for FMEG industries. Scarcity of rawmaterial, unpredictable delay in transportation from foreign suppliers, non-availability of affordable skilled labor, changing customer demand have completely transformed the electric goods industries. Increasing demand for good-quality low-cost products is giving abundant opportunities of growth to FMEG industries if they manage issues and challenges of supply chain management (SCM). In this research paper, authors have studied the issues and challenges of FMEG supply chain using situation actor process-learning action performance (SAP-LAP) methodology. Authors have observed that Indian FMEG are facing challenges like supply chain collaboration (outsourcing of non-core operations, vertically integrating core competencies), retail integration, supply risk management (natural, terrorism, demand and supply), supply chain planning (demand-driven supply chains), reverse logistic, sustainability, and green supply chain and digital supply chain.",About Sufficiency
Attracting supermarkets to inner-city neighborhoods: Economic development outside the box,"The paucity of accessible supermarkets is a continuing concern in inner-city communities. Based on a survey of planners in 32 communities, this article examines initiatives to encourage grocery retail investment, reasons for the existence or absence of initiatives, and factors in successful developments. This research shows that systematic, citywide grocery initiatives are rare, with such efforts limited to particular sites or developments. Reliance on private initiatives, absence of grassroots requests for action, and assignment of lower priority to grocery stores in commercial revitalization programs explain planner inaction. Successful initiatives are characterized by political leadership, competent public agency participation, and, often, partnerships with nonprofit agencies. This article also presents recommendations for community and economic development planners to increase grocery investment in underserved areas.",About Sufficiency
Who's behind the wheel? Visioning the future users and urban contexts of connected and autonomous vehicle technologies,"Connected and Autonomous Vehicles (CAVs) are promised by their developers to transform mobilities, making travel accessible to all - including those unable to drive due to age, affordability or disability - and thereby widen the distribution of what Urry calls 'network capital'. This paper interrogates promotional visualizations about CAVs as they imagine future automated mobilities and the scaling up of the technologies from small trials to mass roll-out. It analyses a wide range of images from a CAV trial in a UK city and demonstrates that these images reinforce rather than disrupt traditional gendered associations of automobility. This study further develops this work and notes other ways in which visualizations of CAV-enabled network mobility reiterate existing network capital inequalities. It also pays careful attention to the background urban environment in which CAVS are pictured. The paper argues that an absence of people and place specificity enable CAV technologies to be imagined as being used in other locations and contexts. Hence the visualizations of CAV that picture only specific forms corporeal mobility also work to envision the mobility of entrepreneurial capital, as the software and hardware behind the driverless vehicle is shown as transferable to, and profitable in, different contexts and situations.",About Sufficiency
Impact of smart logistics on smart city sustainable performance: an empirical investigation,"Purpose Technologies continue to disrupt logistics and freight transport (known as smart logistics), but their impacts on smart city sustainability is underinvestigated. Drawing on technology, organisation and environment (TOE) perspective, the objective of this study is to empirically investigate the hierarchical effects of smart logistics on smart city sustainable dimensions (i.e. environmental, social and economic). Design/methodology/approach The study used cross-sectional survey to collect data from urban transporters, warehouse managers, retailers and information technology (IT) managers in Australia. Data were analysed using structural equation modeling (SEM) to test the hypothesised relationship between constructs of smart logistics and smart city sustainable performance. Findings The findings reveal that information and communications technologies (ICTs) use and IT capability (ITC) have positive and significant effects on smart logistics. Technology-enabled smart logistics have an immediate positive effect on smart city environment, which in turn has positive impacts on social and economic performance. Practical implications The study informs managers that smart logistics equipped with freight transport telematics can improve smart city environment through enhanced tracking and tracing of goods movement. The improved environmental stewardship is likely to support social and economic performance. Originality/value Smart city research remains primarily theoretical and focussed on concerns surrounding sustainable growth amid urbanisation and digitalisation. City logistics and urban freights play key role in smart city economic growth, but vehicular pollution pose social and environmental challenges. Technology-assisted smart logistics are likely to improve smart city sustainable performance but yet to find how they affect each other.",About Sufficiency
Manufacturer's online recycling channel intrusion strategies considering digital technology enabling and government subsidies,"We constructed closed-loop supply chain decision-making models for digital technology empowerment considering government subsidies and analyzed the interaction between subsidy strategies and digital technology empowerment decisions. We found that when manufacturer does not (does) implement online recycling channel intrusion, government subsidies have (have not) a transmission effect. However, when the government subsidizes manufacturer (recycling platform), channel intrusion will increase (decrease) the digital technology empowerment level of recycling platform. Meanwhile, excessive subsidies for recycling platform are not conducive to remanufacturing. In addition, when consumers' digital sensitivity is weak, government subsidies to manufacturer is more conducive to increasing social welfare.",About Sufficiency
Optimal Green Operation and Information Leakage Decisions under Government Subsidy and Supply Uncertainty,"This study investigated optimal green operation and information leakage decisions in a green supply chain system. The system consists of one supplier, one leader retailer 1, one follower retailer 2, and the government. In this system, the government subsidizes each retailer based on the selling price of the product. The supplier is subject to a yield uncertainty process. The suppler decides whether to leak leader retailer 1 ' s order quantity to follower retailer 2 or not. In this study, we first built a Stackelberg game to address the equilibrium green operation decisions, when the supplier has and has not information leakage behavior, respectively. Subsequently, we identify the supplier's information leakage equilibrium and how such behavior affects retailers' ex ante profits, consumer surplus, and social welfare through a numerical study. Interestingly, we obtained the following results: (1) Supplier leaks are the unique equilibrium of the supplier. The product's green degree and wholesale price at supplier's equilibrium are higher under information leakage than under no information leakage. (2) The supplier's information leakage behavior is good for leader retailer 1 and bad for follower retailer 2. (3) Information leakage behavior increases both consumer surplus and social welfare under certain conditions. (4) In general, key system parameters (e.g., the subsidy rate, supply uncertainty, supply correlation, and forecast accuracies) positively correlate with consumer surplus and social welfare in the same direction, while they affect retailer 1 ' s and retailer 2 ' s ex ante profit in the opposite direction. These findings provide useful insights for businesses to manage demand forecast information and make decisions on the green level of the product in green supply chain management.",About Sufficiency
Optimal pricing and collection decisions in a two-period closed-loop supply chain considering channel inconvenience,"Improving recovery efficiency is a key concern for collectors in a closed-loop supply chain (CLSC) with remanufacturing, as customers often consider the inconvenience of recycling channels when returning used products. This issue profoundly affects collectors' capacity to recover materials. In a two-period CLSC with remanufacturing, including a manufacturer and a retailer, we develop game-theoretical models in the centralized and decentralized scenarios and compare the optimal solutions, consumer surplus, social welfare and environmental impact of different models through analytical and numerical analysis. Our aim is to examine firms' dynamic pricing strategies and collection investment decisions by considering customers' perception of channel inconvenience. There are four main findings. Firstly, in the centralized model and the retailer collection model, the decision-maker lowers the retail price in the first period. However, in the competitive collection model, the manufacturer and the retailer raise the wholesale and retail prices in the first period, respectively. Secondly, in the retailer collection model, as the recycling revenue increases, the manufacturer, although not involved in collecting, the profit also increases due to the free-rider behavior. Thirdly, in the competitive collection model, when the remanufacturing cost savings is relatively high, the collection investment of the manufacturer is much larger than that of the retailer, resulting in the retailer failing to collect any product and giving up collecting. Finally, the collection competition improves the total collection rate and environmental performance but reduces the profit of the manufacturer and the retailer, as well as the consumer surplus and social welfare. Therefore, we design a two-part tariff contract to coordinate the decentralized model and effectively improve the performance of the supply chain.",About Sufficiency
Contractor Cooperation Mechanism and Evolution of the Green Supply Chain in Mega Projects,"The large scale of construction in mega projects leads to significant environmental and socioeconomic impacts; thus, the projects should also exhibit greater social responsibility. Adopting green supply chain management in the construction process is an important way to realize the goal of sustainable development of mega projects. Because the green supply chain behavior during construction is mainly demonstrated by contractors, it is especially important to study the evolutionary trend of their behavior. Thus, to explore the cooperative relationship among contractors, this paper considers a lengthy construction period, multi-agent participation dynamics, and opportunistic behaviorall are key features of mega projectsand establishes an evolutionary game model. Specifically, a replicator dynamic equation is used to describe the long-term effects of the contractor's decisions. Equilibrium determinants are then analyzed and simulated. The results show that the initial probabilities of the two types of contractors (main contractor and subcontractor) demonstrating opportunistic behaviors have a significant effect on the direction of evolution of the mega project. The main contractor, who dominates the subsidy allocation by the government and project owner, should pay attention to maintaining a balance in the income distribution between him or her and the subcontractor. Additionally, there is an optimal distribution coefficient for the subsidy that minimizes the overall probability of opportunistic behavior. This paper provides a point of reference for the decisions of the main contractor and the subcontractor in the green supply chain of mega projects.",About Sufficiency
Impact of Information Asymmetry on the Operation of Green Closed-Loop Supply Chain under Government Regulation,"Recycling subsidy and carbon tax policies are ways to achieve energy and environmental sustainability. The implementation of these policies has changed the operating environment of traditional closed-loop supply chains, while the privacy of relevant information increases the difficulty of decision-making. Under the background, this paper considers the green closed-loop supply chain (GCLSC) under the hybrid policy of recycling subsidy and carbon tax where the manufacturer is in charge of recycling and the retailer invests in green marketing. Taking green marketing cost coefficient as the retailer's private information, this paper explores the influence of information asymmetry on optimal decisions and performance of the GCLSC. By constructing game models of information symmetry and asymmetry, the optimal decisions, economic and environmental performance, and social welfare are provided. Combined with numerical analysis, the influence of uncertainty of the manufacturer's estimation, subsidies and carbon tax on the GCLSC is proposed. The results indicate that the uncertainty in the manufacturer's estimation can improve the social welfare under certain conditions, but it cannot reduce carbon emissions. Recycling subsidy and carbon tax policies oppositely affect the manufacturer's optimal decisions and carbon emissions. Information asymmetry is beneficial to the retailer. However, less uncertainty in estimation is not always better for the manufacturer. The manufacturer needs to proactively adopt strategies to stimulate the retailer's information sharing.",About Sufficiency
What is the role of product services in the dual-channel closed-loop supply chain?,"With the continuous development of internet technology and increasing awareness of environmental protection among consumers, the dual-channel closed-loop supply chain management combining online direct sales channel and retail channel has received more and more attention, however, the existing research has neglected the service research, so this paper focuses on the service strategy in the dual-channel closed-loop supply chain. In this paper, we propose three service strategy models for dual-channel closed-loop supply chains, including when there is no service provision, after-sales service and recycling service provided by retailers and after-sales service provided by manufacturers and recycling service provided by retailers, compare and analyze the optimal decisions of supply chain members under the three service strategy models, discuss the role of service generated in dual-channel closed-loop supply chains, and propose a service cost-sharing strategy to achieve a Pareto improvement in the effectiveness of a dual-channel closed-loop supply chain. The study finds that service plays a positive role in the dual-channel closed-loop supply chain; the model of manufacturer providing after-sales service and retailer providing recycling service is the best service strategy choice in terms of economic and social benefits; the service cooperation mechanism of service cost sharing can achieve Pareto improvement of the supply chain within a reasonable range.",About Sufficiency
Cargo prioritization and terminal allocation problem for inland waterway disruptions,"To mitigate inland waterway disruption impacts, we introduce the cargo prioritization and terminal allocation problem (CPTAP) to minimize the total value loss of disrupted barge cargoes. CPTAP is formulated as a nonlinear binary integer program, and problems of realistic size can be efficiently and effectively solved with a genetic algorithm approach. The final solution identifies an accessible alternative terminal for each disrupted barge and the prioritized offloading turn that each barge takes at its assigned terminal. Implementation of CPTAP results in reduced cargo value loss and response time when compared with a naive minimize distance approach.",About Sufficiency
Evaluating the circular supply chain adoption in manufacturing sectors: A picture fuzzy approach,"In order to address the environmental and climate change challenges, the circular economy has gotten much attention from the worldwide economic and corporate platforms and policy domains. More specifically, the manufacturing sectors have played a crucial part in the global improvement of living standards. They are also associated with unsustainable production and consumption patterns in a linear system; thus, the circular economy has emerged as an alternative to the linear system. The companies should engage in more sustainable activities to transit to a circular economy and reap its advantages; thus, the present study aims to extend a novel picture fuzzy SWARA-COPRAS framework to assess the manufacturing sectors concerning circular supply chain management indicators. Therefore, sixteen indicators were identified through literature review, and then their weight was determined using picture fuzzy SWARA. Subsequently, the picture fuzzy COPRAS was applied to rank three manufacturing sectors in a home appliance company concerning the weighted indicators. The results indicated that ""waste reduction"" is the most influential indicator, followed by ""circular supplier selection."" Furthermore, comparative and sensitivity analyses have been conducted to evaluate the proposed method's performance compared to the other methods and weight changes.",About Sufficiency
The adoption of Industry 4.0 technologies in SMEs: results of an international study,"Purpose The purpose of this paper is to investigate whether the financial resources invested in advanced manufacturing technologies (AMTs) and social capital (SC), in terms of the set of internal and external relationships a firm holds, have a positive effect on the adoption of Industry 4.0 (I4.0) technologies. Furthermore, it tests whether the organizational context strengthens these relationships. Design/methodology/approach The authors used regression models to analyze data collected through an international survey carried out within the scope of a European project involving six Central European regions. Findings Results show that small- and medium-sized enterprises (SMEs) having stronger internal and external SC have a higher propensity to adopt I4.0 technologies, and both management support and absorptive capacity (AC) strengthen these relationships, whereas investments in AMTs within the manufacturing area and internal SC have a positive association with the intensity of I4.0 adoption. However, in presence of a high level of management support and AC, the relationship between external SC and I4.0 adoption becomes positive and significant. Management support also moderates the impact that investments in AMTs in the manufacturing area and internal SC have on the intensity of adoption of I4.0 technologies. Originality/value This paper is one of the first to investigate the context of SMEs that, having fewer resources, face some difficulties in exploiting the potential of I4.0 revolution. Moreover, it adopts a broad perspective on the factors that facilitate the adoption of I4.0.",About Sufficiency
The Reverse Supply Chain of the E-Waste Management Processes in a Circular Economy Framework: Evidence from Italy,"In the last several decades, Waste Electrical and Electronic Equipment (WEEE) reverse supply chain management has increasingly gained more attention due to the development of an environmental awareness, the rapid raise of e-wasted products and the EU regulations. In particular, although the new EU WEEE collection target has not been reached by many EU countries, several studies show that an optimized WEEE wastes management processes could represent a relevant way to achieve economic, environmental and social benefits expected by the adoption of circular economy approaches. According to this, the paper aims to evaluate the extent to which the current Italian organization of the WEEE management system and the related legislation are able to support the achievement of the targets defined by EU with a specific focus on the collection centers (CCs) which play a key role being the initial point of the WEEE reverse logistic cycle. An illustrative analysis based on the transition probability matrix regarding both the e-waste collecting performance and the distribution of collecting centers in the Italian provinces is illustrated. Furthermore, we have analyzed the presence of a correlation between the WEEE collection rate and the presence of the CCs in different provinces in order to better comprehend the role that can play both the investments in CC system and other soft measures in achieving the WEEE collection targets. Results show that the current Italian organization of the WEEE management system and the related legislations are not so effective in supporting the achievement of EU WEEE collection targets at the national level, although some geographical areas and provinces outperform the EU targets.",About Sufficiency
Enhancing Fastener Standardization for Disassembly: A Path to Sustainable Laptop Design,"This project aims to design a laptop for easy of disassembly, thereby reducing disassembly time. The challenges associated with product disassembly include the complexity of replacing worn-out components, managing various types of fasteners, and finding solutions for end-of- life products. This paper focuses on the design for disassembly of laptops by standardizing fasteners to minimize disassembly time. A modified design is proposed to decrease the disassembly time of the laptop compared to the reference design. The laptop components and fasteners are designed using Autodesk Inventor, and the disassembly time is calculated accordingly. The comparison between the new design and the reference design reveals significant improvements in disassembly time for both the laptop screen and keyboard. Specifically, the disassembly time is reduced by a factor of 4 for the laptop screen and 4.5 for the keyboard. Furthermore, the number of required disassembly tools is reduced from three different screwdrivers to just one. The primary objective of the new laptop design is to ensure easy disassembly and reassembly. By making product components easily accessible for repair, replacement, recycling, and reuse, this design not only saves time and resources for manufacturing and recycling companies but also reduces labor costs. It enhances customer satisfaction and minimizes the time and effort required for quick disassembly during product repair and refurbishment. In conclusion, Design for Disassembly is an effective approach for creating sustainable products that can be easily disassembled, facilitating simpler maintenance, repair, recovery, and reuse of components and materials.",About Sufficiency
"Supply chain environmental and social sustainability practice diffusion: Bibliometrics, content analysis and conceptual framework","The purpose of this paper is to uncover how environmental and social sustainability practices are diffused across the supply chain tiers through supplier development initiatives. In particular, the work seeks to uncover the initiatives developed directly or indirectly by manufacturing firms and the factors that enhance them. A systematic literature review is used to examine the breadth of the sustainable supply chain literature. The papers obtained are screened and analysed using established procedures to produce bibliometric and thematic analyses. The findings show the evolution of this young field around key research groups with few papers looking beyond focal firms to immediate suppliers and even fewer examining multiple tiers. Whilst numerous organisational factors are identified, few works consider most of them together and none capture their interrelationships at such breadth. Within this field lacking in theory, the originality of the work is the assembly of environmental and social practices into an integrated framework for their diffusion across supply chain tiers in the design and implementation of supplier development initiatives. There is recognition of where in the supply chain these practices are applied. The implications of this research are a framework around which supply chain diffusion theory can be tested and subsequent potential for its deployment in business to guide sustainable practice adoption.",About Sufficiency
