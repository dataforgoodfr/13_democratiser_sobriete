# ğŸ“„ Paper Processing Pipeline

**Safe refactoring for 250k document processing with targeted scraping and parallel metadata extraction**

This is a well-organized, production-ready system for scraping academic papers from OpenAlex and extracting metadata using LLM-powered processing.

## ğŸ“ Project Structure

```
pipeline_scripts/
â”œâ”€â”€ ğŸ“‚ scraping/              # Paper scraping components
â”‚   â”œâ”€â”€ targeted_scraper.py   # â­ Main scraper (targets specific OpenAlex IDs)
â”‚   â”œâ”€â”€ test_scraping.py      # Test scraping with 10 papers
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“‚ database/              # Database models and management  
â”‚   â”œâ”€â”€ models.py             # SQLModel classes and operations
â”‚   â”œâ”€â”€ manage_queue.py       # â­ Central queue management CLI
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“‚ docs/                  # Documentation
â”‚   â”œâ”€â”€ README.md             # This file
â”‚   â”œâ”€â”€ USAGE_EXAMPLES.md     # Detailed usage examples
â”‚   â””â”€â”€ install_chrome.md     # Chrome installation guide
â”œâ”€â”€ historized_ingestion_pipeline.py  # âœï¸ Original pipeline (folder mode added)
â””â”€â”€ run_metadata_extraction.sh        # Bash script for parallel processing
```

## ğŸš€ Quick Start

### **Step 0: Install Chrome (REQUIRED for scraping)**
```bash
# macOS
brew install --cask google-chrome && pip install selenium webdriver-manager

# Ubuntu  
wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add - && \
sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list' && \
sudo apt update && sudo apt install -y google-chrome-stable && \
pip install selenium webdriver-manager

# Install CLI dependencies
pip install typer[all] rich

# Test Chrome setup (optional but recommended)
python cli.py scrape --test-paper "https://openalex.org/W2741809807"
```

### **Step 1: Explore the CLI (RECOMMENDED)**
```bash
# See beautiful help with all commands
python cli.py --help

# Get detailed help for specific commands
python cli.py scrape --help
python cli.py queue --help
```

### **Step 2: Test with 10 Papers (RECOMMENDED)**
```bash
# Test the complete pipeline with just 10 papers
python cli.py test

# This will:
# 1. Create test database queue
# 2. Test scraping functionality  
# 3. Create folder structure
# 4. Show you what to expect
```

### **Step 3: Test Single File (Original Mode)**
```bash
# This still works exactly as before
python historized_ingestion_pipeline.py --file-path path/to/your.pdf
```

### **Step 4: Test Folder Mode (New)**
```bash
# New: Process entire folder
python historized_ingestion_pipeline.py --folder-path ./test_scraping_output/folder_00
```

### **Step 5: Production Scraping (When Ready)**
```bash
# Use the new targeted scraper (RECOMMENDED)
python cli.py scrape --batch-size 10

# Or test a specific paper first
python cli.py scrape --test-paper "https://openalex.org/W2741809807"

# Check progress
python cli.py queue --stats
```

### **Step 6: Run 12 Parallel Processes**
```bash
# Make executable (one time)  
chmod +x run_metadata_extraction.sh

# Run on all 12 folders
./run_metadata_extraction.sh
```

## ğŸ¯ Components Overview

### 0. **cli.py** (NEW - Modern CLI Interface)
- ğŸ¨ **Built with Typer and Rich** for beautiful, colored output
- ğŸ“– **Auto-generated help** with examples and rich formatting
- ğŸ”§ **Type-safe arguments** with automatic validation
- ğŸ’¡ **Intuitive command structure** with subcommands

### 1. **scraping/targeted_scraper.py** (NEW - Main Scraper)
- ğŸ¯ **Downloads specific papers by OpenAlex ID** from database queue
- ğŸŒ **Calls OpenAlex API directly** for each paper (no random search)
- ğŸ”„ **DOI retry logic**: If OpenAlex ID fails, automatically retries with DOI
- ğŸ“ **Distributes papers to 12 folders automatically**
- ğŸ—„ï¸ **Tracks progress and retry attempts in database**
- ğŸ›¡ï¸ **Batch processing** with proper error handling and resilience

### 2. **scraping/test_scraping.py** (Test Suite)
- ğŸ§ª **Tests with 10 papers** before full-scale processing
- ğŸ“ **Creates folder structure** and shows expected results
- ğŸ—„ï¸ **Populates test database queue**
- ğŸ“Š **Shows expected results**

### 3. **database/manage_queue.py** (NEW - Queue Management CLI)
- ğŸ§¹ **Clear/reset** scraping queue for testing
- ğŸ“¥ **Populate queue** from policies_abstracts_all table
- ğŸ“Š **Comprehensive statistics** and progress tracking
- ğŸ” **Show failed papers** with error details
- âœ… **Show recent successes** 
- ğŸ›ï¸ **Central CLI** for all queue operations

### 4. **database/models.py** (Database Support)
- ğŸ“¥ **Simple queue tracking** for papers to scrape
- ğŸ—„ï¸ **Basic database models** (much simpler than complex version)
- ğŸ“Š **Statistics and progress tracking**

### 5. **historized_ingestion_pipeline.py** (âœï¸ Modified Original)
- ğŸ”„ **Added `--folder-path` mode** for batch processing
- âœ… **Maintains backward compatibility** (single file mode still works)
- ğŸ—„ï¸ **Unchanged core logic** for metadata extraction

### 6. **run_metadata_extraction.sh** (Parallel Processing)
- ğŸ”„ **Runs 12 parallel processes** for maximum throughput
- ğŸ“ **Each process handles one folder**
- ğŸ“Š **Progress tracking and error logging**

## ğŸ—„ï¸ Queue Management

The unified CLI provides comprehensive database queue management:

### **Clean Up Test Data**
```bash
# Clear entire queue (for fresh start)
python cli.py queue --clear

# Reset failed entries to try again
python cli.py queue --reset-failed
```

### **Monitor Progress**
```bash
# Show comprehensive statistics
python cli.py queue --stats

# Show recent failures with error messages
python cli.py queue --show-failed --limit 10

# Show recent successes
python cli.py queue --show-successes --limit 10
```

### **Example Output**
```
ğŸ“Š SCRAPING QUEUE STATISTICS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ Total in queue: 1,250
âœ… Successfully scraped: 1,180
âŒ Failed: 45
â³ Pending: 25
ğŸ“ˆ Completion rate: 94.4%
ğŸ“‰ Failure rate: 3.6%

ğŸ“ FOLDER DISTRIBUTION:
   ğŸ“‚ folder_00: 98 papers
   ğŸ“‚ folder_01: 102 papers
   ğŸ“‚ folder_02: 95 papers
   ... (showing distribution across 12 folders)
   ğŸ“Š Total distributed: 1,180 papers
```

## ğŸ› ï¸ Troubleshooting

### **Chrome/Selenium Issues**
```bash
# Test Chrome setup first
python -c "from selenium import webdriver; print('Selenium OK')"

# If Chrome not found, install it:
# macOS:
brew install --cask google-chrome

# Ubuntu:
sudo apt install -y google-chrome-stable

# Install Python dependencies
pip install selenium webdriver-manager requests
```

### **Script Won't Run**
```bash
# Make executable
chmod +x run_metadata_extraction.sh

# Check paths
ls -la historized_ingestion_pipeline.py
```

### **Database Connection Issues**
```bash
# Test database connection
python cli.py queue --stats

# Check environment variables
echo $DATABASE_URL
```

### **No PDFs Found**
```bash
# Check folder structure
ls -la scraping_output/
find scraping_output/ -name "*.pdf" | head -10
```

### **Individual Folder Testing**
```bash
# Test single folder first
python historized_ingestion_pipeline.py --folder-path ./scraping_output/folder_00
```

### **Check Logs**
```bash
# Monitor progress
tail -f failed_extractions.txt

# Check for specific errors
grep "Error" failed_extractions.txt
```

## ğŸ‰ Summary

This is a **minimal, safe refactoring** that:

- âœ… **Keeps your working pipeline intact**
- âœ… **Adds folder processing capability**  
- âœ… **Enables 12x parallelization**
- âœ… **Improves monitoring and error handling**
- âœ… **Maintains backward compatibility**

**Total changes**: 
- âœï¸  **1 file modified**: `historized_ingestion_pipeline.py` (added folder mode)
- ğŸ“„ **8 files added**: 
  - `scraping/targeted_scraper.py` (â­ NEW - targeted production scraper)
  - `scraping/test_scraping.py` (test with 10 papers)
  - `database/manage_queue.py` (â­ NEW - queue management CLI)
  - `database/models.py` (database support)
  - `run_metadata_extraction.sh` (parallel processing)
  - `docs/install_chrome.md` (Chrome installation guide)
  - `docs/USAGE_EXAMPLES.md` (detailed usage examples & workflows)
  - `docs/README.md` (this documentation)
- ğŸš€ **Ready to process 250k documents safely!**

## ğŸ¯ Complete Workflow for 250k Documents

```bash
# 1. Setup Chrome/Selenium (one-time)
# See install_chrome.md for detailed instructions

# 2. Test everything with 10 papers first
python cli.py test

# 3. If test works, populate production queue
python cli.py queue --populate --limit 250000

# 4. Run targeted scraping (repeat as needed)
python cli.py scrape --batch-size 50

# 5. Check scraping progress
python cli.py queue --stats

# 6. When enough papers are scraped, run extraction
./run_metadata_extraction.sh

# 7. Monitor extraction progress
tail -f failed_extractions.txt
```

The original approach still works, but now you can test safely and process folders in parallel for much better throughput.

## ğŸ“š Additional Resources

- **ğŸ“– [USAGE_EXAMPLES.md](USAGE_EXAMPLES.md)** - Detailed workflows and practical examples
- **ğŸ”§ [install_chrome.md](install_chrome.md)** - Chrome installation guide for all platforms  
- **ğŸ§ª Testing**: Start with `python cli.py test` for a safe environment
- **ğŸ—„ï¸ Queue Management**: Use `python cli.py queue --help` for all database operations
- **ğŸ†˜ Need help?** Check the troubleshooting section above or the usage examples 